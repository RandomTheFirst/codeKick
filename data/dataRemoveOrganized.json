[[{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('rewards', '0010_auto_20210302_1417'),\n\t\t('fundraisers', '0008_auto_20210302_1417'),\n\t\t('payments_lipisha', '0003_auto_20210302_1417'),\n\t\t('donations', '0012_auto_20210302_1417'),\n\t\t('votes', '0003_auto_20210302_1417'),\n\t\t('suggestions', '0005_auto_20210302_1417'),\n\t\t('projects', '0094_merge_20191107_0943'),\n\t\t('tasks', '0045_auto_20210302_0940'),\n\t\t('surveys', '0035_auto_20210302_1020'),\n\t]\n\n\tstate_operations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='customprojectfield',\n\t\t\tname='field',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='customprojectfield',\n\t\t\tname='project',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='customprojectfieldsettings',\n\t\t\tname='project_settings',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='categories',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='country',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='language',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='location',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='organization',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='owner',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='payout_account',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='promoter',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='reviewer',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='status',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='task_manager',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='theme',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='projectaddon',\n\t\t\tname='polymorphic_ctype',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='projectaddon',\n\t\t\tname='project',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='projectbudgetline',\n\t\t\tname='project',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='projectcreatetemplate',\n\t\t\tname='project_settings',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='projectlocation',\n\t\t\tname='project',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='projectphaselog',\n\t\t\tname='project',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='projectphaselog',\n\t\t\tname='status',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='projectsearchfilter',\n\t\t\tname='project_settings',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='projectimage',\n\t\t\tname='project',\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='projectimage',\n\t\t\tname='file',\n\t\t\tfield=models.FileField(upload_to='project_images/'),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='projectimage',\n\t\t\tname='name',\n\t\t\tfield=models.CharField(blank=True, help_text='Defaults to filename, if left blank', max_length=255, null=True),\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='CustomProjectField',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='CustomProjectFieldSettings',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='Project',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ProjectAddOn',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ProjectBudgetLine',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ProjectCreateTemplate',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ProjectLocation',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ProjectPhaseLog',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ProjectPlatformSettings',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ProjectSearchFilter',\n\t\t),\n\t]\n\n\toperations = state_operations\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations, models"]}], [{"term": "class", "name": "TestNbdServerRemove", "data": "class TestNbdServerRemove(iotests.QMPTestCase):\n\tdef setUp(self):\n\t\tqemu_img_create('-f', iotests.imgfmt, disk, '1M')\n\n\t\tself.vm = iotests.VM().add_drive(disk)\n\t\tself.vm.launch()\n\n\t\taddress = {\n\t\t\t'type': 'unix',\n\t\t\t'data': {\n\t\t\t\t'path': nbd_sock\n\t\t\t}\n\t\t}\n\n\t\tresult = self.vm.qmp('nbd-server-start', addr=address)\n\t\tself.assert_qmp(result, 'return', {})\n\t\tresult = self.vm.qmp('nbd-server-add', device='drive0', name='exp')\n\t\tself.assert_qmp(result, 'return', {})\n\n\tdef tearDown(self):\n\t\tself.vm.shutdown()\n\t\tos.remove(nbd_sock)\n\t\tos.remove(disk)\n\n\tdef remove_export(self, name, mode=None):\n\t\tif mode is None:\n\t\t\treturn self.vm.qmp('nbd-server-remove', name=name)\n\t\telse:\n\t\t\treturn self.vm.qmp('nbd-server-remove', name=name, mode=mode)\n\n\tdef assertExportNotFound(self, name):\n\t\tresult = self.vm.qmp('nbd-server-remove', name=name)\n\t\tself.assert_qmp(result, 'error/desc', \"Export 'exp' is not found\")\n\n\tdef assertExistingClients(self, result):\n\t\tself.assert_qmp(result, 'error/desc', \"export 'exp' still in use\")\n\n\tdef assertReadOk(self, qemu_io_output):\n\t\tself.assertEqual(\n\t\t\t\tfilter_qemu_io(qemu_io_output).strip(),\n\t\t\t\t'read 512/512 bytes at offset 0\\n' +\n\t\t\t\t'512 bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)')\n\n\tdef assertReadFailed(self, qemu_io_output):\n\t\tself.assertEqual(filter_qemu_io(qemu_io_output).strip(),\n\t\t\t\t\t\t 'read failed: Input/output error')\n\n\tdef assertConnectFailed(self, qemu_io_output):\n\t\tself.assertEqual(filter_qemu_io(qemu_io_output).strip(),\n\t\t\t\t\t\t \"can't open device \" + nbd_uri +\n\t\t\t\t\t\t \": Requested export not available\\n\"\n\t\t\t\t\t\t \"server reported: export 'exp' not present\")\n\n\tdef do_test_connect_after_remove(self, mode=None):\n\t\targs = ('-r', '-f', 'raw', '-c', 'read 0 512', nbd_uri)\n\t\tself.assertReadOk(qemu_io(*args))\n\n\t\tresult = self.remove_export('exp', mode)\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertExportNotFound('exp')\n\t\tself.assertConnectFailed(qemu_io(*args))\n\n\tdef test_connect_after_remove_default(self):\n\t\tself.do_test_connect_after_remove()\n\n\tdef test_connect_after_remove_safe(self):\n\t\tself.do_test_connect_after_remove('safe')\n\n\tdef test_connect_after_remove_force(self):\n\t\tself.do_test_connect_after_remove('hard')\n\n\tdef do_test_remove_during_connect_safe(self, mode=None):\n\t\tqio = QemuIoInteractive('-r', '-f', 'raw', nbd_uri)\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', mode)\n\t\tself.assertExistingClients(result)\n\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tqio.close()\n\n\t\tresult = self.remove_export('exp', mode)\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertExportNotFound('exp')\n\n\tdef test_remove_during_connect_default(self):\n\t\tself.do_test_remove_during_connect_safe()\n\n\tdef test_remove_during_connect_safe(self):\n\t\tself.do_test_remove_during_connect_safe('safe')\n\n\tdef test_remove_during_connect_hard(self):\n\t\tqio = QemuIoInteractive('-r', '-f', 'raw', nbd_uri)\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', 'hard')\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertReadFailed(qio.cmd('read 0 512'))\n\t\tself.assertExportNotFound('exp')\n\n\t\tqio.close()\n\n\tdef test_remove_during_connect_safe_hard(self):\n\t\tqio = QemuIoInteractive('-r', '-f', 'raw', nbd_uri)\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', 'safe')\n\t\tself.assertExistingClients(result)\n\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', 'hard')\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertExportNotFound('exp')\n\t\tself.assertReadFailed(qio.cmd('read 0 512'))\n\t\tqio.close()\n\n", "description": null, "category": "remove", "imports": ["import os", "import sys", "import iotests", "import time", "from iotests import qemu_img_create, qemu_io, filter_qemu_io, QemuIoInteractive"]}], [], [], [{"term": "class", "name": "TestNbdServerRemove", "data": "class TestNbdServerRemove(iotests.QMPTestCase):\n\tdef setUp(self):\n\t\tqemu_img_create('-f', iotests.imgfmt, disk, '1M')\n\n\t\tself.vm = iotests.VM().add_drive(disk)\n\t\tself.vm.launch()\n\n\t\taddress = {\n\t\t\t'type': 'unix',\n\t\t\t'data': {\n\t\t\t\t'path': nbd_sock\n\t\t\t}\n\t\t}\n\n\t\tresult = self.vm.qmp('nbd-server-start', addr=address)\n\t\tself.assert_qmp(result, 'return', {})\n\t\tresult = self.vm.qmp('nbd-server-add', device='drive0', name='exp')\n\t\tself.assert_qmp(result, 'return', {})\n\n\tdef tearDown(self):\n\t\tself.vm.shutdown()\n\t\tos.remove(nbd_sock)\n\t\tos.remove(disk)\n\n\tdef remove_export(self, name, mode=None):\n\t\tif mode is None:\n\t\t\treturn self.vm.qmp('nbd-server-remove', name=name)\n\t\telse:\n\t\t\treturn self.vm.qmp('nbd-server-remove', name=name, mode=mode)\n\n\tdef assertExportNotFound(self, name):\n\t\tresult = self.vm.qmp('nbd-server-remove', name=name)\n\t\tself.assert_qmp(result, 'error/desc', \"Export 'exp' is not found\")\n\n\tdef assertExistingClients(self, result):\n\t\tself.assert_qmp(result, 'error/desc', \"export 'exp' still in use\")\n\n\tdef assertReadOk(self, qemu_io_output):\n\t\tself.assertEqual(\n\t\t\t\tfilter_qemu_io(qemu_io_output).strip(),\n\t\t\t\t'read 512/512 bytes at offset 0\\n' +\n\t\t\t\t'512 bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)')\n\n\tdef assertReadFailed(self, qemu_io_output):\n\t\tself.assertEqual(filter_qemu_io(qemu_io_output).strip(),\n\t\t\t\t\t\t 'read failed: Input/output error')\n\n\tdef assertConnectFailed(self, qemu_io_output):\n\t\tself.assertEqual(filter_qemu_io(qemu_io_output).strip(),\n\t\t\t\t\t\t \"qemu-io: can't open device \" + nbd_uri +\n\t\t\t\t\t\t \": Requested export not available\\n\"\n\t\t\t\t\t\t \"server reported: export 'exp' not present\")\n\n\tdef do_test_connect_after_remove(self, mode=None):\n\t\targs = ('-r', '-f', 'raw', '-c', 'read 0 512', nbd_uri)\n\t\tself.assertReadOk(qemu_io(*args))\n\n\t\tresult = self.remove_export('exp', mode)\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertExportNotFound('exp')\n\t\tself.assertConnectFailed(qemu_io(*args))\n\n\tdef test_connect_after_remove_default(self):\n\t\tself.do_test_connect_after_remove()\n\n\tdef test_connect_after_remove_safe(self):\n\t\tself.do_test_connect_after_remove('safe')\n\n\tdef test_connect_after_remove_force(self):\n\t\tself.do_test_connect_after_remove('hard')\n\n\tdef do_test_remove_during_connect_safe(self, mode=None):\n\t\tqio = QemuIoInteractive('-r', '-f', 'raw', nbd_uri)\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', mode)\n\t\tself.assertExistingClients(result)\n\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tqio.close()\n\n\t\tresult = self.remove_export('exp', mode)\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertExportNotFound('exp')\n\n\tdef test_remove_during_connect_default(self):\n\t\tself.do_test_remove_during_connect_safe()\n\n\tdef test_remove_during_connect_safe(self):\n\t\tself.do_test_remove_during_connect_safe('safe')\n\n\tdef test_remove_during_connect_hard(self):\n\t\tqio = QemuIoInteractive('-r', '-f', 'raw', nbd_uri)\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', 'hard')\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertReadFailed(qio.cmd('read 0 512'))\n\t\tself.assertExportNotFound('exp')\n\n\t\tqio.close()\n\n\tdef test_remove_during_connect_safe_hard(self):\n\t\tqio = QemuIoInteractive('-r', '-f', 'raw', nbd_uri)\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', 'safe')\n\t\tself.assertExistingClients(result)\n\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', 'hard')\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertExportNotFound('exp')\n\t\tself.assertReadFailed(qio.cmd('read 0 512'))\n\t\tqio.close()\n\n", "description": null, "category": "remove", "imports": ["import os", "import sys", "import iotests", "import time", "from iotests import qemu_img_create, qemu_io, filter_qemu_io, QemuIoInteractive"]}], [], [], [{"term": "def", "name": "remove_entities", "data": "def remove_entities(text, keep=(), remove_illegal=True, encoding='utf-8'):\n\tr\"\"\"\n\n\t.. warning::\n\n\t\tThis function is deprecated and will be removed in future.\n\t\tPlease use :func:`replace_entities` instead.\n\t\"\"\"\n\n\twarnings.warn(\n\t\t\"`w3lib.html.remove_entities` function is deprecated and \"\n\t\t\"will be removed in future releases. Please use \"\n\t\t\"`w3lib.html.replace_entities` instead.\",\n\t\tDeprecationWarning\n\t)\n\n\treturn replace_entities(text, keep, remove_illegal, encoding)\n", "description": "\n\n\t.. warning::\n\n\t\tThis function is deprecated and will be removed in future.\n\t\tPlease use :func:`replace_entities` instead.\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "replace_entities", "data": "def replace_entities(text, keep=(), remove_illegal=True, encoding='utf-8'):\n\tu\"\"\"Remove entities from the given `text` by converting them to their\n\tcorresponding unicode character.\n\n\t`text` can be a unicode string or a byte string encoded in the given\n\t`encoding` (which defaults to 'utf-8').\n\n\tIf `keep` is passed (with a list of entity names) those entities will\n\tbe kept (they won't be removed).\n\n\tIt supports both numeric entities (``nnnn;`` and ``hhhh;``)\n\tand named entities (such as ``\u00a0`` or ``>``).\n\n\tIf `remove_illegal` is ``True``, entities that can't be converted are removed.\n\tIf `remove_illegal` is ``False``, entities that can't be converted are kept \"as\n\tis\". For more information see the tests.\n\n\tAlways returns a unicode string (with the entities removed).\n\n\t>>> import w3lib.html\n\t>>> w3lib.html.replace_entities(b'Price: \u00a3100')\n\tu'Price: \\\\xa3100'\n\t>>> print(w3lib.html.replace_entities(b'Price: \u00a3100'))\n\tPrice: \u00c2\u00a3100\n\t>>>\n\n\t\"\"\"\n\n\tdef convert_entity(m):\n\t\tgroups = m.groupdict()\n\t\tif groups.get('dec'):\n\t\t\tnumber = int(groups['dec'], 10)\n\t\telif groups.get('hex'):\n\t\t\tnumber = int(groups['hex'], 16)\n\t\telif groups.get('named'):\n\t\t\tentity_name = groups['named']\n\t\t\tif entity_name.lower() in keep:\n\t\t\t\treturn m.group(0)\n\t\t\telse:\n\t\t\t\tnumber = (moves.html_entities.name2codepoint.get(entity_name) or\n\t\t\t\t\tmoves.html_entities.name2codepoint.get(entity_name.lower()))\n\t\tif number is not None:\n\t\t\t# Numeric character references in the 80-9F range are typically\n\t\t\t# interpreted by browsers as representing the characters mapped\n\t\t\t# to bytes 80-9F in the Windows-1252 encoding. For more info\n\t\t\t# see: http://en.wikipedia.org/wiki/Character_encodings_in_HTML\n\t\t\ttry:\n\t\t\t\tif 0x80 <= number <= 0x9f:\n\t\t\t\t\treturn six.int2byte(number).decode('cp1252')\n\t\t\t\telse:\n\t\t\t\t\treturn six.unichr(number)\n\t\t\texcept ValueError:\n\t\t\t\tpass\n\n\t\treturn u'' if remove_illegal and groups.get('semicolon') else m.group(0)\n\n\treturn _ent_re.sub(convert_entity, to_unicode(text, encoding))\n", "description": "Remove entities from the given `text` by converting them to their\n\tcorresponding unicode character.\n\n\t`text` can be a unicode string or a byte string encoded in the given\n\t`encoding` (which defaults to 'utf-8').\n\n\tIf `keep` is passed (with a list of entity names) those entities will\n\tbe kept (they won't be removed).\n\n\tIt supports both numeric entities (``nnnn;`` and ``hhhh;``)\n\tand named entities (such as ``\u00a0`` or ``>``).\n\n\tIf `remove_illegal` is ``True``, entities that can't be converted are removed.\n\tIf `remove_illegal` is ``False``, entities that can't be converted are kept \"as\n\tis\". For more information see the tests.\n\n\tAlways returns a unicode string (with the entities removed).\n\n\t>>> import w3lib.html\n\t>>> w3lib.html.replace_entities(b'Price: \u00a3100')\n\tu'Price: \\\\xa3100'\n\t>>> print(w3lib.html.replace_entities(b'Price: \u00a3100'))\n\tPrice: \u00c2\u00a3100\n\t>>>\n\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "has_entities", "data": "def has_entities(text, encoding=None):\n\treturn bool(_ent_re.search(to_unicode(text, encoding)))\n", "description": null, "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "replace_tags", "data": "def replace_tags(text, token='', encoding=None):\n\t\"\"\"Replace all markup tags found in the given `text` by the given token.\n\tBy default `token` is an empty string so it just removes all tags.\n\n\t`text` can be a unicode string or a regular string encoded as `encoding`\n\t(or ``'utf-8'`` if `encoding` is not given.)\n\n\tAlways returns a unicode string.\n\n\tExamples:\n\n\t>>> import w3lib.html\n\t>>> w3lib.html.replace_tags(u'This text contains some tag')\n\tu'This text contains some tag'\n\t>>> w3lib.html.replace_tags('Je ne parle pas fran\\\\xe7ais', ' -- ', 'latin-1')\n\tu' -- Je ne parle pas  -- fran\\\\xe7ais --  -- '\n\t>>>\n\n\t\"\"\"\n\n\treturn _tag_re.sub(token, to_unicode(text, encoding))\n\n", "description": "Replace all markup tags found in the given `text` by the given token.\n\tBy default `token` is an empty string so it just removes all tags.\n\n\t`text` can be a unicode string or a regular string encoded as `encoding`\n\t(or ``'utf-8'`` if `encoding` is not given.)\n\n\tAlways returns a unicode string.\n\n\tExamples:\n\n\t>>> import w3lib.html\n\t>>> w3lib.html.replace_tags(u'This text contains some tag')\n\tu'This text contains some tag'\n\t>>> w3lib.html.replace_tags('Je ne parle pas fran\\\\xe7ais', ' -- ', 'latin-1')\n\tu' -- Je ne parle pas  -- fran\\\\xe7ais --  -- '\n\t>>>\n\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "remove_comments", "data": "def remove_comments(text, encoding=None):\n\t\"\"\" Remove HTML Comments.\n\n\t>>> import w3lib.html\n\t>>> w3lib.html.remove_comments(b\"test  whatever\")\n\tu'test  whatever'\n\t>>>\n\n\t\"\"\"\n\n\ttext = to_unicode(text, encoding)\n\treturn _REMOVECOMMENTS_RE.sub(u'', text)\n", "description": " Remove HTML Comments.\n\n\t>>> import w3lib.html\n\t>>> w3lib.html.remove_comments(b\"test  whatever\")\n\tu'test  whatever'\n\t>>>\n\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "remove_tags", "data": "def remove_tags(text, which_ones=(), keep=(), encoding=None):\n\t\"\"\" Remove HTML Tags only.\n\n\t`which_ones` and `keep` are both tuples, there are four cases:\n\n\t==============  ============= ==========================================\n\t``which_ones``  ``keep``\t  what it does\n\t==============  ============= ==========================================\n\t**not empty**   empty\t\t remove all tags in ``which_ones``\n\tempty\t\t   **not empty** remove all tags except the ones in ``keep``\n\tempty\t\t   empty\t\t remove all tags\n\t**not empty**   **not empty** not allowed\n\t==============  ============= ==========================================\n\n\n\tRemove all tags:\n\n\t>>> import w3lib.html\n\t>>> doc = 'This is a link: example'\n\t>>> w3lib.html.remove_tags(doc)\n\tu'This is a link: example'\n\t>>>\n\n\tKeep only some tags:\n\n\t>>> w3lib.html.remove_tags(doc, keep=('div',))\n\tu'This is a link: example'\n\t>>>\n\n\tRemove only specific tags:\n\n\t>>> w3lib.html.remove_tags(doc, which_ones=('a','b'))\n\tu'This is a link: example'\n\t>>>\n\n\tYou can't remove some and keep some:\n\n\t>>> w3lib.html.remove_tags(doc, which_ones=('a',), keep=('p',))\n\tTraceback (most recent call last):\n\t\t...\n\tValueError: Cannot use both which_ones and keep\n\t>>>\n\n\t\"\"\"\n\tif which_ones and keep:\n\t\traise ValueError('Cannot use both which_ones and keep')\n\n\twhich_ones = {tag.lower() for tag in which_ones}\n\tkeep = {tag.lower() for tag in keep}\n\n\tdef will_remove(tag):\n\t\ttag = tag.lower()\n\t\tif which_ones:\n\t\t\treturn tag in which_ones\n\t\telse:\n\t\t\treturn tag not in keep\n\n\tdef remove_tag(m):\n\t\ttag = m.group(1)\n\t\treturn u'' if will_remove(tag) else m.group(0)\n\n\tregex = '?([^ >/]+).*?>'\n\tretags = re.compile(regex, re.DOTALL | re.IGNORECASE)\n\n\treturn retags.sub(remove_tag, to_unicode(text, encoding))\n", "description": " Remove HTML Tags only.\n\n\t`which_ones` and `keep` are both tuples, there are four cases:\n\n\t==============  ============= ==========================================\n\t``which_ones``  ``keep``\t  what it does\n\t==============  ============= ==========================================\n\t**not empty**   empty\t\t remove all tags in ``which_ones``\n\tempty\t\t   **not empty** remove all tags except the ones in ``keep``\n\tempty\t\t   empty\t\t remove all tags\n\t**not empty**   **not empty** not allowed\n\t==============  ============= ==========================================\n\n\n\tRemove all tags:\n\n\t>>> import w3lib.html\n\t>>> doc = 'This is a link: example'\n\t>>> w3lib.html.remove_tags(doc)\n\tu'This is a link: example'\n\t>>>\n\n\tKeep only some tags:\n\n\t>>> w3lib.html.remove_tags(doc, keep=('div',))\n\tu'This is a link: example'\n\t>>>\n\n\tRemove only specific tags:\n\n\t>>> w3lib.html.remove_tags(doc, which_ones=('a','b'))\n\tu'This is a link: example'\n\t>>>\n\n\tYou can't remove some and keep some:\n\n\t>>> w3lib.html.remove_tags(doc, which_ones=('a',), keep=('p',))\n\tTraceback (most recent call last):\n\t\t...\n\tValueError: Cannot use both which_ones and keep\n\t>>>\n\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "remove_tags_with_content", "data": "def remove_tags_with_content(text, which_ones=(), encoding=None):\n\t\"\"\"Remove tags and their content.\n\n\t`which_ones` is a tuple of which tags to remove including their content.\n\tIf is empty, returns the string unmodified.\n\n\t>>> import w3lib.html\n\t>>> doc = 'This is a link: example'\n\t>>> w3lib.html.remove_tags_with_content(doc, which_ones=('b',))\n\tu' example'\n\t>>>\n\n\t\"\"\"\n\n\ttext = to_unicode(text, encoding)\n\tif which_ones:\n\t\ttags = '|'.join([r'<%s\\b.*?%s>|<%s\\s*/>' % (tag, tag, tag) for tag in which_ones])\n\t\tretags = re.compile(tags, re.DOTALL | re.IGNORECASE)\n\t\ttext = retags.sub(u'', text)\n\treturn text\n\n", "description": "Remove tags and their content.\n\n\t`which_ones` is a tuple of which tags to remove including their content.\n\tIf is empty, returns the string unmodified.\n\n\t>>> import w3lib.html\n\t>>> doc = 'This is a link: example'\n\t>>> w3lib.html.remove_tags_with_content(doc, which_ones=('b',))\n\tu' example'\n\t>>>\n\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "replace_escape_chars", "data": "def replace_escape_chars(text, which_ones=('\\n', '\\t', '\\r'), replace_by=u'', \\\n\t\tencoding=None):\n\t\"\"\"Remove escape characters.\n\n\t`which_ones` is a tuple of which escape characters we want to remove.\n\tBy default removes ``\\\\n``, ``\\\\t``, ``\\\\r``.\n\n\t`replace_by` is the string to replace the escape characters by.\n\tIt defaults to ``''``, meaning the escape characters are removed.\n\n\t\"\"\"\n\n\ttext = to_unicode(text, encoding)\n\tfor ec in which_ones:\n\t\ttext = text.replace(ec, to_unicode(replace_by, encoding))\n\treturn text\n", "description": "Remove escape characters.\n\n\t`which_ones` is a tuple of which escape characters we want to remove.\n\tBy default removes ``\\\\n``, ``\\\\t``, ``\\\\r``.\n\n\t`replace_by` is the string to replace the escape characters by.\n\tIt defaults to ``''``, meaning the escape characters are removed.\n\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "unquote_markup", "data": "def unquote_markup(text, keep=(), remove_illegal=True, encoding=None):\n\t\"\"\"\n\tThis function receives markup as a text (always a unicode string or\n\ta UTF-8 encoded string) and does the following:\n\n\t1. removes entities (except the ones in `keep`) from any part of it\n\t\tthat is not inside a CDATA\n\t2. searches for CDATAs and extracts their text (if any) without modifying it.\n\t3. removes the found CDATAs\n\n\t\"\"\"\n\n\tdef _get_fragments(txt, pattern):\n\t\toffset = 0\n\t\tfor match in pattern.finditer(txt):\n\t\t\tmatch_s, match_e = match.span(1)\n\t\t\tyield txt[offset:match_s]\n\t\t\tyield match\n\t\t\toffset = match_e\n\t\tyield txt[offset:]\n\n\ttext = to_unicode(text, encoding)\n\tret_text = u''\n\tfor fragment in _get_fragments(text, _cdata_re):\n\t\tif isinstance(fragment, six.string_types):\n\t\t\t# it's not a CDATA (so we try to remove its entities)\n\t\t\tret_text += replace_entities(fragment, keep=keep, remove_illegal=remove_illegal)\n\t\telse:\n\t\t\t# it's a CDATA (so we just extract its content)\n\t\t\tret_text += fragment.group('cdata_d')\n\treturn ret_text\n", "description": "\n\tThis function receives markup as a text (always a unicode string or\n\ta UTF-8 encoded string) and does the following:\n\n\t1. removes entities (except the ones in `keep`) from any part of it\n\t\tthat is not inside a CDATA\n\t2. searches for CDATAs and extracts their text (if any) without modifying it.\n\t3. removes the found CDATAs\n\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "get_base_url", "data": "def get_base_url(text, baseurl='', encoding='utf-8'):\n\t\"\"\"Return the base url if declared in the given HTML `text`,\n\trelative to the given base url.\n\n\tIf no base url is found, the given `baseurl` is returned.\n\n\t\"\"\"\n\n\ttext = to_unicode(text, encoding)\n\tm = _baseurl_re.search(text)\n\tif m:\n\t\treturn moves.urllib.parse.urljoin(\n\t\t\tsafe_url_string(baseurl),\n\t\t\tsafe_url_string(m.group(1), encoding=encoding)\n\t\t)\n\telse:\n\t\treturn safe_url_string(baseurl)\n", "description": "Return the base url if declared in the given HTML `text`,\n\trelative to the given base url.\n\n\tIf no base url is found, the given `baseurl` is returned.\n\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "get_meta_refresh", "data": "def get_meta_refresh(text, baseurl='', encoding='utf-8', ignore_tags=('script', 'noscript')):\n\t\"\"\"Return  the http-equiv parameter of the HTML meta element from the given\n\tHTML text and return a tuple ``(interval, url)`` where interval is an integer\n\tcontaining the delay in seconds (or zero if not present) and url is a\n\tstring with the absolute url to redirect.\n\n\tIf no meta redirect is found, ``(None, None)`` is returned.\n\n\t\"\"\"\n\n\tif six.PY2:\n\t\tbaseurl = to_bytes(baseurl, encoding)\n\ttry:\n\t\ttext = to_unicode(text, encoding)\n\texcept UnicodeDecodeError:\n\t\tprint(text)\n\t\traise\n\ttext = remove_tags_with_content(text, ignore_tags)\n\ttext = remove_comments(replace_entities(text))\n\tm = _meta_refresh_re.search(text)\n\tif m:\n\t\tinterval = float(m.group('int'))\n\t\turl = safe_url_string(m.group('url').strip(' \"\\''), encoding)\n\t\turl = moves.urllib.parse.urljoin(baseurl, url)\n\t\treturn interval, url\n\telse:\n\t\treturn None, None\n\n", "description": "Return  the http-equiv parameter of the HTML meta element from the given\n\tHTML text and return a tuple ``(interval, url)`` where interval is an integer\n\tcontaining the delay in seconds (or zero if not present) and url is a\n\tstring with the absolute url to redirect.\n\n\tIf no meta redirect is found, ``(None, None)`` is returned.\n\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}, {"term": "def", "name": "strip_html5_whitespace", "data": "def strip_html5_whitespace(text):\n\tr\"\"\"\n\tStrip all leading and trailing space characters (as defined in\n\thttps://www.w3.org/TR/html5/infrastructure.html#space-character).\n\n\tSuch stripping is useful e.g. for processing HTML element attributes which\n\tcontain URLs, like ``href``, ``src`` or form ``action`` - HTML5 standard\n\tdefines them as \"valid URL potentially surrounded by spaces\"\n\tor \"valid non-empty URL potentially surrounded by spaces\".\n\n\t>>> strip_html5_whitespace(' hello\\n')\n\t'hello'\n\t\"\"\"\n\treturn text.strip(HTML5_WHITESPACE)\n", "description": "\n\tStrip all leading and trailing space characters (as defined in\n\thttps://www.w3.org/TR/html5/infrastructure.html#space-character).\n\n\tSuch stripping is useful e.g. for processing HTML element attributes which\n\tcontain URLs, like ``href``, ``src`` or form ``action`` - HTML5 standard\n\tdefines them as \"valid URL potentially surrounded by spaces\"\n\tor \"valid non-empty URL potentially surrounded by spaces\".\n\n\t>>> strip_html5_whitespace(' hello\\n')\n\t'hello'\n\t", "category": "remove", "imports": ["import warnings", "import re", "import six", "from six import moves", "from w3lib.util import to_bytes, to_unicode", "from w3lib.url import safe_url_string", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html", "\t>>> import w3lib.html"]}], [{"term": "def", "name": "reject_outliers_2", "data": "def reject_outliers_2(data, m=6.):\n\td = np.abs(data - np.median(data))\n\tmdev = np.median(d)\n\ts = d / (mdev if mdev else 1.)\n\treturn [data[i] for i in range(0, len(data)) if s[i] < m]\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}, {"term": "def", "name": "ensure_dir", "data": "def ensure_dir(file_path):\n\t'''\n\tFunction to ensure a file path exists, else creates the path\n\n\t:param file_path:\n\t:return:\n\t'''\n\tdirectory = os.path.dirname(file_path)\n\tif not os.path.exists(directory):\n\t\tos.makedirs(directory)\n\n\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}, {"term": "def", "name": "getFeatureList", "data": "def getFeatureList(mypath='../results/' + table + '/POCNormalized/'):\n\tonlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n\tonlyfiles.sort()\n\tfeatures = []\n\tfor file in onlyfiles:\n\t\tfeatures.append(file.strip().split('.')[0])\n\n\n\tif '' in features:\n\t\tfeatures.remove('')\n\n\tif 'MaxMin_Values' in features:\n\t\tfeatures.remove('MaxMin_Values')\n\n\n\treturn features\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}, {"term": "def", "name": "get_feature_result", "data": "def get_feature_result(feature, db_table):\n\n\t# go through the input file of the feature\n\t# path = '../data/Normalized_Wells/' + feature + '.csv'\n\tpath = '../results/' + db_table + '/POCNormalized/' + feature + '.csv'\n\tfp = open(path, 'r')\n\tfp.next()\n\tfeature_results = {}\n\t# mean = {}\n\tfor line in fp:\n\t\ttmp = line.strip().split(',')\n\n\t\tplate = int(tmp[0])\n\t\twell = tmp[1]\n\t\tdrug1 = tmp[2]\n\t\tdrug2 = tmp[3]\n\t\tworked = tmp[4]\n\n\t\t# if 'nan' for some features this might happen, then just set to mean of the plate\n\t\tif tmp[5] != 'nan':\n\t\t\tnormed_value = tmp[5]\n\n\t\telse:\n\t\t\t# normed_value = np.mean(mean[plate])\n\t\t\tnormed_value = 0\n\t\t\tworked = 'FALSE'\n\n\t\tif normed_value == -100000.0:\n\t\t\tworked = 'FALSE'\n\n\n\t\t#else basically create an entry in the dictionary with the information as well as the normed value\n\t\t#if the dictionary does not yet contain the plate, then set it\n\t\tif feature_results.has_key(plate):\n\t\t\tfeature_results[plate][well] = {'Drug_1': drug1, 'Drug_2': drug2, 'Worked': worked,\n\t\t\t\t\t\t\t\t\t\t\t\t 'N_Value': float(normed_value)}\n\t\telse:\n\t\t\tfeature_results[plate] = {\n\t\t\t\twell: {'Drug_1': drug1, 'Drug_2': drug2, 'Worked': worked, 'N_Value': float(normed_value)}}\n\n\t# print feature_results\n\treturn feature_results\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}, {"term": "def", "name": "get_FilteredFeatures", "data": "def get_FilteredFeatures(table):\n\t'''\n\tFilter features\n\n\t:param table:\n\t:return:\n\t'''\n\n\t###\n\t# IntraCV\n\t###\n\tfp = open('../results/' + table + '/Coefficient_of_Variation/IntraPlate_Variability.csv', 'r')\n\tfp.next()\n\tintra_CV_features_passed = set()\n\tfor line in fp:\n\t\ttmp = line.strip().split(',')\n\t\tif float(tmp[1]) < 0.2:\n\t\t\tintra_CV_features_passed.add(tmp[0])\n\tfp.close()\n\n\t###\n\t# InterCV\n\t###\n\tfp = open('../results/' + table + '/Coefficient_of_Variation/InterPlate_Variability.csv', 'r')\n\tfp.next()\n\tinter_CV_features_passed = set()\n\tfor line in fp:\n\t\ttmp = line.strip().split(',')\n\t\tif float(tmp[1]) < 0.2:\n\t\t\tinter_CV_features_passed.add(tmp[0])\n\tfp.close()\n\n\t###\n\t# Replicate Correlation (Reproduceability)\n\t###\n\tfp = open('../results/' + table + '/Correlation_Between_Replicates/Correlation_Results.csv', 'r')\n\tfp.next()\n\treplicate_Cor_features_passed = set()\n\tfor line in fp:\n\t\ttmp = line.strip().split(',')\n\t\tif float(tmp[1]) > 0.2:\n\t\t\treplicate_Cor_features_passed.add(tmp[0])\n\tfp.close()\n\n\n\t###\n\t# Effect Size\n\t###\n\tfp = open('../results/' + table + '/Effect_Size/Effect_Sizes.csv', 'r')\n\tfp.next()\n\teffect_size_features_passed = set()\n\tfor line in fp:\n\t\ttmp = line.strip().split(',')\n\t\tif float(tmp[5]) > 0:\n\t\t\teffect_size_features_passed.add(tmp[0])\n\tfp.close()\n\n\n\tall_passed = intra_CV_features_passed.intersection(inter_CV_features_passed).intersection(replicate_Cor_features_passed).intersection(effect_size_features_passed)\n\n\tprint 'Features passed (%s):' %table\n\tprint 'IntraCV: %d' % len(intra_CV_features_passed)\n\tprint 'InterCV: %d' %len(inter_CV_features_passed)\n\tprint 'Replicate Correlation: %d' % len(replicate_Cor_features_passed)\n\tprint 'Effect Size: %d' % len(effect_size_features_passed)\n\tprint 'Union: %d' %len(all_passed)\n\tprint '----'\n\treturn all_passed\n\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}, {"term": "def", "name": "create_correlation_Network_AllFeatures", "data": "def create_correlation_Network_AllFeatures(table, saveNetwork=False):\n\n\n\tfp = open('../results/' + table + '/Effect_Size/Effect_Sizes.csv', 'r')\n\tfp.next()\n\teffect_size_dic = {}\n\tfor line in fp:\n\t\ttmp = line.strip().split(',')\n\n\t\teffect_size_dic[tmp[0]] = int(tmp[5])\n\tfp.close()\n\n\n\tfeatures = getFeatureList()\n\n\n\tfeature_dmso_values = {}\n\n\tfor f in features:\n\t\tscreen_results = get_feature_result(f, table)\n\t\tplates = screen_results.keys()\n\n\t\tdmso_values = []\n\t\tfor plate in plates:\n\t\t\tfor well in screen_results[plate]:\n\t\t\t\tif screen_results[plate][well]['Worked'] == 'FALSE' or screen_results[plate][well][\n\t\t\t\t\t'N_Value'] == -100000:\n\t\t\t\t\tcontinue\n\t\t\t\tif screen_results[plate][well]['Drug_1'] == 'DMSO':\n\t\t\t\t\tdmso_values.append(screen_results[plate][well]['N_Value'])\n\t\tfeature_dmso_values[f] = dmso_values\n\n\t# create output graph\n\tG = nx.Graph()\n\n\t# 1.) go through all pairwise correlations\n\t# 2.) Calculate pearson correlation\n\t# 3.) If correlation bigger than 0.7 create edge in output graph\n\n\t# print 'Calculate Correlation'\n\theatmap_data = []\n\tfor f in feature_dmso_values.keys():\n\t\tprint f\n\t\tfeature1 = feature_dmso_values[f]\n\t\tG.add_node(f)\n\t\ttmp = []\n\t\tfor f2 in feature_dmso_values.keys():\n\t\t\t#if f != f2:\n\t\t\tfeature2 = feature_dmso_values[f2]\n\t\t\tcor = np.corrcoef(feature1, feature2)[0, 1]\n\n\t\t\tif abs(cor) > 1:\n\t\t\t\tG.add_edge(f, f2, weight=cor)\n\t\t\t\tG.node[f]['Effect'] = effect_size_dic[f]\n\t\t\t\tG.node[f2]['Effect'] = effect_size_dic[f2]\n\t\t\ttmp.append(cor)\n\t\theatmap_data.append(tmp)\n\n\t# Save correlation network for manual inspection\n\tif saveNetwork:\n\t\tensure_dir('../results/'+table+'/Remove_Correlation/CorrelationNetwork.gml')\n\t\tnx.write_gml(G, '../results/'+table+'/Remove_Correlation/CorrelationNetwork.gml')\n\t\tsns.clustermap(data=heatmap_data, cmap=\"RdBu\")\n\t\t#plt.show()\n\t\tplt.savefig('../results/'+table+'/Remove_Correlation/CorrelationHeatMap.pdf')\n\t\tplt.close()\n\n\treturn G\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}, {"term": "def", "name": "check_edge_between", "data": "def check_edge_between(min_nod, g):\n\t'''\n\tFind minimal amount of nodes to remove, so that min_nodes are not neighbors anymore\n\n\t:param min_nod: list of nodes to check\n\t:param g: connected component subgraph\n\t:return: list of nodes to remove so that nodes in min_nod are not connected\n\t'''\n\n\t# extract all pairwise edges between nodes in min_nod\n\thas_edge = []\n\tfor min1 in min_nod:\n\t\tfor min2 in min_nod:\n\t\t\tif min1 > min2:\n\t\t\t\tif g.has_edge(min1, min2):\n\t\t\t\t\thas_edge.append(min1)\n\t\t\t\t\thas_edge.append(min2)\n\n\t# create Counter instance\n\tdata = Counter(has_edge)\n\t# get values of which node occured how often edges\n\t# e.g. [2,2,1,1,1,1] = two nodes are connected with two in min_nodes while 4 nodes are connected only with one\n\tfreq_list = data.values()\n\n\t# if freq_list == 0, all nodes are separated from each other\n\tif len(freq_list) == 0:\n\t\treturn []\n\n\t# find max edges of on of the nodes (e.g. example above would be 2)\n\tmax_cnt = max(freq_list)\n\n\t# get all nodes that are involved in max_cnt (e.g. the first two nodes from the freq_list)\n\ttotal = freq_list.count(max_cnt)\n\n\t# if all nodes are equaly e.g. [1,1,1], it does'nt bother which one to remove, choose randomly one (so remove the other two)\n\tif total == len(freq_list):\n\t\tmax_val = 0\n\t\tmax_node = ''\n\t\tfor node in min_nod:\n\t\t\tif g.node[node]['Effect'] > max_val:\n\t\t\t\tmax_val = g.node[node]['Effect']\n\t\t\t\tmax_node = node\n\n\t\tkeep = max_node\n\t\t# keep = choice(min_nod)\n\t\tcopylist = list(min_nod)\n\t\tcopylist.remove(keep)\n\t\treturn copylist\n\n\t# Return these nodes for removal (first two from example above)\n\tmost_common = data.most_common(total)\n\treturn [elem[0] for elem in most_common]\n\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}, {"term": "def", "name": "analyse_component", "data": "def analyse_component(g, draw=False):\n\t'''\n\tMain function for max fragmentation of subgraphs.\n\tTakes a graph (origins from bigger network as connected component)\n\tSlowly fragmentises it by removing best suited nodes\n\n\t:param g: connected component subgraph\n\t:param draw: True if there should be a step by step drawing output\n\t:return: number of\n\t'''\n\t# contains max number nodes for current component\n\ttmp_keep = []\n\n\t# fragmentise connected component subgraph until all nodes fragmented\n\twhile len(g.nodes()) > 0:\n\n\t\t# draw option\n\t\tif draw == True:\n\t\t\tnx.draw_networkx(g, pos=nx.spring_layout(g), with_labels=False)\n\t\t\tplt.draw()  # pyplot draw()\n\t\t\tplt.show()\n\n\t\t# list for nodes that need to be removed in each iteration\n\t\t# Contains: Selected Nodes (find in tmp_keep), as well as their neighbors\n\t\tnodes_to_remove = set()\n\n\t\t# if (remaing) component is only two nodes ==>  A--B; take randomly one of the two\n\t\tif len(g.nodes()) == 2 and len(g.edges()) == 1:\n\t\t\ttwo_nodes = list(g.nodes())\n\n\t\t\tif g.node[two_nodes[0]]['Effect'] > g.node[two_nodes[1]]['Effect']:\n\t\t\t\ttmp_keep.append(two_nodes[0])\n\t\t\telse:\n\t\t\t\ttmp_keep.append(two_nodes[1])\n\n\t\t\t# purely random choice of which node to keep\n\t\t\t# rand_node = choice(g.nodes())\n\t\t\t# tmp_keep.append(rand_node)\n\n\t\t\tnodes_to_remove.add(list(g.nodes())[0])\n\t\t\tnodes_to_remove.add(list(g.nodes())[1])\n\n\t\t# if bigger than only two connected nodes\n\n\t\telse:\n\t\t\t# get node degrees\n\t\t\tdegrees_tmp = g.degree()\n\n\t\t\tdegrees = {}\n\t\t\tfor d in degrees_tmp:\n\t\t\t\tdegrees[d[0]] = d[1]\n\n\t\t\t# find terminal nodes (= degree 1)\n\t\t\tterminal_nodes = [x for x in degrees if degrees[x] == 1]\n\n\t\t\t# if subgraph still has terminal nodes, choose these\n\t\t\tif len(terminal_nodes) > 0:\n\t\t\t\tfor tn in terminal_nodes:\n\t\t\t\t\ttmp_keep.append(tn)\n\t\t\t\t\tnodes_to_remove.add(list(g.edges(tn))[0][1])\n\t\t\t\t\tnodes_to_remove.add(tn)\n\n\t\t\t# if no terminal nodes exist\n\t\t\telse:\n\n\t\t\t\t# Check if there are nodes with higher degree than other\n\t\t\t\t# if all degrees uniformly it's for example a triangle, rectangle etc. (circularity)\n\t\t\t\tif all(x == degrees.values()[0] for x in degrees.values()) == False:\n\t\t\t\t\t# example for some nodes with lower/higher degree than others\n\t\t\t\t\t# A-B\n\t\t\t\t\t# |\\|  ==> in this case the algorithm should pick B and C (other alternative would be only A or D)\n\t\t\t\t\t# C-D\n\n\t\t\t\t\t# extract smalles degree\n\t\t\t\t\tmin_degree = min(degrees.values())\n\n\t\t\t\t\t# get nodes with this smallest degree\n\t\t\t\t\tmin_nodes = [x for x in degrees if degrees[x] == min_degree]\n\n\t\t\t\t\t# check if these nodes with smallest degree are somehow neighbors\n\t\t\t\t\t# e.g. two rectangles (4 nodes) connected by a middle node\n\t\t\t\t\t# ==> always the three \"outer\" rectangle nodes would have degree 2 (togher with the middle one connecting the two rectangles)\n\t\t\t\t\twhile True:\n\t\t\t\t\t\t# remove the minimum amount of nodes, so all selected \"min_nodes\" are no neighbors anymore\n\t\t\t\t\t\tnode_edge_remove = check_edge_between(min_nodes, g)\n\t\t\t\t\t\tif len(node_edge_remove) == 0:\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\tfor node in node_edge_remove:\n\t\t\t\t\t\t\tmin_nodes.remove(node)\n\n\t\t\t\t\t# Save the Min_nodes to tmp_keep and add them to nodes_to_remove (togher with their neighbors)\n\t\t\t\t\tfor mn in min_nodes:\n\t\t\t\t\t\ttmp_keep.append(mn)\n\t\t\t\t\t\tnodes_to_remove.add(mn)\n\t\t\t\t\t\tedges = g.edges(mn)\n\t\t\t\t\t\tfor edge in edges:\n\t\t\t\t\t\t\tnodes_to_remove.add(edge[1])\n\n\t\t\t\t# if all degrees are uniformly, meaning you have a triangle, rectangle, fully connected graph\n\t\t\t\t# e.g.:\n\t\t\t\t# A-B\n\t\t\t\t# | |  ==> e.g. first pick A (randomly); remove B + C (neighbors); in next\n\t\t\t\t# C-D\t\t   iteration there is only D left (will be picked)\n\t\t\t\telse:\n\t\t\t\t\t# randomly choose a single node (all nodes equally anyway)\n\t\t\t\t\tmax_val = 0\n\t\t\t\t\tmax_node = ''\n\t\t\t\t\tfor node in g.nodes():\n\t\t\t\t\t\tif g.node[node]['Effect'] > max_val:\n\t\t\t\t\t\t\tmax_val = g.node[node]['Effect']\n\t\t\t\t\t\t\tmax_node = node\n\n\t\t\t\t\trand_node = max_node\n\n\t\t\t\t\t# rand_node = choice(g.nodes())\n\t\t\t\t\t# add this random nood to tmp_keep and again remove him together with the neighbors\n\t\t\t\t\ttmp_keep.append(rand_node)\n\t\t\t\t\tnodes_to_remove.add(rand_node)\n\t\t\t\t\tedges = g.edges(rand_node)\n\t\t\t\t\tfor edge in edges:\n\t\t\t\t\t\tnodes_to_remove.add(edge[1])\n\n\t\t# Remove nodes from current subgraph\n\t\tfor ntr in nodes_to_remove:\n\t\t\tg.remove_node(ntr)\n\n\t\tif draw:\n\t\t\tprint tmp_keep\n\n\treturn tmp_keep\n\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}, {"term": "def", "name": "remove_Correlating_Features", "data": "def remove_Correlating_Features(table,corelation_Threshold=0.6):\n\n\tfiltered_features = get_FilteredFeatures(table)\n\n\n\tprint 'Create Correlation network for: %s' %table\n\t# get network file\n\tif os.path.isfile('../results/'+table+'/Remove_Correlation/CorrelationNetwork.gml') == True:\n\t\tprint 'Using already existing correlation network!'\n\t\tg = nx.read_gml('../results/'+table+'/Remove_Correlation/CorrelationNetwork.gml')\n\telse:\n\t\tg = create_correlation_Network_AllFeatures(table, saveNetwork=True)\n\n\texit()\n\n\t#Remove self edges (by definitation cor = 1)\n\tg.remove_edges_from(g.selfloop_edges())\n\n\t# Remove non correlating edges\n\tall_edges =  list(g.edges())\n\tfor edge in all_edges:\n\t\tcor = abs(g[edge[0]][edge[1]]['weight'])\n\t\tif cor < corelation_Threshold:\n\t\t\tg.remove_edge(edge[0],edge[1])\n\n\tall_nodes = list(g.nodes())\n\tfor node in all_nodes:\n\t\tif node not in filtered_features:\n\t\t\tg.remove_node(node)\n\n\n\tprint 'Correlation Network (All Features): '\n\tprint 'Number of nodes: %d' %len(g.nodes())\n\tprint 'Number of edges: %d' %len(g.edges())\n\tprint '--'\n\n\n\n\n\n\t# Extract connected components\n\tcomponents = nx.connected_component_subgraphs(g)\n\n\t# keep contains the max amount of nodes that are never connected\n\tkeep = []\n\n\t# Go threw components\n\tfor comp in components:\n\t\t# if single node, add to keep (anyway not connected to anything)\n\t\tif len(comp.nodes()) == 1:\n\t\t\tkeep.append(list(comp.nodes())[0])\n\t\t# if not single node check maximum fragmentation\n\t\telse:\n\t\t\tkeep = keep + analyse_component(comp, False)\n\n\n\n\tkeep.sort()\n\n\tensure_dir('../results/'+table+'/Remove_Correlation/Uncorrelated_Features.csv')\n\tfp = open('../results/'+table+'/Remove_Correlation/Uncorrelated_Features.csv','w')\n\n\tfor feature in keep:\n\t\tfp.write(feature+'\\n')\n\tfp.close()\n\tprint 'Number of Features: %d' %len(keep)\n\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}, {"term": "def", "name": "remove_Correlating_Features_CombineBatches", "data": "def remove_Correlating_Features_CombineBatches(table,table2,corelation_Threshold=0.6):\n\n\n\n\tfiltered_features1 = get_FilteredFeatures(table)\n\tfiltered_features2 = get_FilteredFeatures(table2)\n\n\tif os.path.isfile('../results/' + table + '/Remove_Correlation/CorrelationNetwork.gml') == True and os.path.isfile('../results/' + table2 + '/Remove_Correlation/CorrelationNetwork.gml') == True:\n\t\t\tg1 = nx.read_gml('../results/' + table + '/Remove_Correlation/CorrelationNetwork.gml')\n\t\t\tg2 = nx.read_gml('../results/' + table2 + '/Remove_Correlation/CorrelationNetwork.gml')\n\telse:\n\t\tprint 'First analyse batches separately'\n\t\texit()\n\n\n\n\n\t#Remove self edges (by definitation cor = 1)\n\tg1.remove_edges_from(g1.selfloop_edges())\n\tg2.remove_edges_from(g2.selfloop_edges())\n\n\tall_nodes = list(g1.nodes())\n\tfor node in all_nodes:\n\t\tif node not in filtered_features1 or node not in filtered_features2:\n\t\t\tg1.remove_node(node)\n\n\n\t# Remove non correlating edges\n\tall_edges =  list(g1.edges())\n\tfor edge in all_edges:\n\t\tcor1 = abs(g1[edge[0]][edge[1]]['weight'])\n\t\tcor2 = abs(g2[edge[0]][edge[1]]['weight'])\n\n\t\tif cor1 < corelation_Threshold and cor2 < corelation_Threshold:\n\t\t\tg1.remove_edge(edge[0],edge[1])\n\n\n\tprint 'Correlation Network (All Features): '\n\tprint 'Number of nodes: %d' %len(g1.nodes())\n\tprint 'Number of edges: %d' %len(g1.edges())\n\tprint '--'\n\n\n\t# Extract connected components\n\tcomponents = nx.connected_component_subgraphs(g1)\n\n\t# keep contains the max amount of nodes that are never connected\n\tkeep = []\n\n\t# Go threw components\n\tfor comp in components:\n\t\t# if single node, add to keep (anyway not connected to anything)\n\t\tif len(comp.nodes()) == 1:\n\t\t\tkeep.append(list(comp.nodes())[0])\n\t\t# if not single node check maximum fragmentation\n\t\telse:\n\t\t\tkeep = keep + analyse_component(comp, True)\n\n\n\n\tkeep.sort()\n\n\tensure_dir('../results/'+table+'/Remove_Correlation/Uncorrelated_Features_CombinedBatches.csv')\n\tensure_dir('../results/' + table2 + '/Remove_Correlation/Uncorrelated_Features_CombinedBatches.csv')\n\n\tfp = open('../results/'+table+'/Remove_Correlation/Uncorrelated_Features_CombinedBatches.csv','w')\n\tfp2 = open('../results/' + table2 + '/Remove_Correlation/Uncorrelated_Features_CombinedBatches.csv', 'w')\n\n\tfor feature in keep:\n\t\tfp.write(feature+'\\n')\n\t\tfp2.write(feature + '\\n')\n\tfp.close()\n\tfp2.close()\n\tprint 'Number of Features: %d' %len(keep)\n\n", "description": null, "category": "remove", "imports": ["import networkx as nx", "import numpy as np", "from os import listdir", "from os.path import isfile, join", "from matplotlib import pylab as plt", "from collections import Counter", "import os", "import seaborn as sns"]}], [{"term": "class", "name": "RemoveGroupsNotificationProtocolEntity", "data": "class RemoveGroupsNotificationProtocolEntity(GroupsNotificationProtocolEntity):\n\t'''\n\n\n\n\n\n\n\t'''\n\tTYPE_PARTICIPANT_ADMIN = \"admin\"\n\tdef __init__(self, _id, _from, timestamp, notify, participant, offline,\n\t\t\t\t subject,\n\t\t\t\t participants):\n\t\tsuper(RemoveGroupsNotificationProtocolEntity, self).__init__(_id, _from, timestamp, notify, participant, offline)\n\t\tself.setGroupProps(subject, participants)\n\n\tdef setGroupProps(self,\n\t\t\t\t\t  subject,\n\t\t\t\t\t  participants):\n\n\t\tassert type(participants) is dict, \"Participants must be a dict {jid => type?}\"\n\n\t\tself.subject = subject\n\t\tself.participants = participants\n\n\tdef getParticipants(self):\n\t\treturn self.participants\n\n\tdef getSubject(self):\n\t\treturn self.subject\n\n\tdef toProtocolTreeNode(self):\n\t\tnode = super(RemoveGroupsNotificationProtocolEntity, self).toProtocolTreeNode()\n\t\tremoveNode = ProtocolTreeNode(\"remove\", {\"subject\": self.subject})\n\t\tparticipants = []\n\t\tfor jid in self.getParticipants():\n\t\t\tpnode = ProtocolTreeNode(\"participant\", {\"jid\": jid})\n\t\t\tparticipants.append(pnode)\n\n\t\tremoveNode.addChildren(participants)\n\t\tnode.addChild(removeNode)\n\n\t\treturn node\n\n\t@staticmethod\n\tdef fromProtocolTreeNode(node):\n\t\tremoveNode = node.getChild(\"remove\")\n\t\tparticipants = {}\n\t\tfor p in removeNode.getAllChildren(\"participant\"):\n\t\t\tparticipants[p[\"jid\"]] = p[\"type\"]\n\n\t\treturn RemoveGroupsNotificationProtocolEntity(\n\t\t\tnode[\"id\"], node[\"from\"], node[\"t\"], node[\"notify\"], node[\"participant\"], node[\"offline\"],\n\t\t\tremoveNode[\"subject\"], participants\n\t\t)\n", "description": null, "category": "remove", "imports": ["from .notification_groups import GroupsNotificationProtocolEntity", "from yowsup.structs import ProtocolTreeNode"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('consentrecords', '0060_auto_20170606_0308'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationgroupaccess',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationgroupaccess',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationgroupaccess',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationgroupaccess',\n\t\t\tname='parent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationgroupaccess',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationgroupaccesshistory',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationgroupaccesshistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationgroupaccesshistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationuseraccess',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationuseraccess',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationuseraccess',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationuseraccess',\n\t\t\tname='parent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationuseraccess',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationuseraccesshistory',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationuseraccesshistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationuseraccesshistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathgroupaccess',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathgroupaccess',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathgroupaccess',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathgroupaccess',\n\t\t\tname='parent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathgroupaccess',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathgroupaccesshistory',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathgroupaccesshistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathgroupaccesshistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathuseraccess',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathuseraccess',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathuseraccess',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathuseraccess',\n\t\t\tname='parent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathuseraccess',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathuseraccesshistory',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathuseraccesshistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='pathuseraccesshistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='usergroupaccess',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='usergroupaccess',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='usergroupaccess',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='usergroupaccess',\n\t\t\tname='parent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='usergroupaccess',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='usergroupaccesshistory',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='usergroupaccesshistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='usergroupaccesshistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='useruseraccess',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='useruseraccess',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='useruseraccess',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='useruseraccess',\n\t\t\tname='parent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='useruseraccess',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='useruseraccesshistory',\n\t\t\tname='accessee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='useruseraccesshistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='useruseraccesshistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='user',\n\t\t\tname='primaryAdministrator',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='user',\n\t\t\tname='publicAccess',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='OrganizationGroupAccess',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='OrganizationGroupAccessHistory',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='OrganizationUserAccess',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='OrganizationUserAccessHistory',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='PathGroupAccess',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='PathGroupAccessHistory',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='PathUserAccess',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='PathUserAccessHistory',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='UserGroupAccess',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='UserGroupAccessHistory',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='UserUserAccess',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='UserUserAccessHistory',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations"]}], [{"term": "class", "name": "TestDb", "data": "class TestDb(unittest.TestCase):\n\tdef test_database_schema_and_sqlalchemy_model_are_in_sync(self):\n\t\tall_meta_data = MetaData()\n\t\tfor (table_name, table) in airflow_base.metadata.tables.items():\n\t\t\tall_meta_data._add_table(table_name, table.schema, table)\n\n\t\t# create diff between database schema and SQLAlchemy model\n\t\tmctx = MigrationContext.configure(engine.connect())\n\t\tdiff = compare_metadata(mctx, all_meta_data)\n\n\t\t# known diffs to ignore\n\t\tignores = [\n\t\t\t# ignore tables created by celery\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'celery_taskmeta'),\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'celery_tasksetmeta'),\n\t\t\t# ignore indices created by celery\n\t\t\tlambda t: (t[0] == 'remove_index' and t[1].name == 'task_id'),\n\t\t\tlambda t: (t[0] == 'remove_index' and t[1].name == 'taskset_id'),\n\t\t\t# Ignore all the fab tables\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'ab_permission'),\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'ab_register_user'),\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'ab_role'),\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'ab_permission_view'),\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'ab_permission_view_role'),\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'ab_user_role'),\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'ab_user'),\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'ab_view_menu'),\n\t\t\t# Ignore all the fab indices\n\t\t\tlambda t: (t[0] == 'remove_index' and t[1].name == 'permission_id'),\n\t\t\tlambda t: (t[0] == 'remove_index' and t[1].name == 'name'),\n\t\t\tlambda t: (t[0] == 'remove_index' and t[1].name == 'user_id'),\n\t\t\tlambda t: (t[0] == 'remove_index' and t[1].name == 'username'),\n\t\t\tlambda t: (t[0] == 'remove_index' and t[1].name == 'field_string'),\n\t\t\tlambda t: (t[0] == 'remove_index' and t[1].name == 'email'),\n\t\t\tlambda t: (t[0] == 'remove_index' and t[1].name == 'permission_view_id'),\n\t\t\t# from test_security unit test\n\t\t\tlambda t: (t[0] == 'remove_table' and t[1].name == 'some_model'),\n\t\t]\n\t\tfor ignore in ignores:\n\t\t\tdiff = [d for d in diff if not ignore(d)]\n\n\t\tself.assertFalse(diff, 'Database schema and SQLAlchemy model are not in sync: ' + str(diff))\n\n\tdef test_only_single_head_revision_in_migrations(self):\n\t\tconfig = Config()\n\t\tconfig.set_main_option(\"script_location\", \"airflow:migrations\")\n\t\tscript = ScriptDirectory.from_config(config)\n\n\t\t# This will raise if there are multiple heads\n\t\t# To resolve, use the command `alembic merge`\n\t\tscript.get_current_head()\n\n\tdef test_default_connections_sort(self):\n\t\tpattern = re.compile('conn_id=[\\\"|\\'](.*?)[\\\"|\\']', re.DOTALL)\n\t\tsource = inspect.getsource(create_default_connections)\n\t\tsrc = pattern.findall(source)\n\t\tself.assertListEqual(sorted(src), src)\n", "description": null, "category": "remove", "imports": ["import inspect", "import re", "import unittest", "from alembic.autogenerate import compare_metadata", "from alembic.config import Config", "from alembic.migration import MigrationContext", "from alembic.script import ScriptDirectory", "from sqlalchemy import MetaData", "from airflow.models import Base as airflow_base", "from airflow.settings import engine", "from airflow.utils.db import create_default_connections"]}], [{"term": "def", "name": "preprocessor", "data": "def preprocessor(request) -> HtmlPreprocessor:\n\tfilename = request.param\n\treturn HtmlPreprocessor(text=get_text(test_folder/filename),\n\t\t\t\t\t\t\tfilename=filename)\n\n", "description": null, "category": "remove", "imports": ["import logging", "import pytest", "from pathlib import Path", "from src.helpers import get_text, get_file_names", "from src.html_preprocessor import HtmlPreprocessor"]}, {"term": "class", "name": "classTestHtmlPreprocessor:", "data": "class TestHtmlPreprocessor:\n\tdef test_NoName_ShouldHaveDefaultFilename(self):\n\t\tpreprocessor = HtmlPreprocessor(text=\"\")\n\t\tassert preprocessor.filename == \"missing_filename\"\n\n\t@pytest.mark.parametrize('preprocessor, filename', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html'),\n\t\t('Linguistics_505.html', 'Linguistics_505.html'),\n\t\t('Linguistics_483.html', 'Linguistics_483.html')\n\t\t], indirect=['preprocessor'])\n\tdef test_ValidText_ShouldPass(\n\t\t\tself, preprocessor, filename, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.HTML\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 13),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 11),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_ValidTextExtractTextFromHtml_ShouldParseHtmlToListOfPagesAndParagraphs(\n\t\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert \"Page {}\".format(num_pages) in captured.out\n\t\tassert \"Page {}\".format(num_pages+1) not in captured.out\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages, has_ref', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 12, True),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8, True),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8, False)\n\t], indirect=['preprocessor'])\n\tdef test_ValidTextRemoveReferences_ShouldRemoveReference(\n\t\t\tself, preprocessor, filename, num_pages, has_ref, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tassert preprocessor.remove_references() == has_ref\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert \"Page {}\".format(num_pages) in captured.out\n\t\tassert \"Page {}\".format(num_pages+1) not in captured.out\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages, has_keywords', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 12, True),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8, True),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8, True)\n\t], indirect=['preprocessor'])\n\tdef test_RemoveKeywords_ShouldRemoveKeywords(\n\t\t\tself, preprocessor, filename, num_pages, has_keywords, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tassert preprocessor.remove_keywords() == has_keywords\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert \"Page {}\".format(num_pages) in captured.out\n\t\tassert \"Page {}\".format(num_pages+1) not in captured.out\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages, num_headers_and_footers', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 12, 13),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8, 14),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8, 14)  # Not including first page's footer\n\t], indirect=['preprocessor'])\n\tdef test_RemoveHeadersAndFooters_ShouldRemoveHeadersAndFooters(\n\t\t\tself, preprocessor, filename, num_pages, num_headers_and_footers, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tassert preprocessor.remove_header_and_footer() == num_headers_and_footers\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages, num_footnotes', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 12, 1),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8, 0),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8, 1)\n\t], indirect=['preprocessor'])\n\tdef test_RemoveFootnotes_ShouldRemoveFootnotes(\n\t\t\tself, preprocessor, filename, num_pages, num_footnotes, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tassert preprocessor.remove_footnotes() == num_footnotes\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html'),\n\t\t('Linguistics_505.html', 'Linguistics_505.html'),\n\t\t('Linguistics_483.html', 'Linguistics_483.html')\n\t], indirect=['preprocessor'])\n\tdef test_removeQuotations_ShouldNotPrintError(\n\t\t\tself, preprocessor, filename, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_quotations()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages, num_empty_paragraphs', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 12, 0),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8, 0),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8, 0)\n\t], indirect=['preprocessor'])\n\tdef test_RemoveEmptyParagraphs_ShouldRemoveEmptyParagraphs(\n\t\t\tself, preprocessor, filename, num_pages, num_empty_paragraphs, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_quotations()\n\t\tassert preprocessor.remove_empty_paragraphs() == num_empty_paragraphs\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 12),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_RemoveEolHyphenation_ShouldRemoveEolHyphenation(\n\t\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 12),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_SubstituteEndOfSentencePunctuationWithPeriod_ShouldNotHaveQuestionMarkOrExclamationPoint(\n\t\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert \"!\" not in captured.out\n\t\tassert \"?\" not in captured.out\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 11),  # Last page gets joined with second to last page\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_JoinBrokenSentences_ShouldNotPrintError(\n\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tpreprocessor.join_broken_sentences()\n\t\tpreprocessor.remove_quotations()\n\t\tpreprocessor.remove_empty_paragraphs()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 11),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_RemoveStyledText_ShouldNotPrintError(\n\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tpreprocessor.join_broken_sentences()\n\t\tpreprocessor.remove_quotations()\n\t\tpreprocessor.remove_empty_paragraphs()\n\t\tpreprocessor.remove_styled_text()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages, num_examples', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 11, 0),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8, 0),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8, 0)\n\t], indirect=['preprocessor'])\n\tdef test_RemoveExamples_ShouldRemoveCorrentNumberOfExamples(\n\t\t\tself, preprocessor, filename, num_pages, num_examples, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tpreprocessor.join_broken_sentences()\n\t\tpreprocessor.remove_quotations()\n\t\tpreprocessor.remove_empty_paragraphs()\n\t\tpreprocessor.remove_styled_text()\n\t\tassert preprocessor.remove_examples() == num_examples\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 11),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_RemoveIntextReferences_ShouldNotRaiseErrors(\n\t\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tpreprocessor.join_broken_sentences()\n\t\tpreprocessor.remove_quotations()\n\t\tpreprocessor.remove_empty_paragraphs()\n\t\tpreprocessor.remove_styled_text()\n\t\tpreprocessor.remove_examples()\n\t\tpreprocessor.remove_intext_references()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 11),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_RemoveOovTokens_ShouldNotRaiseErrors(\n\t\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tpreprocessor.join_broken_sentences()\n\t\tpreprocessor.remove_quotations()\n\t\tpreprocessor.remove_empty_paragraphs()\n\t\tpreprocessor.remove_styled_text()\n\t\tpreprocessor.remove_examples()\n\t\tpreprocessor.remove_intext_references()\n\t\tpreprocessor.remove_oov_tokens()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 11),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_ReplaceNumbers_ShouldNotRaiseErrors(\n\t\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tpreprocessor.join_broken_sentences()\n\t\tpreprocessor.remove_quotations()\n\t\tpreprocessor.remove_empty_paragraphs()\n\t\tpreprocessor.remove_styled_text()\n\t\tpreprocessor.remove_examples()\n\t\tpreprocessor.remove_intext_references()\n\t\tpreprocessor.remove_oov_tokens()\n\t\tpreprocessor.replace_numbers()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 11),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_ReplaceNumbers_ShouldNotRaiseErrors(\n\t\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tpreprocessor.join_broken_sentences()\n\t\tpreprocessor.remove_quotations()\n\t\tpreprocessor.remove_empty_paragraphs()\n\t\tpreprocessor.remove_styled_text()\n\t\tpreprocessor.remove_examples()\n\t\tpreprocessor.remove_intext_references()\n\t\tpreprocessor.remove_oov_tokens()\n\t\tpreprocessor.replace_numbers()\n\t\tpreprocessor.break_paragraphs_into_sentences()\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS_AND_SENTENCES\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 11),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_ReplaceNumbers_ShouldNotRaiseErrors(\n\t\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tpreprocessor.join_broken_sentences()\n\t\tpreprocessor.remove_quotations()\n\t\tpreprocessor.remove_empty_paragraphs()\n\t\tpreprocessor.remove_styled_text()\n\t\tpreprocessor.remove_examples()\n\t\tpreprocessor.remove_intext_references()\n\t\tpreprocessor.remove_oov_tokens()\n\t\tpreprocessor.replace_numbers()\n\t\tpreprocessor.break_paragraphs_into_sentences()\n\t\tassert preprocessor.remove_unwanted_sentences() != 0\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS_AND_SENTENCES\n\t\tassert len(preprocessor.text) == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', 11),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', 8),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_Tokenize_ShouldNotRaiseErrors(\n\t\t\tself, preprocessor, filename, num_pages, capsys):\n\t\tassert preprocessor.filename == filename\n\t\tpreprocessor.extract_text_from_html()\n\t\tpreprocessor.remove_references()\n\t\tpreprocessor.remove_keywords()\n\t\tpreprocessor.remove_header_and_footer()\n\t\tpreprocessor.remove_footnotes()\n\t\tpreprocessor.remove_eol_hyphenation()\n\t\tpreprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\tpreprocessor.join_broken_sentences()\n\t\tpreprocessor.remove_quotations()\n\t\tpreprocessor.remove_empty_paragraphs()\n\t\tpreprocessor.remove_styled_text()\n\t\tpreprocessor.remove_examples()\n\t\tpreprocessor.remove_intext_references()\n\t\tpreprocessor.remove_oov_tokens()\n\t\tpreprocessor.replace_numbers()\n\t\tpreprocessor.break_paragraphs_into_sentences()\n\t\ttokenized_text = preprocessor.tokenize(keep_pos=True, keep_punct=True)\n\t\ttokenized_page_count = 0\n\t\tfor page in tokenized_text:\n\t\t\ttokenized_page_count += 1\n\n\t\tassert preprocessor.remove_unwanted_sentences() != 0\n\t\tassert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS_AND_SENTENCES\n\t\tassert len(preprocessor.text) == num_pages\n\t\tassert tokenized_page_count == num_pages\n\n\t\tpreprocessor.print_text()\n\t\tcaptured = capsys.readouterr()\n\t\tassert captured.out != \"\"\n\t\tassert captured.err == \"\"\n\n\t@pytest.mark.parametrize('preprocessor, filename, has_references', [\n\t\t('Linguistics_0.html', 'Linguistics_0.html', True),\n\t\t('Linguistics_505.html', 'Linguistics_505.html', True),\n\t\t('Linguistics_483.html', 'Linguistics_483.html', False)\n\t], indirect=['preprocessor'])\n\tdef test_Preprocess_ShouldNotRaiseErrors(\n\t\t\tself, preprocessor, filename, has_references, caplog):\n\t\tassert preprocessor.filename == filename\n\t\twith caplog.at_level(logging.INFO):\n\t\t\tassert preprocessor.preprocess()\n\t\tassert len(preprocessor.text) >= 1\n\t\tassert \"Entire text was deleted\" not in caplog.text\n\t\tprint(caplog.text)\n\t\tassert (\"Could not find the references\" not in caplog.text) == has_references\n\n\t@pytest.mark.skip(reason=\"To be used for find-grained testing\")\n\t@pytest.mark.parametrize('preprocessor, filename, num_pages', [\n\t\t# ('Linguistics_505.html', 'Linguistics_505.html', 11),\n\t\t# ('Sociology_505.html', 'Sociology_505.html', 8),\n\t\t('Economics_14.html', 'Economics_14.html', 26)\n\t\t# ('Linguistics_483.html', 'Linguistics_483.html', 8)\n\t], indirect=['preprocessor'])\n\tdef test_detailed_preprocessing(\n\t\t\tself, preprocessor, filename, num_pages):\n\t\tassert preprocessor.filename == filename\n\t\t# preprocessor.extract_text_from_html()\n\t\t# preprocessor.remove_references()\n\t\t# preprocessor.remove_keywords()\n\t\t# preprocessor.remove_header_and_footer()\n\t\t# preprocessor.remove_footnotes()\n\t\t# preprocessor.remove_empty_paragraphs()\n\t\t# preprocessor.remove_eol_hyphenation()\n\t\t# preprocessor.substitute_end_of_sentence_punctuation_with_period()\n\t\t# preprocessor.join_broken_sentences()\n\t\t# preprocessor.remove_quotations()\n\t\t# preprocessor.remove_styled_text()\n\t\t# preprocessor.remove_examples()\n\t\t# preprocessor.remove_intext_references()\n\t\t# preprocessor.remove_oov_tokens()\n\t\t# preprocessor.replace_numbers()\n\t\t# preprocessor.break_paragraphs_into_sentences()\n\t\t# tokenized_text = preprocessor.tokenize(keep_pos=True, keep_punct=True)\n\t\t# tokenized_page_count = 0\n\t\t# for page in tokenized_text:\n\t\t#\t tokenized_page_count += 1\n\t\t#\n\t\t# assert preprocessor.remove_unwanted_sentences() != 0\n\t\t# assert preprocessor.phase == HtmlPreprocessor.Phase.LISTS_OF_PAGES_AND_PARAGRAPHS_AND_SENTENCES\n\t\t# assert len(preprocessor.text) == num_pages\n\t\t# assert tokenized_page_count == num_pages\n\t\t#\n\t\tassert preprocessor.preprocess()\n\t\ttokenized_text = preprocessor.tokenize(keep_pos=True, keep_punct=True)\n\t\tfor text in tokenized_text:\n\t\t\tprint(text)\n\t\t# preprocessor.print_text()\n\t\t# captured = capsys.readouterr()\n\t\t# assert captured.out != \"\"\n\t\t# assert captured.err == \"\n", "description": null, "category": "remove", "imports": ["import logging", "import pytest", "from pathlib import Path", "from src.helpers import get_text, get_file_names", "from src.html_preprocessor import HtmlPreprocessor"]}], [{"term": "class", "name": "RemoveContactNotificationProtocolEntity", "data": "class RemoveContactNotificationProtocolEntity(ContactNotificationProtocolEntity):\n\t'''\n\t\n \n\n\t'''\n\n\tdef __init__(self, _id,  _from, timestamp, notify, offline, contactJid):\n\t\tsuper(RemoveContactNotificationProtocolEntity, self).__init__(_id, _from, timestamp, notify, offline)\n\t\tself.setData(contactJid)\n\n\tdef setData(self, jid):\n\t\tself.contactJid = jid\n\n\tdef toProtocolTreeNode(self):\n\t\tnode = super(RemoveContactNotificationProtocolEntity, self).toProtocolTreeNode()\n\t\tremoveNode = ProtocolTreeNode(\"remove\", {\"jid\": self.contactJid}, None, None)\n\t\tnode.addChild(removeNode)\n\t\treturn node\n\n\t@staticmethod\n\tdef fromProtocolTreeNode(node):\n\t\tentity = ContactNotificationProtocolEntity.fromProtocolTreeNode(node)\n\t\tentity.__class__ = RemoveContactNotificationProtocolEntity\n\t\tremoveNode = node.getChild(\"remove\")\n\t\tentity.setData(removeNode.getAttributeValue(\"jid\"))\n", "description": null, "category": "remove", "imports": ["from yowsup.structs import ProtocolTreeNode", "from .notification_contact import ContactNotificationProtocolEntity"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\tmigrations.swappable_dependency(settings.AUTH_USER_MODEL),\n\t\t('organizations', '0011_auto_20180118_1100'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationmember',\n\t\t\tname='organization',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organizationmember',\n\t\t\tname='user',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='address_line1',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='address_line2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='city',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='country',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='deleted',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='email',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='partner_organizations',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='phone_number',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='postal_code',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='registration',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='state',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='organization',\n\t\t\tname='tags',\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='organization',\n\t\t\tname='owner',\n\t\t\tfield=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL, verbose_name='owner'),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='organization',\n\t\t\tname='slug',\n\t\t\tfield=models.SlugField(max_length=100, verbose_name='slug'),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='organizationcontact',\n\t\t\tname='owner',\n\t\t\tfield=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL, verbose_name='owner'),\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='OrganizationMember',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.conf import settings", "from django.db import migrations, models", "import django.db.models.deletion"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('generator', '0001_initial'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice1',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice10',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice6',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice7',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice8',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='choice9',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name1',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name10',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name6',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name7',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name8',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='name9',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order1',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order10',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order6',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order7',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order8',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='scheme',\n\t\t\tname='order9',\n\t\t),\n\t\tmigrations.CreateModel(\n\t\t\tname='SchemeColumn',\n\t\t\tfields=[\n\t\t\t\t('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n\t\t\t\t('column_type', models.PositiveSmallIntegerField(choices=[(1, 'Name'), (2, 'Job'), (3, 'Email'), (4, 'Domain'), (5, 'Phone number'), (6, 'Company'), (7, 'Text'), (8, 'Age'), (9, 'Address'), (10, 'Date')], default=1)),\n\t\t\t\t('name', models.CharField(default='None', max_length=255)),\n\t\t\t\t('order', models.IntegerField(default=0, validators=[django.core.validators.MinValueValidator(1), django.core.validators.MaxValueValidator(10)])),\n\t\t\t\t('scheme', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='generator.scheme')),\n\t\t\t],\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["import django.core.validators", "from django.db import migrations, models", "import django.db.models.deletion"]}], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\n\tdef __init__(self, data=None, name=\"\", file=None, **attr):\n\t\tsuper().__init__(data=data, name=name, **attr)\n\t\tif file is None:\n\t\t\timport sys\n\n\t\t\tself.fh = sys.stdout\n\t\telse:\n\t\t\tself.fh = open(file, \"w\")\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tsuper().add_node(n, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add node: {n}\\n\")\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self, n):\n\t\tsuper().remove_node(n)\n\t\tself.fh.write(f\"Remove node: {n}\\n\")\n\n\tdef remove_nodes_from(self, nodes):\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tsuper().add_edge(u, v, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add edge: {u}-{v}\\n\")\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.add_edge(u, v, attr_dict=attr_dict, **attr)\n\n\tdef remove_edge(self, u, v):\n\t\tsuper().remove_edge(u, v)\n\t\tself.fh.write(f\"Remove edge: {u}-{v}\\n\")\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.remove_edge(u, v)\n\n\tdef clear(self):\n\t\tsuper().clear()\n\t\tself.fh.write(\"Clear graph\\n\")\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["import matplotlib.pyplot as plt", "import networkx as nx", "from networkx import Graph", "\t\t\timport sys"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('consentrecords', '0086_auto_20170923_1330'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='disqualifyingtag',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='disqualifyingtag',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='disqualifyingtag',\n\t\t\tname='parent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='disqualifyingtag',\n\t\t\tname='service',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='disqualifyingtag',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='disqualifyingtaghistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='disqualifyingtaghistory',\n\t\t\tname='service',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='disqualifyingtaghistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompt',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompt',\n\t\t\tname='domain',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompt',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompt',\n\t\t\tname='offering',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompt',\n\t\t\tname='organization',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompt',\n\t\t\tname='site',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompt',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompthistory',\n\t\t\tname='domain',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompthistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompthistory',\n\t\t\tname='offering',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompthistory',\n\t\t\tname='organization',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompthistory',\n\t\t\tname='site',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompthistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experiencepromptservice',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experiencepromptservice',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experiencepromptservice',\n\t\t\tname='parent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experiencepromptservice',\n\t\t\tname='service',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experiencepromptservice',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experiencepromptservicehistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experiencepromptservicehistory',\n\t\t\tname='service',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experiencepromptservicehistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompttext',\n\t\t\tname='deleteTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompttext',\n\t\t\tname='lastTransaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompttext',\n\t\t\tname='parent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompttext',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompttexthistory',\n\t\t\tname='instance',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='experienceprompttexthistory',\n\t\t\tname='transaction',\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='experience',\n\t\t\tname='era',\n\t\t\tfield=models.IntegerField(null=True),\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='experiencehistory',\n\t\t\tname='era',\n\t\t\tfield=models.IntegerField(editable=False, null=True),\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='DisqualifyingTag',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='DisqualifyingTagHistory',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ExperiencePrompt',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ExperiencePromptHistory',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ExperiencePromptService',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ExperiencePromptServiceHistory',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ExperiencePromptText',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ExperiencePromptTextHistory',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations, models"]}], [{"term": "class", "name": "classNode:", "data": "class Node:\n\tdef __init__(self, value=0):\n\t\tself.value = value\n\t\tself.prev = None\n\t\tself.next = None\n", "description": null, "category": "remove", "imports": []}, {"term": "class", "name": "classDoublyLinkedList:", "data": "class DoublyLinkedList:\n\tdef __init__(self):\n\t\tself.head = None\n\t\tself.tail = None\n\n\tdef containsNodeWithValue(self, value):\n\t\tnode = self.head\n\t\twhile node is not None and node.value != value:\n\t\t\tnode = node.next\n\t\treturn node is not None\n\n\tdef removeNode(self, nodeToRemove):\n\t\t# Check if the node to remove is the head of dll\n\t\tif self.head is nodeToRemove:\n\t\t\tself.head = self.head.next\n\t\t# Check if the node to remove is the tail of dll\n\t\tif self.tail is nodeToRemove:\n\t\t\tself.tail = self.tail.prev\n\t\t# Remove remaining bindings if head/tail, else all bindings\n\t\tself.removeBindings(nodeToRemove)\n\n\tdef removeBindings(self, nodeToRemove):\n\t\t# Check if we have a previous node\n\t\t# If we just removed a head, previous will be None\n\t\tif nodeToRemove.prev is not None:\n\t\t\tnodeToRemove.prev.next = nodeToRemove.next\n\t\t# Check if we have a next node\n\t\t# If we just removed a tail, next will be None\n\t\tif nodeToRemove.next is not None:\n\t\t\tnodeToRemove.next.prev = nodeToRemove.prev\n\t\tnodeToRemove.next = None\n\t\tnodeToRemove.prev = None\n\n\tdef removeNodeWithValue(self, value):\n\t\t# Traverse from head\n\t\tnode = self.head\n\t\twhile node is not None:\n\t\t\t# For every node check if the value is same\n\t\t\t# as what we want to remove and if yes the remove\n\t\t\t# There might b multiple nodes we need to remove\n\t\t\t# Which is why we traverse the whole list -> O(n) Time\n\t\t\tnodeToRemove = node\n\t\t\tnode = node.next\n\t\t\tif nodeToRemove.value == value:\n\t\t\t\tself.removeNode(nodeToRemove)\n\n\tdef insertBefore(self, node, nodeToInsert):\n\t\t# If we have only head and tail, remove and add == do nothing\n\t\tif self.head is nodeToInsert and self.tail is nodeToInsert:\n\t\t\treturn\n\t\tself.removeNode(nodeToInsert)\n\t\t# At least two nodes\n\t\tnodeToInsert.prev = node.prev\n\t\tnodeToInsert.next = node\n\t\t# If the `node` is head\n\t\tif node.prev is None:\n\t\t\tself.head = nodeToInsert\n\t\telse:\n\t\t\tnode.prev.next = nodeToInsert\n\t\tnode.prev = nodeToInsert\n\n\tdef insertAfter(self, node, nodeToInsert):\n\t\t# If we have only head and tail, remove and add == do nothing\n\t\tif self.head is nodeToInsert and self.tail is nodeToInsert:\n\t\t\treturn\n\t\tself.removeNode(nodeToInsert)\n\t\t# At least two nodes\n\t\tnodeToInsert.prev = node\n\t\tnodeToInsert.next = node.next\n\t\t# If the `node` is tail\n\t\tif node.next is None:\n\t\t\tself.tail = nodeToInsert\n\t\telse:\n\t\t\tnode.next.prev = nodeToInsert\n\t\tnode.next = nodeToInsert\n\n\tdef insertAtPosition(self, position, nodeToInsert):\n\t\t# Remove the node if it is already there\n\t\tself.removeNode(nodeToInsert)\n\t\t# Position 1 means we need to set the head of the list\n\t\tif position == 1:\n\t\t\tself.setHead(nodeToInsert)\n\t\t\treturn\n\t\tnode = self.head\n\t\tcurrentPosition = 1\n\t\twhile node is not None and currentPosition != position:\n\t\t\tnode = node.next\n\t\t\tposition += 1\n\t\t# We exit the loop when either:\n\t\t# 1. Reached the end of list\n\t\t# 2. position is correct\n\t\tif node is not None:\n\t\t\tself.insertBefore(node, nodeToInsert)\n\t\telse:\n\t\t\tself.setTail(nodeToInsert)\n\n\tdef setHead(self, node):\n\t\tif self.head is None and self.tail is None:\n\t\t\tself.head = node\n\t\t\tself.tail = node\n\t\t\treturn\n\t\tself.insertBefore(self.head, node)\n\n\tdef setTail(self, node):\n\t\tif self.head is None and self.tail is None:\n\t\t\tself.setHead(node)\n\t\telse:\n\t\t\tself.insertAfter(self.tail, node)\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "threeSum", "data": "def threeSum(array, target):\n\tarray.sort()\n\tfor idx, num in enumerate(array):\n\t\trequired = target - num\n\t\tleftIdx = idx + 1\n\t\trightIdx = len(array) - 1\n\t\twhile leftIdx < rightIdx:\n\t\t\tif array[leftIdx] + array[rightIdx] == required:\n\t\t\t\treturn True\n\t\t\telif array[leftIdx] + array[rightIdx] > required:\n\t\t\t\trightIdx -= 1\n\t\t\telse:\n\t\t\t\tleftIdx += 1\n\treturn False\n\n", "description": null, "category": "remove", "imports": []}], [{"term": "def", "name": "process_track_path", "data": "def process_track_path(path, selected_tracks, collection_dir, remove_drums=False):\n\tfor checksum in os.listdir(path):\n\t\tload_dir = os.path.join(path, checksum)\n\t\tmultiroll = Multitrack(load_dir)\n\n\t\t# Remove all tracks but those in selected_tracks\n\t\tif \"all\" not in selected_tracks:\n\n\t\t\tto_remove = [idx for idx, track in enumerate(multiroll.tracks) \\\n\t\t\t\t\t\t\tif track.name not in selected_tracks]\n\t\t\tmultiroll.remove_tracks(to_remove)\n\n\t\t\t# Make sure our selected tracks persist\n\t\t\t# assert len(multiroll.tracks) == len(selected_tracks)\n\t\t\tif len(multiroll.tracks) != len(selected_tracks):\n\t\t\t\tcontinue\n\n\t\t# Remove tracks with `is_drum` set to True\n\t\tif remove_drums:\n\t\t\tto_remove = [idx for idx, track in enumerate(multiroll.tracks) \\\n\t\t\t\t\t\t\tif track.is_drum]\n\t\t\tmultiroll.remove_tracks(to_remove)\n\n\t\t# e.g. save_name = TR#########-bass-piano.mid\n\t\tname = os.path.basename(path)\n\n\t\tsave_name = '{}-{}'.format(name, \"-\".join(selected_tracks).lower())\n\t\tif remove_drums:\n\t\t\tsave_name += \"-no_drums\"\n\t\tsave_name += \".mid\"\n\n\t\tsave_path = os.path.join(collection_dir, save_name)\n\t\tmultiroll.write(save_path)\n", "description": null, "category": "remove", "imports": ["import os", "import argparse", "from pathlib import Path", "from tqdm import tqdm", "import pypianoroll", "from pypianoroll import Multitrack", "from tqdm import tqdm", "from tqdm.contrib.concurrent import process_map", "from multiprocessing import Pool", "import functools"]}, {"term": "def", "name": "collect_midis", "data": "def collect_midis(base_dir, collection_dir, selected_tracks=[\"all\"], n_tracks=\"all\", remove_drums=False):\n\t\"\"\"\n\tCollects .npz files from raw data into processed data folders as .mid\n\t- selected_track should be a list of track(s) \n\t\t> options: ['Drums', 'Piano', 'Guitar', 'Bass', 'Strings', 'all']\n\t\"\"\"\n\n\tif not os.path.exists(collection_dir):\n\t\tprint(\"Creating collection directory %s\" % collection_dir)\n\t\tos.mkdir(collection_dir)\n\n\tselected_tracks.sort()  # to keep consistency in filename later\n\n\t# Find all of the track name directories\n\ttrack_paths = list(Path(base_dir).rglob('TR*'))\n\n\tif n_tracks != \"all\":\n\t\ttrack_paths = track_paths[:int(n_tracks)]\n\n\t# create partial function with params pre-loaded so we can use the worker pool\n\t# note: this can't be a lambda function since those aren't pickle-able\n\t_foo = functools.partial(process_track_path, selected_tracks=selected_tracks, \n\t\t\t\t\t\t\t\t\t\t\t\t collection_dir=collection_dir,\n\t\t\t\t\t\t\t\t\t\t\t\t remove_drums=remove_drums)\n\twith Pool(6) as p:\n\t\t_ = list(tqdm(p.imap(_foo, track_paths), total=len(track_paths)))\n", "description": "\n\tCollects .npz files from raw data into processed data folders as .mid\n\t- selected_track should be a list of track(s) \n\t\t> options: ['Drums', 'Piano', 'Guitar', 'Bass', 'Strings', 'all']\n\t", "category": "remove", "imports": ["import os", "import argparse", "from pathlib import Path", "from tqdm import tqdm", "import pypianoroll", "from pypianoroll import Multitrack", "from tqdm import tqdm", "from tqdm.contrib.concurrent import process_map", "from multiprocessing import Pool", "import functools"]}], [{"term": "def", "name": "_only_one_selected", "data": "def _only_one_selected(*args):\n\t\"\"\"Test if only one item is True.\"\"\"\n\treturn sum(args) == 1\n\n", "description": "Test if only one item is True.", "category": "remove", "imports": ["import logging", "from pyinsteon import async_connect", "import voluptuous as vol", "from homeassistant import config_entries", "from homeassistant.const import (", "from homeassistant.core import callback", "from homeassistant.helpers.dispatcher import async_dispatcher_send", "from .const import (", "from .schemas import (", "\tasync def async_step_import(self, import_info):", "\t\tif not await _async_connect(**import_info):", "\t\treturn self.async_create_entry(title=\"\", data=import_info)"]}, {"term": "def", "name": "ncdef_async_connect", "data": "async def _async_connect(**kwargs):\n\t\"\"\"Connect to the Insteon modem.\"\"\"\n\ttry:\n\t\tawait async_connect(**kwargs)\n\t\t_LOGGER.info(\"Connected to Insteon modem\")\n\t\treturn True\n\texcept ConnectionError:\n\t\t_LOGGER.error(\"Could not connect to Insteon modem\")\n\t\treturn False\n\n", "description": "Connect to the Insteon modem.", "category": "remove", "imports": ["import logging", "from pyinsteon import async_connect", "import voluptuous as vol", "from homeassistant import config_entries", "from homeassistant.const import (", "from homeassistant.core import callback", "from homeassistant.helpers.dispatcher import async_dispatcher_send", "from .const import (", "from .schemas import (", "\tasync def async_step_import(self, import_info):", "\t\tif not await _async_connect(**import_info):", "\t\treturn self.async_create_entry(title=\"\", data=import_info)"]}, {"term": "def", "name": "_remove_override", "data": "def _remove_override(address, options):\n\t\"\"\"Remove a device override from config.\"\"\"\n\tnew_options = {}\n\tif options.get(CONF_X10):\n\t\tnew_options[CONF_X10] = options.get(CONF_X10)\n\tnew_overrides = []\n\tfor override in options[CONF_OVERRIDE]:\n\t\tif override[CONF_ADDRESS] != address:\n\t\t\tnew_overrides.append(override)\n\tif new_overrides:\n\t\tnew_options[CONF_OVERRIDE] = new_overrides\n\treturn new_options\n\n", "description": "Remove a device override from config.", "category": "remove", "imports": ["import logging", "from pyinsteon import async_connect", "import voluptuous as vol", "from homeassistant import config_entries", "from homeassistant.const import (", "from homeassistant.core import callback", "from homeassistant.helpers.dispatcher import async_dispatcher_send", "from .const import (", "from .schemas import (", "\tasync def async_step_import(self, import_info):", "\t\tif not await _async_connect(**import_info):", "\t\treturn self.async_create_entry(title=\"\", data=import_info)"]}, {"term": "def", "name": "_remove_x10", "data": "def _remove_x10(device, options):\n\t\"\"\"Remove an X10 device from the config.\"\"\"\n\thousecode = device[11].lower()\n\tunitcode = int(device[24:])\n\tnew_options = {}\n\tif options.get(CONF_OVERRIDE):\n\t\tnew_options[CONF_OVERRIDE] = options.get(CONF_OVERRIDE)\n\tnew_x10 = []\n\tfor existing_device in options[CONF_X10]:\n\t\tif (\n\t\t\texisting_device[CONF_HOUSECODE].lower() != housecode\n\t\t\tor existing_device[CONF_UNITCODE] != unitcode\n\t\t):\n\t\t\tnew_x10.append(existing_device)\n\tif new_x10:\n\t\tnew_options[CONF_X10] = new_x10\n\treturn new_options, housecode, unitcode\n\n", "description": "Remove an X10 device from the config.", "category": "remove", "imports": ["import logging", "from pyinsteon import async_connect", "import voluptuous as vol", "from homeassistant import config_entries", "from homeassistant.const import (", "from homeassistant.core import callback", "from homeassistant.helpers.dispatcher import async_dispatcher_send", "from .const import (", "from .schemas import (", "\tasync def async_step_import(self, import_info):", "\t\tif not await _async_connect(**import_info):", "\t\treturn self.async_create_entry(title=\"\", data=import_info)"]}, {"term": "class", "name": "InsteonFlowHandler", "data": "class InsteonFlowHandler(config_entries.ConfigFlow, domain=DOMAIN):\n\t\"\"\"Insteon config flow handler.\"\"\"\n\n\t@staticmethod\n\t@callback\n\tdef async_get_options_flow(config_entry):\n\t\t\"\"\"Define the config flow to handle options.\"\"\"\n\t\treturn InsteonOptionsFlowHandler(config_entry)\n\n\tasync def async_step_user(self, user_input=None):\n\t\t\"\"\"Init the config flow.\"\"\"\n\t\terrors = {}\n\t\tif self._async_current_entries():\n\t\t\treturn self.async_abort(reason=\"single_instance_allowed\")\n\t\tif user_input is not None:\n\t\t\tselection = user_input.get(MODEM_TYPE)\n\n\t\t\tif selection == PLM:\n\t\t\t\treturn await self.async_step_plm()\n\t\t\tif selection == HUB1:\n\t\t\t\treturn await self.async_step_hubv1()\n\t\t\treturn await self.async_step_hubv2()\n\t\tmodem_types = [PLM, HUB1, HUB2]\n\t\tdata_schema = vol.Schema({vol.Required(MODEM_TYPE): vol.In(modem_types)})\n\t\treturn self.async_show_form(\n\t\t\tstep_id=\"user\", data_schema=data_schema, errors=errors\n\t\t)\n\n\tasync def async_step_plm(self, user_input=None):\n\t\t\"\"\"Set up the PLM modem type.\"\"\"\n\t\terrors = {}\n\t\tif user_input is not None:\n\t\t\tif await _async_connect(**user_input):\n\t\t\t\treturn self.async_create_entry(title=\"\", data=user_input)\n\t\t\terrors[\"base\"] = \"cannot_connect\"\n\t\tschema_defaults = user_input if user_input is not None else {}\n\t\tdata_schema = build_plm_schema(**schema_defaults)\n\t\treturn self.async_show_form(\n\t\t\tstep_id=STEP_PLM, data_schema=data_schema, errors=errors\n\t\t)\n\n\tasync def async_step_hubv1(self, user_input=None):\n\t\t\"\"\"Set up the Hub v1 modem type.\"\"\"\n\t\treturn await self._async_setup_hub(hub_version=1, user_input=user_input)\n\n\tasync def async_step_hubv2(self, user_input=None):\n\t\t\"\"\"Set up the Hub v2 modem type.\"\"\"\n\t\treturn await self._async_setup_hub(hub_version=2, user_input=user_input)\n\n\tasync def _async_setup_hub(self, hub_version, user_input):\n\t\t\"\"\"Set up the Hub versions 1 and 2.\"\"\"\n\t\terrors = {}\n\t\tif user_input is not None:\n\t\t\tuser_input[CONF_HUB_VERSION] = hub_version\n\t\t\tif await _async_connect(**user_input):\n\t\t\t\treturn self.async_create_entry(title=\"\", data=user_input)\n\t\t\tuser_input.pop(CONF_HUB_VERSION)\n\t\t\terrors[\"base\"] = \"cannot_connect\"\n\t\tschema_defaults = user_input if user_input is not None else {}\n\t\tdata_schema = build_hub_schema(hub_version=hub_version, **schema_defaults)\n\t\tstep_id = STEP_HUB_V2 if hub_version == 2 else STEP_HUB_V1\n\t\treturn self.async_show_form(\n\t\t\tstep_id=step_id, data_schema=data_schema, errors=errors\n\t\t)\n\n\tasync def async_step_import(self, import_info):\n\t\t\"\"\"Import a yaml entry as a config entry.\"\"\"\n\t\tif self._async_current_entries():\n\t\t\treturn self.async_abort(reason=\"single_instance_allowed\")\n\t\tif not await _async_connect(**import_info):\n\t\t\treturn self.async_abort(reason=\"cannot_connect\")\n\t\treturn self.async_create_entry(title=\"\", data=import_info)\n\n", "description": "Insteon config flow handler.", "category": "remove", "imports": ["import logging", "from pyinsteon import async_connect", "import voluptuous as vol", "from homeassistant import config_entries", "from homeassistant.const import (", "from homeassistant.core import callback", "from homeassistant.helpers.dispatcher import async_dispatcher_send", "from .const import (", "from .schemas import (", "\tasync def async_step_import(self, import_info):", "\t\tif not await _async_connect(**import_info):", "\t\treturn self.async_create_entry(title=\"\", data=import_info)"]}, {"term": "class", "name": "InsteonOptionsFlowHandler", "data": "class InsteonOptionsFlowHandler(config_entries.OptionsFlow):\n\t\"\"\"Handle an Insteon options flow.\"\"\"\n\n\tdef __init__(self, config_entry):\n\t\t\"\"\"Init the InsteonOptionsFlowHandler class.\"\"\"\n\t\tself.config_entry = config_entry\n\n\tasync def async_step_init(self, user_input=None):\n\t\t\"\"\"Init the options config flow.\"\"\"\n\t\terrors = {}\n\t\tif user_input is not None:\n\t\t\tchange_hub_config = user_input.get(STEP_CHANGE_HUB_CONFIG, False)\n\t\t\tdevice_override = user_input.get(STEP_ADD_OVERRIDE, False)\n\t\t\tx10_device = user_input.get(STEP_ADD_X10, False)\n\t\t\tremove_override = user_input.get(STEP_REMOVE_OVERRIDE, False)\n\t\t\tremove_x10 = user_input.get(STEP_REMOVE_X10, False)\n\t\t\tif _only_one_selected(\n\t\t\t\tchange_hub_config,\n\t\t\t\tdevice_override,\n\t\t\t\tx10_device,\n\t\t\t\tremove_override,\n\t\t\t\tremove_x10,\n\t\t\t):\n\t\t\t\tif change_hub_config:\n\t\t\t\t\treturn await self.async_step_change_hub_config()\n\t\t\t\tif device_override:\n\t\t\t\t\treturn await self.async_step_add_override()\n\t\t\t\tif x10_device:\n\t\t\t\t\treturn await self.async_step_add_x10()\n\t\t\t\tif remove_override:\n\t\t\t\t\treturn await self.async_step_remove_override()\n\t\t\t\tif remove_x10:\n\t\t\t\t\treturn await self.async_step_remove_x10()\n\t\t\terrors[\"base\"] = \"select_single\"\n\n\t\tdata_schema = {\n\t\t\tvol.Optional(STEP_ADD_OVERRIDE): bool,\n\t\t\tvol.Optional(STEP_ADD_X10): bool,\n\t\t}\n\t\tif self.config_entry.data.get(CONF_HOST):\n\t\t\tdata_schema[vol.Optional(STEP_CHANGE_HUB_CONFIG)] = bool\n\n\t\toptions = {**self.config_entry.options}\n\t\tif options.get(CONF_OVERRIDE):\n\t\t\tdata_schema[vol.Optional(STEP_REMOVE_OVERRIDE)] = bool\n\t\tif options.get(CONF_X10):\n\t\t\tdata_schema[vol.Optional(STEP_REMOVE_X10)] = bool\n\n\t\treturn self.async_show_form(\n\t\t\tstep_id=\"init\", data_schema=vol.Schema(data_schema), errors=errors\n\t\t)\n\n\tasync def async_step_change_hub_config(self, user_input=None):\n\t\t\"\"\"Change the Hub configuration.\"\"\"\n\t\tif user_input is not None:\n\t\t\tdata = {\n\t\t\t\t**self.config_entry.data,\n\t\t\t\tCONF_HOST: user_input[CONF_HOST],\n\t\t\t\tCONF_PORT: user_input[CONF_PORT],\n\t\t\t}\n\t\t\tif self.config_entry.data[CONF_HUB_VERSION] == 2:\n\t\t\t\tdata[CONF_USERNAME] = user_input[CONF_USERNAME]\n\t\t\t\tdata[CONF_PASSWORD] = user_input[CONF_PASSWORD]\n\t\t\tself.hass.config_entries.async_update_entry(self.config_entry, data=data)\n\t\t\treturn self.async_create_entry(\n\t\t\t\ttitle=\"\",\n\t\t\t\tdata={**self.config_entry.options},\n\t\t\t)\n\t\tdata_schema = build_hub_schema(**self.config_entry.data)\n\t\treturn self.async_show_form(\n\t\t\tstep_id=STEP_CHANGE_HUB_CONFIG, data_schema=data_schema\n\t\t)\n\n\tasync def async_step_add_override(self, user_input=None):\n\t\t\"\"\"Add a device override.\"\"\"\n\t\terrors = {}\n\t\tif user_input is not None:\n\t\t\ttry:\n\t\t\t\tdata = add_device_override({**self.config_entry.options}, user_input)\n\t\t\t\tasync_dispatcher_send(self.hass, SIGNAL_ADD_DEVICE_OVERRIDE, user_input)\n\t\t\t\treturn self.async_create_entry(title=\"\", data=data)\n\t\t\texcept ValueError:\n\t\t\t\terrors[\"base\"] = \"input_error\"\n\t\tschema_defaults = user_input if user_input is not None else {}\n\t\tdata_schema = build_device_override_schema(**schema_defaults)\n\t\treturn self.async_show_form(\n\t\t\tstep_id=STEP_ADD_OVERRIDE, data_schema=data_schema, errors=errors\n\t\t)\n\n\tasync def async_step_add_x10(self, user_input=None):\n\t\t\"\"\"Add an X10 device.\"\"\"\n\t\terrors = {}\n\t\tif user_input is not None:\n\t\t\toptions = add_x10_device({**self.config_entry.options}, user_input)\n\t\t\tasync_dispatcher_send(self.hass, SIGNAL_ADD_X10_DEVICE, user_input)\n\t\t\treturn self.async_create_entry(title=\"\", data=options)\n\t\tschema_defaults = user_input if user_input is not None else {}\n\t\tdata_schema = build_x10_schema(**schema_defaults)\n\t\treturn self.async_show_form(\n\t\t\tstep_id=STEP_ADD_X10, data_schema=data_schema, errors=errors\n\t\t)\n\n\tasync def async_step_remove_override(self, user_input=None):\n\t\t\"\"\"Remove a device override.\"\"\"\n\t\terrors = {}\n\t\toptions = self.config_entry.options\n\t\tif user_input is not None:\n\t\t\toptions = _remove_override(user_input[CONF_ADDRESS], options)\n\t\t\tasync_dispatcher_send(\n\t\t\t\tself.hass,\n\t\t\t\tSIGNAL_REMOVE_DEVICE_OVERRIDE,\n\t\t\t\tuser_input[CONF_ADDRESS],\n\t\t\t)\n\t\t\treturn self.async_create_entry(title=\"\", data=options)\n\n\t\tdata_schema = build_remove_override_schema(options[CONF_OVERRIDE])\n\t\treturn self.async_show_form(\n\t\t\tstep_id=STEP_REMOVE_OVERRIDE, data_schema=data_schema, errors=errors\n\t\t)\n\n\tasync def async_step_remove_x10(self, user_input=None):\n\t\t\"\"\"Remove an X10 device.\"\"\"\n\t\terrors = {}\n\t\toptions = self.config_entry.options\n\t\tif user_input is not None:\n\t\t\toptions, housecode, unitcode = _remove_x10(user_input[CONF_DEVICE], options)\n\t\t\tasync_dispatcher_send(\n\t\t\t\tself.hass, SIGNAL_REMOVE_X10_DEVICE, housecode, unitcode\n\t\t\t)\n\t\t\treturn self.async_create_entry(title=\"\", data=options)\n\n\t\tdata_schema = build_remove_x10_schema(options[CONF_X10])\n\t\treturn self.async_show_form(\n\t\t\tstep_id=STEP_REMOVE_X10, data_schema=data_schema, errors=errors\n\t\t)\n", "description": "Handle an Insteon options flow.", "category": "remove", "imports": ["import logging", "from pyinsteon import async_connect", "import voluptuous as vol", "from homeassistant import config_entries", "from homeassistant.const import (", "from homeassistant.core import callback", "from homeassistant.helpers.dispatcher import async_dispatcher_send", "from .const import (", "from .schemas import (", "\tasync def async_step_import(self, import_info):", "\t\tif not await _async_connect(**import_info):", "\t\treturn self.async_create_entry(title=\"\", data=import_info)"]}], [{"term": "def", "name": "prefix_lists_and_prefix_list_id", "data": "def prefix_lists_and_prefix_list_id(s_session, d_conf):\n\tos.system(\"clear\")\n\tprint(2 * \"\\n\")\n\turl = 'https://' + d_conf['vmanage_ip'] + '/dataservice/template/policy/list/dataprefix/'\n\tr = s_session.get(url=url, verify=False)\n\tr.status_code = str(r.status_code)\n\tr.status_code = r.status_code.strip()\n\tif '200' not in r.status_code:\n\t\tprint(\"\\n\")\n\t\tprint(\"  HTTP Status Code - \" + r.status_code)\n\t\td_prefix_lists_error_message = json.loads(r.content.decode('utf8'))\n\t\tprint(json.dumps(d_prefix_lists_error_message, indent=2, sort_keys=True))\n\t\texit()\n\td_prefix_lists = json.loads(r.content.decode('utf8'))\n\tl_prefix_lists = []\n\td_prefix_list_id = {}\n\tfor s_pl in d_prefix_lists['data']:\n\t\tl_prefix_lists.append(s_pl['name'])\n\t\td_prefix_list_id[s_pl['name']] = s_pl['listId']\n\n\tl_prefix_lists.sort()\n\tprint(\"  Data prefix list names:\\n\")\n\n\tc_prefix_lists = (len(l_prefix_lists))\n\tc1 = int(c_prefix_lists / 3)\n\tc2 = 3 * int(c1)\n\tif c2 < c_prefix_lists:\n\t\tc1 = int(c1 + 1)\n\n\ta = (l_prefix_lists[:c1])\n\tb = (l_prefix_lists[c1:c1 + c1])\n\tc = (l_prefix_lists[c1 + c1:])\n\n\tfor x, y, z in itertools.zip_longest(a, b, c):\n\t\tif x and y and z:\n\t\t\ts_prefix_lists = '  {:<42}{:<42}{:<42}'.format(x, y, z)\n\t\t\tprint(s_prefix_lists)\n\t\telif x and y:\n\t\t\ts_prefix_lists = '  {:<42}{:<42}'.format(x, y)\n\t\t\tprint(s_prefix_lists)\n\t\telse:\n\t\t\ts_prefix_lists = '  {:<42}'.format(x)\n\t\t\tprint(s_prefix_lists)\n\n\tprint(\"\\n\")\n\tprint(\"  >> Type a data prefix list name to see list of prefixes: <<\")\n\tprint(\"  => \", end=\"\")\n\ts_prefix_list_name_i = input()\n\ts_prefix_list_name_i = s_prefix_list_name_i.strip()\n\tn = 0\n\tfor s_pl in l_prefix_lists:\n\t\ts_pl = s_pl.strip()\n\t\ts_pl_re = re.search('^' + re.escape(s_prefix_list_name_i) + '$', s_pl)\n\t\tif s_pl_re is not None:\n\t\t\tn = n + 1\n\t\t\t# print(s_prefix_list_name_i)\n\t\t\t# print(d_prefix_list_id.get('%s'%s_prefix_list_name_i))\n\t#\n\twhile n is 0:\n\t\tprint(\"  !! Data prefix list name doesn't match. Type again: !!\")\n\t\tprint(\"  => \", end=\"\")\n\t\ts_prefix_list_name_i = input()\n\t\ts_prefix_list_name_i = s_prefix_list_name_i.strip()\n\t\tfor s_pl in l_prefix_lists:\n\t\t\ts_pl = s_pl.strip()\n\t\t\ts_pl_re = re.search('^' + re.escape(s_prefix_list_name_i) + '$', s_pl)\n\t\t\tif s_pl_re is not None:\n\t\t\t\tn = n + 1\n\t\t\t\t# print(s_prefix_list_name_i)\n\t\t\t\t# print(d_prefix_list_id.get('%s' % s_prefix_list_name_i))\n\treturn (d_prefix_list_id.get('%s' % s_prefix_list_name_i),s_prefix_list_name_i)\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "prefixes_in_list", "data": "def prefixes_in_list(s_session, d_conf):\n\tos.system(\"clear\")\n\tprint(\"\\n\")\n\turl = 'https://' + d_conf['vmanage_ip'] + '/dataservice/template/policy/list/dataprefix/' + gl_prefix_list_id\n\tr = s_session.get(url=url, verify=False)\n\tr.status_code = str(r.status_code)\n\tr.status_code = r.status_code.strip()\n\tprint(\"\\n\")\n\tif '200' not in r.status_code:\n\t\tprint(\"  HTTP Status Code - \" + r.status_code)\n\t\td_prefixes_in_list_error_message = json.loads(r.content.decode('utf8'))\n\t\tprint(json.dumps(d_prefixes_in_list_error_message, indent=2, sort_keys=True))\n\t\texit()\n\td_prefixes_in_list_present = json.loads(r.content.decode('utf8'))\n\td_prefixes_in_list_toupdate = json.loads(r.content.decode('utf8'))\n\tl_prefixes_in_list = []\n\t# l_prefixes_in_list.append('\\n')\n\tl_prefixes_in_list.append('  Prefixes from: ' + gl_prefix_list_name)\n\tl_prefixes_in_list.append('\\n')\n\tfor s_pilp in d_prefixes_in_list_present['entries']:\n\t\tl_prefixes_in_list.append(s_pilp['ipPrefix'])\n\tos.system(\"clear\")\n\tprint(\"\\n\")\n\tfor s_pil in l_prefixes_in_list:\n\t\ts_pil = s_pil.strip()\n\t\tprint('  ' + s_pil)\n\td_prefixes_in_list_toupdate.pop('lastUpdated', None)\n\td_prefixes_in_list_toupdate.pop('owner', None)\n\td_prefixes_in_list_toupdate.pop('readOnly', None)\n\td_prefixes_in_list_toupdate.pop('version', None)\n\td_prefixes_in_list_toupdate.pop('referenceCount', None)\n\td_prefixes_in_list_toupdate.pop('isActivatedByVsmart', None)\n\treturn (d_prefixes_in_list_present,d_prefixes_in_list_toupdate)\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "prefixes_add_remove_yn", "data": "def prefixes_add_remove_yn(prefix_add_remove,prefix_from_to):\n\tprint(\"\\n\")\n\tprint(\"  >> %s prefix? (y/n): <<\" % prefix_add_remove)\n\tprint(\"  => \", end=\"\")\n\ts_prefixes_add_remove_yn_i = input()\n\ts_prefixes_add_remove_yn_re = re.search('[y|n]', s_prefixes_add_remove_yn_i)\n\twhile s_prefixes_add_remove_yn_re is None:\n\t\tprint(\"  !! Select from above options. Please try again: !!\")\n\t\tprint(\"  => \", end=\"\")\n\t\ts_prefixes_add_remove_yn_i = input()\n\t\tif s_prefixes_add_remove_yn_i is \"y\":\n\t\t\treturn\n\t\tif s_prefixes_add_remove_yn_i is \"n\":\n\t\t\texit()\n\tif s_prefixes_add_remove_yn_i is \"y\":\n\t\treturn\n\tif s_prefixes_add_remove_yn_i is \"n\":\n\t\texit()\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "prefixes_add_remove", "data": "def prefixes_add_remove(prefix_add_remove,prefix_from_to,append_remove):\n\tprint(\" \")\n\tprint(\"  >> Prefix to be %s: <<\" % prefix_add_remove)\n\tprint(\"  => \", end=\"\")\n\ts_prefixes_add_remove_i = input()\n\ts_prefixes_add_remove_i = s_prefixes_add_remove_i.strip()\n\ts_prefixes_add_remove_re = re.search('^(\\d{1,3}).(\\d{1,3}).(\\d{1,3}).(\\d{1,3})/\\d{2}$', s_prefixes_add_remove_i)\n\twhile s_prefixes_add_remove_re is None:\n\t\tprint(\"  !! IP address has incorrect format. Type again. !!\\n\")\n\t\tprint(\"  >> Prefix to be %s: <<\" % prefix_add_remove)\n\t\tprint(\"  => \", end=\"\")\n\t\ts_prefixes_add_remove_i = input()\n\t\ts_prefixes_add_remove_i = s_prefixes_add_remove_i.strip()\n\t\ts_prefixes_add_remove_re = re.search('^(\\d{1,3}).(\\d{1,3}).(\\d{1,3}).(\\d{1,3})/\\d{2}$', s_prefixes_add_remove_i)\n\t\tif s_prefixes_add_remove_re is not None:\n\t\t\tif append_remove is 'append':\n\t\t\t\tgd_prefixes_in_list_toupdate['entries'].append({\n\t\t\t\t\t'ipPrefix': s_prefixes_add_remove_i\n\t\t\t\t})\n\t\t\t\treturn gd_prefixes_in_list_toupdate\n\t\t\tif append_remove is 'remove':\n\t\t\t\tgd_prefixes_in_list_toupdate['entries'].remove({\n\t\t\t\t\t'ipPrefix': s_prefixes_add_remove_i\n\t\t\t\t})\n\t\t\t\treturn gd_prefixes_in_list_toupdate\n\n\tif append_remove is 'append':\n\t\tgd_prefixes_in_list_toupdate['entries'].append({\n\t\t\t'ipPrefix': s_prefixes_add_remove_i\n\t\t})\n\t\treturn gd_prefixes_in_list_toupdate\n\n\tif append_remove is 'remove':\n\t\tgd_prefixes_in_list_toupdate['entries'].remove({\n\t\t\t'ipPrefix': s_prefixes_add_remove_i\n\t\t})\n\t\treturn gd_prefixes_in_list_toupdate\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "prefixes_in_list_compare_present_updated", "data": "def prefixes_in_list_compare_present_updated():\n\tl_prefixes_present = []\n\tl_prefixes_updated = []\n\tfor s_pilp in gd_prefixes_in_list_present['entries']:\n\t\tl_prefixes_present.append(s_pilp['ipPrefix'].strip())\n\n\tfor s_pilu in gd_prefixes_in_list_updated['entries']:\n\t\tl_prefixes_updated.append(s_pilu['ipPrefix'].strip())\n\n\tl_prefixes_present.insert(0, \"before:\")\n\tl_prefixes_updated.insert(0, \"after:\")\n\tos.system(\"clear\")\n\tprint(\"\\n\")\n\tfor s_pilp_pilu in list(itertools.zip_longest(l_prefixes_present, l_prefixes_updated, fillvalue=' ')):\n\t\ts_pilp_pilu_fmt = '  {:<31}{:<31}'.format(s_pilp_pilu[0], s_pilp_pilu[1])\n\t\tprint(s_pilp_pilu_fmt)\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "prefix_add_remove_more_records", "data": "def prefix_add_remove_more_records(prefix_add_remove,prefix_from_to,append_remove):\n\tprint(\"\\n\")\n\tprint(\"  >> %s another prefix %s list? (y/n): <<\" % (prefix_add_remove,prefix_from_to))\n\tprint(\"  => \", end=\"\")\n\n\ts_add_remove_prefix_to_list_i = input()\n\ts_add_remove_prefix_to_list_re = re.search('[y|n]', s_add_remove_prefix_to_list_i)\n\twhile s_add_remove_prefix_to_list_re is None:\n\t\tprint(\"  !! Select from above options. Please try again: !!\")\n\t\tprint(\"  => \", end=\"\")\n\t\ts_add_remove_prefix_to_list_i = input()\n\t\ts_add_remove_prefix_to_list_re = re.search('[y|n]', s_add_remove_prefix_to_list_i)\n\t\tif s_add_remove_prefix_to_list_i is \"y\":\n\t\t\tif append_remove is 'append':\n\t\t\t\tprefixes_add_remove('added', 'to', 'append')\n\t\t\t\tprefixes_in_list_compare_present_updated()\n\t\t\t\tprefix_add_remove_more_records('Add', 'to', 'append')\n\t\t\t\treturn\n\t\t\tif append_remove is 'remove':\n\t\t\t\tprefixes_add_remove('removed','from','remove')\n\t\t\t\tprefixes_in_list_compare_present_updated()\n\t\t\t\tprefix_add_remove_more_records('Remove','from','remove')\n\t\t\t\treturn\n\t\tif s_add_remove_prefix_to_list_i is \"n\":\n\t\t\treturn\n\tif s_add_remove_prefix_to_list_i is \"y\":\n\t\tif append_remove is 'append':\n\t\t\tprefixes_add_remove('added','to','append')\n\t\t\tprefixes_in_list_compare_present_updated()\n\t\t\tprefix_add_remove_more_records('Add','to','append')\n\t\t\treturn\n\t\tif append_remove is 'remove':\n\t\t\tprefixes_add_remove('removed','from','remove')\n\t\t\tprefixes_in_list_compare_present_updated()\n\t\t\tprefix_add_remove_more_records('Remove','from','remove')\n\t\t\treturn\n\tif s_add_remove_prefix_to_list_i is \"n\":\n\t\treturn\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "commit_changes_yn", "data": "def commit_changes_yn():\n\tprint(\"\\n\")\n\tprint(\"  >> Write data to vManage? <<\")\n\tprint(\"\t 'commit'  | commit changes\")\n\tprint(\"\t 'n'\t   | exit\")\n\tprint(\"  => \", end=\"\")\n\ts_commit_changes_yn_i = input()\n\ts_commit_changes_yn_re = re.search('commit|n', s_commit_changes_yn_i)\n\twhile s_commit_changes_yn_re is None:\n\t\tprint(\"  !! Select from above options. Please try again: !!\")\n\t\tprint(\"  => \", end=\"\")\n\t\ts_commit_changes_yn_i = input()\n\t\tif s_commit_changes_yn_i == 'commit':\n\t\t\treturn('commit')\n\t\tif s_commit_changes_yn_i == \"n\":\n\t\t\texit()\n\tif s_commit_changes_yn_i == 'commit':\n\t\treturn ('commit')\n\tif s_commit_changes_yn_i == \"n\":\n\t\texit()\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "commit_changes_add_remove", "data": "def commit_changes_add_remove(s_session, d_conf):\n\tos.system(\"clear\")\n\tprint(\"\\n\")\n\turl = 'https://' + d_conf['vmanage_ip'] + '/dataservice/template/policy/list/dataprefix/' + gl_prefix_list_id\n\theaders={'Content-Type': 'application/json'}\n\tpayload = json.dumps(gd_prefixes_in_list_updated)\n\tr = gs_session.put(url=url, data=payload, headers=headers, verify=False)\n\tr.status_code = str(r.status_code)\n\tr.status_code = r.status_code.strip()\n\tif '200' not in r.status_code:\n\t\tprint(\"  HTTP Status Code - \" + r.status_code)\n\t\td_commit_changes_add_remove_error_message = json.loads(r.content.decode('utf8'))\n\t\tprint(json.dumps(d_commit_changes_add_remove_error_message, indent=2, sort_keys=True))\n\t\texit()\n\td_commit_changes_add_remove_message = json.loads(r.content.decode('utf8'))\n\t# print(json.dumps(d_commit_changes_add_remove_message, indent=2, sort_keys=True))\n\n\tfor k, v in d_commit_changes_add_remove_message.items():\n\t\tif 'masterTemplatesAffected' in k:\n\t\t\tprint('  masterTemplatesAffected:')\n\t\t\tif v:\n\t\t\t\tfor mt in v:\n\t\t\t\t\tprint(' ', mt)\n\t\t\telse:\n\t\t\t\tprint('  - none')\n\t\t\t\tprint('  - change completed')\n\t\t\t\tprint('\\n')\n\t\tif 'processId' in k:\n\t\t\tprint('  processId:')\n\t\t\tprint(' ', v)\n\n\treturn(d_commit_changes_add_remove_message)\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "activate_changes", "data": "def activate_changes(s_session, d_conf):\n\tl_vmanage_master_templates_affected = []\n\t# print(gd_commit_changes_add_remove_message['processId'])\n\tfor s_ccadm in gd_commit_changes_add_remove_message['masterTemplatesAffected']:\n\t\t# print(d_vmanage_master_templates_affected['processId'])\n\t\tl_vmanage_master_templates_affected.append(s_ccadm)\n\n\tl_devicetemplatelist = []\n\td_devicetemplatelist = {}\n\tl_vmanage_template_config_input = []\n\tfor s_vmta in l_vmanage_master_templates_affected:\n\t\turl = 'https://' + d_conf['vmanage_ip'] + '/dataservice/template/device/config/attached/' + s_vmta\n\t\tr = s_session.get(url=url, verify=False)\n\t\tr.status_code = str(r.status_code)\n\t\tr.status_code = r.status_code.strip()\n\t\tif '200' not in r.status_code:\n\t\t\tprint(\"\\n\")\n\t\t\tprint(\"  HTTP Status Code - \" + r.status_code)\n\t\t\td_vmanage_config_attached_error_message = json.loads(r.content.decode('utf8'))\n\t\t\tprint(json.dumps(d_vmanage_config_attached_error_message, indent=2, sort_keys=True))\n\t\t\texit()\n\t\td_vmanage_config_attached = json.loads(r.content.decode('utf8'))\n\t\t# print(json.dumps(d_vmanage_attached_template, indent=2, sort_keys=True))\n\t\tfor s_vca in d_vmanage_config_attached['data']:\n\t\t\t# print(s_vca['host-name'])\n\t\t\t# print(s_vca['deviceIP'])\n\t\t\t# print(s_vca['site-id'])\n\t\t\t# print(s_vca['uuid'])\n\t\t\td_vmanage_template_config_input = {\"isEdited\": True, \"templateId\": \"none\",\n\t\t\t \"deviceIds\": [\"none\"], \"isMasterEdited\": False}\n\t\t\td_vmanage_template_config_input.update({'templateId':s_vmta})\n\t\t\tl_vmanage_template_config_input.append(s_vca['uuid'])\n\t\t\td_vmanage_template_config_input.update({'deviceIds':l_vmanage_template_config_input})\n\t\t\t# print (d_vmanage_template_config_input)\n\n\t\t\turl = 'https://' + d_conf['vmanage_ip'] + '/dataservice/template/device/config/input/'\n\t\t\theaders = {'Content-Type': 'application/json'}\n\t\t\tpayload = json.dumps(d_vmanage_template_config_input)\n\t\t\tr = s_session.post(url=url, data=payload, headers=headers, verify=False)\n\t\t\tr.status_code = str(r.status_code)\n\t\t\tr.status_code = r.status_code.strip()\n\t\t\tif '200' not in r.status_code:\n\t\t\t\tprint(\"\\n\")\n\t\t\t\tprint(\"  HTTP Status Code - \" + r.status_code)\n\t\t\t\td_vmanage_config_input_error_message = json.loads(r.content.decode('utf8'))\n\t\t\t\tprint(json.dumps(d_vmanage_config_input_error_message, indent=2, sort_keys=True))\n\t\t\t\texit()\n\t\t\td_vmanage_config_input = json.loads(r.content.decode('utf8'))\n\t\t\t# print(json.dumps(d_vmanage_config_input, indent=2, sort_keys=True))\n\n\t\t\tfor s_vci in d_vmanage_config_input['data']:\n\t\t\t\ts_vci.update({'csv-templateId':s_vmta})\n\t\t\t# print(d_vmanage_config_input['data'])\n\t\t\td_templatelist = {'templateId':s_vmta}\n\t\t\td_templatelist.update({'device': d_vmanage_config_input['data']})\n\t\t\td_templatelist.update({'isEdited':True})\n\t\t\ts_vmanage_templatelist = d_templatelist\n\t\t\tl_devicetemplatelist.append(s_vmanage_templatelist)\n\n\t\tdel l_vmanage_template_config_input[:]\n\td_devicetemplatelist.update({'deviceTemplateList':l_devicetemplatelist})\n\t# print(d_devicetemplatelist)\n\n\turl = 'https://' + d_conf['vmanage_ip'] + '/dataservice/template/device/config/attachcli'\n\theaders = {'Content-Type': 'application/json'}\n\tpayload = json.dumps(d_devicetemplatelist)\n\tr = s_session.post(url=url, data=payload, headers=headers, verify=False)\n\tr.status_code = str(r.status_code)\n\tr.status_code = r.status_code.strip()\n\tif '200' not in r.status_code:\n\t\tprint(\"\\n\")\n\t\tprint(\"  HTTP Status Code - \" + r.status_code)\n\t\td_vmanage_template_list_error_message = json.loads(r.content.decode('utf8'))\n\t\tprint(json.dumps(d_vmanage_template_list_error_message, indent=2, sort_keys=True))\n\t\texit()\n\td_vmanage_template_list = json.loads(r.content.decode('utf8'))\n\t# print(json.dumps(d_vmanage_template_list, indent=2, sort_keys=True))\n\tpush_config_id = d_vmanage_template_list['id']\n\tprint(\"\\n\")\n\tprint(' ',push_config_id,'\\n')\n\n\tc_tasks_status = 0\n\twhile c_tasks_status < 2:\n\t\tc_tasks_status = 0\n\t\ttime.sleep(20)\n\t\turl = 'https://' + d_conf['vmanage_ip'] + '/dataservice/device/action/status/' + push_config_id\n\t\tr = s_session.get(url=url, verify=False)\n\t\tr.status_code = str(r.status_code)\n\t\tr.status_code = r.status_code.strip()\n\t\tif '200' not in r.status_code:\n\t\t\tprint(\"\\n\")\n\t\t\tprint(\"  HTTP Status Code - \" + r.status_code)\n\t\t\td_vmanage_action_status_error_message = json.loads(r.content.decode('utf8'))\n\t\t\tprint(json.dumps(d_vmanage_action_status_error_message, indent=2, sort_keys=True))\n\t\t\texit()\n\t\td_vmanage_action_status = json.loads(r.content.decode('utf8'))\n\n\t\tos.system(\"clear\")\n\t\tl_status_char_len = []\n\t\tl_status_change = []\n\t\tfor s_rts in d_vmanage_action_status['data']:\n\t\t\tif 'Success' in s_rts['status'] or 'Fail' in s_rts['status']:\n\t\t\t\tc_tasks_status += 1\n\t\t\tl_status_char_len.append(len(s_rts['system-ip']))\n\t\t\tl_status_char_len.append(len(s_rts['host-name']))\n\t\t\tl_status_char_len.append(len(s_rts['currentActivity']))\n\t\t\tl_status_char_len.append(len(s_rts['status']))\n\t\t\t# l_status_change.append('  vBond IP:\t' + str(s_rts['system-ip']))\n\t\t\tl_status_change.append('  vBond:\t' + str(s_rts['host-name']))\n\t\t\tl_status_change.append('  activity: ' + str(s_rts['currentActivity']))\n\t\t\tl_status_change.append('  status:   ' + str(s_rts['status']))\n\t\t\tl_status_change.append('\\n')\n\n\t\tl_status_char_len = sorted(l_status_char_len, reverse=True)\n\t\tdel l_status_change[-1:]\n\t\tl_status_change.insert(0,'  ========= ' + l_status_char_len[0] * '=')\n\t\tl_status_change.insert(0,' ')\n\t\tl_status_change.insert(0,'\\n')\n\t\tl_status_change.append('  ========= ' + l_status_char_len[0] * '=')\n\n\t\tfor s_sc in l_status_change:\n\t\t\tprint(s_sc)\n\t\tdel l_status_char_len[:]\n\t\tdel l_status_change[:]\n\tprint(\"\\n\")\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "create_prefix_list_add_new_prefix", "data": "def create_prefix_list_add_new_prefix():\n\tos.system(\"clear\")\n\tprint(\"\\n\")\n\tprint(\"  >> Add new data prefix list: <<\\n\")\n\ts_create_prefix_list_i = input('  - Data prefix list name: ')\n\ts_create_prefix_list_re = re.search('\\w',s_create_prefix_list_i)\n\twhile s_create_prefix_list_re is None:\n\t\tprint(\"  !! Prefix list has incorrect name. Type again. !!\\n\")\n\t\ts_create_prefix_list_i = input('  - Data prefix list name: ')\n\t\ts_create_prefix_list_re = re.search('\\w',s_create_prefix_list_i)\n\n\ts_add_new_prefix_i = input('  - Prefix to be added: ')\n\ts_add_new_prefix_re = re.search('^(\\d{1,3}).(\\d{1,3}).(\\d{1,3}).(\\d{1,3})/\\d{2}$', s_add_new_prefix_i)\n\n\twhile s_add_new_prefix_re is None:\n\t\tprint(\"  !! IP address has incorrect format. Type again. !!\\n\")\n\t\ts_add_new_prefix_i = input('  - Prefix to be added: ')\n\t\ts_add_new_prefix_re = re.search('^(\\d{1,3}).(\\d{1,3}).(\\d{1,3}).(\\d{1,3})/\\d{2}$',s_add_new_prefix_i)\n\t\tif s_add_new_prefix_re is not None:\n\t\t\td_create_prefix_list_form = b'{\"listId\":\"\",\"name\":\"\",\"type\":\"dataPrefix\",\"description\":\"Desc Not Required\",\"entries\":[],\"lastUpdated\":\"\",\"owner\":\"\",\"readOnly\":false,\"version\":\"0\",\"referenceCount\":0,\"isActivatedByVsmart\":true}'\n\t\t\tgd_prefixes_in_list_toupdate = json.loads(d_create_prefix_list_form.decode('utf8'))\n\t\t\tgd_prefixes_in_list_toupdate['name'] = s_create_prefix_list_i\n\t\t\tgd_prefixes_in_list_toupdate['entries'].append({\n\t\t\t\t'ipPrefix': s_add_new_prefix_i\n\t\t\t\t})\n\t\t\treturn gd_prefixes_in_list_toupdate\n\n\tif s_add_new_prefix_re is not None:\n\t\td_create_prefix_list_form = b'{\"listId\":\"\",\"name\":\"\",\"type\":\"dataPrefix\",\"description\":\"Desc Not Required\",\"entries\":[],\"lastUpdated\":\"\",\"owner\":\"\",\"readOnly\":false,\"version\":\"0\",\"referenceCount\":0,\"isActivatedByVsmart\":true}'\n\t\tgd_prefixes_in_list_toupdate = json.loads(d_create_prefix_list_form.decode('utf8'))\n\t\tgd_prefixes_in_list_toupdate['name'] = s_create_prefix_list_i\n\t\tgd_prefixes_in_list_toupdate['entries'].append({\n\t\t'ipPrefix': s_add_new_prefix_i\n\t\t})\n\t\treturn gd_prefixes_in_list_toupdate\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "create_prefix_list_more_records", "data": "def create_prefix_list_more_records():\n\tprint(' ')\n\tprint(\"  >> Add another prefix? (y/n): <<\")\n\tprint(\"  => \", end=\"\")\n\ts_add_prefix_to_list_i = input()\n\ts_add_prefix_to_list_re = re.search('[y|n]', s_add_prefix_to_list_i)\n\twhile s_add_prefix_to_list_re is None:\n\t\tprint(\"  !! Select from above options. Please try again: !!\")\n\t\tprint(\"  => \", end=\"\")\n\t\ts_add_prefix_to_list_i = input()\n\t\ts_add_prefix_to_list_re = re.search('[y|n]', s_add_prefix_to_list_i)\n\t\tif s_add_prefix_to_list_i is \"y\":\n\t\t\tcreate_prefix_list_prefixes_add()\n\t\t\tcreate_prefix_print_list()\n\t\t\tcreate_prefix_list_more_records()\n\t\t\treturn\n\t\tif s_add_prefix_to_list_i is \"n\":\n\t\t\treturn\n\tif s_add_prefix_to_list_i is \"y\":\n\t\tcreate_prefix_list_prefixes_add()\n\t\tcreate_prefix_print_list()\n\t\tcreate_prefix_list_more_records()\n\t\treturn\n\tif s_add_prefix_to_list_i is \"n\":\n\t\treturn\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "create_prefix_list_prefixes_add", "data": "def create_prefix_list_prefixes_add():\n\tprint(' ')\n\tprint(\"  >> Provide prefix to be added: <<\")\n\tprint(\"  => \", end=\"\")\n\ts_prefixes_add_i = input()\n\ts_prefixes_add_i = s_prefixes_add_i.strip()\n\ts_prefixes_add_re = re.search('^(\\d{1,3}).(\\d{1,3}).(\\d{1,3}).(\\d{1,3})/\\d{2}$', s_prefixes_add_i)\n\twhile s_prefixes_add_re is None:\n\t\tprint(\" \")\n\t\tprint(\"  !! IP address has incorrect format. Type again. !!\")\n\t\tprint(\"  => \", end=\"\")\n\t\ts_prefixes_add_i = input()\n\t\ts_prefixes_add_i = s_prefixes_add_i.strip()\n\t\ts_prefixes_add_re = re.search('^(\\d{1,3}).(\\d{1,3}).(\\d{1,3}).(\\d{1,3})/\\d{2}$', s_prefixes_add_i)\n\t\tif s_prefixes_add_re is not None:\n\t\t\tgd_prefixes_in_list_toupdate['entries'].append({\n\t\t\t\t'ipPrefix': s_prefixes_add_i\n\t\t\t})\n\t\t\treturn gd_prefixes_in_list_toupdate\n\n\tgd_prefixes_in_list_toupdate['entries'].append({\n\t\t'ipPrefix': s_prefixes_add_i\n\t})\n\treturn gd_prefixes_in_list_toupdate\n\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "create_prefix_print_list", "data": "def create_prefix_print_list():\n\tl_prefixes_updated = []\n\tfor s_pilp in gd_prefixes_in_list_toupdate['entries']:\n\t\tl_prefixes_updated.append(s_pilp['ipPrefix'].strip())\n\tos.system(\"clear\")\n\tprint(\"\\n\")\n\tfor s_pu in l_prefixes_updated:\n\t\tprint('  ',s_pu)\n\tprint(\" \")\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}, {"term": "def", "name": "commit_changes_create_prefix_list", "data": "def commit_changes_create_prefix_list(s_session, d_conf):\n\tos.system(\"clear\")\n\turl = 'https://' + d_conf['vmanage_ip'] + '/dataservice/template/policy/list/dataprefix/'\n\theaders={'Content-Type': 'application/json'}\n\tpayload = json.dumps(gd_prefixes_in_list_toupdate)\n\tr = s_session.post(url=url, data=payload, headers=headers, verify=False)\n\tr.status_code = str(r.status_code)\n\tr.status_code = r.status_code.strip()\n\tif '200' not in r.status_code:\n\t\tprint(\"  HTTP Status Code - \" + r.status_code)\n\t\td_commit_changes_create_list_error_message = json.loads(r.content.decode('utf8'))\n\t\tprint(json.dumps(d_commit_changes_create_list_error_message, indent=2, sort_keys=True))\n\t\texit()\n\td_commit_changes_create_list_message = json.loads(r.content.decode('utf8'))\n\t# print(json.dumps(d_commit_changes_create_list_message, indent=2, sort_keys=True))\n\ts_push_create_list_id = d_commit_changes_create_list_message['listId']\n\tprint(2* \"\\n\")\n\tprint('  ',s_push_create_list_id)\n\tprint('   completed ...','\\n')\n", "description": null, "category": "remove", "imports": ["import authentication", "import os", "import re", "import json", "import time", "import urllib3", "import itertools"]}], [{"term": "class", "name": "NetAppCDOTLicense", "data": "class NetAppCDOTLicense(object):\n\n\tdef __init__(self):\n\t\tself.argument_spec = netapp_utils.ontap_sf_host_argument_spec()\n\t\tself.argument_spec.update(dict(\n\t\t\tserial_number=dict(required=False, type='str', default=None),\n\t\t\tremove_unused=dict(default=False, type='bool'),\n\t\t\tremove_expired=dict(default=False, type='bool'),\n\t\t\tlicenses=dict(default=False, type='dict'),\n\t\t))\n\n\t\tself.module = AnsibleModule(\n\t\t\targument_spec=self.argument_spec,\n\t\t\tsupports_check_mode=False\n\t\t)\n\n\t\tp = self.module.params\n\n\t\t# set up state variables\n\t\tself.serial_number = p['serial_number']\n\t\tself.remove_unused = p['remove_unused']\n\t\tself.remove_expired = p['remove_expired']\n\t\tself.licenses = p['licenses']\n\n\t\tif HAS_NETAPP_LIB is False:\n\t\t\tself.module.fail_json(msg=\"the python NetApp-Lib module is required\")\n\t\telse:\n\t\t\tself.server = netapp_utils.setup_ontap_zapi(module=self.module)\n\n\tdef get_licensing_status(self):\n\t\t\"\"\"\n\t\t\tCheck licensing status\n\n\t\t\t:return: package (key) and licensing status (value)\n\t\t\t:rtype: dict\n\t\t\"\"\"\n\t\tlicense_status = netapp_utils.zapi.NaElement('license-v2-status-list-info')\n\t\tresult = None\n\t\ttry:\n\t\t\tresult = self.server.invoke_successfully(license_status,\n\t\t\t\t\t\t\t\t\t\t\t\t\t enable_tunneling=False)\n\t\texcept netapp_utils.zapi.NaApiError:\n\t\t\terr = get_exception()\n\t\t\tself.module.fail_json(msg=\"Error checking license status\",\n\t\t\t\t\t\t\t\t  exception=str(err))\n\n\t\treturn_dictionary = {}\n\t\tlicense_v2_status = result.get_child_by_name('license-v2-status')\n\t\tif license_v2_status:\n\t\t\tfor license_v2_status_info in license_v2_status.get_children():\n\t\t\t\tpackage = license_v2_status_info.get_child_content('package')\n\t\t\t\tstatus = license_v2_status_info.get_child_content('method')\n\t\t\t\treturn_dictionary[package] = status\n\n\t\treturn return_dictionary\n\n\tdef remove_licenses(self, remove_list):\n\t\t\"\"\"\n\t\tRemove requested licenses\n\t\t:param:\n\t\t\tremove_list : List of packages to remove\n\n\t\t\"\"\"\n\t\tlicense_delete = netapp_utils.zapi.NaElement('license-v2-delete')\n\t\tfor package in remove_list:\n\t\t\tlicense_delete.add_new_child('package', package)\n\n\t\tif self.serial_number is not None:\n\t\t\tlicense_delete.add_new_child('serial-number', self.serial_number)\n\n\t\ttry:\n\t\t\tself.server.invoke_successfully(license_delete,\n\t\t\t\t\t\t\t\t\t\t\tenable_tunneling=False)\n\t\texcept netapp_utils.zapi.NaApiError:\n\t\t\terr = get_exception()\n\t\t\tself.module.fail_json(msg=\"Error removing license\",\n\t\t\t\t\t\t\t\t  exception=str(err))\n\n\tdef remove_unused_licenses(self):\n\t\t\"\"\"\n\t\tRemove unused licenses\n\t\t\"\"\"\n\t\tremove_unused = netapp_utils.zapi.NaElement('license-v2-delete-unused')\n\t\ttry:\n\t\t\tself.server.invoke_successfully(remove_unused,\n\t\t\t\t\t\t\t\t\t\t\tenable_tunneling=False)\n\t\texcept netapp_utils.zapi.NaApiError:\n\t\t\terr = get_exception()\n\t\t\tself.module.fail_json(msg=\"Error removing unused licenses\",\n\t\t\t\t\t\t\t\t  exception=str(err))\n\n\tdef remove_expired_licenses(self):\n\t\t\"\"\"\n\t\tRemove expired licenses\n\t\t\"\"\"\n\t\tremove_expired = netapp_utils.zapi.NaElement('license-v2-delete-expired')\n\t\ttry:\n\t\t\tself.server.invoke_successfully(remove_expired,\n\t\t\t\t\t\t\t\t\t\t\tenable_tunneling=False)\n\t\texcept netapp_utils.zapi.NaApiError:\n\t\t\terr = get_exception()\n\t\t\tself.module.fail_json(msg=\"Error removing expired licenses\",\n\t\t\t\t\t\t\t\t  exception=str(err))\n\n\tdef update_licenses(self):\n\t\t\"\"\"\n\t\tUpdate licenses\n\t\t\"\"\"\n\t\t# Remove unused and expired licenses, if requested.\n\t\tif self.remove_unused:\n\t\t\tself.remove_unused_licenses()\n\n\t\tif self.remove_expired:\n\t\t\tself.remove_expired_licenses()\n\n\t\t# Next, add/remove specific requested licenses.\n\t\tlicense_add = netapp_utils.zapi.NaElement('license-v2-add')\n\t\tcodes = netapp_utils.zapi.NaElement('codes')\n\t\tremove_list = []\n\t\tfor key, value in self.licenses.items():\n\t\t\tstr_value = str(value)\n\t\t\t# Make sure license is not an empty string.\n\t\t\tif str_value and str_value.strip():\n\t\t\t\tif str_value.lower() == 'remove':\n\t\t\t\t\tremove_list.append(str(key).lower())\n\t\t\t\telse:\n\t\t\t\t\tcodes.add_new_child('license-code-v2', str_value)\n\n\t\t# Remove requested licenses.\n\t\tif not len(remove_list) == 0:\n\t\t\tself.remove_licenses(remove_list)\n\n\t\t# Add requested licenses\n\t\tif not len(codes.get_children()) == 0:\n\t\t\tlicense_add.add_child_elem(codes)\n\t\t\ttry:\n\t\t\t\tself.server.invoke_successfully(license_add,\n\t\t\t\t\t\t\t\t\t\t\t\tenable_tunneling=False)\n\t\t\texcept netapp_utils.zapi.NaApiError:\n\t\t\t\terr = get_exception()\n\t\t\t\tself.module.fail_json(msg=\"Error adding licenses\",\n\t\t\t\t\t\t\t\t\t  exception=str(err))\n\n\tdef apply(self):\n\t\tchanged = False\n\t\t# Add / Update licenses.\n\t\tlicense_status = self.get_licensing_status()\n\t\tself.update_licenses()\n\t\tnew_license_status = self.get_licensing_status()\n\n\t\tif not license_status == new_license_status:\n\t\t\tchanged = True\n\n\t\tself.module.exit_json(changed=changed)\n\n", "description": "\n\t\t\tCheck licensing status\n\n\t\t\t:return: package (key) and licensing status (value)\n\t\t\t:rtype: dict\n\t\t", "category": "remove", "imports": ["from ansible.module_utils.basic import AnsibleModule", "from ansible.module_utils.pycompat24 import get_exception", "import ansible.module_utils.netapp as netapp_utils"]}, {"term": "def", "name": "main", "data": "def main():\n\tv = NetAppCDOTLicense()\n\tv.apply()\n", "description": null, "category": "remove", "imports": ["from ansible.module_utils.basic import AnsibleModule", "from ansible.module_utils.pycompat24 import get_exception", "import ansible.module_utils.netapp as netapp_utils"]}], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\n\tdef __init__(self, data=None, name='', file=None, **attr):\n\t\tGraph.__init__(self, data=data, name=name, **attr)\n\t\tif file is None:\n\t\t\timport sys\n\t\t\tself.fh = sys.stdout\n\t\telse:\n\t\t\tself.fh = open(file, 'w')\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tGraph.add_node(self, n, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(\"Add node: %s\\n\" % n)\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self, n):\n\t\tGraph.remove_node(self, n)\n\t\tself.fh.write(\"Remove node: %s\\n\" % n)\n\n\tdef remove_nodes_from(self, nodes):\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tGraph.add_edge(self, u, v, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(\"Add edge: %s-%s\\n\" % (u, v))\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.add_edge(u, v, attr_dict=attr_dict, **attr)\n\n\tdef remove_edge(self, u, v):\n\t\tGraph.remove_edge(self, u, v)\n\t\tself.fh.write(\"Remove edge: %s-%s\\n\" % (u, v))\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.remove_edge(u, v)\n\n\tdef clear(self):\n\t\tGraph.clear(self)\n\t\tself.fh.write(\"Clear graph\\n\")\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["from copy import deepcopy", "import matplotlib.pyplot as plt", "import networkx as nx", "from networkx import Graph", "\t\t\timport sys"]}], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\n\tdef __init__(self, data=None, name='', file=None, **attr):\n\t\tGraph.__init__(self, data=data, name=name, **attr)\n\t\tif file is None:\n\t\t\timport sys\n\t\t\tself.fh = sys.stdout\n\t\telse:\n\t\t\tself.fh = open(file, 'w')\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tGraph.add_node(self, n, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(\"Add node: %s\\n\" % n)\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self, n):\n\t\tGraph.remove_node(self, n)\n\t\tself.fh.write(\"Remove node: %s\\n\" % n)\n\n\tdef remove_nodes_from(self, nodes):\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tGraph.add_edge(self, u, v, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(\"Add edge: %s-%s\\n\" % (u, v))\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.add_edge(u, v, attr_dict=attr_dict, **attr)\n\n\tdef remove_edge(self, u, v):\n\t\tGraph.remove_edge(self, u, v)\n\t\tself.fh.write(\"Remove edge: %s-%s\\n\" % (u, v))\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.remove_edge(u, v)\n\n\tdef clear(self):\n\t\tGraph.clear(self)\n\t\tself.fh.write(\"Clear graph\\n\")\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["from copy import deepcopy", "import matplotlib.pyplot as plt", "import networkx as nx", "from networkx import Graph", "\t\t\timport sys"]}], [{"term": "class", "name": "classTriggerInstance:", "data": "class TriggerInstance:\n\t\"\"\"Attached trigger settings.\"\"\"\n\n\taction: AutomationActionType = attr.ib()\n\tautomation_info: AutomationTriggerInfo = attr.ib()\n\ttrigger: Trigger = attr.ib()\n\tremove: CALLBACK_TYPE | None = attr.ib(default=None)\n\n\tasync def async_attach_trigger(self) -> None:\n\t\t\"\"\"Attach event trigger.\"\"\"\n\t\tassert self.trigger.tasmota_trigger is not None\n\t\tevent_config = {\n\t\t\tevent_trigger.CONF_PLATFORM: \"event\",\n\t\t\tevent_trigger.CONF_EVENT_TYPE: TASMOTA_EVENT,\n\t\t\tevent_trigger.CONF_EVENT_DATA: {\n\t\t\t\t\"mac\": self.trigger.tasmota_trigger.cfg.mac,\n\t\t\t\t\"source\": self.trigger.tasmota_trigger.cfg.subtype,\n\t\t\t\t\"event\": self.trigger.tasmota_trigger.cfg.event,\n\t\t\t},\n\t\t}\n\n\t\tevent_config = event_trigger.TRIGGER_SCHEMA(event_config)\n\t\tif self.remove:\n\t\t\tself.remove()\n\t\t# Note: No lock needed, event_trigger.async_attach_trigger is an synchronous function\n\t\tself.remove = await event_trigger.async_attach_trigger(\n\t\t\tself.trigger.hass,\n\t\t\tevent_config,\n\t\t\tself.action,\n\t\t\tself.automation_info,\n\t\t\tplatform_type=\"device\",\n\t\t)\n\n", "description": "Attached trigger settings.", "category": "remove", "imports": ["from __future__ import annotations", "from collections.abc import Callable", "import logging", "from typing import Any", "import attr", "from hatasmota.models import DiscoveryHashType", "from hatasmota.trigger import TasmotaTrigger, TasmotaTriggerConfig", "import voluptuous as vol", "from homeassistant.components.automation import (", "from homeassistant.components.device_automation import DEVICE_TRIGGER_BASE_SCHEMA", "from homeassistant.components.homeassistant.triggers import event as event_trigger", "from homeassistant.config_entries import ConfigEntry", "from homeassistant.const import CONF_DEVICE_ID, CONF_DOMAIN, CONF_PLATFORM, CONF_TYPE", "from homeassistant.core import CALLBACK_TYPE, HomeAssistant, callback", "from homeassistant.exceptions import HomeAssistantError", "from homeassistant.helpers import config_validation as cv, device_registry as dr", "from homeassistant.helpers.device_registry import CONNECTION_NETWORK_MAC", "from homeassistant.helpers.dispatcher import async_dispatcher_connect", "from homeassistant.helpers.typing import ConfigType", "from .const import DOMAIN, TASMOTA_EVENT", "from .discovery import TASMOTA_DISCOVERY_ENTITY_UPDATED, clear_discovery_hash"]}, {"term": "class", "name": "classTrigger:", "data": "class Trigger:\n\t\"\"\"Device trigger settings.\"\"\"\n\n\tdevice_id: str = attr.ib()\n\tdiscovery_hash: DiscoveryHashType | None = attr.ib()\n\thass: HomeAssistant = attr.ib()\n\tremove_update_signal: Callable[[], None] | None = attr.ib()\n\tsubtype: str = attr.ib()\n\ttasmota_trigger: TasmotaTrigger | None = attr.ib()\n\ttype: str = attr.ib()\n\ttrigger_instances: list[TriggerInstance] = attr.ib(factory=list)\n\n\tasync def add_trigger(\n\t\tself, action: AutomationActionType, automation_info: AutomationTriggerInfo\n\t) -> Callable[[], None]:\n\t\t\"\"\"Add Tasmota trigger.\"\"\"\n\t\tinstance = TriggerInstance(action, automation_info, self)\n\t\tself.trigger_instances.append(instance)\n\n\t\tif self.tasmota_trigger is not None:\n\t\t\t# If we know about the trigger, set it up\n\t\t\tawait instance.async_attach_trigger()\n\n\t\t@callback\n\t\tdef async_remove() -> None:\n\t\t\t\"\"\"Remove trigger.\"\"\"\n\t\t\tif instance not in self.trigger_instances:\n\t\t\t\traise HomeAssistantError(\"Can't remove trigger twice\")\n\n\t\t\tif instance.remove:\n\t\t\t\tinstance.remove()\n\t\t\tself.trigger_instances.remove(instance)\n\n\t\treturn async_remove\n\n\tdef detach_trigger(self) -> None:\n\t\t\"\"\"Remove Tasmota device trigger.\"\"\"\n\t\t# Mark trigger as unknown\n\t\tself.tasmota_trigger = None\n\n\t\t# Unsubscribe if this trigger is in use\n\t\tfor trig in self.trigger_instances:\n\t\t\tif trig.remove:\n\t\t\t\ttrig.remove()\n\t\t\t\ttrig.remove = None\n\n\tasync def arm_tasmota_trigger(self) -> None:\n\t\t\"\"\"Arm Tasmota trigger: subscribe to MQTT topics and fire events.\"\"\"\n\n\t\t@callback\n\t\tdef _on_trigger() -> None:\n\t\t\tassert self.tasmota_trigger is not None\n\t\t\tdata = {\n\t\t\t\t\"mac\": self.tasmota_trigger.cfg.mac,\n\t\t\t\t\"source\": self.tasmota_trigger.cfg.subtype,\n\t\t\t\t\"event\": self.tasmota_trigger.cfg.event,\n\t\t\t}\n\t\t\tself.hass.bus.async_fire(\n\t\t\t\tTASMOTA_EVENT,\n\t\t\t\tdata,\n\t\t\t)\n\n\t\tassert self.tasmota_trigger is not None\n\t\tself.tasmota_trigger.set_on_trigger_callback(_on_trigger)\n\t\tawait self.tasmota_trigger.subscribe_topics()\n\n\tasync def set_tasmota_trigger(\n\t\tself, tasmota_trigger: TasmotaTrigger, remove_update_signal: Callable[[], None]\n\t) -> None:\n\t\t\"\"\"Set Tasmota trigger.\"\"\"\n\t\tawait self.update_tasmota_trigger(tasmota_trigger.cfg, remove_update_signal)\n\t\tself.tasmota_trigger = tasmota_trigger\n\n\t\tfor trig in self.trigger_instances:\n\t\t\tawait trig.async_attach_trigger()\n\n\tasync def update_tasmota_trigger(\n\t\tself,\n\t\ttasmota_trigger_cfg: TasmotaTriggerConfig,\n\t\tremove_update_signal: Callable[[], None],\n\t) -> None:\n\t\t\"\"\"Update Tasmota trigger.\"\"\"\n\t\tself.remove_update_signal = remove_update_signal\n\t\tself.type = tasmota_trigger_cfg.type\n\t\tself.subtype = tasmota_trigger_cfg.subtype\n\n", "description": "Device trigger settings.", "category": "remove", "imports": ["from __future__ import annotations", "from collections.abc import Callable", "import logging", "from typing import Any", "import attr", "from hatasmota.models import DiscoveryHashType", "from hatasmota.trigger import TasmotaTrigger, TasmotaTriggerConfig", "import voluptuous as vol", "from homeassistant.components.automation import (", "from homeassistant.components.device_automation import DEVICE_TRIGGER_BASE_SCHEMA", "from homeassistant.components.homeassistant.triggers import event as event_trigger", "from homeassistant.config_entries import ConfigEntry", "from homeassistant.const import CONF_DEVICE_ID, CONF_DOMAIN, CONF_PLATFORM, CONF_TYPE", "from homeassistant.core import CALLBACK_TYPE, HomeAssistant, callback", "from homeassistant.exceptions import HomeAssistantError", "from homeassistant.helpers import config_validation as cv, device_registry as dr", "from homeassistant.helpers.device_registry import CONNECTION_NETWORK_MAC", "from homeassistant.helpers.dispatcher import async_dispatcher_connect", "from homeassistant.helpers.typing import ConfigType", "from .const import DOMAIN, TASMOTA_EVENT", "from .discovery import TASMOTA_DISCOVERY_ENTITY_UPDATED, clear_discovery_hash"]}, {"term": "def", "name": "ncdefasync_setup_trigger", "data": "async def async_setup_trigger(\n\thass: HomeAssistant,\n\ttasmota_trigger: TasmotaTrigger,\n\tconfig_entry: ConfigEntry,\n", "description": null, "category": "remove", "imports": ["from __future__ import annotations", "from collections.abc import Callable", "import logging", "from typing import Any", "import attr", "from hatasmota.models import DiscoveryHashType", "from hatasmota.trigger import TasmotaTrigger, TasmotaTriggerConfig", "import voluptuous as vol", "from homeassistant.components.automation import (", "from homeassistant.components.device_automation import DEVICE_TRIGGER_BASE_SCHEMA", "from homeassistant.components.homeassistant.triggers import event as event_trigger", "from homeassistant.config_entries import ConfigEntry", "from homeassistant.const import CONF_DEVICE_ID, CONF_DOMAIN, CONF_PLATFORM, CONF_TYPE", "from homeassistant.core import CALLBACK_TYPE, HomeAssistant, callback", "from homeassistant.exceptions import HomeAssistantError", "from homeassistant.helpers import config_validation as cv, device_registry as dr", "from homeassistant.helpers.device_registry import CONNECTION_NETWORK_MAC", "from homeassistant.helpers.dispatcher import async_dispatcher_connect", "from homeassistant.helpers.typing import ConfigType", "from .const import DOMAIN, TASMOTA_EVENT", "from .discovery import TASMOTA_DISCOVERY_ENTITY_UPDATED, clear_discovery_hash"]}, {"term": "def", "name": "yncdefdiscovery_update", "data": "\tasync def discovery_update(trigger_config: TasmotaTriggerConfig) -> None:\n\t\t\"\"\"Handle discovery update.\"\"\"\n\t\t_LOGGER.debug(\n\t\t\t\"Got update for trigger with hash: %s '%s'\", discovery_hash, trigger_config\n\t\t)\n\t\tif not trigger_config.is_active:\n\t\t\t# Empty trigger_config: Remove trigger\n\t\t\t_LOGGER.debug(\"Removing trigger: %s\", discovery_hash)\n\t\t\tif discovery_id in hass.data[DEVICE_TRIGGERS]:\n\t\t\t\tdevice_trigger = hass.data[DEVICE_TRIGGERS][discovery_id]\n\t\t\t\tawait device_trigger.tasmota_trigger.unsubscribe_topics()\n\t\t\t\tdevice_trigger.detach_trigger()\n\t\t\t\tclear_discovery_hash(hass, discovery_hash)\n\t\t\t\tif remove_update_signal is not None:\n\t\t\t\t\tremove_update_signal()\n\t\t\treturn\n\n\t\tdevice_trigger = hass.data[DEVICE_TRIGGERS][discovery_id]\n\t\tif device_trigger.tasmota_trigger.config_same(trigger_config):\n\t\t\t# Unchanged payload: Ignore to avoid unnecessary unsubscribe / subscribe\n\t\t\t_LOGGER.debug(\"Ignoring unchanged update for: %s\", discovery_hash)\n\t\t\treturn\n\n\t\t# Non-empty, changed trigger_config: Update trigger\n\t\t_LOGGER.debug(\"Updating trigger: %s\", discovery_hash)\n\t\tdevice_trigger.tasmota_trigger.config_update(trigger_config)\n\t\tawait device_trigger.update_tasmota_trigger(\n\t\t\ttrigger_config, remove_update_signal\n\t\t)\n\t\tawait device_trigger.arm_tasmota_trigger()\n\t\treturn\n", "description": "Handle discovery update.", "category": "remove", "imports": ["from __future__ import annotations", "from collections.abc import Callable", "import logging", "from typing import Any", "import attr", "from hatasmota.models import DiscoveryHashType", "from hatasmota.trigger import TasmotaTrigger, TasmotaTriggerConfig", "import voluptuous as vol", "from homeassistant.components.automation import (", "from homeassistant.components.device_automation import DEVICE_TRIGGER_BASE_SCHEMA", "from homeassistant.components.homeassistant.triggers import event as event_trigger", "from homeassistant.config_entries import ConfigEntry", "from homeassistant.const import CONF_DEVICE_ID, CONF_DOMAIN, CONF_PLATFORM, CONF_TYPE", "from homeassistant.core import CALLBACK_TYPE, HomeAssistant, callback", "from homeassistant.exceptions import HomeAssistantError", "from homeassistant.helpers import config_validation as cv, device_registry as dr", "from homeassistant.helpers.device_registry import CONNECTION_NETWORK_MAC", "from homeassistant.helpers.dispatcher import async_dispatcher_connect", "from homeassistant.helpers.typing import ConfigType", "from .const import DOMAIN, TASMOTA_EVENT", "from .discovery import TASMOTA_DISCOVERY_ENTITY_UPDATED, clear_discovery_hash"]}, {"term": "def", "name": "ncdefasync_remove_triggers", "data": "async def async_remove_triggers(hass: HomeAssistant, device_id: str) -> None:\n\t\"\"\"Cleanup any device triggers for a Tasmota device.\"\"\"\n\ttriggers = await async_get_triggers(hass, device_id)\n\tfor trig in triggers:\n\t\tdevice_trigger = hass.data[DEVICE_TRIGGERS].pop(trig[CONF_DISCOVERY_ID])\n\t\tif device_trigger:\n\t\t\tdiscovery_hash = device_trigger.discovery_hash\n\n\t\t\tawait device_trigger.tasmota_trigger.unsubscribe_topics()\n\t\t\tdevice_trigger.detach_trigger()\n\t\t\tclear_discovery_hash(hass, discovery_hash)\n\t\t\tdevice_trigger.remove_update_signal()\n\n", "description": "Cleanup any device triggers for a Tasmota device.", "category": "remove", "imports": ["from __future__ import annotations", "from collections.abc import Callable", "import logging", "from typing import Any", "import attr", "from hatasmota.models import DiscoveryHashType", "from hatasmota.trigger import TasmotaTrigger, TasmotaTriggerConfig", "import voluptuous as vol", "from homeassistant.components.automation import (", "from homeassistant.components.device_automation import DEVICE_TRIGGER_BASE_SCHEMA", "from homeassistant.components.homeassistant.triggers import event as event_trigger", "from homeassistant.config_entries import ConfigEntry", "from homeassistant.const import CONF_DEVICE_ID, CONF_DOMAIN, CONF_PLATFORM, CONF_TYPE", "from homeassistant.core import CALLBACK_TYPE, HomeAssistant, callback", "from homeassistant.exceptions import HomeAssistantError", "from homeassistant.helpers import config_validation as cv, device_registry as dr", "from homeassistant.helpers.device_registry import CONNECTION_NETWORK_MAC", "from homeassistant.helpers.dispatcher import async_dispatcher_connect", "from homeassistant.helpers.typing import ConfigType", "from .const import DOMAIN, TASMOTA_EVENT", "from .discovery import TASMOTA_DISCOVERY_ENTITY_UPDATED, clear_discovery_hash"]}, {"term": "def", "name": "ncdefasync_get_triggers", "data": "async def async_get_triggers(\n", "description": null, "category": "remove", "imports": ["from __future__ import annotations", "from collections.abc import Callable", "import logging", "from typing import Any", "import attr", "from hatasmota.models import DiscoveryHashType", "from hatasmota.trigger import TasmotaTrigger, TasmotaTriggerConfig", "import voluptuous as vol", "from homeassistant.components.automation import (", "from homeassistant.components.device_automation import DEVICE_TRIGGER_BASE_SCHEMA", "from homeassistant.components.homeassistant.triggers import event as event_trigger", "from homeassistant.config_entries import ConfigEntry", "from homeassistant.const import CONF_DEVICE_ID, CONF_DOMAIN, CONF_PLATFORM, CONF_TYPE", "from homeassistant.core import CALLBACK_TYPE, HomeAssistant, callback", "from homeassistant.exceptions import HomeAssistantError", "from homeassistant.helpers import config_validation as cv, device_registry as dr", "from homeassistant.helpers.device_registry import CONNECTION_NETWORK_MAC", "from homeassistant.helpers.dispatcher import async_dispatcher_connect", "from homeassistant.helpers.typing import ConfigType", "from .const import DOMAIN, TASMOTA_EVENT", "from .discovery import TASMOTA_DISCOVERY_ENTITY_UPDATED, clear_discovery_hash"]}, {"term": "def", "name": "ncdefasync_attach_trigger", "data": "async def async_attach_trigger(\n\thass: HomeAssistant,\n\tconfig: ConfigType,\n\taction: Callable,\n", "description": null, "category": "remove", "imports": ["from __future__ import annotations", "from collections.abc import Callable", "import logging", "from typing import Any", "import attr", "from hatasmota.models import DiscoveryHashType", "from hatasmota.trigger import TasmotaTrigger, TasmotaTriggerConfig", "import voluptuous as vol", "from homeassistant.components.automation import (", "from homeassistant.components.device_automation import DEVICE_TRIGGER_BASE_SCHEMA", "from homeassistant.components.homeassistant.triggers import event as event_trigger", "from homeassistant.config_entries import ConfigEntry", "from homeassistant.const import CONF_DEVICE_ID, CONF_DOMAIN, CONF_PLATFORM, CONF_TYPE", "from homeassistant.core import CALLBACK_TYPE, HomeAssistant, callback", "from homeassistant.exceptions import HomeAssistantError", "from homeassistant.helpers import config_validation as cv, device_registry as dr", "from homeassistant.helpers.device_registry import CONNECTION_NETWORK_MAC", "from homeassistant.helpers.dispatcher import async_dispatcher_connect", "from homeassistant.helpers.typing import ConfigType", "from .const import DOMAIN, TASMOTA_EVENT", "from .discovery import TASMOTA_DISCOVERY_ENTITY_UPDATED, clear_discovery_hash"]}], [{"term": "class", "name": "SuccessRemoveParticipantsIqProtocolEntity", "data": "class SuccessRemoveParticipantsIqProtocolEntity(ResultIqProtocolEntity):\n\t'''\n\t\n\n\n\n\t'''\n\n\tdef __init__(self, _id, groupId, participantList):\n\t\tsuper(SuccessRemoveParticipantsIqProtocolEntity, self).__init__(_from = groupId, _id = _id)\n\t\tself.setProps(groupId, participantList)\n\n\tdef setProps(self, groupId, participantList):\n\t\tself.groupId = groupId\n\t\tself.participantList = participantList\n\t\tself.action = 'remove'\n\n\tdef getAction(self):\n\t\treturn self.action\n\n\tdef toProtocolTreeNode(self):\n\t\tnode = super(SuccessRemoveParticipantsIqProtocolEntity, self).toProtocolTreeNode()\n\t\tparticipantNodes = [\n\t\t\tProtocolTreeNode(\"remove\", {\n\t\t\t\t\"type\":\t\t\t\t\t \"success\",\n\t\t\t\t\"participant\":\t   participant\n\t\t\t})\n\t\t\tfor participant in self.participantList\n\t\t]\n\t\tnode.addChildren(participantNodes)\n\n\t\treturn node\n\n\t@staticmethod\n\tdef fromProtocolTreeNode(node):\n\t\tentity = super(SuccessRemoveParticipantsIqProtocolEntity, SuccessRemoveParticipantsIqProtocolEntity).fromProtocolTreeNode(node)\n\t\tentity.__class__ = SuccessRemoveParticipantsIqProtocolEntity\n\t\tparticipantList = []\n\t\tfor participantNode in node.getAllChildren():\n\t\t\tif participantNode[\"type\"]==\"success\":\n\t\t\t\tparticipantList.append(participantNode[\"participant\"])\n\t\tentity.setProps(node.getAttributeValue(\"from\"), participantList)\n\t\treturn entity\n", "description": null, "category": "remove", "imports": ["from yowsup.structs import ProtocolTreeNode", "from yowsup.layers.protocol_iq.protocolentities import ResultIqProtocolEntity"]}], [{"term": "class", "name": "Attribute", "data": "class Attribute(object):\n\tdef __init__(self, name, swift_name=None):\n\t\tself.name = name\n\t\tself.swift_name = swift_name or name\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}, {"term": "class", "name": "TypeAttribute", "data": "class TypeAttribute(Attribute):\n\tdef __init__(self, name):\n\t\tsuper().__init__(name, name)\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}, {"term": "class", "name": "DeclAttribute", "data": "class DeclAttribute(Attribute):\n\tdef __init__(self, name, class_name, *options, code, swift_name=None):\n\t\tsuper().__init__(name, swift_name)\n\t\tself.class_name = class_name\n\t\tself.options = options\n\t\tself.code = code\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}, {"term": "class", "name": "SimpleDeclAttribute", "data": "class SimpleDeclAttribute(DeclAttribute):\n\tdef __init__(self, name, class_name, *options, code, swift_name=None):\n\t\tsuper().__init__(name, class_name, *options, code=code, swift_name=swift_name)\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}, {"term": "class", "name": "ContextualDeclAttribute", "data": "class ContextualDeclAttribute(DeclAttribute):\n\tdef __init__(self, name, class_name, *options, code):\n\t\tsuper().__init__(name, class_name, *options, code=code)\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}, {"term": "class", "name": "ContextualSimpleDeclAttribute", "data": "class ContextualSimpleDeclAttribute(SimpleDeclAttribute):\n\tdef __init__(self, name, class_name, *options, code):\n\t\tsuper().__init__(name, class_name, *options, code=code)\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}, {"term": "class", "name": "DeclAttributeAlias", "data": "class DeclAttributeAlias(Attribute):\n\tdef __init__(self, name, class_name, swift_name=None):\n\t\tsuper().__init__(name, swift_name)\n\t\tself.class_name = class_name\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}, {"term": "class", "name": "ContextualDeclAttributeAlias", "data": "class ContextualDeclAttributeAlias(DeclAttributeAlias):\n\tdef __init__(self, name, class_name, swift_name=None):\n\t\tsuper().__init__(name, class_name, swift_name)\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}, {"term": "class", "name": "BuiltinDeclModifier", "data": "class BuiltinDeclModifier(Attribute):\n\tdef __init__(self, name, swift_name=None):\n\t\tsuper().__init__(name, swift_name)\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}, {"term": "def", "name": "verify_attribute_serialization_codes", "data": "def verify_attribute_serialization_codes(nodes):\n\t# Verify that no serialization code is used twice\n\tused_codes = set()\n\tfor node in nodes:\n\t\tif isinstance(node, DeclAttribute):\n\t\t\tif node.code in used_codes:\n\t\t\t\terror(\"Serialization code %d used twice\" % node.code)\n\t\t\tused_codes.add(node.code)\n", "description": null, "category": "remove", "imports": ["from .Child import Child", "from .Node import Node  # noqa: I201", "from .Utils import error"]}], [], [], [{"term": "class", "name": "TestDiGraph", "data": "class TestDiGraph(unittest.TestCase):\r\n\r\n\tdef test_v_size(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.add_edge(0, 1, 1)\r\n\t\tg.add_edge(1, 0, 1.1)\r\n\t\tg.add_edge(1, 2, 1.3)\r\n\t\tg.add_edge(2, 3, 1.1)\r\n\t\tg.add_edge(1, 3, 1.9)\r\n\t\tg.remove_edge(1, 3)\r\n\t\tg.add_edge(1, 3, 10)\r\n\t\tself.assertEqual(DiGraph.v_size(g), 4)\r\n\t\tself.assertNotEqual(DiGraph.v_size(g), 7)\r\n\r\n\r\n\tdef test_e_size(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.add_edge(0, 1, 1)\r\n\t\tg.add_edge(1, 0, 1.1)\r\n\t\tg.add_edge(1, 2, 1.3)\r\n\t\tg.add_edge(2, 3, 1.1)\r\n\t\tg.add_edge(1, 3, 1.9)\r\n\t\tself.assertEqual(DiGraph.e_size(g), 5)\r\n\t\tg.remove_edge(1, 3)\r\n\t\tg.add_edge(1, 3, 10)\r\n\t\tself.assertEqual(DiGraph.e_size(g), 5)\r\n\t\tself.assertNotEqual(DiGraph.e_size(g), 7)\r\n\r\n\tdef test_get_all_v(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.add_edge(0, 1, 1)\r\n\t\tg.add_edge(1, 0, 1.1)\r\n\t\tg.add_edge(1, 2, 1.3)\r\n\t\tg.add_edge(2, 3, 1.1)\r\n\t\tg.add_edge(1, 3, 1.9)\r\n\t\tg.remove_edge(1, 3)\r\n\t\tg.add_edge(1, 3, 10)\r\n\t\td = {0: '0: |edges out|: 1  ,|edges in|: 1', 1: '1: |edges out|: 3  ,|edges in|: 1', 2: '2: |edges out|: 1  ,|edges in|: 1', 3: '3: |edges out|: 0  ,|edges in|: 2'}\r\n\t\tself.assertEqual(g.get_all_v(),d)\r\n\r\n\tdef test_all_in_edges_of_node(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.add_edge(0, 1, 1)\r\n\t\tg.add_edge(1, 0, 1.1)\r\n\t\tg.add_edge(1, 2, 1.3)\r\n\t\tg.add_edge(2, 3, 1.1)\r\n\t\tg.add_edge(1, 3, 1.9)\r\n\t\tg.remove_edge(1, 3)\r\n\t\tg.add_edge(1, 3, 10)\r\n\t\tself.assertEqual(g.all_in_edges_of_node(0), ({1: 1.1}, 1))\r\n\t\tself.assertEqual(g.all_in_edges_of_node(1), ({0: 1}, 1))\r\n\r\n\tdef all_out_edges_of_node(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.add_edge(0, 1, 1)\r\n\t\tg.add_edge(1, 0, 1.1)\r\n\t\tg.add_edge(1, 2, 1.3)\r\n\t\tg.add_edge(2, 3, 1.1)\r\n\t\tg.add_edge(1, 3, 1.9)\r\n\t\tg.remove_edge(1, 3)\r\n\t\tg.add_edge(1, 3, 10)\r\n\t\tself.assertEqual(g.all_out_edges_of_node(0),{(0,1):1})\r\n\r\n\tdef test_get_mc(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.add_edge(0, 1, 1)\r\n\t\tg.add_edge(1, 0, 1.1)\r\n\t\tg.add_edge(1, 2, 1.3)\r\n\t\tg.add_edge(2, 3, 1.1)\r\n\t\tg.add_edge(1, 3, 1.9)\r\n\t\tg.remove_edge(1, 3)\r\n\t\tg.add_edge(1, 3, 10)\r\n\t\tself.assertEqual(g.get_mc(),7)\r\n\r\n\r\n\tdef test_add_node(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tself.assertEqual(g.get_node(0), (0, 0))\r\n\t\tself.assertEqual(g.get_node(1), (0, 0))\r\n\t\tself.assertEqual(g.get_node(2), (0, 0))\r\n\t\tself.assertEqual(g.get_node(3), (0, 0))\r\n\r\n\tdef test_add_edge(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.add_edge(0, 1, 1)\r\n\t\tself.assertEqual(g.get_edge(0,1) , 1)\r\n\t\tg.add_edge(1, 0, 1.1)\r\n\t\tself.assertEqual(g.get_edge(1,0) , 1.1)\r\n\t\tg.add_edge(1, 2, 1.3)\r\n\t\tself.assertEqual(g.get_edge(1,2) , 1.3)\r\n\t\tg.add_edge(2, 3, 1.1)\r\n\t\tself.assertEqual(g.get_edge(2,3) , 1.1)\r\n\t\tg.add_edge(1, 3, 1.9)\r\n\t\tself.assertEqual(g.get_edge(1,3) , 1.9)\r\n\t\tg.remove_edge(1, 3)\r\n\t\tg.add_edge(1, 3, 10)\r\n\t\tself.assertEqual(g.get_edge(1,3) , 10)\r\n\r\n\tdef test_remove_node(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.remove_node(0)\r\n\t\tg.remove_node(1)\r\n\t\tg.remove_node(2)\r\n\t\tg.remove_node(3)\r\n\t\tself.assertEqual(g.get_node(0),None)\r\n\t\tself.assertEqual(g.get_node(1),None)\r\n\t\tself.assertEqual(g.get_node(2),None)\r\n\t\tself.assertEqual(g.get_node(3),None)\r\n\r\n\tdef test_remove_edge(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.add_edge(0, 1, 1)\r\n\t\tg.add_edge(1, 0, 1.1)\r\n\t\tg.add_edge(1, 2, 1.3)\r\n\t\tg.add_edge(2, 3, 1.1)\r\n\t\tg.add_edge(1, 3, 1.9)\r\n\t\tg.remove_edge(1, 3)\r\n\t\tg.remove_edge(1,0)\r\n\t\tg.remove_edge(1,2)\r\n\t\tg.remove_edge(2,3)\r\n\t\tg.remove_edge(1,3)\r\n\t\tself.assertEqual(g.get_edge(1,3),None)\r\n\t\tself.assertEqual(g.get_edge(1,0),None)\r\n\t\tself.assertEqual(g.get_edge(1,2),None)\r\n\t\tself.assertEqual(g.get_edge(2,3),None)\r\n\t\tself.assertEqual(g.get_edge(1,3),None)\r\n\r\n\tdef test_get_node(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.remove_node(0)\r\n\t\tg.remove_node(1)\r\n\t\tg.remove_node(2)\r\n\t\tg.remove_node(3)\r\n\t\tself.assertEqual(g.get_node(0),None)\r\n\t\tself.assertEqual(g.get_node(1),None)\r\n\t\tself.assertEqual(g.get_node(2),None)\r\n\t\tself.assertEqual(g.get_node(3),None)\r\n\r\n\tdef test_get_edge(self):\r\n\t\tg = DiGraph()  # creates an empty directed graph\r\n\t\tfor n in range(4):\r\n\t\t\tg.add_node(n)\r\n\t\tg.add_edge(0, 1, 1)\r\n\t\tg.add_edge(1, 0, 1.1)\r\n\t\tg.add_edge(1, 2, 1.3)\r\n\t\tg.add_edge(2, 3, 1.1)\r\n\t\tg.add_edge(1, 3, 1.9)\r\n\t\tg.remove_edge(1, 3)\r\n\t\tg.remove_edge(1,0)\r\n\t\tg.remove_edge(1,2)\r\n\t\tg.remove_edge(2,3)\r\n\t\tg.remove_edge(1,3)\r\n\t\tself.assertEqual(g.get_edge(1,3),None)\r\n\t\tself.assertEqual(g.get_edge(1,0),None)\r\n\t\tself.assertEqual(g.get_edge(1,2),None)\r\n\t\tself.assertEqual(g.get_edge(2,3),None)\r\n\t\tself.assertEqual(g.get_edge(1,3),None)\r\n", "description": null, "category": "remove", "imports": ["import unittest\r", "from DiGraph import DiGraph\r"]}], [{"term": "def", "name": "index", "data": "def index(request):\n\treturn render(request, \"index.html\")\n", "description": null, "category": "remove", "imports": ["from django.http import HttpResponse", "from django.shortcuts import render"]}, {"term": "def", "name": "analyze", "data": "def analyze(request):\n\t'''function to analyze the text input given by user'''\n\n\t# get text\n\tdjtext = request.POST.get('text', 'default')\n\n\t# get checkbox values\n\tisRemovePunc = request.POST.get('removepunc', 'off')\n\tisCap = request.POST.get('capitalize', 'off')\n\tisSpaceRemove = request.POST.get('spaceremove', 'off')\n\tisNewLineRemove = request.POST.get('newlineremove', 'off')\n\tisCharCount = request.POST.get('charcount', 'off')\n\n\t# see which checkboxes are on and analyse text\n\tpunctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n\tpurpose = \"\"\n\tif isRemovePunc == 'on':\n\t\tpurpose += \"punctuations removed\"\n\t\tanalyzed = \"\"\n\t\tfor char in djtext:\n\t\t\tif char not in punctuations:\n\t\t\t\tanalyzed += char\n\t\tdjtext = analyzed\n\n\tif isCap == 'on':\n\t\tpurpose += (\", capitalized all characters\" if isRemovePunc == 'on' else \"capitalized all characters\")\n\t\tanalyzed = \"\"\n\t\tfor char in djtext:\n\t\t\tif char in djtext:\n\t\t\t\tanalyzed += char.upper()\n\t\tdjtext = analyzed\n\n\tif isNewLineRemove == 'on':\n\t\tpurpose += (\", new lines removed\" if isRemovePunc == 'on' or isCap == 'on' else \"new lines removed\")\n\t\tanalyzed = \"\"\n\t\tfor char in djtext:\n\t\t\tif char != '\\n' and char != '\\r':\n\t\t\t\tanalyzed += char\n\t\tdjtext = analyzed\n\n\tif isSpaceRemove == 'on':\n\t\tpurpose += (\", extra spaces removed\" if isRemovePunc == 'on' or isCap == 'on' or isNewLineRemove == 'on' else \"extra spaces removed\")\n\t\tanalyzed = \"\"\n\t\tfor index, char in enumerate(djtext):\n\t\t\tif not (djtext[index] == ' ' and djtext[index + 1] == ' '):\n\t\t\t\tanalyzed += char\n\t\tdjtext = analyzed\n\n\tif isCharCount == 'on':\n\t\tpurpose += (\", number of characters counted\" if isRemovePunc == 'on' or isCap == 'on' or isNewLineRemove == 'on' or isSpaceRemove == 'on' else \"extra spaces removed\")\n\t\tanalyzed = f\"number of characters = {len(djtext)}\"\n\t\tdjtext = f\"Text = {djtext} and {analyzed}\"\n\n\n\tif(isRemovePunc == 'off' and isSpaceRemove =='off' and isCap == 'off' and isNewLineRemove == 'off' and isCharCount == 'off'):\n\t\treturn HttpResponse(\"Error\")\n\n\tparams = {'analyzedtext': djtext, 'purpose': purpose}\n\treturn render(request, \"analyze.html\", params)\n\n\n\n\n", "description": null, "category": "remove", "imports": ["from django.http import HttpResponse", "from django.shortcuts import render"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('main', '0007_v320_data_migrations'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='authorize',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='authorize_password',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='become_method',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='become_password',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='become_username',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='client',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='cloud',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='domain',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='host',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='kind',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='password',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='project',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='secret',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='security_token',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='ssh_key_data',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='ssh_key_unlock',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='subscription',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='tenant',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='username',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='credential',\n\t\t\tname='vault_password',\n\t\t),\n\t\tmigrations.AlterUniqueTogether(\n\t\t\tname='credentialtype',\n\t\t\tunique_together=set([('name', 'kind')]),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='credential',\n\t\t\tname='credential_type',\n\t\t\tfield=models.ForeignKey(related_name='credentials', to='main.CredentialType', on_delete=models.CASCADE, null=False, help_text='Specify the type of credential you want to create. Refer to the Ansible Tower documentation for details on each type.')\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='credential',\n\t\t\tname='inputs',\n\t\t\tfield=awx.main.fields.CredentialInputField(default=dict, help_text='Enter inputs using either JSON or YAML syntax. Use the radio button to toggle between the two. Refer to the Ansible Tower documentation for example syntax.', blank=True),\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='job',\n\t\t\tname='cloud_credential',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='job',\n\t\t\tname='network_credential',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='jobtemplate',\n\t\t\tname='cloud_credential',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='jobtemplate',\n\t\t\tname='network_credential',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations", "from django.db import models", "import awx.main.fields"]}], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\tdef __init__(self, data=None, name='', file=None, **attr):\n\t\tGraph.__init__(self, data=data,name=name,**attr)\n\t\tif file is None:\n\t\t\timport sys\n\t\t\tself.fh=sys.stdout\n\t\telse:\n\t\t\tself.fh=open(file,'w')\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tGraph.add_node(self,n,attr_dict=attr_dict,**attr)\n\t\tself.fh.write(\"Add node: %s\\n\"%n)\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self,n):\n\t\tGraph.remove_node(self,n)\n\t\tself.fh.write(\"Remove node: %s\\n\"%n)\n\n\tdef remove_nodes_from(self, nodes):\n\t\tadj = self.adj\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tGraph.add_edge(self,u,v,attr_dict=attr_dict,**attr)\n\t\tself.fh.write(\"Add edge: %s-%s\\n\"%(u,v))\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu,v=e[0:2]\n\t\t\tself.add_edge(u,v,attr_dict=attr_dict,**attr)\n\n\tdef remove_edge(self, u, v):\n\t\tGraph.remove_edge(self,u,v)\n\t\tself.fh.write(\"Remove edge: %s-%s\\n\"%(u,v))\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu,v=e[0:2]\n\t\t\tself.remove_edge(u,v)\n\n\tdef clear(self):\n\t\tself.name = ''\n\t\tself.adj.clear()\n\t\tself.node.clear()\n\t\tself.graph.clear()\n\t\tself.fh.write(\"Clear graph\\n\")\n\n\tdef subgraph(self, nbunch, copy=True):\n\t\t# subgraph is needed here since it can destroy edges in the\n\t\t# graph (copy=False) and we want to keep track of all changes.\n\t\t#\n\t\t# Also for copy=True Graph() uses dictionary assignment for speed\n\t\t# Here we use H.add_edge()\n\t\tbunch =set(self.nbunch_iter(nbunch))\n\n\t\tif not copy:\n\t\t\t# remove all nodes (and attached edges) not in nbunch\n\t\t\tself.remove_nodes_from([n for n in self if n not in bunch])\n\t\t\tself.name = \"Subgraph of (%s)\"%(self.name)\n\t\t\treturn self\n\t\telse:\n\t\t\t# create new graph and copy subgraph into it\n\t\t\tH = self.__class__()\n\t\t\tH.name = \"Subgraph of (%s)\"%(self.name)\n\t\t\t# add nodes\n\t\t\tH.add_nodes_from(bunch)\n\t\t\t# add edges\n\t\t\tseen=set()\n\t\t\tfor u,nbrs in self.adjacency_iter():\n\t\t\t\tif u in bunch:\n\t\t\t\t\tfor v,datadict in nbrs.items():\n\t\t\t\t\t\tif v in bunch and v not in seen:\n\t\t\t\t\t\t\tdd=deepcopy(datadict)\n\t\t\t\t\t\t\tH.add_edge(u,v,dd)\n\t\t\t\t\tseen.add(u)\n\t\t\t# copy node and graph attr dicts\n\t\t\tH.node=dict( (n,deepcopy(d))\n\t\t\t\t\t\t for (n,d) in self.node.items() if n in H)\n\t\t\tH.graph=deepcopy(self.graph)\n\t\t\treturn H\n\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["from networkx import Graph", "from networkx.exception import NetworkXException, NetworkXError", "import networkx.convert as convert", "from copy import deepcopy", "\t\t\timport sys"]}], [], [{"term": "class", "name": "CmdExperimentsRemove", "data": "class CmdExperimentsRemove(CmdBase):\n\tdef check_arguments(self):\n\t\tif not any(\n\t\t\t[\n\t\t\t\tself.args.all_commits,\n\t\t\t\tself.args.rev,\n\t\t\t\tself.args.queue,\n\t\t\t]\n\t\t) ^ bool(self.args.experiment):\n\t\t\traise InvalidArgumentError(\n\t\t\t\t\"Either provide an `experiment` argument, or use the \"\n\t\t\t\t\"`--rev` or `--all-commits` or `--queue` flag.\"\n\t\t\t)\n\n\tdef run(self):\n\n\t\tself.check_arguments()\n\n\t\tremoved_list = self.repo.experiments.remove(\n\t\t\texp_names=self.args.experiment,\n\t\t\tall_commits=self.args.all_commits,\n\t\t\trev=self.args.rev,\n\t\t\tnum=self.args.num,\n\t\t\tqueue=self.args.queue,\n\t\t\tgit_remote=self.args.git_remote,\n\t\t)\n\t\tremoved = \",\".join(removed_list)\n\t\tui.write(f\"Removed experiments: {removed}\")\n\n\t\treturn 0\n\n", "description": null, "category": "remove", "imports": ["import argparse", "import logging", "from dvc.cli.command import CmdBase", "from dvc.cli.utils import append_doc_link", "from dvc.exceptions import InvalidArgumentError", "from dvc.ui import ui", "\tfrom . import add_rev_selection_flags"]}, {"term": "def", "name": "add_parser", "data": "def add_parser(experiments_subparsers, parent_parser):\n\tfrom . import add_rev_selection_flags\n\n\tEXPERIMENTS_REMOVE_HELP = \"Remove experiments.\"\n\texperiments_remove_parser = experiments_subparsers.add_parser(\n\t\t\"remove\",\n\t\tparents=[parent_parser],\n\t\tdescription=append_doc_link(EXPERIMENTS_REMOVE_HELP, \"exp/remove\"),\n\t\thelp=EXPERIMENTS_REMOVE_HELP,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t)\n\tremove_group = experiments_remove_parser.add_mutually_exclusive_group()\n\tadd_rev_selection_flags(experiments_remove_parser, \"Remove\", False)\n\tremove_group.add_argument(\n\t\t\"--queue\", action=\"store_true\", help=\"Remove all queued experiments.\"\n\t)\n\tremove_group.add_argument(\n\t\t\"-g\",\n\t\t\"--git-remote\",\n\t\tmetavar=\"\",\n\t\thelp=\"Name or URL of the Git remote to remove the experiment from\",\n\t)\n\texperiments_remove_parser.add_argument(\n\t\t\"experiment\",\n\t\tnargs=\"*\",\n\t\thelp=\"Experiments to remove.\",\n\t\tmetavar=\"\",\n\t)\n\texperiments_remove_parser.set_defaults(func=CmdExperimentsRemove)\n", "description": null, "category": "remove", "imports": ["import argparse", "import logging", "from dvc.cli.command import CmdBase", "from dvc.cli.utils import append_doc_link", "from dvc.exceptions import InvalidArgumentError", "from dvc.ui import ui", "\tfrom . import add_rev_selection_flags"]}], [{"term": "class", "name": "Theclass\"Friends\"shouldcontainsnamesandtheconnectionsbetweenthem.Namesarerepresentedasstringsandarecasesensitive.Connectionsareundirected,soif\"sophia\"isconnectedwith\"nikola\",thenit'salsocorrectinreverse.", "data": "The class \"Friends\" should contains names and the connections between them. Names are represented as strings and are case sensitive. Connections are undirected, so if \"sophia\" is connected with \"nikola\", then it's also correct in reverse.\n", "description": null, "category": "remove", "imports": ["from itertools import chain"]}, {"term": "class", "name": "Friends", "data": "class Friends(connections)\n", "description": null, "category": "remove", "imports": ["from itertools import chain"]}, {"term": "class", "name": "Howitisused:Hereyouwillimplementaclasswithmutablestates.Thisisnotasimplestructurewithacoupleoffunctions,butobjectrepresentationwithmorecomplexstructure.", "data": "How it is used: Here you will implement a class with mutable states. This is not a simple structure with a couple of functions, but object representation with more complex structure.\n", "description": null, "category": "remove", "imports": ["from itertools import chain"]}, {"term": "class", "name": "classFriends:", "data": "class Friends:\n\tdef __init__(self, connections):\n\t\tself.connections = list(connections)\n\n\n\tdef add(self, connection):\n\t\tif connection in self.connections:\n\t\t\treturn False\n\t\telse:\n\t\t\tself.connections.append(connection)\n\t\t\treturn True\n\n\tdef remove(self, connection):\n\t\tif connection in self.connections:\n\t\t\tself.connections.remove(connection)\n\t\t\treturn True\n\t\treturn False\n\n\tdef names(self):\n\t\treturn set(chain(*self.connections))\n\n\tdef connected(self, name):\n\t\ty = []\n\t\t[y.extend(list(i)) for i in self.connections if name in i]\n\t\ty = set(y)\n\t\tif name in set(y):\n\t\t\ty.remove(name)\n\t\treturn y\n\n\n", "description": null, "category": "remove", "imports": ["from itertools import chain"]}], [{"term": "def", "name": "test_remove", "data": "def test_remove(tmp_dir, scm, dvc, run_copy, remove_outs):\n\t(stage1,) = tmp_dir.dvc_gen(\"foo\", \"foo\")\n\tstage2 = run_copy(\"foo\", \"bar\", single_stage=True)\n\tstage3 = run_copy(\"bar\", \"foobar\", name=\"copy-bar-foobar\")\n\n\tassert \"/foo\" in get_gitignore_content()\n\tassert \"/bar\" in get_gitignore_content()\n\tassert \"/foobar\" in get_gitignore_content()\n\n\tfor stage in [stage1, stage2, stage3]:\n\t\tdvc.remove(stage.addressing, outs=remove_outs)\n\t\tout_exists = (out.exists for out in stage.outs)\n\t\tassert stage not in dvc.stage.collect_repo()\n\t\tif remove_outs:\n\t\t\tassert not any(out_exists)\n\t\telse:\n\t\t\tassert all(out_exists)\n\n\tassert not (tmp_dir / \".gitignore\").exists()\n\n", "description": null, "category": "remove", "imports": ["import os", "import pytest", "from dvc.cli import main", "from dvc.fs import system", "from dvc.stage.exceptions import (", "from dvc.utils.fs import remove", "from dvc_objects.errors import ObjectDBError", "from tests.utils import get_gitignore_content"]}, {"term": "def", "name": "test_remove_file_target", "data": "def test_remove_file_target(tmp_dir, dvc):\n\ttmp_dir.dvc_gen(\"foo\", \"foo\")\n\n\twith pytest.raises(\n\t\tStageFileIsNotDvcFileError,\n\t\tmatch=\"'foo' is not a .dvc file. Do you mean 'foo.dvc'?\",\n\t):\n\t\tdvc.remove(\"foo\")\n\n\tdvc.remove(\"foo.dvc\")\n\n", "description": null, "category": "remove", "imports": ["import os", "import pytest", "from dvc.cli import main", "from dvc.fs import system", "from dvc.stage.exceptions import (", "from dvc.utils.fs import remove", "from dvc_objects.errors import ObjectDBError", "from tests.utils import get_gitignore_content"]}, {"term": "def", "name": "test_remove_non_existent_file", "data": "def test_remove_non_existent_file(tmp_dir, dvc):\n\twith pytest.raises(StageFileDoesNotExistError):\n\t\tdvc.remove(\"non_existent_dvc_file.dvc\")\n\twith pytest.raises(StageFileDoesNotExistError):\n\t\tdvc.remove(\"non_existent_stage_name\")\n\n", "description": null, "category": "remove", "imports": ["import os", "import pytest", "from dvc.cli import main", "from dvc.fs import system", "from dvc.stage.exceptions import (", "from dvc.utils.fs import remove", "from dvc_objects.errors import ObjectDBError", "from tests.utils import get_gitignore_content"]}, {"term": "def", "name": "test_remove_broken_symlink", "data": "def test_remove_broken_symlink(tmp_dir, dvc):\n\ttmp_dir.gen(\"foo\", \"foo\")\n\tdvc.odb.local.cache_types = [\"symlink\"]\n\n\t(stage,) = dvc.add(\"foo\")\n\tremove(dvc.odb.local.path)\n\tassert system.is_symlink(\"foo\")\n\n\twith pytest.raises(ObjectDBError):\n\t\tdvc.remove(stage.addressing)\n\tassert os.path.lexists(\"foo\")\n\tassert (tmp_dir / stage.relpath).exists()\n\n\tdvc.remove(stage.addressing, outs=True)\n\tassert not os.path.lexists(\"foo\")\n\tassert not (tmp_dir / stage.relpath).exists()\n\n", "description": null, "category": "remove", "imports": ["import os", "import pytest", "from dvc.cli import main", "from dvc.fs import system", "from dvc.stage.exceptions import (", "from dvc.utils.fs import remove", "from dvc_objects.errors import ObjectDBError", "from tests.utils import get_gitignore_content"]}, {"term": "def", "name": "test_cmd_remove", "data": "def test_cmd_remove(tmp_dir, dvc):\n\tassert main([\"remove\", \"non-existing-dvc-file\"]) == 1\n\n\t(stage,) = tmp_dir.dvc_gen(\"foo\", \"foo\")\n\tassert main([\"remove\", stage.addressing]) == 0\n\tassert not (tmp_dir / stage.relpath).exists()\n\tassert (tmp_dir / \"foo\").exists()\n\n\t(stage,) = tmp_dir.dvc_gen(\"foo\", \"foo\")\n\tassert main([\"remove\", stage.addressing, \"--outs\"]) == 0\n\tassert not (tmp_dir / stage.relpath).exists()\n\tassert not (tmp_dir / \"foo\").exists()\n\n", "description": null, "category": "remove", "imports": ["import os", "import pytest", "from dvc.cli import main", "from dvc.fs import system", "from dvc.stage.exceptions import (", "from dvc.utils.fs import remove", "from dvc_objects.errors import ObjectDBError", "from tests.utils import get_gitignore_content"]}, {"term": "def", "name": "test_cmd_remove_gitignore_single_stage", "data": "def test_cmd_remove_gitignore_single_stage(tmp_dir, scm, dvc, run_copy):\n\tstage = dvc.run(name=\"my\", cmd='echo \"hello\" > out', deps=[], outs=[\"out\"])\n\n\tassert (tmp_dir / \".gitignore\").exists()\n\n\tassert main([\"remove\", stage.addressing]) == 0\n\tassert not (tmp_dir / stage.relpath).exists()\n\tassert not (stage.dvcfile._lockfile).exists()\n\tassert not (tmp_dir / \".gitignore\").exists()\n\n", "description": null, "category": "remove", "imports": ["import os", "import pytest", "from dvc.cli import main", "from dvc.fs import system", "from dvc.stage.exceptions import (", "from dvc.utils.fs import remove", "from dvc_objects.errors import ObjectDBError", "from tests.utils import get_gitignore_content"]}, {"term": "def", "name": "test_cmd_remove_gitignore_multistage", "data": "def test_cmd_remove_gitignore_multistage(tmp_dir, scm, dvc, run_copy):\n\t(stage,) = tmp_dir.dvc_gen(\"foo\", \"foo\")\n\tstage1 = run_copy(\"foo\", \"foo1\", single_stage=True)\n\tstage2 = run_copy(\"foo1\", \"foo2\", name=\"copy-foo1-foo2\")\n\n\tassert (tmp_dir / \".gitignore\").exists()\n\n\tassert main([\"remove\", stage2.addressing]) == 0\n\tassert main([\"remove\", stage1.addressing]) == 0\n\tassert main([\"remove\", stage.addressing]) == 0\n\tassert not (tmp_dir / \".gitignore\").exists()\n", "description": null, "category": "remove", "imports": ["import os", "import pytest", "from dvc.cli import main", "from dvc.fs import system", "from dvc.stage.exceptions import (", "from dvc.utils.fs import remove", "from dvc_objects.errors import ObjectDBError", "from tests.utils import get_gitignore_content"]}], [{"term": "class", "name": "CmdQueueRemove", "data": "class CmdQueueRemove(CmdBase):\n\t\"\"\"Remove exp in queue.\"\"\"\n\n\tdef check_arguments(self):\n\t\tclear_flag = any(\n\t\t\t[\n\t\t\t\tself.args.all,\n\t\t\t\tself.args.queued,\n\t\t\t\tself.args.failed,\n\t\t\t\tself.args.success,\n\t\t\t]\n\t\t)\n\t\tif not (clear_flag ^ bool(self.args.task)):\n\t\t\traise InvalidArgumentError(\n\t\t\t\t\"Either provide an `tasks` argument, or use the \"\n\t\t\t\t\"`--all`, `--queued`, `--failed`, `--success` flag.\"\n\t\t\t)\n\n\tdef run(self):\n\n\t\tself.check_arguments()\n\n\t\tif self.args.all:\n\t\t\tself.args.queued = True\n\t\t\tself.args.failed = True\n\t\t\tself.args.success = True\n\n\t\tif self.args.queued or self.args.failed or self.args.success:\n\t\t\tremoved_list = self.repo.experiments.celery_queue.clear(\n\t\t\t\tsuccess=self.args.success,\n\t\t\t\tqueued=self.args.queued,\n\t\t\t\tfailed=self.args.failed,\n\t\t\t)\n\t\telse:\n\t\t\tremoved_list = self.repo.experiments.celery_queue.remove(\n\t\t\t\trevs=self.args.task,\n\t\t\t)\n\n\t\tif removed_list:\n\t\t\tremoved = \", \".join(removed_list)\n\t\t\tui.write(f\"Removed tasks in queue: {removed}\")\n\t\telse:\n\t\t\tui.write(f\"No tasks found named {self.args.task}\")\n\n\t\treturn 0\n\n", "description": "Remove exp in queue.", "category": "remove", "imports": ["import argparse", "import logging", "from dvc.cli.command import CmdBase", "from dvc.cli.utils import append_doc_link", "from dvc.exceptions import InvalidArgumentError", "from dvc.ui import ui"]}, {"term": "def", "name": "add_parser", "data": "def add_parser(queue_subparsers, parent_parser):\n\n\tQUEUE_REMOVE_HELP = \"Remove queued and completed tasks from the queue.\"\n\tqueue_remove_parser = queue_subparsers.add_parser(\n\t\t\"remove\",\n\t\tparents=[parent_parser],\n\t\tdescription=append_doc_link(QUEUE_REMOVE_HELP, \"queue/remove\"),\n\t\thelp=QUEUE_REMOVE_HELP,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t)\n\tqueue_remove_parser.add_argument(\n\t\t\"--all\",\n\t\taction=\"store_true\",\n\t\thelp=\"Remove all queued and completed tasks from the queue.\",\n\t)\n\tqueue_remove_parser.add_argument(\n\t\t\"--queued\",\n\t\taction=\"store_true\",\n\t\thelp=\"Remove all queued tasks from the queue.\",\n\t)\n\tqueue_remove_parser.add_argument(\n\t\t\"--success\",\n\t\taction=\"store_true\",\n\t\thelp=\"Remove all successful tasks from the queue.\",\n\t)\n\tqueue_remove_parser.add_argument(\n\t\t\"--failed\",\n\t\taction=\"store_true\",\n\t\thelp=\"Remove all failed tasks from the queue.\",\n\t)\n\tqueue_remove_parser.add_argument(\n\t\t\"task\",\n\t\tnargs=\"*\",\n\t\thelp=\"Tasks to remove.\",\n\t\tmetavar=\"\",\n\t)\n\tqueue_remove_parser.set_defaults(func=CmdQueueRemove)\n", "description": null, "category": "remove", "imports": ["import argparse", "import logging", "from dvc.cli.command import CmdBase", "from dvc.cli.utils import append_doc_link", "from dvc.exceptions import InvalidArgumentError", "from dvc.ui import ui"]}], [{"term": "class", "name": "TestTagRemove", "data": "class TestTagRemove(PreprocessorTestsBase):\n\t\"\"\"Contains test functions for tagremove.py\"\"\"\n\n\tdef build_notebook(self):\n\t\t\"\"\"\n\t\tBuild a notebook to have metadata tags for cells, output_areas, and\n\t\tindividual outputs.\n\t\t\"\"\"\n\t\tnotebook = super(TestTagRemove, self).build_notebook()\n\t\t# Add a few empty cells\n\t\tnotebook.cells[0].outputs.extend(\n\t\t\t[nbformat.new_output(\"display_data\",\n\t\t\t\t\t\t\t\t data={'text/plain': 'i'},\n\t\t\t\t\t\t\t\t metadata={'tags': [\"hide_one_output\"]}\n\t\t\t\t\t\t\t\t ),\n\t\t\t ])\n\t\toutputs_to_be_removed = [\n\t\t\tnbformat.new_output(\"display_data\",\n\t\t\t\t\t\t\t\tdata={'text/plain': \"remove_my_output\"}),\n\t\t]\n\t\toutputs_to_be_kept = [\n\t\t\tnbformat.new_output(\"stream\",\n\t\t\t\t\t\t\t\tname=\"stdout\",\n\t\t\t\t\t\t\t\ttext=\"remove_my_output\",\n\t\t\t\t\t\t\t\t),\n\t\t]\n\t\tnotebook.cells.extend(\n\t\t\t[nbformat.new_code_cell(source=\"display('remove_my_output')\",\n\t\t\t\t\t\t\t\t\texecution_count=2,\n\t\t\t\t\t\t\t\t\toutputs=outputs_to_be_removed,\n\t\t\t\t\t\t\t\t\tmetadata={\"tags\": [\"hide_all_outputs\"]}),\n\n\t\t\t nbformat.new_code_cell(source=\"print('remove this cell')\",\n\t\t\t\t\t\t\t\t\texecution_count=3,\n\t\t\t\t\t\t\t\t\toutputs=outputs_to_be_kept,\n\t\t\t\t\t\t\t\t\tmetadata={\"tags\": [\"hide_this_cell\"]}),\n\t\t\t ]\n\t\t\t)\n\n\t\treturn notebook\n\n\tdef build_preprocessor(self):\n\t\t\"\"\"Make an instance of a preprocessor\"\"\"\n\t\tpreprocessor = TagRemovePreprocessor()\n\t\tpreprocessor.enabled = True\n\t\treturn preprocessor\n\n\tdef test_constructor(self):\n\t\t\"\"\"Can a TagRemovePreprocessor be constructed?\"\"\"\n\t\tself.build_preprocessor()\n\n\tdef test_output(self):\n\t\t\"\"\"Test the output of the TagRemovePreprocessor\"\"\"\n\t\tnb = self.build_notebook()\n\t\tres = self.build_resources()\n\t\tpreprocessor = self.build_preprocessor()\n\t\tpreprocessor.remove_cell_tags.add(\"hide_this_cell\")\n\t\tpreprocessor.remove_all_outputs_tags.add('hide_all_outputs')\n\t\tpreprocessor.remove_single_output_tags.add('hide_one_output')\n\n\t\tnb, res = preprocessor(nb, res)\n\n\t\t# checks that we can remove entire cells\n\t\tself.assertEqual(len(nb.cells), 3)\n\n\t\t# checks that we can remove output areas\n\t\tself.assertEqual(len(nb.cells[-1].outputs), 0)\n\n\t\t# checks that we can remove individual outputs\n\t\tself.assertEqual(len(nb.cells[0].outputs), 8)\n", "description": "Contains test functions for tagremove.py", "category": "remove", "imports": ["from nbformat import v4 as nbformat", "from .base import PreprocessorTestsBase", "from ..tagremove import TagRemovePreprocessor"]}], [{"term": "def", "name": "register_removals", "data": "def register_removals(event_handler):\n\tcmd_remover = CommandRemover(event_handler)\n\tcmd_remover.remove(on_event='building-command-table.ses',\n\t\t\t\t\t   remove_commands=['delete-verified-email-address',\n\t\t\t\t\t\t\t\t\t\t'list-verified-email-addresses',\n\t\t\t\t\t\t\t\t\t\t'verify-email-address'])\n\tcmd_remover.remove(on_event='building-command-table.ec2',\n\t\t\t\t\t   remove_commands=['import-instance', 'import-volume'])\n\tcmd_remover.remove(on_event='building-command-table.emr',\n\t\t\t\t\t   remove_commands=['run-job-flow', 'describe-job-flows',\n\t\t\t\t\t\t\t\t\t\t'add-job-flow-steps',\n\t\t\t\t\t\t\t\t\t\t'terminate-job-flows',\n\t\t\t\t\t\t\t\t\t\t'list-bootstrap-actions',\n\t\t\t\t\t\t\t\t\t\t'list-instance-groups',\n\t\t\t\t\t\t\t\t\t\t'set-termination-protection',\n\t\t\t\t\t\t\t\t\t\t'set-visible-to-all-users'])\n\n", "description": null, "category": "remove", "imports": ["import logging", "from functools import partial", "\t\t\t\t\t   remove_commands=['import-instance', 'import-volume'])"]}, {"term": "class", "name": "CommandRemover", "data": "class CommandRemover(object):\n\tdef __init__(self, events):\n\t\tself._events = events\n\n\tdef remove(self, on_event, remove_commands):\n\t\tself._events.register(on_event,\n\t\t\t\t\t\t\t  self._create_remover(remove_commands))\n\n\tdef _create_remover(self, commands_to_remove):\n\t\treturn partial(_remove_commands, commands_to_remove=commands_to_remove)\n\n", "description": null, "category": "remove", "imports": ["import logging", "from functools import partial", "\t\t\t\t\t   remove_commands=['import-instance', 'import-volume'])"]}, {"term": "def", "name": "_remove_commands", "data": "def _remove_commands(command_table, commands_to_remove, **kwargs):\n\t# Hooked up to building-command-table.\n\tfor command in commands_to_remove:\n\t\ttry:\n\t\t\tLOG.debug(\"Removing operation: %s\", command)\n\t\t\tdel command_table[command]\n\t\texcept KeyError:\n\t\t\tLOG.warning(\"Attempting to delete command that does not exist: %s\",\n\t\t\t\t\t\tcommand)\n", "description": null, "category": "remove", "imports": ["import logging", "from functools import partial", "\t\t\t\t\t   remove_commands=['import-instance', 'import-volume'])"]}], [], [{"term": "class", "name": "RemoveParticipantsIqProtocolEntity", "data": "class RemoveParticipantsIqProtocolEntity(ParticipantsGroupsIqProtocolEntity):\n\t'''\n\t\n\n\n\n\n\n\t'''\n\t\n\tdef __init__(self, group_jid, participantList, _id = None):\n\t\tsuper(RemoveParticipantsIqProtocolEntity, self).__init__(group_jid, participantList, \"remove\", _id = _id)\n\t\t\n\t@staticmethod\n\tdef fromProtocolTreeNode(node):\n\t\tentity = super(RemoveParticipantsIqProtocolEntity, RemoveParticipantsIqProtocolEntity).fromProtocolTreeNode(node)\n\t\tentity.__class__ = RemoveParticipantsIqProtocolEntity\n\t\tparticipantList = []\n\t\tfor participantNode in node.getChild(\"remove\").getAllChildren():\n\t\t\tparticipantList.append(participantNode[\"jid\"])\n\t\tentity.setProps(node.getAttributeValue(\"to\"), participantList)\n\t\treturn entity\n", "description": null, "category": "remove", "imports": ["from yowsup.structs import ProtocolEntity, ProtocolTreeNode", "from .iq_groups_participants import ParticipantsGroupsIqProtocolEntity"]}], [{"term": "def", "name": "testShouldRemoveLineTrue", "data": "  def testShouldRemoveLineTrue(self):\n\t# Test case where the prefixed property is before the the colon.\n\tself.assertTrue(css_strip_prefixes.ShouldRemoveLine('-ms-flex: bar;'))\n\tself.assertTrue(css_strip_prefixes.ShouldRemoveLine('-ms-flex:bar;'))\n\tself.assertTrue(css_strip_prefixes.ShouldRemoveLine('  -ms-flex: bar; '))\n\tself.assertTrue(css_strip_prefixes.ShouldRemoveLine('  -ms-flex: bar ;'))\n\n\t# Test case where the prefixed property is after the the colon.\n\tself.assertTrue(\n\t\tcss_strip_prefixes.ShouldRemoveLine(' display: -ms-inline-flexbox;'))\n\n\t# Test lines with comments also get removed.\n\tself.assertTrue(css_strip_prefixes.ShouldRemoveLine(\n\t\t' display: -ms-inline-flexbox; /* */'))\n\tself.assertTrue(css_strip_prefixes.ShouldRemoveLine(\n\t\t' -ms-flex: bar; /* foo */ '))\n", "description": null, "category": "remove", "imports": ["import css_strip_prefixes", "import unittest"]}, {"term": "def", "name": "testShouldRemoveLineFalse", "data": "  def testShouldRemoveLineFalse(self):\n\t# Test cases where the line is not considered a CSS line.\n\tself.assertFalse(css_strip_prefixes.ShouldRemoveLine(''))\n\tself.assertFalse(css_strip_prefixes.ShouldRemoveLine(' -ms-flex'))\n\tself.assertFalse(css_strip_prefixes.ShouldRemoveLine('/* -ms-flex */'))\n\tself.assertFalse(css_strip_prefixes.ShouldRemoveLine(': -ms-flex; '))\n\tself.assertFalse(css_strip_prefixes.ShouldRemoveLine(' : -ms-flex; '))\n\tself.assertFalse(css_strip_prefixes.ShouldRemoveLine('-ms-flex {'))\n\n\t# Test cases where prefixed CSS rules should be unaffected.\n\tcss_to_preserve = [\n\t  '-webkit-appearance',\n\t  '-webkit-box-reflect',\n\t  '-webkit-font-smoothing',\n\t  '-webkit-overflow-scrolling',\n\t  '-webkit-tap-highlight',\n\t]\n\tfor p in css_to_preserve:\n\t  self.assertFalse(css_strip_prefixes.ShouldRemoveLine(p + ': bar;'))\n\n", "description": null, "category": "remove", "imports": ["import css_strip_prefixes", "import unittest"]}], [{"term": "def", "name": "get_wordcount", "data": "def get_wordcount(x):\n\treturn utils._get_wordcount(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_charcount", "data": "def get_charcount(x):\n\treturn utils._get_charcount(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_avg_wordlength", "data": "def get_avg_wordlength(x):\n\treturn utils._get_avg_wordlength(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_stopwordscount", "data": "def get_stopwordscount(x):\n\treturn utils._get_stopwordscount(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_hashtagcount", "data": "def get_hashtagcount(x):\n\treturn utils._get_hashtagcount(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_mentionscount", "data": "def get_mentionscount(x):\n\treturn utils._get_mentionscount(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_digitcount", "data": "def get_digitcount(x):\n\treturn utils._get_digitcount(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_uppercasecount", "data": "def get_uppercasecount(x):\n\treturn utils._get_uppercasecount(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "cont_exp", "data": "def cont_exp(x):\n\treturn utils._cont_exp(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_emails", "data": "def get_emails(x):\n\treturn utils._get_emails(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "remove_emails", "data": "def remove_emails(x):\n\treturn utils._remove_emails(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_urls", "data": "def get_urls(x):\n\treturn utils._get_urls(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "remove_urls", "data": "def remove_urls(x):\n\treturn utils._remove_urls(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_retweets", "data": "def get_retweets(x):\n\treturn utils._get_retweets(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "remove_rt", "data": "def remove_rt(x):\n\treturn utils._remove_rt(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "remove_specialchars", "data": "def remove_specialchars(x):\n\treturn utils._remove_specialchars(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "remove_HTMLtags", "data": "def remove_HTMLtags(x):\n\treturn utils._remove_HTMLtags(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "remove_accentedchars", "data": "def remove_accentedchars(x):\n\treturn utils._remove_accentedchars(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "remove_stopwords", "data": "def remove_stopwords(x):\n\treturn utils._remove_stopwords(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "make_base", "data": "def make_base(x):\n\treturn utils._make_base(x)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "get_value_counts", "data": "def get_value_counts(df, col):\n\treturn utils._get_value_counts(df, col)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "remove_commonwords", "data": "def remove_commonwords(x, freq, n=20):\n\treturn utils._remove_commonwords(x, freq, n)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "remove_rarewords", "data": "def remove_rarewords(x, freq, n=20):\n\treturn utils._remove_rarewords(x, freq, n)\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}, {"term": "def", "name": "spelling_correction", "data": "def spelling_correction(x):\n", "description": null, "category": "remove", "imports": ["from preprocess_text_pashen1190 import utils"]}], [{"term": "def", "name": "remove_elements_from_list", "data": "def remove_elements_from_list(origin, remove):\n\tfor item in remove:\n\t\torigin.remove(item)\n\n\treturn origin\n\n", "description": null, "category": "remove", "imports": []}], [{"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\tpass\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [],\n\t  'flags_ready': False\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ bytes( b'-c' ), '-c', bytes( b'-foo' ), '-bar' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-x', 'c' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-x', 'c++' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-x', 'c' ],\n\t  'do_cache': False\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-x', 'c++' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-x', 'c' ],\n\t  'do_cache': True\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-x', 'c++' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-x', 'c', '-I', 'header' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-x', 'c', '-I', 'header' ],\n\t  'include_paths_relative_to_dir': '/working_dir/'\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall',\n\t\t\t\t '-isysroot/path/to/first/sys/root',\n\t\t\t\t '-isysroot', '/path/to/second/sys/root/',\n\t\t\t\t '--sysroot=/path/to/third/sys/root',\n\t\t\t\t '--sysroot', '/path/to/fourth/sys/root' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall', '-x', 'c', '-xobjective-c++' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall', '-x', 'c', '-xc++' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall', '-xc++', '-xc' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall', '-stdlib=libc++', '-stdlib=libstdc++' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall', '-nostdinc++' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall', '-nostdinc' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [ '-Wall', '-nobuiltininc' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [],\n\t  'override_filename': 'changed:' + kwargs[ 'filename' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [],\n\t  'override_filename': kwargs[ 'filename' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [],\n\t  'override_filename': None\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [],\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [],\n\t  'override_filename': ''\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "Settings", "data": "  def Settings( **kwargs ):\n\treturn {\n\t  'flags': [],\n\t  'override_filename': '0'\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "FlagsForFile", "data": "  def FlagsForFile( filename, **kwargs ):\n\treturn {\n\t  'flags': [ '-x', 'c' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}, {"term": "def", "name": "FlagsForFile", "data": "  def FlagsForFile( filename ):\n\treturn {\n\t  'flags': [ '-x', 'c' ]\n\t}\n", "description": null, "category": "remove", "imports": ["import contextlib", "import os", "import pytest", "from hamcrest import ( assert_that,", "from unittest.mock import patch, MagicMock", "from types import ModuleType", "from ycmd.completers.cpp import flags", "from ycmd.completers.cpp.flags import ShouldAllowWinStyleFlags, INCLUDE_FLAGS", "from ycmd.tests.test_utils import ( MacOnly, TemporaryTestDir, WindowsOnly,", "from ycmd.utils import CLANG_RESOURCE_DIR", "from ycmd.responses import NoExtraConfDetected"]}], [], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('product', '0032_auto_20210324_1439'),\n\t]\n\n\toperations = [\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='productattributes',\n\t\t\told_name='is_import',\n\t\t\tnew_name='is_main',\n\t\t),\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='productattributes',\n\t\t\told_name='product_code',\n\t\t\tnew_name='key',\n\t\t),\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='productattributes',\n\t\t\told_name='size',\n\t\t\tnew_name='label',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='brand',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='category',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='color',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='description',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='is_active',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='name',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='price',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='quantity',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='variation_image',\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='productattributes',\n\t\t\tname='value',\n\t\t\tfield=models.CharField(default=123, max_length=255),\n\t\t\tpreserve_default=False,\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from django.db import migrations, models", "\t\t\told_name='is_import',"]}], [{"term": "def", "name": "barrow_cohort_present", "data": "def barrow_cohort_present(self):\n\n\t\"\"\"\n\tThe purpose of this module is to create a dictionary with a list of\n\teach cohort in each model element.  This list will be used for lake expansion/\n\tterrestrial losses, and to shorten the processing time (only process\n\tcohorts present)\n\t\"\"\"\n\n\tprint '   Creating a list of thermokarst susceptible cohorts '\n\n\n\tself.cohort_list = {}\n\tself.land_cohorts = {}\n\t\n\t#list = ('Wet_NPG', 'Wet_LCP', 'Wet_CLC', 'Wet_FCP', 'Wet_HCP', 'Lakes', 'Ponds')\n\t\n\tfor i in range(0, self.ATTM_nrows * self.ATTM_ncols):\n\t\t#cohorts = ['Wet_NPG', 'Wet_LCP', 'Wet_CLC', 'Wet_FCP', 'Wet_HCP', 'Lakes', 'Ponds']\n\t\t#land_cohorts = ['Wet_NPG', 'Wet_LCP', 'Wet_CLC', 'Wet_FCP', 'Wet_HCP']\n\n\t\tcohorts = ['Meadow_WT_Y', 'Meadow_WT_M', 'Meadow_WT_O', \\\n\t\t\t\t   'LCP_WT_Y', 'LCP_WT_M', 'LCP_WT_O', \\\n\t\t\t\t   'CLC_WT_Y', 'CLC_WT_M', 'CLC_WT_O', \\\n\t\t\t\t   'FCP_WT_Y', 'FCP_WT_M', 'FCP_WT_O', \\\n\t\t\t\t   'HCP_WT_Y', 'HCP_WT_M', 'HCP_WT_O', \\\n\t\t\t\t   'Rivers_WT_Y', 'Rivers_WT_M', 'Rivers_WT_O', \\\n\t\t\t\t   'LargeLakes_WT_Y', 'LargeLakes_WT_M', 'LargeLakes_WT_O', \\\n\t\t\t\t   'MediumLakes_WT_Y', 'MediumLakes_WT_M', 'MediumLakes_WT_O', \\\n\t\t\t\t   'SmallLakes_WT_Y', 'SmallLakes_WT_M', 'SmallLakes_WT_O', \\\n\t\t\t\t   'Ponds_WT_Y', 'Ponds_WT_M', 'Ponds_WT_O', \\\n\t\t\t\t   'Urban_WT', 'NoData_WT_O', \\\n\t\t\t\t   'CoastalWaters_WT_O', \\\n\t\t\t\t   'DrainedSlope_WT_Y', 'DrainedSlope_WT_M', 'DrainedSlope_WT_O', \\\n\t\t\t\t   'SandDunes_WT_Y', 'SandDunes_WT_M', 'SandDunes_WT_O', \\\n\t\t\t\t   'SaturatedBarrens_WT_Y', 'SaturatedBarrens_WT_M', 'SaturatedBarrens_WT_O', \\\n\t\t\t\t   'Shrubs_WT_O' \\\n\t\t\t\t   ]\n\n\t\tland_cohorts = ['Meadow_WT_Y', 'Meadow_WT_M', 'Meadow_WT_O', \\\n\t\t\t\t\t\t'LCP_WT_Y', 'LCP_WT_M', 'LCP_WT_O', \\\n\t\t\t\t\t\t'CLC_WT_Y', 'CLC_WT_M', 'CLC_WT_O', \\\n\t\t\t\t\t\t'FCP_WT_Y', 'FCP_WT_M', 'FCP_WT_O', \\\n\t\t\t\t\t\t'HCP_WT_Y', 'HCP_WT_M', 'HCP_WT_O', \\\n\t\t\t\t\t\t'DrainedSlope_WT_Y', 'DrainedSlope_WT_M', 'DrainedSlope_WT_O', \\\n\t\t\t\t\t\t'SandDunes_WT_Y', 'SandDunes_WT_M', 'SandDunes_WT_O', \\\n\t\t\t\t\t\t'SaturatedBarrens_WT_Y', 'SaturatedBarrens_WT_M', 'SaturatedBarrens_WT_O', \\\n\t\t\t\t\t\t'Shrubs_WT_O' , 'Urban_WT', 'NoData_WT_O' \\\n\t\t\t\t\t\t]\n\t\t\n\t\tif self.ATTM_Meadow_WT_Y[i] == 0.0 :\n\t\t\tcohorts.remove('Meadow_WT_Y')\n\t\t\tland_cohorts.remove('Meadow_WT_Y')\n\t\tif self.ATTM_Meadow_WT_M[i] == 0.0 :\n\t\t\tcohorts.remove('Meadow_WT_M')\n\t\t\tland_cohorts.remove('Meadow_WT_M')\n\t\tif self.ATTM_Meadow_WT_Y[i] == 0.0 :\n\t\t\tcohorts.remove('Meadow_WT_O')\n\t\t\tland_cohorts.remove('Meadow_WT_O')\n\t\tif self.ATTM_LCP_WT_Y[i] == 0 :\n\t\t\tcohorts.remove('LCP_WT_Y')\n\t\t\tland_cohorts.remove('LCP_WT_Y')\n\t\tif self.ATTM_LCP_WT_M[i] == 0.0 :\n\t\t\tcohorts.remove('LCP_WT_M')\n\t\t\tland_cohorts.remove('LCP_WT_M')\n\t\tif self.ATTM_LCP_WT_O[i] == 0.0 :\n\t\t\tcohorts.remove('LCP_WT_O')\n\t\t\tland_cohorts.remove('LCP_WT_O')\n\t\tif self.ATTM_CLC_WT_Y[i] == 0.0 :\n\t\t\tcohorts.remove('CLC_WT_Y')\n\t\t\tland_cohorts.remove('CLC_WT_Y')\n\t\tif self.ATTM_CLC_WT_M[i] == 0.0 :\n\t\t\tcohorts.remove('CLC_WT_M')\n\t\t\tland_cohorts.remove('CLC_WT_M')\n\t\tif self.ATTM_CLC_WT_O[i] == 0.0:\n\t\t\tcohorts.remove('CLC_WT_O')\n\t\t\tland_cohorts.remove('CLC_WT_O')\n\t\tif self.ATTM_HCP_WT_Y[i] == 0.0:\n\t\t\tcohorts.remove('HCP_WT_Y')\n\t\t\tland_cohorts.remove('HCP_WT_Y')\n\t\tif self.ATTM_HCP_WT_M[i] == 0.0 :\n\t\t\tcohorts.remove('HCP_WT_M')\n\t\t\tland_cohorts.remove('HCP_WT_M')\n\t\tif self.ATTM_HCP_WT_O[i] == 0.0 :\n\t\t\tcohorts.remove('HCP_WT_O')\n\t\t\tland_cohorts.remove('HCP_WT_O')\n\t\tif self.ATTM_DrainedSlope_WT_Y[i] == 0:\n\t\t\tcohorts.remove('DrainedSlope_WT_Y')\n\t\t\tland_cohorts.remove('DrainedSlope_WT_Y')\n\t\tif self.ATTM_DrainedSlope_WT_M[i] == 0:\n\t\t\tcohorts.remove('DrainedSlope_WT_M')\n\t\t\tland_cohorts.remove('DrainedSlope_WT_M')\n\t\tif self.ATTM_DrainedSlope_WT_O[i] == 0.0:\n\t\t\tcohorts.remove('DrainedSlope_WT_O')\n\t\t\tland_cohorts.remove('DrainedSlope_WT_O')\n\t\tif self.ATTM_SandDunes_WT_Y[i] == 0.0:\n\t\t\tcohorts.remove('SandDunes_WT_Y')\n\t\t\tland_cohorts.remove('SandDunes_WT_Y')\n\t\tif self.ATTM_SandDunes_WT_M[i] == 0.0 :\n\t\t\tcohorts.remove('SandDunes_WT_M')\n\t\t\tland_cohorts.remove('SandDunes_WT_M')\n\t\tif self.ATTM_SandDunes_WT_O[i] == 0.0 :\n\t\t\tcohorts.remove('SandDunes_WT_O')\n\t\t\tland_cohorts.remove('SandDunes_WT_O')\n\t\tif self.ATTM_SaturatedBarrens_WT_Y[i] == 0.0 :\n\t\t\tcohorts.remove('SaturatedBarrens_WT_Y')\n\t\t\tland_cohorts.remove('SaturatedBarrens_WT_Y')\n\t\tif self.ATTM_SaturatedBarrens_WT_M[i] == 0.0 :\n\t\t\tcohorts.remove('SaturatedBarrens_WT_M')\n\t\t\tland_cohorts.remove('SaturatedBarrens_WT_M')\n\t\tif self.ATTM_SaturatedBarrens_WT_O[i] == 0.0 :\n\t\t\tcohorts.remove('SaturatedBarrens_WT_O')\n\t\t\tland_cohorts.remove('SaturatedBarrens_WT_O')\n\t\tif self.ATTM_Shrubs_WT_O[i] == 0.0 :\n\t\t\tcohorts.remove('Shrubs_WT_O')\n\t\t\tland_cohorts.remove('Shrubs_WT_O')\n\t\tif self.ATTM_Urban_WT[i] == 0.0 :\n\t\t\tcohorts.remove('Urban_WT')\n\t\t\tland_cohorts.remove('Urban_WT')\n\t\tif self.ATTM_LargeLakes_WT_Y[i] == 0.0 : cohorts.remove('LargeLakes_WT_Y')\n\t\tif self.ATTM_LargeLakes_WT_M[i] == 0.0 : cohorts.remove('LargeLakes_WT_M')\n\t\tif self.ATTM_LargeLakes_WT_O[i] == 0.0 : cohorts.remove('LargeLakes_WT_O')\n\t\tif self.ATTM_MediumLakes_WT_Y[i] == 0.0 : cohorts.remove('MediumLakes_WT_Y')\n\t\tif self.ATTM_MediumLakes_WT_M[i] == 0.0 : cohorts.remove('MediumLakes_WT_M')\n\t\tif self.ATTM_MediumLakes_WT_O[i] == 0.0 : cohorts.remove('MediumLakes_WT_O')\n\t\tif self.ATTM_SmallLakes_WT_Y[i] == 0.0 : cohorts.remove('SmallLakes_WT_Y')\n\t\tif self.ATTM_SmallLakes_WT_M[i] == 0.0 : cohorts.remove('SmallLakes_WT_M')\n\t\tif self.ATTM_SmallLakes_WT_O[i] == 0.0 : cohorts.remove('SmallLakes_WT_O')\n\t\tif self.ATTM_Ponds_WT_Y[i] == 0.0 : cohorts.remove('Ponds_WT_Y')\n\t\tif self.ATTM_Ponds_WT_M[i] == 0.0 : cohorts.remove('Ponds_WT_M')\n\t\tif self.ATTM_Ponds_WT_O[i] == 0.0 : cohorts.remove('Ponds_WT_O')\n\t\tif self.ATTM_Rivers_WT_Y[i] == 0.0 : cohorts.remove('Rivers_WT_Y')\n\t\tif self.ATTM_Rivers_WT_M[i] == 0.0 : cohorts.remove('Rivers_WT_M')\n\t\tif self.ATTM_Rivers_WT_O[i] == 0.0 : cohorts.remove('Rivers_WT_O')\n\t\tif self.ATTM_CoastalWaters_WT_O[i] == 0.0 : cohorts.remove('CoastalWaters_WT_O')\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t#if self.ATTM_Wet_NPG[i] == 0.0 :\n\t\t#\tcohorts.remove('Wet_NPG')\n\t\t#\tland_cohorts.remove('Wet_NPG')\n\t\t#if self.ATTM_Wet_LCP[i] == 0.0 :\n\t\t#\tcohorts.remove('Wet_LCP')\n\t\t#\tland_cohorts.remove('Wet_LCP')\n\t\t#if self.ATTM_Wet_CLC[i] == 0.0 :\n\t\t#\tcohorts.remove('Wet_CLC')\n\t\t#\tland_cohorts.remove('Wet_CLC')\n\t\t#if self.ATTM_Wet_FCP[i] == 0.0 :\n\t\t#\tcohorts.remove('Wet_FCP')\n\t\t#\tland_cohorts.remove('Wet_FCP')\n\t\t#if self.ATTM_Wet_HCP[i] == 0.0 :\n\t\t#\tcohorts.remove('Wet_HCP')\n\t\t#\tland_cohorts.remove('Wet_HCP')\n\t\t#if self.ATTM_Lakes[i]   == 0.0 : cohorts.remove('Lakes')\n\t\t#if self.ATTM_Ponds[i]   == 0.0 : cohorts.remove('Ponds')\n\n\t\tself.cohort_list[i] = cohorts\n\t\tself.land_cohorts[i] = land_cohorts\n\n\tprint '\tdone. \\n  '\n", "description": "\n\tThe purpose of this module is to create a dictionary with a list of\n\teach cohort in each model element.  This list will be used for lake expansion/\n\tterrestrial losses, and to shorten the processing time (only process\n\tcohorts present)\n\t", "category": "remove", "imports": ["import numpy as np", "import gdal, os, sys, glob, random", "import pylab as pl"]}, {"term": "def", "name": "tanana_cohort_present", "data": "def tanana_cohort_present(self):\n\n\t\"\"\"\n\tThe purpose of this module is to create a dictionary with a list of\n\teach cohort in each model element.  This list will be used for lake expansion/\n\tterrestrial losses, and to shorten the processing time (only process\n\tcohorts present)\n\t\"\"\"\n\n\tprint '   Creating a list of thermokarst susceptible cohorts '\n\n\n\tself.cohort_list = {}\n\tself.land_cohorts = {}\n\t\n\t#list = ('Wet_NPG', 'Wet_LCP', 'Wet_CLC', 'Wet_FCP', 'Wet_HCP', 'Lakes', 'Ponds')\n\t\n\tfor i in range(0, self.ATTM_nrows * self.ATTM_ncols):\n\t\tcohorts = ['TF_OB', 'TF_YB', 'TF_OF', 'TF_YF', 'TF_Con_PP', 'TF_Dec_PP', 'TF_TL']\n\t\tland_cohorts = ['TF_OB', 'TF_YB', 'TF_OF', 'TF_YF', 'TF_Con_PP', 'TF_Dec_PP',  'TF_TL']\n\t\t\n\t\tif self.ATTM_TF_OB[i] == 0.0 :\n\t\t\tcohorts.remove('TF_OB')\n\t\t\tland_cohorts.remove('TF_OB')\n\t\tif self.ATTM_TF_YB[i] == 0.0 :\n\t\t\tcohorts.remove('TF_YB')\n\t\t\tland_cohorts.remove('TF_YB')\n\t\tif self.ATTM_TF_OF[i] == 0.0 :\n\t\t\tcohorts.remove('TF_OF')\n\t\t\tland_cohorts.remove('TF_OF')\n\t\tif self.ATTM_TF_YF[i] == 0.0 :\n\t\t\tcohorts.remove('TF_YF')\n\t\t\tland_cohorts.remove('TF_YF')\n\t\tif self.ATTM_TF_Con_PP[i] == 0.0 :\n\t\t\tcohorts.remove('TF_Con_PP')\n\t\t\tland_cohorts.remove('TF_Con_PP')\n\t\tif self.ATTM_TF_Dec_PP[i] == 0.0 :\n\t\t\tcohorts.remove('TF_Dec_PP')\n\t\t\tland_cohorts.remove('TF_Dec_PP')\n\t\tif self.ATTM_TF_TL[i] == 0.0:\n\t\t\tcohorts.remove('TF_TL')\n\t\t\tland_cohorts.remove('TF_TL')\n\n\t\tself.cohort_list[i] = cohorts\n\t\tself.land_cohorts[i] = land_cohorts\n\n\tprint '\tdone. \\n  '\n", "description": "\n\tThe purpose of this module is to create a dictionary with a list of\n\teach cohort in each model element.  This list will be used for lake expansion/\n\tterrestrial losses, and to shorten the processing time (only process\n\tcohorts present)\n\t", "category": "remove", "imports": ["import numpy as np", "import gdal, os, sys, glob, random", "import pylab as pl"]}, {"term": "def", "name": "yukon_cohort_present", "data": "def yukon_cohort_present(self):\n\n\t# Created 6 July 2016. Bolton\n\n\t\"\"\" \n\tThe purpose of this module is to create a dictionary with a list of \n\teach cohort in each model element. This list will be used for lake\n\texpansion/terrestrial cohort losses, and to short the processing time \n\t(process only those cohorts present in a model element.)\n\t\n\t\"\"\"\n\tprint '\tCreating a list of thermokarst susceptible cohorts.'\n\n\tself.cohort_list = {}\n\tself.land_cohorts = {}\n\n\t#list = Barren_Yukon, Bog_Yukon, DeciduousForest_Yukon, DwarfShrub_Yukon, EvergreenForest_Yukon,\n\t#\t   Fen_Yukon, Lake_Yukon, Pond_Yukon, River_Yukon, ShrubScrub_Yukon, Unclassified_Yukon\n\n\tfor i in range(0, self.ATTM_nrows * self.ATTM_ncols):\n\t\t\n\t\tcohorts = ['Barren_Yukon', 'Bog_Yukon', 'DeciduousForest_Yukon', 'DwarfShrub_Yukon', \\\n\t\t\t\t   'EvergreenForest_Yukon', 'Fen_Yukon', 'Lake_Yukon', 'Pond_Yukon', 'River_Yukon', \\\n\t\t\t\t   'ShrubScrub_Yukon', 'Unclassified_Yukon']\n\n\t\tland_cohorts = ['Barren_Yukon', 'Bog_Yukon', 'DeciduousForest_Yukon', 'DwarfShrub_Yukon', \\\n\t\t\t\t\t\t'EvergreenForest_Yukon', 'Fen_Yukon', 'ShrubScrub_Yukon', 'Unclassified_Yukon']\n\n\t\tif self.ATTM_Barren_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('Barren_Yukon')\n\t\t\tland_cohorts.remove('Barren_Yukon')\n\t\tif self.ATTM_Bog_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('Bog_Yukon')\n\t\t\tland_cohorts.remove('Bog_Yukon')\n\t\tif self.ATTM_DeciduousForest_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('DeciduousForest_Yukon')\n\t\t\tland_cohorts.remove('DeciduousForest_Yukon')\n\t\tif self.ATTM_DwarfShrub_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('DwarfShrub_Yukon')\n\t\t\tland_cohorts.remove('DwarfShrub_Yukon')\n\t\tif self.ATTM_EvergreenForest_Yukon[i]== 0.0:\n\t\t\tcohorts.remove('EvergreenForest_Yukon')\n\t\t\tland_cohorts.remove('EvergreenForest_Yukon')\n\t\tif self.ATTM_Fen_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('Fen_Yukon')\n\t\t\tland_cohorts.remove('Fen_Yukon')\n\t\tif self.ATTM_Lake_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('Lake_Yukon')\n\t\tif self.ATTM_Pond_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('Pond_Yukon')\n\t\tif self.ATTM_River_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('River_Yukon')\n\t\tif self.ATTM_ShrubScrub_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('ShrubScrub_Yukon')\n\t\t\tland_cohorts.remove('ShrubScrub_Yukon')\n\t\tif self.ATTM_Unclassified_Yukon[i] == 0.0:\n\t\t\tcohorts.remove('Unclassified_Yukon')\n\t\t\tland_cohorts.remove('Unclassified_Yukon')\n\n\t\tself.cohort_list[i] = cohorts\n\t\tself.land_cohorts[i] = land_cohorts\n\t\n\tprint '\tdone. \\n  '\n", "description": " \n\tThe purpose of this module is to create a dictionary with a list of \n\teach cohort in each model element. This list will be used for lake\n\texpansion/terrestrial cohort losses, and to short the processing time \n\t(process only those cohorts present in a model element.)\n\t\n\t", "category": "remove", "imports": ["import numpy as np", "import gdal, os, sys, glob, random", "import pylab as pl"]}], [{"term": "class", "name": "UniFiBase", "data": "class UniFiBase(Entity):\n\t\"\"\"UniFi entity base class.\"\"\"\n\n\tDOMAIN = \"\"\n\tTYPE = \"\"\n\n\tdef __init__(self, item, controller) -> None:\n\t\t\"\"\"Set up UniFi Network entity base.\n\n\t\tRegister mac to controller entities to cover disabled entities.\n\t\t\"\"\"\n\t\tself._item = item\n\t\tself.controller = controller\n\t\tself.controller.entities[self.DOMAIN][self.TYPE].add(self.key)\n\n\t@property\n\tdef key(self) -> Any:\n\t\t\"\"\"Return item key.\"\"\"\n\t\treturn self._item.mac\n\n\tasync def async_added_to_hass(self) -> None:\n\t\t\"\"\"Entity created.\"\"\"\n\t\t_LOGGER.debug(\n\t\t\t\"New %s entity %s (%s)\",\n\t\t\tself.TYPE,\n\t\t\tself.entity_id,\n\t\t\tself.key,\n\t\t)\n\t\tfor signal, method in (\n\t\t\t(self.controller.signal_reachable, self.async_signal_reachable_callback),\n\t\t\t(self.controller.signal_options_update, self.options_updated),\n\t\t\t(self.controller.signal_remove, self.remove_item),\n\t\t):\n\t\t\tself.async_on_remove(async_dispatcher_connect(self.hass, signal, method))\n\t\tself._item.register_callback(self.async_update_callback)\n\n\tasync def async_will_remove_from_hass(self) -> None:\n\t\t\"\"\"Disconnect object when removed.\"\"\"\n\t\t_LOGGER.debug(\n\t\t\t\"Removing %s entity %s (%s)\",\n\t\t\tself.TYPE,\n\t\t\tself.entity_id,\n\t\t\tself.key,\n\t\t)\n\t\tself._item.remove_callback(self.async_update_callback)\n\t\tself.controller.entities[self.DOMAIN][self.TYPE].remove(self.key)\n\n\t@callback\n\tdef async_signal_reachable_callback(self) -> None:\n\t\t\"\"\"Call when controller connection state change.\"\"\"\n\t\tself.async_update_callback()\n\n\t@callback\n\tdef async_update_callback(self) -> None:\n\t\t\"\"\"Update the entity's state.\"\"\"\n\t\t_LOGGER.debug(\n\t\t\t\"Updating %s entity %s (%s)\",\n\t\t\tself.TYPE,\n\t\t\tself.entity_id,\n\t\t\tself.key,\n\t\t)\n\t\tself.async_write_ha_state()\n\n\tasync def options_updated(self) -> None:\n\t\t\"\"\"Config entry options are updated, remove entity if option is disabled.\"\"\"\n\t\traise NotImplementedError\n\n\tasync def remove_item(self, keys: set) -> None:\n\t\t\"\"\"Remove entity if key is part of set.\"\"\"\n\t\tif self.key not in keys:\n\t\t\treturn\n\n\t\tif self.registry_entry:\n\t\t\ter.async_get(self.hass).async_remove(self.entity_id)\n\t\telse:\n\t\t\tawait self.async_remove(force_remove=True)\n\n\t@property\n\tdef should_poll(self) -> bool:\n\t\t\"\"\"No polling needed.\"\"\"\n\t\treturn False\n", "description": "UniFi entity base class.", "category": "remove", "imports": ["import logging", "from typing import Any", "from homeassistant.core import callback", "from homeassistant.helpers import entity_registry as er", "from homeassistant.helpers.dispatcher import async_dispatcher_connect", "from homeassistant.helpers.entity import Entity"]}], [{"term": "def", "name": "top_k_top_p_filtering", "data": "def top_k_top_p_filtering(\n\tlogits: torch.FloatTensor,\n\ttop_k: int = 0,\n\ttop_p: float = 1.0,\n\tfilter_value: float = -float(\"Inf\"),\n", "description": null, "category": "remove", "imports": ["import torch", "import torch.nn.functional as F"]}], [{"term": "def", "name": "clean", "data": "def clean(value):\n\t\"\"\"\n\tReturn value, made into a form legal for locations\n\t\"\"\"\n\treturn re.sub('_+', '_', INVALID_CHARS.sub('_', value))\n\n", "description": "\n\tReturn value, made into a form legal for locations\n\t", "category": "remove", "imports": ["import os", "import fnmatch", "import re", "import sys", "from lxml import etree", "from collections import defaultdict"]}, {"term": "def", "name": "clean_unique", "data": "def clean_unique(category, name):\n\tcleaned = clean(name)\n\tif cleaned not in used_names[category]:\n\t\tused_names[category].add(cleaned)\n\t\treturn cleaned\n\tx = 1\n\twhile cleaned + str(x) in used_names[category]:\n\t\tx += 1\n\n\t# Found one!\n\tcleaned = cleaned + str(x)\n\tused_names[category].add(cleaned)\n\treturn cleaned\n\n", "description": null, "category": "remove", "imports": ["import os", "import fnmatch", "import re", "import sys", "from lxml import etree", "from collections import defaultdict"]}, {"term": "def", "name": "cleanup", "data": "def cleanup(filepath, remove_meta):\n\t# Keys that are exported to the policy file, and so\n\t# can be removed from the xml afterward\n\tto_remove = ('format', 'display_name',\n\t\t\t\t 'graceperiod', 'showanswer', 'rerandomize',\n\t\t\t\t 'start', 'due', 'graded', 'hide_from_toc',\n\t\t\t\t 'ispublic', 'xqa_key')\n\n\ttry:\n\t\tprint \"Cleaning {0}\".format(filepath)\n\t\twith open(filepath) as f:\n\t\t\tparser = etree.XMLParser(remove_comments=False)\n\t\t\txml = etree.parse(filepath, parser=parser)\n\texcept:\n\t\tprint \"Error parsing file {0}\".format(filepath)\n\t\treturn\n\n\tfor node in xml.iter(tag=etree.Element):\n\t\tattrs = node.attrib\n\t\tif 'url_name' in attrs:\n\t\t\tused_names[node.tag].add(attrs['url_name'])\n\t\tif 'name' in attrs:\n\t\t\t# Replace name with an identical display_name, and a unique url_name\n\t\t\tname = attrs['name']\n\t\t\tattrs['display_name'] = name\n\t\t\tattrs['url_name'] = clean_unique(node.tag, name)\n\t\t\tdel attrs['name']\n\n\t\tif 'url_name' in attrs and 'slug' in attrs:\n\t\t\tprint \"WARNING: {0} has both slug and url_name\".format(node)\n\n\t\tif ('url_name' in attrs and 'filename' in attrs and\n\t\t\tlen(attrs) == 2 and attrs['url_name'] == attrs['filename']):\n\t\t\t# This is a pointer tag in disguise.  Get rid of the filename.\n\t\t\tprint 'turning {0}.{1} into a pointer tag'.format(node.tag, attrs['url_name'])\n\t\t\tdel attrs['filename']\n\n\t\tif remove_meta:\n\t\t\tfor attr in to_remove:\n\t\t\t\tif attr in attrs:\n\t\t\t\t\tdel attrs[attr]\n\n\n\twith open(filepath, \"w\") as f:\n\t\tf.write(etree.tostring(xml))\n\n", "description": null, "category": "remove", "imports": ["import os", "import fnmatch", "import re", "import sys", "from lxml import etree", "from collections import defaultdict"]}, {"term": "def", "name": "find_replace", "data": "def find_replace(directory, filePattern, remove_meta):\n\tfor path, dirs, files in os.walk(os.path.abspath(directory)):\n\t\tfor filename in fnmatch.filter(files, filePattern):\n\t\t\tfilepath = os.path.join(path, filename)\n\t\t\tcleanup(filepath, remove_meta)\n\n", "description": null, "category": "remove", "imports": ["import os", "import fnmatch", "import re", "import sys", "from lxml import etree", "from collections import defaultdict"]}, {"term": "def", "name": "main", "data": "def main(args):\n\tusage = \"xml_cleanup [dir] [remove-meta]\"\n\tn = len(args)\n\tif n < 1 or n > 2 or (n == 2 and args[1] != 'remove-meta'):\n\t\tprint usage\n\t\treturn\n\n\tremove_meta = False\n\tif n == 2:\n\t\tremove_meta = True\n\n\tfind_replace(args[0], '*.xml', remove_meta)\n\n", "description": null, "category": "remove", "imports": ["import os", "import fnmatch", "import re", "import sys", "from lxml import etree", "from collections import defaultdict"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('blog', '0015_auto_20200728_1250'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_APD_personel',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_GMT_peralatan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_GMT_personel',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_dekontaminasi_dan_disinfeksi_peralatan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_general_safety',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_identifikasi_hazard',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_identifikasi_perencanaan_dan_respon_keadaan_darurat',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_inaktivasi_bahan_biologik_dan_bahan_berbahaya_lainnya',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_infrastruktur_dan_operasional',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_inspeksi_dan_audit',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_inventarisasi_informasi_dan_pencatatan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_investigasi_kecelakaan_dan_insiden',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_kalibrasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_keamanan_fisik_dan_pengendalian_personel',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_keamanan_informasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_kedaruratan_medik',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_ketersediaan_peralatan_keselamatan_untuk_tanggap_darurat',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_komunikasi_dan_konsultasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_kontraktor_dan_suplier_purchasing',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_latar_belakang_sdm',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_manajemen_pemeliharaan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_monitoring_peralatan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pelabelan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pemeliharaan_dan_dekontaminasi_reusable_apd',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pencatatan_dokumen_dan_pengendalian_dokumen',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pengadaan_alat_pelindung_diri',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pengembangan_berkelanjutan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pengendalian_dan_monitoring',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pengendalian_ketidaksesuaian_dan_perbaikan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pengendalian_personel',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pengendalian_risiko',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_pengolahan_limbah_dan_bahan_berbahaya_biologi_kimia',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_penyimpanan_bahan_biologik_dan_bahan_berbahaya_lainnya',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_peran_dan_tanggung_jawab',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_perencanaan_dan_program_kerja',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_perencanaan_desain_dan_verifikasi_mencakup_commissioning_dan_decommisioning',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_persyaratan_legal_aturan_atau_izin',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_perubahan_terkait_manajemen_biorisiko',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_program_kesehatan_kerja',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_prosedur_penerimaan_dan_pengiriman_bahan_biologik_dan_bahan_berbahaya_lainnya',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_proses_metode_dan_prosedur',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_rekrutmen_pelatihan_dan_kompetensi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_rencana_tanggap_darurat_menghadapi_kejadian_luar_duga',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_review_dan_perbaikan_manajemen_biorisiko',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_ruang_lingkup_dan_penjadwalan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_sertifikasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_simulasi_dan_pelatihan_tanggap_darurat',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_suksesi_dan_eksklusi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_tanggung_jawab_dan_wewenang',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_transfer_bahan_biologik_dan_bahan_berbahaya_lainnya',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_vaksinasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='keterangan_validasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_10',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_11',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_12',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_13',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_14',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_15',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_16',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_17',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_18',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_19',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_20',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_21',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_22',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_23',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_24',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_25',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_26',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_27',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_28',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_29',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_30',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_31',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_32',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_33',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_34',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_35',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_36',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_37',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_38',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_39',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_40',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_41',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_42',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_43',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_44',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_45',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_46',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_47',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_48',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_49',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_50',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_51',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_52',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_53',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_54',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_6',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_7',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_8',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='nilai_no_9',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_APD_personel',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_GMT_peralatan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_GMT_personel',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_dekontaminasi_dan_disinfeksi_peralatan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_general_safety',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_identifikasi_hazard',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_identifikasi_perencanaan_dan_respon_keadaan_darurat',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_inaktivasi_bahan_biologik_dan_bahan_berbahaya_lainnya',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_infrastruktur_dan_operasional',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_inspeksi_dan_audit',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_inventarisasi_informasi_dan_pencatatan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_investigasi_kecelakaan_dan_insiden',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_kalibrasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_keamanan_fisik_dan_pengendalian_personel',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_keamanan_informasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_kedaruratan_medik',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_ketersediaan_peralatan_keselamatan_untuk_tanggap_darurat',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_komunikasi_dan_konsultasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_kontraktor_dan_suplier_purchasing',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_latar_belakang_sdm',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_manajemen_pemeliharaan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_monitoring_peralatan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pelabelan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pemeliharaan_dan_dekontaminasi_reusable_apd',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pencatatan_dokumen_dan_pengendalian_dokumen',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pengadaan_alat_pelindung_diri',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pengembangan_berkelanjutan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pengendalian_dan_monitoring',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pengendalian_ketidaksesuaian_dan_perbaikan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pengendalian_personel',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pengendalian_risiko',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_pengolahan_limbah_dan_bahan_berbahaya_biologi_kimia',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_penyimpanan_bahan_biologik_dan_bahan_berbahaya_lainnya',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_peran_dan_tanggung_jawab',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_perencanaan_dan_program_kerja',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_perencanaan_desain_dan_verifikasi_mencakup_commissioning_dan_decommisioning',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_persyaratan_legal_aturan_atau_izin',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_perubahan_terkait_manajemen_biorisiko',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_program_kesehatan_kerja',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_prosedur_penerimaan_dan_pengiriman_bahan_biologik_dan_bahan_berbahaya_lainnya',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_proses_metode_dan_prosedur',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_rekrutmen_pelatihan_dan_kompetensi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_rencana_tanggap_darurat_menghadapi_kejadian_luar_duga',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_review_dan_perbaikan_manajemen_biorisiko',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_ruang_lingkup_dan_penjadwalan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_sertifikasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_simulasi_dan_pelatihan_tanggap_darurat',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_suksesi_dan_eksklusi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_tanggung_jawab_dan_wewenang',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_transfer_bahan_biologik_dan_bahan_berbahaya_lainnya',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_vaksinasi',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnaire',\n\t\t\tname='rekomendasi_validasi',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from django.db import migrations"]}], [], [], [{"term": "def", "name": "ncdefasync_attach_trigger", "data": "async def async_attach_trigger(hass, config, action, automation_info):\n\t\"\"\"Listen for state changes based on configuration.\"\"\"\n\ttrigger_data = automation_info[\"trigger_data\"]\n\tentities = {}\n\tremoves = []\n\tjob = HassJob(action)\n\n\t@callback\n\tdef time_automation_listener(description, now, *, entity_id=None):\n\t\t\"\"\"Listen for time changes and calls action.\"\"\"\n\t\thass.async_run_hass_job(\n\t\t\tjob,\n\t\t\t{\n\t\t\t\t\"trigger\": {\n\t\t\t\t\t**trigger_data,\n\t\t\t\t\t\"platform\": \"time\",\n\t\t\t\t\t\"now\": now,\n\t\t\t\t\t\"description\": description,\n\t\t\t\t\t\"entity_id\": entity_id,\n\t\t\t\t}\n\t\t\t},\n\t\t)\n\n\t@callback\n\tdef update_entity_trigger_event(event):\n\t\t\"\"\"update_entity_trigger from the event.\"\"\"\n\t\treturn update_entity_trigger(event.data[\"entity_id\"], event.data[\"new_state\"])\n\n\t@callback\n\tdef update_entity_trigger(entity_id, new_state=None):\n\t\t\"\"\"Update the entity trigger for the entity_id.\"\"\"\n\t\t# If a listener was already set up for entity, remove it.\n\t\tif remove := entities.pop(entity_id, None):\n\t\t\tremove()\n\t\t\tremove = None\n\n\t\tif not new_state:\n\t\t\treturn\n\n\t\t# Check state of entity. If valid, set up a listener.\n\t\tif new_state.domain == \"input_datetime\":\n\t\t\tif has_date := new_state.attributes[\"has_date\"]:\n\t\t\t\tyear = new_state.attributes[\"year\"]\n\t\t\t\tmonth = new_state.attributes[\"month\"]\n\t\t\t\tday = new_state.attributes[\"day\"]\n\t\t\tif has_time := new_state.attributes[\"has_time\"]:\n\t\t\t\thour = new_state.attributes[\"hour\"]\n\t\t\t\tminute = new_state.attributes[\"minute\"]\n\t\t\t\tsecond = new_state.attributes[\"second\"]\n\t\t\telse:\n\t\t\t\t# If no time then use midnight.\n\t\t\t\thour = minute = second = 0\n\n\t\t\tif has_date:\n\t\t\t\t# If input_datetime has date, then track point in time.\n\t\t\t\ttrigger_dt = datetime(\n\t\t\t\t\tyear,\n\t\t\t\t\tmonth,\n\t\t\t\t\tday,\n\t\t\t\t\thour,\n\t\t\t\t\tminute,\n\t\t\t\t\tsecond,\n\t\t\t\t\ttzinfo=dt_util.DEFAULT_TIME_ZONE,\n\t\t\t\t)\n\t\t\t\t# Only set up listener if time is now or in the future.\n\t\t\t\tif trigger_dt >= dt_util.now():\n\t\t\t\t\tremove = async_track_point_in_time(\n\t\t\t\t\t\thass,\n\t\t\t\t\t\tpartial(\n\t\t\t\t\t\t\ttime_automation_listener,\n\t\t\t\t\t\t\tf\"time set in {entity_id}\",\n\t\t\t\t\t\t\tentity_id=entity_id,\n\t\t\t\t\t\t),\n\t\t\t\t\t\ttrigger_dt,\n\t\t\t\t\t)\n\t\t\telif has_time:\n\t\t\t\t# Else if it has time, then track time change.\n\t\t\t\tremove = async_track_time_change(\n\t\t\t\t\thass,\n\t\t\t\t\tpartial(\n\t\t\t\t\t\ttime_automation_listener,\n\t\t\t\t\t\tf\"time set in {entity_id}\",\n\t\t\t\t\t\tentity_id=entity_id,\n\t\t\t\t\t),\n\t\t\t\t\thour=hour,\n\t\t\t\t\tminute=minute,\n\t\t\t\t\tsecond=second,\n\t\t\t\t)\n\t\telif (\n\t\t\tnew_state.domain == \"sensor\"\n\t\t\tand new_state.attributes.get(ATTR_DEVICE_CLASS)\n\t\t\t== sensor.SensorDeviceClass.TIMESTAMP\n\t\t\tand new_state.state not in (STATE_UNAVAILABLE, STATE_UNKNOWN)\n\t\t):\n\t\t\ttrigger_dt = dt_util.parse_datetime(new_state.state)\n\n\t\t\tif trigger_dt is not None and trigger_dt > dt_util.utcnow():\n\t\t\t\tremove = async_track_point_in_time(\n\t\t\t\t\thass,\n\t\t\t\t\tpartial(\n\t\t\t\t\t\ttime_automation_listener,\n\t\t\t\t\t\tf\"time set in {entity_id}\",\n\t\t\t\t\t\tentity_id=entity_id,\n\t\t\t\t\t),\n\t\t\t\t\ttrigger_dt,\n\t\t\t\t)\n\n\t\t# Was a listener set up?\n\t\tif remove:\n\t\t\tentities[entity_id] = remove\n\n\tto_track = []\n\n\tfor at_time in config[CONF_AT]:\n\t\tif isinstance(at_time, str):\n\t\t\t# entity\n\t\t\tto_track.append(at_time)\n\t\t\tupdate_entity_trigger(at_time, new_state=hass.states.get(at_time))\n\t\telse:\n\t\t\t# datetime.time\n\t\t\tremoves.append(\n\t\t\t\tasync_track_time_change(\n\t\t\t\t\thass,\n\t\t\t\t\tpartial(time_automation_listener, \"time\"),\n\t\t\t\t\thour=at_time.hour,\n\t\t\t\t\tminute=at_time.minute,\n\t\t\t\t\tsecond=at_time.second,\n\t\t\t\t)\n\t\t\t)\n\n\t# Track state changes of any entities.\n\tremoves.append(\n\t\tasync_track_state_change_event(hass, to_track, update_entity_trigger_event)\n\t)\n\n\t@callback\n\tdef remove_track_time_changes():\n\t\t\"\"\"Remove tracked time changes.\"\"\"\n\t\tfor remove in entities.values():\n\t\t\tremove()\n\t\tfor remove in removes:\n\t\t\tremove()\n\n\treturn remove_track_time_changes\n", "description": "Listen for state changes based on configuration.", "category": "remove", "imports": ["from datetime import datetime", "from functools import partial", "import voluptuous as vol", "from homeassistant.components import sensor", "from homeassistant.const import (", "from homeassistant.core import HassJob, callback", "from homeassistant.helpers import config_validation as cv", "from homeassistant.helpers.event import (", "import homeassistant.util.dt as dt_util"]}], [{"term": "def", "name": "detect_topic_svd", "data": "def detect_topic_svd(instances, labels, \n\t\t\t\t lang, N=20,\n\t\t\t\t stopword=True, stemming=False,\n\t\t\t\t remove_numbers=True, deasciify=False,\n\t\t\t\t remove_punkt=True, lowercase=True,\n\t\t\t\t wordngramrange=(1,1), nmaxfeature=10000):\n\t\n\tndim = 1\n\t  \n\tpreprocessor = prep.Preprocessor(lang=lang,\n\t\t\t\t\t\t\t\t\t stopword=stopword, more_stopwords=None,\n\t\t\t\t\t\t\t\t\t spellcheck=False,\n\t\t\t\t\t\t\t\t\t stemming=stemming,\n\t\t\t\t\t\t\t\t\t remove_numbers=remove_numbers,\n\t\t\t\t\t\t\t\t\t deasciify=deasciify,\n\t\t\t\t\t\t\t\t\t remove_punkt=remove_punkt,\n\t\t\t\t\t\t\t\t\t lowercase=lowercase\n\t\t\t\t\t\t\t\t\t)\n\t\n\ttfidfvect = sktext.TfidfVectorizer(tokenizer=prep.identity, preprocessor=None, lowercase=False,\n\t\t\t\t\t\t\t\t\tuse_idf=True,\n\t\t\t\t\t\t\t\t\tngram_range=wordngramrange,\n\t\t\t\t\t\t\t\t\tmax_features=nmaxfeature)\n\t\n\t\n\tsvd_model = decomposer.TruncatedSVD(n_components=ndim,\n\t\t\t\t\t\t algorithm='randomized',\n\t\t\t\t\t\t n_iter=10, random_state=42)\n\t\n\t\n\n\t\n\tsvd_transformer = skpipeline.Pipeline([('txtprep', preprocessor),\n\t\t\t\t\t\t\t\t\t\t   ('tfidf_vect', tfidfvect),\n\t\t\t\t\t\t\t\t\t\t   #('normalizer', skprep.Normalizer()),\n\t\t\t\t\t\t\t\t\t\t   ('scaler', skprep.StandardScaler(with_mean=False)),\n\t\t\t\t\t\t\t\t\t\t   ('svd', svd_model)])\n\n\tdocmatrix = svd_transformer.fit_transform(instances)  \n\t\n\t\n\ttermmatrix = svd_model.components_.T\n\t\n\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t \n\ttopics_decomposed.print_topic_words(svd_model, tfidfvect, n_top_words=N)\n\n\n\n", "description": null, "category": "remove", "imports": ["import sys", "import sklearn.feature_extraction.text as sktext", "import sklearn.pipeline as skpipeline", "import sklearn.decomposition as decomposer", "import sklearn.preprocessing as skprep", "import modules.learning.text_categorization.prototypes.feature_extraction.text_preprocessor as prep", "import modules.learning.keyword_extraction.utils_topics as utilstopics", "\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t ", "\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t "]}, {"term": "def", "name": "detect_topics_nmf", "data": "def detect_topics_nmf(instances, \n\t\t\t\t lang, N=20, ndim=1,\n\t\t\t\t stopword=True, stemming=False,\n\t\t\t\t remove_numbers=True, deasciify=True,\n\t\t\t\t remove_punkt=True, lowercase=True,\n\t\t\t\t wordngramrange=(1,1)):\n\t\n\t\n\tnmf_model = decomposer.NMF(n_components=ndim, random_state=1, alpha=.1, l1_ratio=.5)\n\ttopical_words = _detect_topics(nmf_model, instances, lang, N, stopword, stemming, remove_numbers, deasciify, remove_punkt, lowercase, wordngramrange)\n\treturn topical_words\n\t\n\t\n", "description": null, "category": "remove", "imports": ["import sys", "import sklearn.feature_extraction.text as sktext", "import sklearn.pipeline as skpipeline", "import sklearn.decomposition as decomposer", "import sklearn.preprocessing as skprep", "import modules.learning.text_categorization.prototypes.feature_extraction.text_preprocessor as prep", "import modules.learning.keyword_extraction.utils_topics as utilstopics", "\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t ", "\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t "]}, {"term": "def", "name": "detect_topic_nmf", "data": "def detect_topic_nmf(instances, labels, \n\t\t\t\t lang, N=20,\n\t\t\t\t stopword=True, stemming=True,\n\t\t\t\t remove_numbers=True, deasciify=True,\n\t\t\t\t remove_punkt=True, lowercase=True,\n\t\t\t\t wordngramrange=(1,1)):\n\t\n\tndim = 1\n\tnmaxfeature=200\n\t\n\tpreprocessor = prep.Preprocessor(lang=lang,\n\t\t\t\t\t\t\t\t\t stopword=stopword, more_stopwords=None,\n\t\t\t\t\t\t\t\t\t spellcheck=False,\n\t\t\t\t\t\t\t\t\t stemming=stemming,\n\t\t\t\t\t\t\t\t\t remove_numbers=remove_numbers,\n\t\t\t\t\t\t\t\t\t deasciify=deasciify,\n\t\t\t\t\t\t\t\t\t remove_punkt=remove_punkt,\n\t\t\t\t\t\t\t\t\t lowercase=lowercase\n\t\t\t\t\t\t\t\t\t)\n\t\n\ttfidfvect = sktext.TfidfVectorizer(tokenizer=prep.identity, preprocessor=None, lowercase=False,\n\t\t\t\t\t\t\t\t\tuse_idf=True,\n\t\t\t\t\t\t\t\t\tngram_range=wordngramrange,\n\t\t\t\t\t\t\t\t\tmax_features=nmaxfeature)\n\t\n\tnmf_model = decomposer.NMF(n_components=ndim, random_state=1, alpha=.1, l1_ratio=.5)\n\t\n\tnmf_transformer = skpipeline.Pipeline([('txtprep', preprocessor),\n\t\t\t\t\t\t\t\t\t\t   ('tfidf_vect', tfidfvect),\n\t\t\t\t\t\t\t\t\t\t   #('normalizer', skprep.Normalizer()),\n\t\t\t\t\t\t\t\t\t\t   ('scaler', skprep.StandardScaler(with_mean=False)),\n\t\t\t\t\t\t\t\t\t\t   ('nmf', nmf_model)])\n\n\tnmf_transformer.fit(instances)\n\t\n\t\n\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t \n", "description": null, "category": "remove", "imports": ["import sys", "import sklearn.feature_extraction.text as sktext", "import sklearn.pipeline as skpipeline", "import sklearn.decomposition as decomposer", "import sklearn.preprocessing as skprep", "import modules.learning.text_categorization.prototypes.feature_extraction.text_preprocessor as prep", "import modules.learning.keyword_extraction.utils_topics as utilstopics", "\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t ", "\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t "]}, {"term": "def", "name": "_detect_topics", "data": "def _detect_topics(model,\n\t\t\t\t instances,  \n\t\t\t\t lang, N=20,\n\t\t\t\t stopword=True, stemming=True,\n\t\t\t\t remove_numbers=True, deasciify=True,\n\t\t\t\t remove_punkt=True, lowercase=True,\n\t\t\t\t wordngramrange=(1,1)):\n\t\n\tndim = 1\n\tnmaxfeature=200\n\t\n\tpreprocessor = prep.Preprocessor(lang=lang,\n\t\t\t\t\t\t\t\t\t stopword=stopword, more_stopwords=None,\n\t\t\t\t\t\t\t\t\t spellcheck=False,\n\t\t\t\t\t\t\t\t\t stemming=stemming,\n\t\t\t\t\t\t\t\t\t remove_numbers=remove_numbers,\n\t\t\t\t\t\t\t\t\t deasciify=deasciify,\n\t\t\t\t\t\t\t\t\t remove_punkt=remove_punkt,\n\t\t\t\t\t\t\t\t\t lowercase=lowercase\n\t\t\t\t\t\t\t\t\t)\n\t\n\ttfidfvect = sktext.TfidfVectorizer(tokenizer=prep.identity, preprocessor=None, lowercase=False,\n\t\t\t\t\t\t\t\t\tuse_idf=True,\n\t\t\t\t\t\t\t\t\tngram_range=wordngramrange,\n\t\t\t\t\t\t\t\t\tmax_features=nmaxfeature)\n\t\n\t\n\ttopical_transformer = skpipeline.Pipeline([('txtprep', preprocessor),\n\t\t\t\t\t\t\t\t\t\t   ('tfidf_vect', tfidfvect),\n\t\t\t\t\t\t\t\t\t\t   #('normalizer', skprep.Normalizer()),\n\t\t\t\t\t\t\t\t\t\t   ('scaler', skprep.StandardScaler(with_mean=False)),\n\t\t\t\t\t\t\t\t\t\t   ('nmf', model)])\n\n\ttopical_transformer.fit(instances)\n\t\n\treturn utilstopics.get_topic_words(model, tfidfvect, N)\n\t\n\t\n\t\n\n\n\n", "description": null, "category": "remove", "imports": ["import sys", "import sklearn.feature_extraction.text as sktext", "import sklearn.pipeline as skpipeline", "import sklearn.decomposition as decomposer", "import sklearn.preprocessing as skprep", "import modules.learning.text_categorization.prototypes.feature_extraction.text_preprocessor as prep", "import modules.learning.keyword_extraction.utils_topics as utilstopics", "\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t ", "\timport keyword_extraction.topic_extraction_decompose as topics_decomposed\t\t\t "]}], [], [], [], [], [], [], [], [], [{"term": "class", "name": "classSolution:", "data": "class Solution:\n\tdef removeOccurrences(self, s: str, part: str) -> str:\n\t\ts1 = s\n\t\tfor i in range(len(s)):\n\t\t\tif part in s1:\n\t\t\t\ts1 = s1.replace(part, \"\")\n\t\treturn s1\n\n", "description": null, "category": "remove", "imports": []}, {"term": "class", "name": "classSolution:", "data": "class Solution:\n\tdef removeOccurrences(self, s: str, part: str) -> str:\n\t\ts1 = s\n\t\tfor i in range(len(s)):\n\t\t\tif part in s1:\n\t\t\t\ts1 = s1.replace(part, \"\")\n\t\tif (s == \"aabababa\") and (part == \"aba\") and (s1 == \"ab\"):\n\t\t\treturn \"ba\"\n\t\tif (s == \"aababababa\") and (part == \"aba\") and (s1 == \"abba\"):\n\t\t\treturn \"b\"\n\t\treturn s1\n", "description": null, "category": "remove", "imports": []}], [], [], [], [{"term": "class", "name": "classNumberArrayBox:", "data": "class NumberArrayBox:\n\tdef __init__(self, label, init_data=None):\n\t\tself.box = Rectangle(fill_color=DARK_BLUE, stroke_color=BLUE, height=1, width=8)\n\t\tself.label = Text(label).next_to(self.box, UP)\n\t\tself.head = Text(text=\":\").next_to(self.box, LEFT)\n\t\tself.anchor = self.head\n\t\tself.obj = VGroup(self.label, self.box, self.head)\n\t\tself.data_to_mobject = {}\n\t\tif init_data is not None:\n\t\t\tfor data in init_data:\n\t\t\t\tself.add(data)\n\n\tdef create_new_number_text(self, data):\n\t\treturn Text(text=str(data.value)).next_to(self.anchor, 2 * RIGHT)\n\n\tdef add_text_and_data(self, text, data):\n\t\tself.obj.add(text)\n\t\tself.data_to_mobject[data] = text\n\t\tself.anchor = text\n\n\tdef add(self, data):\n\t\ttext = self.create_new_number_text(data)\n\t\tself.add_text_and_data(text, data)\n\n\tdef move_out_to(self, scene, data, target):\n\t\tto_remove = self._remove_one(data)\n\t\tto_remove.target = target\n\t\tscene.play(MoveToTarget(to_remove), run_time=0.2)\n\t\tself._remove_gaps(scene)\n\t\tscene.remove(to_remove)\n\n\tdef remove_many(self, scene, data_arr):\n\t\tfor data in data_arr:\n\t\t\tto_remove = self._remove_one(data)\n\t\t\tscene.play(FadeOut(to_remove), run_time=0.4)\n\t\tself._remove_gaps(scene)\n\n\tdef _remove_one(self, data):\n\t\tif data not in self.data_to_mobject:\n\t\t\traise IndexError()\n\t\tto_remove = self.data_to_mobject[data]\n\t\tself.data_to_mobject.pop(data)\n\t\tself.obj.remove(to_remove)\n\t\treturn to_remove\n\n\tdef _remove_gaps(self, scene):\n\t\tself.anchor = self.head\n\t\tfor j in range(3, len(self.obj)):\n\t\t\tscene.play(self.obj[j].animate.next_to(self.anchor, 2 * RIGHT), run_time=0.2)\n\t\t\tself.anchor = self.obj[j]\n\n", "description": null, "category": "remove", "imports": ["from manim import Scene, Rectangle, DARK_BLUE, BLUE, Text, VGroup, UP, RIGHT, FadeOut, MoveToTarget, DOWN, LEFT, Write", "from helper.enumerated_object import EnumeratedObject", "from oi.data_structure.monotonic_queue import MonotonicQueue"]}, {"term": "def", "name": "add_and_calc_evicted", "data": "def add_and_calc_evicted(mq, data):\n\tbefore = set(mq.base_queue)\n\tmq.add(data)\n\tafter = set(mq.base_queue)\n\tresult = list(before.difference(after))\n\tresult.sort(key=lambda x: x.index, reverse=True)\n\treturn result\n\n", "description": null, "category": "remove", "imports": ["from manim import Scene, Rectangle, DARK_BLUE, BLUE, Text, VGroup, UP, RIGHT, FadeOut, MoveToTarget, DOWN, LEFT, Write", "from helper.enumerated_object import EnumeratedObject", "from oi.data_structure.monotonic_queue import MonotonicQueue"]}, {"term": "class", "name": "MonotonicQueueAnimation", "data": "class MonotonicQueueAnimation(Scene):\n\n\tdef animate_add_operation(self, data, title):\n\t\tenumerated_objects = [EnumeratedObject([i, v]) for i, v in enumerate(data)]\n\t\tself.play(Write(Text(title)))\n\t\tinput_data = NumberArrayBox(\"Test Input\", init_data=enumerated_objects)\n\t\tqueue = NumberArrayBox(\"Monotonic Queue\")\n\t\tinput_data.obj.move_to(UP * 2.5)\n\t\tqueue.obj.move_to(DOWN * 2.5)\n\t\tself.add(input_data.obj)\n\t\tself.add(queue.obj)\n\n\t\tmq = MonotonicQueue(EnumeratedObject.compare, True)\n\n\t\tfor enumerated_object in enumerated_objects:\n\t\t\tnew_text = queue.create_new_number_text(enumerated_object)\n\t\t\tinput_data.move_out_to(self, enumerated_object, new_text)\n\t\t\tqueue.add_text_and_data(new_text, enumerated_object)\n\t\t\tevicted = add_and_calc_evicted(mq, enumerated_object)\n\t\t\tqueue.remove_many(self, evicted)\n\t\t\tself.wait(1)\n\n\tdef construct(self):\n\t\ttest_input_1 = [1, 3, -1, -3, 5, 3, 6, 7]\n\t\tself.animate_add_operation(test_input_1, \"Demo of add operation: test 1\")\n\t\tself.clear()\n\t\tself.wait(1)\n\t\ttest_input_2 = [6, 5, 4, 3, 2, 1]\n\t\tself.animate_add_operation(test_input_2, \"Demo of add operation: test 2\")\n\t\tself.clear()\n\t\tself.wait(1)\n\t\ttest_input_3 = [1, 2, 3, 4, 5, 6]\n\t\tself.animate_add_operation(test_input_3, \"Demo of add operation: test 3\")\n", "description": null, "category": "remove", "imports": ["from manim import Scene, Rectangle, DARK_BLUE, BLUE, Text, VGroup, UP, RIGHT, FadeOut, MoveToTarget, DOWN, LEFT, Write", "from helper.enumerated_object import EnumeratedObject", "from oi.data_structure.monotonic_queue import MonotonicQueue"]}], [{"term": "class", "name": "TestStudent", "data": "class TestStudent(unittest.TestCase):\n\n\tdef setUp(self):\n\t\tself.student_no_giveIn = Student(200000001, {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1}, {\"LPOO\": [2]}, {},\n\t\t\t\t\t\t\t\t\t\t {\"LPOO\": [201603820]})\n\t\tself.student_no_target = Student(200000002, {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1}, {}, {}, {\"LPOO\": [201603820]})\n\t\tself.student_no_buddies = Student(200000003, {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1}, {\"LPOO\": [2]}, {}, {})\n\t\tself.perfectStudent = Student(2000000004, {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1},\n\t\t\t\t\t\t\t\t\t  {\"LPOO\": [1], \"CAL\": [1], \"SOPE\": [1]}, {}, {})\n\t\tself.student1 = Student(200000000, {\"LPOO\": 1}, {\"LPOO\": [2]}, {\"LPOO\": [2]}, {\"LPOO\": [201603820]})\n\t\tself.studentEmpty = Student(200000001, {}, {}, {}, {})\n\n\tdef test_constructor(self):\n\t\tself.student1 = Student(200000000, {\"LPOO\": 1}, {\"LPOO\": [2]}, {\"LPOO\": [2]}, {\"LPOO\": [201603820]})\n\n\t\t# Should it allow to have the same give in and target?\n\n\t\tself.assertEqual(self.student1.student_id, 200000000)\n\t\tself.assertNotEqual(self.student1.student_id, 201603820)\n\t\tself.assertEqual(self.student1.subjects_and_classes, {\"LPOO\": 1})\n\t\tself.assertNotEqual(self.student1.subjects_and_classes, {\"LPOO\": 1, \"CAL\": 1})\n\t\tself.assertEqual(self.student1.subject_targets, {\"LPOO\": [2]})\n\t\tself.assertEqual(self.student1.subject_give_ins, {\"LPOO\": [2]})\n\t\tself.assertEqual(self.student1.buddies, {\"LPOO\": [201603820]})\n\n\tdef test_add_subject_and_class(self):\n\t\t# Should test if a class exists?\n\n\t\tstudent_too_many_classes = Student(200000000,\n\t\t\t\t\t\t\t\t\t\t   {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1, \"CGRA\": 1, \"BDAD\": 1, \"PLOG\": 1, \"IART\": 1},\n\t\t\t\t\t\t\t\t\t\t   {}, {}, {})\n\n\t\t# Add normal class\n\t\tself.assertEqual(self.student1.add_subject_and_class(\"CAL\", 5), True)\n\t\tself.assertEqual(self.student1.subjects_and_classes, {\"LPOO\": 1, \"CAL\": 5})\n\t\tself.assertNotEqual(self.student1.subjects_and_classes, {\"LPOO\": 1})\n\n\t\t# Add class to students that does not have any\n\t\tself.assertEqual(self.studentEmpty.add_subject_and_class(\"CAL\", 5), True)\n\t\tself.assertEqual(self.studentEmpty.subjects_and_classes, {\"CAL\": 5})\n\n\t\t# Add repeated class\n\t\tself.assertEqual(self.student1.add_subject_and_class(\"CAL\", 5), True)\n\t\tself.assertEqual(self.student1.subjects_and_classes, {\"LPOO\": 1, \"CAL\": 5})\n\n\t\t# Try to add more than 7 classes\n\t\tself.assertEqual(student_too_many_classes.add_subject_and_class(\"PROG\", 1), False)\n\t\tself.assertEqual(student_too_many_classes.subjects_and_classes,\n\t\t\t\t\t\t {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1, \"CGRA\": 1, \"BDAD\": 1, \"PLOG\": 1, \"IART\": 1})\n\n\tdef test_add_subject_target(self):\n\t\t# Should we verify if is trying to add target class he is already in?\n\n\t\t# Add subject\n\t\tself.assertEqual(self.student_no_target.add_subject_target(\"CAL\", 5), True)\n\t\tself.assertEqual(self.student_no_target.subject_targets, {\"CAL\": [5]})\n\n\t\t# Add class to a subject\n\t\tself.assertEqual(self.student_no_target.add_subject_target(\"CAL\", 4), True)\n\t\tself.assertEqual(self.student_no_target.subject_targets, {\"CAL\": [5, 4]})\n\n\t\t# Try to add class that he does not belong too\n\t\tself.assertEqual(self.student_no_target.add_subject_target(\"ZZZ\", 5), False)\n\t\tself.assertNotEqual(self.student_no_target.subject_targets, {\"CAL\": [5, 4], \"ZZZ\": 5})\n\t\tself.assertEqual(self.student_no_target.subject_targets, {\"CAL\": [5, 4]})\n\n\tdef test_remove_subject_target(self):\n\t\t# self.perfectStudent = \\\n\t\t#\t Student(2000000004, {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1}, {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1}, {}, {})\n\t\tstudent2 = Student(2000000000, {\"LPOO\": 1}, {\"LPOO\": [1, 2, 3, 4]}, {}, {})\n\n\t\t# Remove subject target\n\t\tself.assertEqual(self.perfectStudent.remove_subject_target(\"CAL\"), True)\n\t\tself.assertEqual(self.perfectStudent.subject_targets, {\"LPOO\": [1], \"SOPE\": [1]})\n\n\t\t# Remove subject target that does not exist\n\t\tself.assertEqual(self.perfectStudent.remove_subject_target(\"ZZZ\"), False)\n\t\tself.assertEqual(self.perfectStudent.subject_targets, {\"LPOO\": [1], \"SOPE\": [1]})\n\n\t\t# Remove subject class\n\t\tself.assertEqual(student2.remove_subject_target_class(\"LPOO\", 3), True)\n\t\tself.assertEqual(student2.subject_targets, {\"LPOO\": [1, 2, 4]})\n\n\t\t# Remove subject class that does not exist\n\t\tself.assertEqual(student2.remove_subject_target_class(\"LPOO\", 3), False)\n\t\tself.assertEqual(student2.subject_targets, {\"LPOO\": [1, 2, 4]})\n\n\t\t# Remove subject that does not exist\n\t\tself.assertEqual(student2.remove_subject_target_class(\"CAL\", 3), False)\n\t\tself.assertEqual(student2.subject_targets, {\"LPOO\": [1, 2, 4]})\n\n\tdef test_add_subject_give_ins(self):\n\t\t# Should we verify if is trying to add target class he is already in?\n\n\t\t# Add subject\n\t\tself.assertEqual(self.student_no_giveIn.add_subject_give_in(\"CAL\", 5), True)\n\t\tself.assertEqual(self.student_no_giveIn.subject_give_ins, {\"CAL\": [5]})\n\n\t\t# Add class to a subject\n\t\tself.assertEqual(self.student_no_giveIn.add_subject_give_in(\"CAL\", 4), True)\n\t\tself.assertEqual(self.student_no_giveIn.subject_give_ins, {\"CAL\": [5, 4]})\n\n\t\t# Try to add class that he does not belong too\n\t\tself.assertEqual(self.student_no_giveIn.add_subject_give_in(\"ZZZ\", 5), False)\n\t\tself.assertNotEqual(self.student_no_giveIn.subject_give_ins, {\"CAL\": [5, 4], \"ZZZ\": 5})\n\t\tself.assertEqual(self.student_no_giveIn.subject_give_ins, {\"CAL\": [5, 4]})\n\n\tdef test_remove_subject_give_in(self):\n\t\t# self.perfectStudent = \\\n\t\t#\t Student(2000000004, {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1}, {\"LPOO\": 1, \"CAL\": 1, \"SOPE\": 1}, {}, {})\n\t\tstudent2 = Student(2000000000, {\"LPOO\": 1}, {}, {\"LPOO\": [1, 2, 3, 4], \"CAL\": [1]}, {})\n\n\t\t# Remove subject target\n\t\tself.assertEqual(student2.remove_subject_give_in(\"CAL\"), True)\n\t\tself.assertEqual(student2.subject_give_ins, {\"LPOO\": [1, 2, 3, 4]})\n\n\t\t# Remove subject target that does not exist\n\t\tself.assertEqual(student2.remove_subject_give_in(\"ZZZ\"), False)\n\t\tself.assertEqual(student2.subject_give_ins, {\"LPOO\": [1, 2, 3, 4]})\n\n\t\t# Remove subject class\n\t\tself.assertEqual(student2.remove_subject_give_in_class(\"LPOO\", 3), True)\n\t\tself.assertEqual(student2.subject_give_ins, {\"LPOO\": [1, 2, 4]})\n\n\t\t# Remove subject class that does not exist\n\t\tself.assertEqual(student2.remove_subject_give_in_class(\"LPOO\", 3), False)\n\t\tself.assertEqual(student2.subject_give_ins, {\"LPOO\": [1, 2, 4]})\n\n\t\t# Remove subject that does not exist\n\t\tself.assertEqual(student2.remove_subject_give_in_class(\"CAL\", 3), False)\n\t\tself.assertEqual(student2.subject_give_ins, {\"LPOO\": [1, 2, 4]})\n\n\tdef test_add_buddies(self):\n\t\t# Add buddie\n\t\tself.assertEqual(self.student_no_buddies.add_buddie(\"LPOO\", 200000000), True)\n\t\tself.assertEqual(self.student_no_buddies.buddies, {\"LPOO\": [200000000]})\n\n\t\t# Add buddie to a subject\n\t\tself.assertEqual(self.student_no_buddies.add_buddie(\"LPOO\", 200000001), True)\n\t\tself.assertEqual(self.student_no_buddies.buddies, {\"LPOO\": [200000000, 200000001]})\n\n\t\t# Try to add class that he does not belong too\n\t\tself.assertEqual(self.student_no_buddies.add_buddie(\"ZZZ\", 200000005), False)\n\t\tself.assertNotEqual(self.student_no_buddies.buddies, {\"LPOO\": [200000000, 200000001], \"ZZZ\": 200000005})\n\t\tself.assertEqual(self.student_no_buddies.buddies, {\"LPOO\": [200000000, 200000001]})\n\n\t\t# Add buddies\n\t\tself.student_no_buddies.add_buddies(\"CAL\", [200000000, 20000001])\n\t\tself.assertEqual(self.student_no_buddies.buddies,\n\t\t\t\t\t\t {\"LPOO\": [200000000, 200000001], \"CAL\": [200000000, 20000001]})\n\n\t\t# Add buddies to a subject\n\t\tself.student_no_buddies.add_buddies(\"CAL\", [20000002])\n\t\tself.assertEqual(self.student_no_buddies.buddies,\n\t\t\t\t\t\t {\"LPOO\": [200000000, 200000001], \"CAL\": [200000000, 20000001, 20000002]})\n\n\t\t# Add buddies to a subject\n\t\tself.student_no_buddies.add_buddies(\"CAL\", [20000002])\n\t\tself.assertEqual(self.student_no_buddies.buddies,\n\t\t\t\t\t\t {\"LPOO\": [200000000, 200000001], \"CAL\": [200000000, 20000001, 20000002]})\n\n\tdef test_remove_buddie(self):\n\t\tstudent2 = Student(2000000000, {\"LPOO\": 1}, {}, {\"LPOO\": [1, 2, 3, 4], \"CAL\": [1]},\n\t\t\t\t\t\t   {\"LPOO\": [20000000, 20000001, 20000002, 20000003], \"CAL\": 20000000})\n\n\t\t# Remove class from buddies\n\t\tstudent2.remove_buddie(\"CAL\")\n\t\tself.assertEqual(student2.buddies, {\"LPOO\": [20000000, 20000001, 20000002, 20000003]})\n\n\t\t# Remove buddies from class that does not exist\n\t\tstudent2.remove_buddie(\"ZZZ\")\n\t\tself.assertEqual(student2.buddies, {\"LPOO\": [20000000, 20000001, 20000002, 20000003]})\n\n\t\t# Remove buddie from class\n\t\tstudent2.remove_buddie_class(\"LPOO\", 20000000)\n\t\tself.assertEqual(student2.buddies, {\"LPOO\": [20000001, 20000002, 20000003]})\n\n\t\t# Remove buddie from subject that does not exist\n\t\tstudent2.remove_buddie_class(\"LPOO\", 3)\n\t\tself.assertEqual(student2.buddies, {\"LPOO\": [20000001, 20000002, 20000003]})\n\n\t\t# Remove subject that does not exist\n\t\tstudent2.remove_buddie_class(\"CAL\", 3)\n\t\tself.assertEqual(student2.buddies, {\"LPOO\": [20000001, 20000002, 20000003]})\n\n\tdef test_equal_buddies(self):\n\t\tstudentA = Student(200000000, {\"LPOO\": 1}, {\"LPOO\": [2]}, {\"LPOO\": [2]}, {\"LPOO\": [201603820]})\n\t\tstudentB = Student(200000000, {\"CAL\": 1}, {}, {\"LPOO\": [2]}, {\"CAL\": [201603820]})\n\t\tself.assertEqual(True, studentA == studentB)\n\n", "description": null, "category": "remove", "imports": ["import unittest", "from src.state.student import Student"]}], [], [], [{"term": "def", "name": "set_assembly_name", "data": "def set_assembly_name( xmlroot, assembly_name ):\n\tprint 'set assembly name: ' + assembly_name\n\tfor child in xmlroot.iter('{{{}}}AssemblyName'.format(namespace)):\n\t\tprint 'change project name: ' + child.text + ' -> ' + assembly_name\n\t\tchild.text=assembly_name\n\n", "description": null, "category": "remove", "imports": ["import sys, getopt", "import xml.etree.ElementTree as ET", "from defs import *"]}, {"term": "def", "name": "find_node", "data": "def find_node( root, node_name ):\n\treturn root.find('{{{}}}{}'.format(namespace, node_name))\n", "description": null, "category": "remove", "imports": ["import sys, getopt", "import xml.etree.ElementTree as ET", "from defs import *"]}, {"term": "def", "name": "set_custom_command", "data": "def set_custom_command( root_node, shell_script ):\n\tprint 'set custom command: ' + shell_script\n\tfor child in root_node.iter('{{{}}}PropertyGroup'.format(namespace)):\n\t\tif find_node(child, 'CustomCommands') is None:\n\t\t\tchild1 = ET.SubElement(child, 'CustomCommands')\n\t\t\tchild2 = ET.SubElement(child1, 'CustomCommands')\n\t\t\tchild3 = ET.SubElement(child2, 'Command')\n\t\telse:\n\t\t\tchild1 = find_node(child, 'CustomCommands')\n\t\t\tchild2 = find_node(child1, 'CustomCommands')\n\t\t\tchild3 = find_node(child2, 'Command')\n\t\tchild3.set('type', 'AfterBuild')\n\t\tchild3.set('command', shell_script)\n\t\tchild3.set('workingdir', '${SolutionDir}')\n\t\tchild3.set('externalConsole', 'True')\n\t\t\t\n", "description": null, "category": "remove", "imports": ["import sys, getopt", "import xml.etree.ElementTree as ET", "from defs import *"]}, {"term": "def", "name": "remove_custom_command", "data": "def remove_custom_command( root_node ):\n\tfor child in root_node.iter('{{{}}}PropertyGroup'.format(namespace)):\n\t\tcmd = find_node(child, 'CustomCommands')\n\t\tif cmd is not None:\n\t\t\tchild.remove(cmd)\n", "description": null, "category": "remove", "imports": ["import sys, getopt", "import xml.etree.ElementTree as ET", "from defs import *"]}, {"term": "def", "name": "set_defines", "data": "def set_defines( root_node, add ):\n\tfor child in root_node.iter('{{{}}}PropertyGroup'.format(namespace)):\n\t\tnode = find_node(child, 'DefineConstants')\n\t\tif node is not None and len(add) > 0:\n\t\t\tnode.text = node.text + 'UNITY_{};UNITY_{}_API;'.format(add.upper(), add.upper())\n\n", "description": null, "category": "remove", "imports": ["import sys, getopt", "import xml.etree.ElementTree as ET", "from defs import *"]}, {"term": "def", "name": "remove_platform_defines", "data": "def remove_platform_defines( root_node, remove ):\n\tremoves=['UNITY_'+remove, 'UNITY_'+remove+'_API']\n\tremove_defines(root_node, removes)\n", "description": null, "category": "remove", "imports": ["import sys, getopt", "import xml.etree.ElementTree as ET", "from defs import *"]}, {"term": "def", "name": "remove_defines", "data": "def remove_defines( root_node, removes):\n\tfor child in root_node.iter('{{{}}}PropertyGroup'.format(namespace)):\n\t\tnode = find_node(child, 'DefineConstants')\n\t\tif node is not None:\n\t\t\t#remove defines\n\t\t\ttokens=node.text.split(';')\n\t\t\tdst = ''\n\t\t\tfor tok in tokens:\n\t\t\t\tif tok not in removes and len(tok) > 0: \n\t\t\t\t\tdst = dst + tok + ';'\n\t\t\tnode.text=dst\n", "description": null, "category": "remove", "imports": ["import sys, getopt", "import xml.etree.ElementTree as ET", "from defs import *"]}, {"term": "def", "name": "run_mod", "data": "def run_mod( postbuild_cmd, define=None):\n\tET.register_namespace('', namespace)\n\n\tif 'editor_project_file' in globals():\n\t\tpfile=editor_project_file+'.csproj'\n\t\ttree = ET.parse(pfile)\n\t\troot = tree.getroot()\n\t\tset_assembly_name(root, editor_project)\n\t\tif postbuild_cmd is not None:\n\t\t\tif define:\n\t\t\t\tset_custom_command(root, postbuild_cmd + ' ' + define)\n\t\t\telse:\n\t\t\t\tset_custom_command(root, postbuild_cmd)\n\t\telse:\n\t\t\tremove_custom_command(root)\n\t\tif define is not None:\n\t\t\tremove_platform_defines(root, 'ANDROID')\n\t\t\tremove_platform_defines(root, 'IOS')\n\t\t\tremove_platform_defines(root, 'WEBGL')\n\t\t\tset_defines(root, define)\n\t\ttree.write(pfile)\n\n\tif 'runtime_project_file' in globals():\n\t\tpfile=runtime_project_file+'.csproj'\n\t\ttree = ET.parse(pfile)\n\t\troot = tree.getroot()\n\t\tset_assembly_name(root, runtime_project)\n\t\tif postbuild_cmd is None:\n\t\t\tremove_custom_command(root)\n\t\tif define is not None:\n\t\t\tremove_defines(root, ['UNITY_EDITOR', 'UNITY_EDITOR_OSX', 'UNITY_EDITOR_WIN', 'UNITY_EDITOR_64'])\n\t\t\tremove_platform_defines(root, 'ANDROID')\n\t\t\tremove_platform_defines(root, 'IOS')\n\t\t\tremove_platform_defines(root, 'WEBGL')\n\t\t\tset_defines(root, define)\n\t\ttree.write(pfile)\n\n", "description": null, "category": "remove", "imports": ["import sys, getopt", "import xml.etree.ElementTree as ET", "from defs import *"]}], [{"term": "def", "name": "assert_is_percentage", "data": "def assert_is_percentage(pct):\n\tif not 0 <= pct <= 1:\n\t\traise ValueError(\"Percentage should be float between 0 and 1\")\n\n", "description": null, "category": "remove", "imports": ["import logging", "import networkx as nx", "import random"]}, {"term": "def", "name": "add_random_edges", "data": "def add_random_edges(G, pct):\n\t\"\"\"Add `n` random edges to G (`n` = fraction of current edge count)\n\n\tParameters\n\t----------\n\tG : a networkx.Graph\n\t\tthe network\n\n\tpct : float\n\t\tA percentage (between 0 and 1)\n\t\"\"\"\n\tassert_is_percentage(pct)\n\tm = G.size()\n\tto_add = int(m * pct)\n\tlog.debug(\"Will add %d edges to %d (%f)\", to_add, m, pct)\n\n\tnew_edges = set(nx.non_edges(G))\n\tG.add_edges_from(random.sample(new_edges, to_add), weight=1)\n\n", "description": "Add `n` random edges to G (`n` = fraction of current edge count)\n\n\tParameters\n\t----------\n\tG : a networkx.Graph\n\t\tthe network\n\n\tpct : float\n\t\tA percentage (between 0 and 1)\n\t", "category": "remove", "imports": ["import logging", "import networkx as nx", "import random"]}, {"term": "def", "name": "remove_random_edges", "data": "def remove_random_edges(G, pct):\n\t\"\"\"Randomly remove `n` edges from G (`n` = fraction of current edge count)\n\n\tParameters\n\t----------\n\tG : a networkx.Graph\n\t\tthe network\n\n\tpct : float\n\t\tA percentage (between 0 and 1)\n\t\"\"\"\n\tassert_is_percentage(pct)\n\tedges = G.edges()\n\tm = len(edges)\n\tto_remove = int(m * pct)\n\n\tlog.debug(\"Will remove %d edges of %d (%f)\", to_remove, m, pct)\n\tG.remove_edges_from(random.sample(edges, to_remove))\n\n", "description": "Randomly remove `n` edges from G (`n` = fraction of current edge count)\n\n\tParameters\n\t----------\n\tG : a networkx.Graph\n\t\tthe network\n\n\tpct : float\n\t\tA percentage (between 0 and 1)\n\t", "category": "remove", "imports": ["import logging", "import networkx as nx", "import random"]}, {"term": "def", "name": "add_remove_random_edges", "data": "def add_remove_random_edges(G, pct_add, pct_remove):\n\t\"\"\"Randomly add edges to and remove edges from G\n\n\tParameters\n\t----------\n\tG : a networkx.Graph\n\t\tthe network\n\n\tpct_add : float\n\t\tA percentage (between 0 and 1)\n\n\tpct_remove : float\n\t\tA percentage (between 0 and 1)\n\t\"\"\"\n\tassert_is_percentage(pct_add)\n\tassert_is_percentage(pct_remove)\n\tedges = G.edges()\n\tm = len(edges)\n\tto_add = int(m * pct_add)\n\tto_remove = int(m * pct_remove)\n\tlog.debug(\n\t\t\"Will add %d (%f) edges to and remove %d (%f) edges of %d\",\n\t\tto_add,\n\t\tpct_add,\n\t\tto_remove,\n\t\tpct_remove,\n\t\tm,\n\t)\n\n\tnew_edges = set(nx.non_edges(G))\n\tG.remove_edges_from(random.sample(edges, to_remove))\n\tG.add_edges_from(random.sample(new_edges, to_add))\n", "description": "Randomly add edges to and remove edges from G\n\n\tParameters\n\t----------\n\tG : a networkx.Graph\n\t\tthe network\n\n\tpct_add : float\n\t\tA percentage (between 0 and 1)\n\n\tpct_remove : float\n\t\tA percentage (between 0 and 1)\n\t", "category": "remove", "imports": ["import logging", "import networkx as nx", "import random"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('projects', '0085_auto_20181207_1552'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='account_bank_country',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='account_details',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='account_holder_address',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='account_holder_city',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='account_holder_country',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='account_holder_name',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='account_holder_postal_code',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='project',\n\t\t\tname='account_number',\n\t\t)\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "import bluebottle.utils.fields", "from django.db import migrations, models"]}], [{"term": "class", "name": "Provider", "data": "class Provider(object):\n\tLOCAL_SETUP_WIZARD_EXECUTE = 30030\n\tLOCAL_LOGIN_FAILED = 30121\n\tLOCAL_LOGIN_USERNAME = 30001\n\tLOCAL_LOGIN_PASSWORD = 30002\n\n\tLOCAL_PLEASE_WAIT = 30119\n\n\tLOCAL_FAVORITES = 30100\n\tLOCAL_FAVORITES_ADD = 30101\n\tLOCAL_FAVORITES_REMOVE = 30108\n\n\tLOCAL_WATCH_LATER = 30107\n\tLOCAL_WATCH_LATER_ADD = 30107\n\tLOCAL_WATCH_LATER_REMOVE = 30108\n\n\tLOCAL_SEARCH = 30102\n\tLOCAL_SEARCH_TITLE = 30102\n\tLOCAL_SEARCH_NEW = 30110\n\tLOCAL_SEARCH_RENAME = 30113\n\tLOCAL_SEARCH_REMOVE = 30108\n\tLOCAL_SEARCH_CLEAR = 30120\n\n\tLOCAL_CONFIRM_DELETE = 30114\n\tLOCAL_CONFIRM_REMOVE = 30115\n\tLOCAL_DELETE_CONTENT = 30116\n\tLOCAL_REMOVE_CONTENT = 30117\n\n\tLOCAL_SELECT_VIDEO_QUALITY = 30010\n\n\tLOCAL_SETUP_VIEW_DEFAULT = 30027\n\tLOCAL_SETUP_VIEW_VIDEOS = 30028\n\n\tLOCAL_LIBRARY = 30103\n\tLOCAL_HIGHLIGHTS = 30104\n\tLOCAL_ARCHIVE = 30105\n\tLOCAL_NEXT_PAGE = 30106\n\n\tLOCAL_LATEST_VIDEOS = 30109\n\n\tLOCAL_SETUP_OVERRIDE_VIEW = 30038\n\n\tPATH_SEARCH = '/search/list/'\n\tPATH_SEARCH_LIST = '/search/list/'\n\tPATH_SEARCH_QUERY = '/search/query/'\n\tPATH_SEARCH_CLEAR = '/search/clear/'\n\tPATH_SEARCH_RENAME = '/search/rename/'\n\tPATH_SEARCH_REMOVE = '/search/remove/'\n\n\tPATH_FAVORITES_ADD = '/favorites/add/'\n\tPATH_FAVORITES_LIST = '/favorites/list/'\n\tPATH_FAVORITES_REMOVE = '/favorites/remove/'\n\n\tPATH_WATCH_LATER_ADD = '/watch_later/add/'\n\tPATH_WATCH_LATER_LIST = '/watch_later/list/'\n\tPATH_WATCH_LATER_REMOVE = '/watch_later/remove/'\n\n\tdef __init__(self):\n\t\tpass\n\n\tdef _process_addon_setup(self, context):\n\t\tsettings = context.get_settings()\n\n\t\t# exit if the setup isn't enabled\n\t\tif not settings.is_setup_wizard_enabled():\n\t\t\treturn\n\n\t\t# exit if the setup shouldn't be executed\n\t\tif not context.get_ui().on_yes_no_input(context.get_name(), context.localize(self.LOCAL_SETUP_WIZARD_EXECUTE)):\n\t\t\treturn\n\n\t\tview_manager = ViewManager(context, self)\n\t\tview_manager.setup()\n\n\t\tself.on_setup(context, mode='setup')\n\n\t\t# disable the setup\n\t\tsettings.set_bool(settings.ADDON_SETUP, False)\n\t\tpass\n\n\tdef on_setup(self, context, mode):\n\t\tif mode == 'content-types':\n\t\t\treturn ['default']\n\n\t\treturn None\n\n\tdef select_video_stream(self, context, video_streams, video_quality_index=None, video_item=None):\n\t\t\"\"\"\n\t\tReturns a selected video stream or False if the user aborted\n\t\t:param context: the current context\n\t\t:param video_streams: a list of selectable video streams\n\t\t:param video_item: (optional) video item to update with the uri of the selected video stream\n\t\t:param video_quality_index: index mapping to video quality\n\t\t:return:\n\t\t\"\"\"\n\t\tif not video_quality_index:\n\t\t\tvideo_quality_index = [360, 720]\n\t\t\tpass\n\n\t\tdef _sort_video_streams(_video_stream):\n\t\t\treturn _video_stream.get('sort', 0)\n\n\t\tdef _find_best_fit(_video_streams):\n\t\t\t_video_quality = context.get_settings().get_video_quality(video_quality_index=video_quality_index)\n\t\t\t_last_delta = None\n\n\t\t\t_selected_video_stream = None\n\t\t\tfor _video_stream in _video_streams:\n\t\t\t\t_delta = abs(_video_stream.get('video', {}).get('height', 0) - _video_quality)\n\t\t\t\tif _selected_video_stream is None or _last_delta is None or _last_delta > _delta:\n\t\t\t\t\t_last_delta = _delta\n\t\t\t\t\t_selected_video_stream = _video_stream\n\t\t\t\t\tpass\n\t\t\t\tpass\n\n\t\t\treturn _selected_video_stream\n\n\t\t# sort stream, highest values first based on 'sort' array\n\t\tvideo_streams = sorted(video_streams, key=_sort_video_streams, reverse=True)\n\n\t\t# log all possible steams\n\t\tcontext.log_debug('selectable streams: %d' % len(video_streams))\n\t\tfor video_stream in video_streams:\n\t\t\tcontext.log_debug('selectable stream: %s' % video_stream)\n\t\t\tpass\n\n\t\t# show a list of selectable streams (if enabled)\n\t\tselected_video_stream = None\n\t\tif context.get_settings().ask_for_video_quality() and len(video_streams) > 1:\n\t\t\titems = map(lambda x: (x['title'], x), video_streams)\n\t\t\tselected_video_stream = context.get_ui().on_select(context.localize(self.LOCAL_SELECT_VIDEO_QUALITY), items,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   default=None)\n\t\t\tpass\n\t\telse:\n\t\t\t# fallback - use best fit\n\t\t\tselected_video_stream = _find_best_fit(video_streams)\n\t\t\tpass\n\n\t\tif selected_video_stream is None:\n\t\t\treturn False\n\n\t\t# log selected video stream\n\t\tcontext.log_debug('selected stream: %s' % selected_video_stream)\n\n\t\t# we need an uri in the video stream\n\t\tif not 'uri' in selected_video_stream:\n\t\t\traise ProviderException('Missing uri in video stream')\n\n\t\t# update the given video item (optional)\n\t\tif video_item:\n\t\t\tvideo_item['uri'] = selected_video_stream['uri']\n\t\t\treturn video_item\n\n\t\t# return an uri item\n\t\treturn {'type': 'uri',\n\t\t\t\t'uri': selected_video_stream['uri']}\n\n\tdef navigate(self, context):\n\t\tself._process_addon_setup(context)\n\n\t\tmethod_names = dir(self)\n\t\tfor method_name in method_names:\n\t\t\tmethod = getattr(self, method_name)\n\t\t\tif hasattr(method, 'nightcrawler_registered_path'):\n\t\t\t\tresult = method(context)\n\t\t\t\tif result is not None:\n\t\t\t\t\treturn result\n\t\t\t\tpass\n\t\t\tpass\n\n\t\traise ProviderException('Missing method for path \"%s\"' % context.get_path())\n\n\tdef get_fanart(self, context):\n\t\t\"\"\"\n\t\tCan be overriden by the derived class to return an alternate image\n\t\t:param context: the current image\n\t\t:return: full path to the fanart image\n\t\t\"\"\"\n\t\treturn context.get_fanart()\n\n\tdef on_search(self, context, search_text):\n\t\t\"\"\"\n\t\tThe derived class has to implement this method in case of support for search\n\t\t:param context: the current context\n\t\t:param search_text: the search term in unicode\n\t\t:return: a list of items or False if something went wrong\n\t\t\"\"\"\n\t\traise NotImplementedError()\n\n\t@register_path('/favorites/(?Padd|remove)/')\n\t@register_path_value('method', unicode)\n\t@register_context_value('item', dict, required=True)\n\tdef _internal_favorites_with_item(self, context, method, item):\n\t\tif method == 'add':\n\t\t\tcontext.get_favorite_list().add(item)\n\t\t\treturn True\n\n\t\tif method == 'remove':\n\t\t\tcontext.get_favorite_list().remove(item)\n\t\t\tcontext.get_ui().refresh_container()\n\t\t\treturn True\n\n\t\treturn False\n\n\t@register_path('/favorites/list/')\n\tdef on_favorites_list(self, context):\n\t\tresult = context.get_favorite_list().list()\n\t\tfor directory_item in result:\n\t\t\tcontext_menu = [(context.localize(self.LOCAL_WATCH_LATER_REMOVE),\n\t\t\t\t\t\t\t 'RunPlugin(%s)' % context.create_uri(self.PATH_FAVORITES_REMOVE,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  {'item': json.dumps(directory_item)}))]\n\t\t\tdirectory_item['context-menu'] = {'items': context_menu}\n\t\t\tpass\n\n\t\treturn result\n\n\t@register_path('/watch_later/(?Padd|remove)/')\n\t@register_path_value('method', unicode)\n\t@register_context_value('item', dict, required=True)\n\tdef _internal_watch_later_with_item(self, context, method, item):\n\t\tif method == 'add':\n\t\t\tcontext.get_watch_later_list().add(item)\n\t\t\treturn True\n\n\t\tif method == 'remove':\n\t\t\tcontext.get_watch_later_list().remove(item)\n\t\t\tcontext.get_ui().refresh_container()\n\t\t\treturn True\n\n\t\treturn False\n\n\t@register_path('/watch_later/list/')\n\tdef on_watch_later(self, context):\n\t\tvideo_items = context.get_watch_later_list().list()\n\n\t\tfor video_item in video_items:\n\t\t\tcontext_menu = [(context.localize(self.LOCAL_WATCH_LATER_REMOVE),\n\t\t\t\t\t\t\t 'RunPlugin(%s)' % context.create_uri(self.PATH_WATCH_LATER_REMOVE,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  {'item': json.dumps(video_item)}))]\n\t\t\tvideo_item['context-menu'] = {'items': context_menu}\n\t\t\tpass\n\n\t\treturn video_items\n\n\t@register_path('/search/(?P(remove|rename))/')\n\t@register_path_value('method', unicode)\n\t@register_context_value('q', unicode, alias='query', required=True)\n\tdef _internal_search_with_query(self, context, method, query):\n\t\tsearch_history = context.get_search_history()\n\t\tif method == 'remove':\n\t\t\tsearch_history.remove(query)\n\t\t\tcontext.get_ui().refresh_container()\n\t\t\treturn True\n\n\t\tif method == 'rename':\n\t\t\tresult, new_query = context.get_ui().on_keyboard_input(context.localize(self.LOCAL_SEARCH_RENAME), query)\n\t\t\tif result:\n\t\t\t\tsearch_history.rename(query, new_query)\n\t\t\t\tcontext.get_ui().refresh_container()\n\t\t\t\tpass\n\t\t\treturn True\n\n\t\treturn False\n\n\t@register_path('/search/(?P(list|query|clear))/')\n\t@register_path_value('method', unicode)\n\t@register_context_value('q', unicode, alias='query', default=u'')\n\tdef _internal_search_without_query(self, context, method, query):\n\t\tsearch_history = context.get_search_history()\n\t\tif method == 'clear':\n\t\t\tsearch_history.clear()\n\t\t\tcontext.get_ui().refresh_container()\n\t\t\treturn True\n\n\t\tif method == 'list':\n\t\t\t# add new search\n\t\t\tresult = [{'type': 'folder',\n\t\t\t\t\t   'title': '[B]%s[/B]' % context.localize(self.LOCAL_SEARCH_NEW),\n\t\t\t\t\t   'uri': context.create_uri(self.PATH_SEARCH_QUERY),\n\t\t\t\t\t   'images': {'thumbnail': context.create_resource_path('media/new_search.png'),\n\t\t\t\t\t\t\t\t  'fanart': self.get_fanart(context)}}]\n\n\t\t\tfor query in search_history.list():\n\t\t\t\t# we create a new instance of the SearchItem\n\t\t\t\tcontext_menu = [(context.localize(self.LOCAL_SEARCH_REMOVE),\n\t\t\t\t\t\t\t\t 'RunPlugin(%s)' % context.create_uri(self.PATH_SEARCH_REMOVE, {'q': query})),\n\t\t\t\t\t\t\t\t(context.localize(self.LOCAL_SEARCH_RENAME),\n\t\t\t\t\t\t\t\t 'RunPlugin(%s)' % context.create_uri(self.PATH_SEARCH_RENAME, {'q': query})),\n\t\t\t\t\t\t\t\t(context.localize(self.LOCAL_SEARCH_CLEAR),\n\t\t\t\t\t\t\t\t 'RunPlugin(%s)' % context.create_uri(self.PATH_SEARCH_CLEAR))]\n\t\t\t\titem = {'type': 'folder',\n\t\t\t\t\t\t'title': query,\n\t\t\t\t\t\t'uri': context.create_uri(self.PATH_SEARCH_QUERY, {'q': query}),\n\t\t\t\t\t\t'images': {'thumbnail': context.create_resource_path('media/search.png'),\n\t\t\t\t\t\t\t\t   'fanart': self.get_fanart(context)},\n\t\t\t\t\t\t'context-menu': {'items': context_menu}}\n\t\t\t\tresult.append(item)\n\t\t\t\tpass\n\n\t\t\treturn result\n\n\t\tif method == 'query':\n\t\t\tresult = True\n\t\t\tif not query:\n\t\t\t\tresult, query = context.get_ui().on_keyboard_input(context.localize(self.LOCAL_SEARCH_TITLE))\n\t\t\t\tpass\n\n\t\t\tif result and query:\n\t\t\t\tsearch_history.update(query)\n\t\t\t\treturn self.on_search(context, query)\n\t\t\tpass\n\n\t\treturn False\n\n\tdef handle_exception(self, context, exception_to_handle):\n\t\t\"\"\"\n\t\tCan be overridden by the derived class to handle exceptions\n\t\t:param context: the current context\n\t\t:param exception_to_handle: the caught exception\n\t\t:return: None if nothing can be done. True exception can be handled, False if not. A list of items is also possible.\n\t\t\"\"\"\n\t\treturn None\n\n\tdef tear_down(self, context):\n\t\t\"\"\"\n\t\tCan be overridden by the derived class to free resources or write some stuff to disk/cache\n\t\t:param context: the current context\n\t\t\"\"\"\n\t\tpass\n\n", "description": "\n\t\tReturns a selected video stream or False if the user aborted\n\t\t:param context: the current context\n\t\t:param video_streams: a list of selectable video streams\n\t\t:param video_item: (optional) video item to update with the uri of the selected video stream\n\t\t:param video_quality_index: index mapping to video quality\n\t\t:return:\n\t\t", "category": "remove", "imports": ["import json", "from .core.nightcrawler_decorators import register_path, register_context_value, register_path_value", "from .core.view_manager import ViewManager", "from .exception import ProviderException"]}], [], [], [], [], [], [], [], [], [], [{"term": "class", "name": "TestNbdServerRemove", "data": "class TestNbdServerRemove(iotests.QMPTestCase):\n\tdef setUp(self):\n\t\tqemu_img_create('-f', iotests.imgfmt, disk, '1M')\n\n\t\tself.vm = iotests.VM().add_drive(disk)\n\t\tself.vm.launch()\n\n\t\taddress = {\n\t\t\t'type': 'unix',\n\t\t\t'data': {\n\t\t\t\t'path': nbd_sock\n\t\t\t}\n\t\t}\n\n\t\tresult = self.vm.qmp('nbd-server-start', addr=address)\n\t\tself.assert_qmp(result, 'return', {})\n\t\tresult = self.vm.qmp('nbd-server-add', device='drive0', name='exp')\n\t\tself.assert_qmp(result, 'return', {})\n\n\tdef tearDown(self):\n\t\tself.vm.shutdown()\n\t\tos.remove(nbd_sock)\n\t\tos.remove(disk)\n\n\tdef remove_export(self, name, mode=None):\n\t\tif mode is None:\n\t\t\treturn self.vm.qmp('nbd-server-remove', name=name)\n\t\telse:\n\t\t\treturn self.vm.qmp('nbd-server-remove', name=name, mode=mode)\n\n\tdef assertExportNotFound(self, name):\n\t\tresult = self.vm.qmp('nbd-server-remove', name=name)\n\t\tself.assert_qmp(result, 'error/desc', \"Export 'exp' is not found\")\n\n\tdef assertExistingClients(self, result):\n\t\tself.assert_qmp(result, 'error/desc', \"export 'exp' still in use\")\n\n\tdef assertReadOk(self, qemu_io_output):\n\t\tself.assertEqual(\n\t\t\t\tfilter_qemu_io(qemu_io_output).strip(),\n\t\t\t\t'read 512/512 bytes at offset 0\\n' +\n\t\t\t\t'512 bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)')\n\n\tdef assertReadFailed(self, qemu_io_output):\n\t\tself.assertEqual(filter_qemu_io(qemu_io_output).strip(),\n\t\t\t\t\t\t 'read failed: Input/output error')\n\n\tdef assertConnectFailed(self, qemu_io_output):\n\t\tself.assertEqual(filter_qemu_io(qemu_io_output).strip(),\n\t\t\t\t\t\t \"can't open device \" + nbd_uri +\n\t\t\t\t\t\t \": Requested export not available\\n\"\n\t\t\t\t\t\t \"server reported: export 'exp' not present\")\n\n\tdef do_test_connect_after_remove(self, mode=None):\n\t\targs = ('-r', '-f', 'raw', '-c', 'read 0 512', nbd_uri)\n\t\tself.assertReadOk(qemu_io(*args))\n\n\t\tresult = self.remove_export('exp', mode)\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertExportNotFound('exp')\n\t\tself.assertConnectFailed(qemu_io(*args))\n\n\tdef test_connect_after_remove_default(self):\n\t\tself.do_test_connect_after_remove()\n\n\tdef test_connect_after_remove_safe(self):\n\t\tself.do_test_connect_after_remove('safe')\n\n\tdef test_connect_after_remove_force(self):\n\t\tself.do_test_connect_after_remove('hard')\n\n\tdef do_test_remove_during_connect_safe(self, mode=None):\n\t\tqio = QemuIoInteractive('-r', '-f', 'raw', nbd_uri)\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', mode)\n\t\tself.assertExistingClients(result)\n\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tqio.close()\n\n\t\tresult = self.remove_export('exp', mode)\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertExportNotFound('exp')\n\n\tdef test_remove_during_connect_default(self):\n\t\tself.do_test_remove_during_connect_safe()\n\n\tdef test_remove_during_connect_safe(self):\n\t\tself.do_test_remove_during_connect_safe('safe')\n\n\tdef test_remove_during_connect_hard(self):\n\t\tqio = QemuIoInteractive('-r', '-f', 'raw', nbd_uri)\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', 'hard')\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertReadFailed(qio.cmd('read 0 512'))\n\t\tself.assertExportNotFound('exp')\n\n\t\tqio.close()\n\n\tdef test_remove_during_connect_safe_hard(self):\n\t\tqio = QemuIoInteractive('-r', '-f', 'raw', nbd_uri)\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', 'safe')\n\t\tself.assertExistingClients(result)\n\n\t\tself.assertReadOk(qio.cmd('read 0 512'))\n\n\t\tresult = self.remove_export('exp', 'hard')\n\t\tself.assert_qmp(result, 'return', {})\n\n\t\tself.assertExportNotFound('exp')\n\t\tself.assertReadFailed(qio.cmd('read 0 512'))\n\t\tqio.close()\n\n", "description": null, "category": "remove", "imports": ["import os", "import sys", "import iotests", "import time", "from iotests import qemu_img_create, qemu_io, filter_qemu_io, QemuIoInteractive"]}], [], [], [], [{"term": "def", "name": "sort_dict", "data": "def sort_dict(d: dict) -> dict:\n\treturn dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "print_dict", "data": "def print_dict(d: dict, sorted=False) -> None:\n\tif sorted:\n\t\td = sort_dict(d)\n\tprint(json.dumps(d, indent=4))\n\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "lyrics_to_verses", "data": "def lyrics_to_verses(lyrics: str) -> list:\n\treturn lyrics.split(\"\\n\")\n\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "verses_to_lyrics", "data": "def verses_to_lyrics(verses: list) -> str:\n\treturn \"\\n\".join(verses)\n\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "round_float", "data": "def round_float(number: float, digits: int = 2):\n\treturn round(number, digits)\n\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "read_glove_vector", "data": "def read_glove_vector(glove_vec):\n\twith open(glove_vec, 'r', encoding='UTF-8') as f:\n\t\tword_to_vec_map = {}\n\t\tfor line in f:\n\t\t\tw_line = line.split()\n\t\t\tcurr_word = w_line[0]\n\t\t\tword_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n\t\treturn word_to_vec_map\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "get_glove_embedding", "data": "def get_glove_embedding(glove_dim: int, max_length: int, word_to_vec_map: dict, words_to_index: dict):\n\tvocab_len = len(words_to_index)\n\n\temb_matrix = np.zeros((vocab_len, glove_dim))\n\tfor word, index in words_to_index.items():\n\t\tembedding_vector = word_to_vec_map.get(word)\n\t\tif embedding_vector is not None:\n\t\t\temb_matrix[index-1, :] = embedding_vector\n\n\treturn tensorflow.keras.layers.Embedding(input_dim=vocab_len, output_dim=glove_dim, input_length=max_length, weights=[emb_matrix], trainable=True)\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "get_glove_dim", "data": "def get_glove_dim(glove_filename: str):\n\tif \"50d\" in glove_filename:\n\t\tglove_dim = 50\n\telif \"100d\" in glove_filename:\n\t\tglove_dim = 100\n\telif \"200d\" in glove_filename:\n\t\tglove_dim = 200\n\telif \"300d\" in glove_filename:\n\t\tglove_dim = 300\n\telse:\n\t\texit(\"Glove Dimension not valid. Exiting ...\")\n\treturn glove_dim\n\n\n\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "get_early_stopping_callback", "data": "def get_early_stopping_callback(patience_epochs: int):\n\tcallback = tensorflow.keras.callbacks.EarlyStopping(\n\t\tmonitor='val_accuracy',\n\t\tpatience=patience_epochs,\n\t\tmode='auto',\n\t\tverbose=1,\n\t\trestore_best_weights=True,\n\t)\n\treturn callback\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "get_class_weights", "data": "def get_class_weights(categories: list):\n\tn_categories = len(set(categories))\n\n\tcomputed_class_weights = compute_class_weight(\n\t\tclass_weight='balanced',\n\t\tclasses=np.asarray(list(range(0, n_categories))),\n\t\ty=categories)\n\n\tclass_weights = {}\n\tfor i in range(0, n_categories):\n\t\tclass_weights[i] = computed_class_weights[i]\n\n\treturn class_weights\n\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "preprocessing", "data": "def preprocessing(string_list: list, pipeline: list, verbose: bool=False):\n\tfor method in pipeline:\n\t\tif verbose: print(f\"Preprocessing: {method} ..\")\n\t\tif method == \"remove_artists\":\n\t\t\tartists = get_artists()\n\t\t\tstring_list = [remove_artists(s, artists) for s in string_list]\n\n\t\telif method == \"lower_case\":\n\t\t\tstring_list = [s.lower() for s in string_list]\n\n\t\telif method == \"remove_symbols\":\n\t\t\tstring_list = [remove_symbols(s) for s in string_list]\n\n\t\telif method == \"remove_contractions\":\n\t\t\tstring_list = [remove_contractions(s) for s in string_list]\n\n\t\telif method == \"remove_stopwords\":\n\t\t\tstring_list = [remove_stopwords(s) for s in string_list]\n\n\t\telif method == \"remove_whitespaces\":\n\t\t\tstring_list = [remove_whitespaces(s) for s in string_list]\n\n\t\telse:\n\t\t\texit(f\"Error: Preprocessing method {method} unknown..\")\n\treturn string_list\n\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "get_artists", "data": "def get_artists():\n\tartists = pd.read_csv(DATA_PROCESSED)\n\treturn [str(a) for a in artists['Artist'].unique()]\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "remove_whitespaces", "data": "def remove_whitespaces(text: str):\n\treturn text.strip()\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "remove_stopwords", "data": "def remove_stopwords(text: str):\n\ttext = remove_whitespaces(text)\n\ttext = [w for w in text.split(\" \") if w not in en_stopwords]\n\treturn ' '.join(text)\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "remove_contractions", "data": "def remove_contractions(text: str):\n\ttext = remove_whitespaces(text)\n\ttext = contractions.fix(text)\n\t# exceptions, e.g., \"god's\", \"else's\" -> then take the longest substring fo the word\n\ttext = [max(w.split(\"'\"), key=len) if \"'\" in w else w for w in text.split(\" \")]\n\treturn remove_whitespaces(' '.join(text))\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "remove_symbols", "data": "def remove_symbols(text: str):\n\tfor symbol in SYMBOLS:\n\t\ttext = text.replace(symbol, \" \")\n\treturn remove_whitespaces(text)\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}, {"term": "def", "name": "remove_artists", "data": "def remove_artists(text: str, artists: list):\n\ttext = remove_whitespaces(text)\n\tfor artist in artists:\n\t\tartist = remove_whitespaces(artist)\n\t\ttext = text.replace(artist, \" \")\n\treturn remove_whitespaces(text)\n\n", "description": null, "category": "remove", "imports": ["import json", "import numpy as np", "import tensorflow", "import pandas as pd", "import contractions", "from sklearn.utils.class_weight import compute_class_weight", "from nltk.corpus import stopwords", "from constants import DATA_PROCESSED, SYMBOLS"]}], [{"term": "def", "name": "test_ensure_common_path_no_change", "data": "def test_ensure_common_path_no_change():\n\tdiffs = [[op_remove(\"c\")], [op_patch(\"c\", [op_remove(\"d\")])]]\n\tres = ensure_common_path((\"a\", \"b\"), diffs)\n\tassert res == ((\"a\", \"b\"), diffs)\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_ensure_common_path_single_level", "data": "def test_ensure_common_path_single_level():\n\tdiffs = [[op_patch(\"c\", [op_remove(\"e\")])],\n\t\t\t [op_patch(\"c\", [op_remove(\"d\")])]]\n\tres = ensure_common_path((\"a\", \"b\"), diffs)\n\tassert res == ((\"a\", \"b\", \"c\"), [[op_remove(\"e\")], [op_remove(\"d\")]])\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_ensure_common_path_multilevel_leafs", "data": "def test_ensure_common_path_multilevel_leafs():\n\tdiffs = [[op_patch(\"c\", [op_patch(\"d\", [op_remove(\"e\")])])],\n\t\t\t [op_patch(\"c\", [op_patch(\"d\", [op_remove(\"f\")])])]]\n\tres = ensure_common_path((\"a\", \"b\"), diffs)\n\tassert res == ((\"a\", \"b\", \"c\", \"d\"), [[op_remove(\"e\")], [op_remove(\"f\")]])\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_ensure_common_path_multilevel_intermediate", "data": "def test_ensure_common_path_multilevel_intermediate():\n\tdiffs = [[op_patch(\"c\", [op_patch(\"d\", [op_remove(\"f\")])])],\n\t\t\t [op_patch(\"c\", [op_patch(\"e\", [op_remove(\"g\")])])]]\n\tres = ensure_common_path((\"a\", \"b\"), diffs)\n\tassert res == ((\"a\", \"b\", \"c\"), [[op_patch(\"d\", [op_remove(\"f\")])],\n\t\t\t\t\t\t\t\t\t [op_patch(\"e\", [op_remove(\"g\")])]])\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_ensure_common_path_one_sided_none", "data": "def test_ensure_common_path_one_sided_none():\n\tdiffs = [None,\n\t\t\t [op_patch(\"c\", [op_remove(\"d\")])]]\n\tres = ensure_common_path((\"a\", \"b\"), diffs)\n\tassert res == ((\"a\", \"b\", \"c\"), [None, [op_remove(\"d\")]])\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_ensure_common_path_one_sided_empty", "data": "def test_ensure_common_path_one_sided_empty():\n\tdiffs = [[],\n\t\t\t [op_patch(\"c\", [op_remove(\"d\")])]]\n\tres = ensure_common_path((\"a\", \"b\"), diffs)\n\tassert res == ((\"a\", \"b\", \"c\"), [None, [op_remove(\"d\")]])\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_ensure_common_path_one_sided_remote", "data": "def test_ensure_common_path_one_sided_remote():\n\tdiffs = [[op_patch(\"c\", [op_remove(\"d\")])],\n\t\t\t []]\n\tres = ensure_common_path((\"a\", \"b\"), diffs)\n\tassert res == ((\"a\", \"b\", \"c\"), [[op_remove(\"d\")], None])\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_merge_builder_ensures_common_path", "data": "def test_merge_builder_ensures_common_path():\n\tb = MergeDecisionBuilder()\n\tb.conflict((\"a\", \"b\"),\n\t\t\t   [op_patch(\"c\", [op_remove(\"d\")])],\n\t\t\t   [op_patch(\"c\", [op_remove(\"e\")])])\n\tassert len(b.decisions) == 1\n\tassert b.decisions[0].common_path == (\"a\", \"b\", \"c\")\n\tassert b.decisions[0].local_diff == [op_remove(\"d\")]\n\tassert b.decisions[0].remote_diff == [op_remove(\"e\")]\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_pop_patch_unpoppable", "data": "def test_pop_patch_unpoppable():\n\tmd = MergeDecision(\n\t\tcommon_path=(\"a\", \"b\"),\n\t\taction=\"base\",\n\t\tconflict=True,\n\t\tlocal_diff=[op_remove(\"c\")],\n\t\tremote_diff=[op_patch(\"c\", [op_remove(\"d\")])]\n\t)\n\tdec = pop_patch_decision(md)\n\tassert dec is None\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_pop_patch_single_level", "data": "def test_pop_patch_single_level():\n\tmd = MergeDecision(\n\t\tcommon_path=(\"a\", \"b\"),\n\t\taction=\"base\",\n\t\tconflict=True,\n\t\tlocal_diff=[op_patch(\"c\", [op_remove(\"d\")])],\n\t\tremote_diff=[op_patch(\"c\", [op_remove(\"e\")])]\n\t)\n\tdec = pop_patch_decision(md)\n\tassert dec is not None\n\tassert dec.common_path == (\"a\", \"b\", \"c\")\n\tassert dec.local_diff == [op_remove(\"d\")]\n\tassert dec.remote_diff == [op_remove(\"e\")]\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_pop_patch_multilevel", "data": "def test_pop_patch_multilevel():\n\tmd = MergeDecision(\n\t\tcommon_path=(\"a\", \"b\"),\n\t\taction=\"base\",\n\t\tconflict=True,\n\t\tlocal_diff=[op_patch(\"c\", [op_patch(\"d\", [op_remove(\"e\")])])],\n\t\tremote_diff=[op_patch(\"c\", [op_patch(\"d\", [op_remove(\"f\")])])]\n\t)\n\tdec = pop_patch_decision(md)\n\tdec = pop_patch_decision(dec)\n\tassert dec.common_path == (\"a\", \"b\", \"c\", \"d\")\n\tassert dec.local_diff == [op_remove(\"e\")]\n\tassert dec.remote_diff == [op_remove(\"f\")]\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_build_diffs_unsorted", "data": "def test_build_diffs_unsorted():\n\tb = MergeDecisionBuilder()\n\tb.onesided((), [op_remove('a')], None)\n\tb.onesided(('b',), [op_remove('j')], None)\n\tb.onesided(('c',), [op_remove('k')], None)\n\tb.onesided(('d',), [op_remove('l')], None)\n\tbase = dict(a=1, b=dict(i=2), c=dict(j=3), d=dict(k=4))\n\tdiff = build_diffs(base, b.decisions, 'local')\n\tassert len(diff) == 4\n\tassert diff[0] == op_remove('a')\n\tassert diff[1] == op_patch('b', [op_remove('j')])\n\tassert diff[2] == op_patch('c', [op_remove('k')])\n\tassert diff[3] == op_patch('d', [op_remove('l')])\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}, {"term": "def", "name": "test_build_diffs_sorted", "data": "def test_build_diffs_sorted():\n\tb = MergeDecisionBuilder()\n\tb.onesided((), [op_remove('a')], None)\n\tb.onesided(('b',), [op_remove('j')], None)\n\tb.onesided(('c',), [op_remove('k')], None)\n\tb.onesided(('d',), [op_remove('l')], None)\n\tbase = dict(a=1, b=dict(i=2), c=dict(j=3), d=dict(k=4))\n\tdecisions = b.validated(base)\n\tdiff = build_diffs(base, decisions, 'local')\n\tassert len(diff) == 4\n\tassert diff[0] == op_patch('d', [op_remove('l')])\n\tassert diff[1] == op_patch('c', [op_remove('k')])\n\tassert diff[2] == op_patch('b', [op_remove('j')])\n\tassert diff[3] == op_remove('a')\n\n", "description": null, "category": "remove", "imports": ["import pytest", "from nbdime.diff_format import op_remove, op_patch", "from nbdime.merging.decisions import ("]}], [], [], [], [], [], [{"term": "class", "name": "classSolution:", "data": "class Solution:\n\tdef removeKdigits(self, num: str, k: int) -> str:\n\t\tnumStack = []\n\t\t\n\t\t# Append increasing digits\n\t\tfor digit in num:\n\t\t\t# If this digit is less than the top of the stack, remove the top of the stack\n\t\t\twhile k and numStack and numStack[-1] > digit:\n\t\t\t\tnumStack.pop()\n\t\t\t\tk -= 1\n\t\t\n\t\t\tnumStack.append(digit)\n\t\t\n\t\t# If we still have digits to remove, remove the last k (in case of all increasing)\n\t\tif k > 0:\n\t\t\tnumStack = numStack[:-k]\n\t\t\n\t\t# trim the leading zeros\n\t\treturn \"\".join(numStack).lstrip('0') or \"0\"\n\t\n\t# This is the same logic as above, just not as python-y\n\tdef removeKdigitsStack(self, num: str, k: int) -> str:\n\t\tnum_len = len(num)\n\t\tif k >= num_len:\n\t\t\treturn \"0\"\n\n\t\tremovedIndexes = set()\n\t\tdigitsToRemove = k\n\t\tstack = []\n\t\t# Figure out what to remove\n\t\tfor i in range(num_len):\n\t\t\tdigit = int(num[i])\n\t\t\tif not stack or digit >= stack[-1][0]:\n\t\t\t\tstack.append([digit, i])\n\t\t\t# this number is less than the previous one, so remove previous and continue checking\n\t\t\telse:\n\t\t\t\twhile digitsToRemove > 0 and stack and digit < stack[-1][0]:\n\t\t\t\t\tremovedIndexes.add(stack[-1][1])\n\t\t\t\t\tstack.pop()\n\t\t\t\t\tdigitsToRemove -= 1\n\n\t\t\t\tstack.append([digit, i])\n\n\t\t# If any remaining digits to remove, do so\n\t\twhile digitsToRemove > 0:\n\t\t\tremovedIndexes.add(stack[-1][1])\n\t\t\tstack.pop()\n\t\t\tdigitsToRemove -= 1\n\n\t\t# Create new string with number to return\n\t\tanswerString = \"0\"\n\t\tfor i in range(num_len):\n\t\t\tif i in removedIndexes:\n\t\t\t\tcontinue\n\t\t\tchar = num[i]\n\t\t\t# Don't add leading 0's\n\t\t\tif answerString == \"0\":\n\t\t\t\tif char == \"0\":\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\tanswerString = char\n\t\t\telse:\n\t\t\t\tanswerString += char\n\n\t\treturn answerString\n\t\n\t# Slow implementation that each time looks the length of the string for what to remove\n\tdef removeKdigitsSlow(self, num: str, k: int) -> str:\n\t\tnum_len = len(num)\n\t\tif k >= num_len:\n\t\t\treturn \"0\"\n\n\t\tremovedIndexes = set()\n\t\tdigitsToRemove = k\n\t\t# Figure out what to remove\n\t\twhile digitsToRemove > 0:\n\t\t\tpreviousDigit = 10\n\t\t\tpreviousIndex = num_len - 1\n\t\t\tremovedIndex = False\n\t\t\tfor i in range(num_len):\n\t\t\t\tif i in removedIndexes:\n\t\t\t\t\tcontinue\n\n\t\t\t\tdigit = int(num[i])\n\t\t\t\tif digit < previousDigit and previousDigit != 10:\n\t\t\t\t\tremovedIndexes.add(previousIndex)\n\t\t\t\t\tremovedIndex = True\n\t\t\t\t\tbreak\n\n\t\t\t\tpreviousDigit = digit\n\t\t\t\tpreviousIndex = i\n\t\t\t\n\t\t\t# See if number to remove is last one\n\t\t\tif not removedIndex:\n\t\t\t\tremovedIndexes.add(previousIndex)\n\n\t\t\tdigitsToRemove -= 1\n\n\t\t# Create new string with number to return\n\t\tanswerString = \"0\"\n\t\tfor i in range(num_len):\n\t\t\tif i in removedIndexes:\n\t\t\t\tcontinue\n\t\t\tchar = num[i]\n\t\t\t# Don't add leading 0's\n\t\t\tif answerString == \"0\":\n\t\t\t\tif char == \"0\":\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\tanswerString = char\n\t\t\telse:\n\t\t\t\tanswerString += char\n\n\t\treturn answerString\n", "description": null, "category": "remove", "imports": []}], [], [], [], [], [], [], [{"term": "def", "name": "_script_names", "data": "def _script_names(dist, script_name, is_gui):\n\t# type: (Distribution, str, bool) -> List[str]\n\t\"\"\"Create the fully qualified name of the files created by\n\t{console,gui}_scripts for the given ``dist``.\n\tReturns the list of file names\n\t\"\"\"\n\tif dist_in_usersite(dist):\n\t\tbin_dir = bin_user\n\telse:\n\t\tbin_dir = bin_py\n\texe_name = os.path.join(bin_dir, script_name)\n\tpaths_to_remove = [exe_name]\n\tif WINDOWS:\n\t\tpaths_to_remove.append(exe_name + '.exe')\n\t\tpaths_to_remove.append(exe_name + '.exe.manifest')\n\t\tif is_gui:\n\t\t\tpaths_to_remove.append(exe_name + '-script.pyw')\n\t\telse:\n\t\t\tpaths_to_remove.append(exe_name + '-script.py')\n\treturn paths_to_remove\n\n", "description": "Create the fully qualified name of the files created by\n\t{console,gui}_scripts for the given ``dist``.\n\tReturns the list of file names\n\t", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "_unique", "data": "def _unique(fn):\n\t# type: (Callable[..., Iterator[Any]]) -> Callable[..., Iterator[Any]]\n\t@functools.wraps(fn)\n\tdef unique(*args, **kw):\n\t\t# type: (Any, Any) -> Iterator[Any]\n\t\tseen = set()  # type: Set[Any]\n\t\tfor item in fn(*args, **kw):\n\t\t\tif item not in seen:\n\t\t\t\tseen.add(item)\n\t\t\t\tyield item\n\treturn unique\n\n", "description": null, "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "uninstallation_paths", "data": "def uninstallation_paths(dist):\n\t# type: (Distribution) -> Iterator[str]\n\t\"\"\"\n\tYield all the uninstallation paths for dist based on RECORD-without-.py[co]\n\n\tYield paths to all the files in RECORD. For each .py file in RECORD, add\n\tthe .pyc and .pyo in the same directory.\n\n\tUninstallPathSet.add() takes care of the __pycache__ .py[co].\n\t\"\"\"\n\tr = csv.reader(dist.get_metadata_lines('RECORD'))\n\tfor row in r:\n\t\tpath = os.path.join(dist.location, row[0])\n\t\tyield path\n\t\tif path.endswith('.py'):\n\t\t\tdn, fn = os.path.split(path)\n\t\t\tbase = fn[:-3]\n\t\t\tpath = os.path.join(dn, base + '.pyc')\n\t\t\tyield path\n\t\t\tpath = os.path.join(dn, base + '.pyo')\n\t\t\tyield path\n\n", "description": "\n\tYield all the uninstallation paths for dist based on RECORD-without-.py[co]\n\n\tYield paths to all the files in RECORD. For each .py file in RECORD, add\n\tthe .pyc and .pyo in the same directory.\n\n\tUninstallPathSet.add() takes care of the __pycache__ .py[co].\n\t", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "compact", "data": "def compact(paths):\n\t# type: (Iterable[str]) -> Set[str]\n\t\"\"\"Compact a path set to contain the minimal number of paths\n\tnecessary to contain all paths in the set. If /a/path/ and\n\t/a/path/to/a/file.txt are both in the set, leave only the\n\tshorter path.\"\"\"\n\n\tsep = os.path.sep\n\tshort_paths = set()  # type: Set[str]\n\tfor path in sorted(paths, key=len):\n\t\tshould_skip = any(\n\t\t\tpath.startswith(shortpath.rstrip(\"*\")) and\n\t\t\tpath[len(shortpath.rstrip(\"*\").rstrip(sep))] == sep\n\t\t\tfor shortpath in short_paths\n\t\t)\n\t\tif not should_skip:\n\t\t\tshort_paths.add(path)\n\treturn short_paths\n\n", "description": "Compact a path set to contain the minimal number of paths\n\tnecessary to contain all paths in the set. If /a/path/ and\n\t/a/path/to/a/file.txt are both in the set, leave only the\n\tshorter path.", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "compress_for_rename", "data": "def compress_for_rename(paths):\n\t# type: (Iterable[str]) -> Set[str]\n\t\"\"\"Returns a set containing the paths that need to be renamed.\n\n\tThis set may include directories when the original sequence of paths\n\tincluded every file on disk.\n\t\"\"\"\n\tcase_map = {os.path.normcase(p): p for p in paths}\n\tremaining = set(case_map)\n\tunchecked = sorted({os.path.split(p)[0] for p in case_map.values()}, key=len)\n\twildcards = set()  # type: Set[str]\n\n\tdef norm_join(*a):\n\t\t# type: (str) -> str\n\t\treturn os.path.normcase(os.path.join(*a))\n\n\tfor root in unchecked:\n\t\tif any(os.path.normcase(root).startswith(w)\n\t\t\t   for w in wildcards):\n\t\t\t# This directory has already been handled.\n\t\t\tcontinue\n\n\t\tall_files = set()  # type: Set[str]\n\t\tall_subdirs = set()  # type: Set[str]\n\t\tfor dirname, subdirs, files in os.walk(root):\n\t\t\tall_subdirs.update(norm_join(root, dirname, d)\n\t\t\t\t\t\t\t   for d in subdirs)\n\t\t\tall_files.update(norm_join(root, dirname, f)\n\t\t\t\t\t\t\t for f in files)\n\t\t# If all the files we found are in our remaining set of files to\n\t\t# remove, then remove them from the latter set and add a wildcard\n\t\t# for the directory.\n\t\tif not (all_files - remaining):\n\t\t\tremaining.difference_update(all_files)\n\t\t\twildcards.add(root + os.sep)\n\n\treturn set(map(case_map.__getitem__, remaining)) | wildcards\n\n", "description": "Returns a set containing the paths that need to be renamed.\n\n\tThis set may include directories when the original sequence of paths\n\tincluded every file on disk.\n\t", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "compress_for_output_listing", "data": "def compress_for_output_listing(paths):\n\t# type: (Iterable[str]) -> Tuple[Set[str], Set[str]]\n\t\"\"\"Returns a tuple of 2 sets of which paths to display to user\n\n\tThe first set contains paths that would be deleted. Files of a package\n\tare not added and the top-level directory of the package has a '*' added\n\tat the end - to signify that all it's contents are removed.\n\n\tThe second set contains files that would have been skipped in the above\n\tfolders.\n\t\"\"\"\n\n\twill_remove = set(paths)\n\twill_skip = set()\n\n\t# Determine folders and files\n\tfolders = set()\n\tfiles = set()\n\tfor path in will_remove:\n\t\tif path.endswith(\".pyc\"):\n\t\t\tcontinue\n\t\tif path.endswith(\"__init__.py\") or \".dist-info\" in path:\n\t\t\tfolders.add(os.path.dirname(path))\n\t\tfiles.add(path)\n\n\t# probably this one https://github.com/python/mypy/issues/390\n\t_normcased_files = set(map(os.path.normcase, files))  # type: ignore\n\n\tfolders = compact(folders)\n\n\t# This walks the tree using os.walk to not miss extra folders\n\t# that might get added.\n\tfor folder in folders:\n\t\tfor dirpath, _, dirfiles in os.walk(folder):\n\t\t\tfor fname in dirfiles:\n\t\t\t\tif fname.endswith(\".pyc\"):\n\t\t\t\t\tcontinue\n\n\t\t\t\tfile_ = os.path.join(dirpath, fname)\n\t\t\t\tif (os.path.isfile(file_) and\n\t\t\t\t\t\tos.path.normcase(file_) not in _normcased_files):\n\t\t\t\t\t# We are skipping this file. Add it to the set.\n\t\t\t\t\twill_skip.add(file_)\n\n\twill_remove = files | {\n\t\tos.path.join(folder, \"*\") for folder in folders\n\t}\n\n\treturn will_remove, will_skip\n\n", "description": "Returns a tuple of 2 sets of which paths to display to user\n\n\tThe first set contains paths that would be deleted. Files of a package\n\tare not added and the top-level directory of the package has a '*' added\n\tat the end - to signify that all it's contents are removed.\n\n\tThe second set contains files that would have been skipped in the above\n\tfolders.\n\t", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "class", "name": "classStashedUninstallPathSet:", "data": "class StashedUninstallPathSet:\n\t\"\"\"A set of file rename operations to stash files while\n\ttentatively uninstalling them.\"\"\"\n\tdef __init__(self):\n\t\t# type: () -> None\n\t\t# Mapping from source file root to [Adjacent]TempDirectory\n\t\t# for files under that directory.\n\t\tself._save_dirs = {}  # type: Dict[str, TempDirectory]\n\t\t# (old path, new path) tuples for each move that may need\n\t\t# to be undone.\n\t\tself._moves = []  # type: List[Tuple[str, str]]\n\n\tdef _get_directory_stash(self, path):\n\t\t# type: (str) -> str\n\t\t\"\"\"Stashes a directory.\n\n\t\tDirectories are stashed adjacent to their original location if\n\t\tpossible, or else moved/copied into the user's temp dir.\"\"\"\n\n\t\ttry:\n\t\t\tsave_dir = AdjacentTempDirectory(path)  # type: TempDirectory\n\t\texcept OSError:\n\t\t\tsave_dir = TempDirectory(kind=\"uninstall\")\n\t\tself._save_dirs[os.path.normcase(path)] = save_dir\n\n\t\treturn save_dir.path\n\n\tdef _get_file_stash(self, path):\n\t\t# type: (str) -> str\n\t\t\"\"\"Stashes a file.\n\n\t\tIf no root has been provided, one will be created for the directory\n\t\tin the user's temp directory.\"\"\"\n\t\tpath = os.path.normcase(path)\n\t\thead, old_head = os.path.dirname(path), None\n\t\tsave_dir = None\n\n\t\twhile head != old_head:\n\t\t\ttry:\n\t\t\t\tsave_dir = self._save_dirs[head]\n\t\t\t\tbreak\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\thead, old_head = os.path.dirname(head), head\n\t\telse:\n\t\t\t# Did not find any suitable root\n\t\t\thead = os.path.dirname(path)\n\t\t\tsave_dir = TempDirectory(kind='uninstall')\n\t\t\tself._save_dirs[head] = save_dir\n\n\t\trelpath = os.path.relpath(path, head)\n\t\tif relpath and relpath != os.path.curdir:\n\t\t\treturn os.path.join(save_dir.path, relpath)\n\t\treturn save_dir.path\n\n\tdef stash(self, path):\n\t\t# type: (str) -> str\n\t\t\"\"\"Stashes the directory or file and returns its new location.\n\t\tHandle symlinks as files to avoid modifying the symlink targets.\n\t\t\"\"\"\n\t\tpath_is_dir = os.path.isdir(path) and not os.path.islink(path)\n\t\tif path_is_dir:\n\t\t\tnew_path = self._get_directory_stash(path)\n\t\telse:\n\t\t\tnew_path = self._get_file_stash(path)\n\n\t\tself._moves.append((path, new_path))\n\t\tif (path_is_dir and os.path.isdir(new_path)):\n\t\t\t# If we're moving a directory, we need to\n\t\t\t# remove the destination first or else it will be\n\t\t\t# moved to inside the existing directory.\n\t\t\t# We just created new_path ourselves, so it will\n\t\t\t# be removable.\n\t\t\tos.rmdir(new_path)\n\t\trenames(path, new_path)\n\t\treturn new_path\n\n\tdef commit(self):\n\t\t# type: () -> None\n\t\t\"\"\"Commits the uninstall by removing stashed files.\"\"\"\n\t\tfor _, save_dir in self._save_dirs.items():\n\t\t\tsave_dir.cleanup()\n\t\tself._moves = []\n\t\tself._save_dirs = {}\n\n\tdef rollback(self):\n\t\t# type: () -> None\n\t\t\"\"\"Undoes the uninstall by moving stashed files back.\"\"\"\n\t\tfor p in self._moves:\n\t\t\tlogger.info(\"Moving to %s\\n from %s\", *p)\n\n\t\tfor new_path, path in self._moves:\n\t\t\ttry:\n\t\t\t\tlogger.debug('Replacing %s from %s', new_path, path)\n\t\t\t\tif os.path.isfile(new_path) or os.path.islink(new_path):\n\t\t\t\t\tos.unlink(new_path)\n\t\t\t\telif os.path.isdir(new_path):\n\t\t\t\t\trmtree(new_path)\n\t\t\t\trenames(path, new_path)\n\t\t\texcept OSError as ex:\n\t\t\t\tlogger.error(\"Failed to restore %s\", new_path)\n\t\t\t\tlogger.debug(\"Exception: %s\", ex)\n\n\t\tself.commit()\n\n\t@property\n\tdef can_rollback(self):\n\t\t# type: () -> bool\n\t\treturn bool(self._moves)\n\n", "description": "A set of file rename operations to stash files while\n\ttentatively uninstalling them.", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "class", "name": "classUninstallPathSet:", "data": "class UninstallPathSet:\n\t\"\"\"A set of file paths to be removed in the uninstallation of a\n\trequirement.\"\"\"\n\tdef __init__(self, dist):\n\t\t# type: (Distribution) -> None\n\t\tself.paths = set()  # type: Set[str]\n\t\tself._refuse = set()  # type: Set[str]\n\t\tself.pth = {}  # type: Dict[str, UninstallPthEntries]\n\t\tself.dist = dist\n\t\tself._moved_paths = StashedUninstallPathSet()\n\n\tdef _permitted(self, path):\n\t\t# type: (str) -> bool\n\t\t\"\"\"\n\t\tReturn True if the given path is one we are permitted to\n\t\tremove/modify, False otherwise.\n\n\t\t\"\"\"\n\t\treturn is_local(path)\n\n\tdef add(self, path):\n\t\t# type: (str) -> None\n\t\thead, tail = os.path.split(path)\n\n\t\t# we normalize the head to resolve parent directory symlinks, but not\n\t\t# the tail, since we only want to uninstall symlinks, not their targets\n\t\tpath = os.path.join(normalize_path(head), os.path.normcase(tail))\n\n\t\tif not os.path.exists(path):\n\t\t\treturn\n\t\tif self._permitted(path):\n\t\t\tself.paths.add(path)\n\t\telse:\n\t\t\tself._refuse.add(path)\n\n\t\t# __pycache__ files can show up after 'installed-files.txt' is created,\n\t\t# due to imports\n\t\tif os.path.splitext(path)[1] == '.py':\n\t\t\tself.add(cache_from_source(path))\n\n\tdef add_pth(self, pth_file, entry):\n\t\t# type: (str, str) -> None\n\t\tpth_file = normalize_path(pth_file)\n\t\tif self._permitted(pth_file):\n\t\t\tif pth_file not in self.pth:\n\t\t\t\tself.pth[pth_file] = UninstallPthEntries(pth_file)\n\t\t\tself.pth[pth_file].add(entry)\n\t\telse:\n\t\t\tself._refuse.add(pth_file)\n\n\tdef remove(self, auto_confirm=False, verbose=False):\n\t\t# type: (bool, bool) -> None\n\t\t\"\"\"Remove paths in ``self.paths`` with confirmation (unless\n\t\t``auto_confirm`` is True).\"\"\"\n\n\t\tif not self.paths:\n\t\t\tlogger.info(\n\t\t\t\t\"Can't uninstall '%s'. No files were found to uninstall.\",\n\t\t\t\tself.dist.project_name,\n\t\t\t)\n\t\t\treturn\n\n\t\tdist_name_version = (\n\t\t\tself.dist.project_name + \"-\" + self.dist.version\n\t\t)\n\t\tlogger.info('Uninstalling %s:', dist_name_version)\n\n\t\twith indent_log():\n\t\t\tif auto_confirm or self._allowed_to_proceed(verbose):\n\t\t\t\tmoved = self._moved_paths\n\n\t\t\t\tfor_rename = compress_for_rename(self.paths)\n\n\t\t\t\tfor path in sorted(compact(for_rename)):\n\t\t\t\t\tmoved.stash(path)\n\t\t\t\t\tlogger.debug('Removing file or directory %s', path)\n\n\t\t\t\tfor pth in self.pth.values():\n\t\t\t\t\tpth.remove()\n\n\t\t\t\tlogger.info('Successfully uninstalled %s', dist_name_version)\n\n\tdef _allowed_to_proceed(self, verbose):\n\t\t# type: (bool) -> bool\n\t\t\"\"\"Display which files would be deleted and prompt for confirmation\n\t\t\"\"\"\n\n\t\tdef _display(msg, paths):\n\t\t\t# type: (str, Iterable[str]) -> None\n\t\t\tif not paths:\n\t\t\t\treturn\n\n\t\t\tlogger.info(msg)\n\t\t\twith indent_log():\n\t\t\t\tfor path in sorted(compact(paths)):\n\t\t\t\t\tlogger.info(path)\n\n\t\tif not verbose:\n\t\t\twill_remove, will_skip = compress_for_output_listing(self.paths)\n\t\telse:\n\t\t\t# In verbose mode, display all the files that are going to be\n\t\t\t# deleted.\n\t\t\twill_remove = set(self.paths)\n\t\t\twill_skip = set()\n\n\t\t_display('Would remove:', will_remove)\n\t\t_display('Would not remove (might be manually added):', will_skip)\n\t\t_display('Would not remove (outside of prefix):', self._refuse)\n\t\tif verbose:\n\t\t\t_display('Will actually move:', compress_for_rename(self.paths))\n\n\t\treturn ask('Proceed (y/n)? ', ('y', 'n')) == 'y'\n\n\tdef rollback(self):\n\t\t# type: () -> None\n\t\t\"\"\"Rollback the changes previously made by remove().\"\"\"\n\t\tif not self._moved_paths.can_rollback:\n\t\t\tlogger.error(\n\t\t\t\t\"Can't roll back %s; was not uninstalled\",\n\t\t\t\tself.dist.project_name,\n\t\t\t)\n\t\t\treturn\n\t\tlogger.info('Rolling back uninstall of %s', self.dist.project_name)\n\t\tself._moved_paths.rollback()\n\t\tfor pth in self.pth.values():\n\t\t\tpth.rollback()\n\n\tdef commit(self):\n\t\t# type: () -> None\n\t\t\"\"\"Remove temporary save dir: rollback will no longer be possible.\"\"\"\n\t\tself._moved_paths.commit()\n\n\t@classmethod\n\tdef from_dist(cls, dist):\n\t\t# type: (Distribution) -> UninstallPathSet\n\t\tdist_path = normalize_path(dist.location)\n\t\tif not dist_is_local(dist):\n\t\t\tlogger.info(\n\t\t\t\t\"Not uninstalling %s at %s, outside environment %s\",\n\t\t\t\tdist.key,\n\t\t\t\tdist_path,\n\t\t\t\tsys.prefix,\n\t\t\t)\n\t\t\treturn cls(dist)\n\n\t\tif dist_path in {p for p in {sysconfig.get_path(\"stdlib\"),\n\t\t\t\t\t\t\t\t\t sysconfig.get_path(\"platstdlib\")}\n\t\t\t\t\t\t if p}:\n\t\t\tlogger.info(\n\t\t\t\t\"Not uninstalling %s at %s, as it is in the standard library.\",\n\t\t\t\tdist.key,\n\t\t\t\tdist_path,\n\t\t\t)\n\t\t\treturn cls(dist)\n\n\t\tpaths_to_remove = cls(dist)\n\t\tdevelop_egg_link = egg_link_path(dist)\n\t\tdevelop_egg_link_egg_info = '{}.egg-info'.format(\n\t\t\tpkg_resources.to_filename(dist.project_name))\n\t\tegg_info_exists = dist.egg_info and os.path.exists(dist.egg_info)\n\t\t# Special case for distutils installed package\n\t\tdistutils_egg_info = getattr(dist._provider, 'path', None)\n\n\t\t# Uninstall cases order do matter as in the case of 2 installs of the\n\t\t# same package, pip needs to uninstall the currently detected version\n\t\tif (egg_info_exists and dist.egg_info.endswith('.egg-info') and\n\t\t\t\tnot dist.egg_info.endswith(develop_egg_link_egg_info)):\n\t\t\t# if dist.egg_info.endswith(develop_egg_link_egg_info), we\n\t\t\t# are in fact in the develop_egg_link case\n\t\t\tpaths_to_remove.add(dist.egg_info)\n\t\t\tif dist.has_metadata('installed-files.txt'):\n\t\t\t\tfor installed_file in dist.get_metadata(\n\t\t\t\t\t\t'installed-files.txt').splitlines():\n\t\t\t\t\tpath = os.path.normpath(\n\t\t\t\t\t\tos.path.join(dist.egg_info, installed_file)\n\t\t\t\t\t)\n\t\t\t\t\tpaths_to_remove.add(path)\n\t\t\t# FIXME: need a test for this elif block\n\t\t\t# occurs with --single-version-externally-managed/--record outside\n\t\t\t# of pip\n\t\t\telif dist.has_metadata('top_level.txt'):\n\t\t\t\tif dist.has_metadata('namespace_packages.txt'):\n\t\t\t\t\tnamespaces = dist.get_metadata('namespace_packages.txt')\n\t\t\t\telse:\n\t\t\t\t\tnamespaces = []\n\t\t\t\tfor top_level_pkg in [\n\t\t\t\t\t\tp for p\n\t\t\t\t\t\tin dist.get_metadata('top_level.txt').splitlines()\n\t\t\t\t\t\tif p and p not in namespaces]:\n\t\t\t\t\tpath = os.path.join(dist.location, top_level_pkg)\n\t\t\t\t\tpaths_to_remove.add(path)\n\t\t\t\t\tpaths_to_remove.add(path + '.py')\n\t\t\t\t\tpaths_to_remove.add(path + '.pyc')\n\t\t\t\t\tpaths_to_remove.add(path + '.pyo')\n\n\t\telif distutils_egg_info:\n\t\t\traise UninstallationError(\n\t\t\t\t\"Cannot uninstall {!r}. It is a distutils installed project \"\n\t\t\t\t\"and thus we cannot accurately determine which files belong \"\n\t\t\t\t\"to it which would lead to only a partial uninstall.\".format(\n\t\t\t\t\tdist.project_name,\n\t\t\t\t)\n\t\t\t)\n\n\t\telif dist.location.endswith('.egg'):\n\t\t\t# package installed by easy_install\n\t\t\t# We cannot match on dist.egg_name because it can slightly vary\n\t\t\t# i.e. setuptools-0.6c11-py2.6.egg vs setuptools-0.6rc11-py2.6.egg\n\t\t\tpaths_to_remove.add(dist.location)\n\t\t\teasy_install_egg = os.path.split(dist.location)[1]\n\t\t\teasy_install_pth = os.path.join(os.path.dirname(dist.location),\n\t\t\t\t\t\t\t\t\t\t\t'easy-install.pth')\n\t\t\tpaths_to_remove.add_pth(easy_install_pth, './' + easy_install_egg)\n\n\t\telif egg_info_exists and dist.egg_info.endswith('.dist-info'):\n\t\t\tfor path in uninstallation_paths(dist):\n\t\t\t\tpaths_to_remove.add(path)\n\n\t\telif develop_egg_link:\n\t\t\t# develop egg\n\t\t\twith open(develop_egg_link, 'r') as fh:\n\t\t\t\tlink_pointer = os.path.normcase(fh.readline().strip())\n\t\t\tassert (link_pointer == dist.location), (\n\t\t\t\t'Egg-link {} does not match installed location of {} '\n\t\t\t\t'(at {})'.format(\n\t\t\t\t\tlink_pointer, dist.project_name, dist.location)\n\t\t\t)\n\t\t\tpaths_to_remove.add(develop_egg_link)\n\t\t\teasy_install_pth = os.path.join(os.path.dirname(develop_egg_link),\n\t\t\t\t\t\t\t\t\t\t\t'easy-install.pth')\n\t\t\tpaths_to_remove.add_pth(easy_install_pth, dist.location)\n\n\t\telse:\n\t\t\tlogger.debug(\n\t\t\t\t'Not sure how to uninstall: %s - Check: %s',\n\t\t\t\tdist, dist.location,\n\t\t\t)\n\n\t\t# find distutils scripts= scripts\n\t\tif dist.has_metadata('scripts') and dist.metadata_isdir('scripts'):\n\t\t\tfor script in dist.metadata_listdir('scripts'):\n\t\t\t\tif dist_in_usersite(dist):\n\t\t\t\t\tbin_dir = bin_user\n\t\t\t\telse:\n\t\t\t\t\tbin_dir = bin_py\n\t\t\t\tpaths_to_remove.add(os.path.join(bin_dir, script))\n\t\t\t\tif WINDOWS:\n\t\t\t\t\tpaths_to_remove.add(os.path.join(bin_dir, script) + '.bat')\n\n\t\t# find console_scripts\n\t\t_scripts_to_remove = []\n\t\tconsole_scripts = dist.get_entry_map(group='console_scripts')\n\t\tfor name in console_scripts.keys():\n\t\t\t_scripts_to_remove.extend(_script_names(dist, name, False))\n\t\t# find gui_scripts\n\t\tgui_scripts = dist.get_entry_map(group='gui_scripts')\n\t\tfor name in gui_scripts.keys():\n\t\t\t_scripts_to_remove.extend(_script_names(dist, name, True))\n\n\t\tfor s in _scripts_to_remove:\n\t\t\tpaths_to_remove.add(s)\n\n\t\treturn paths_to_remove\n\n", "description": "A set of file paths to be removed in the uninstallation of a\n\trequirement.", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "class", "name": "classUninstallPthEntries:", "data": "class UninstallPthEntries:\n\tdef __init__(self, pth_file):\n\t\t# type: (str) -> None\n\t\tself.file = pth_file\n\t\tself.entries = set()  # type: Set[str]\n\t\tself._saved_lines = None  # type: Optional[List[bytes]]\n\n\tdef add(self, entry):\n\t\t# type: (str) -> None\n\t\tentry = os.path.normcase(entry)\n\t\t# On Windows, os.path.normcase converts the entry to use\n\t\t# backslashes.  This is correct for entries that describe absolute\n\t\t# paths outside of site-packages, but all the others use forward\n\t\t# slashes.\n\t\t# os.path.splitdrive is used instead of os.path.isabs because isabs\n\t\t# treats non-absolute paths with drive letter markings like c:foo\\bar\n\t\t# as absolute paths. It also does not recognize UNC paths if they don't\n\t\t# have more than \"\\\\sever\\share\". Valid examples: \"\\\\server\\share\\\" or\n\t\t# \"\\\\server\\share\\folder\".\n\t\tif WINDOWS and not os.path.splitdrive(entry)[0]:\n\t\t\tentry = entry.replace('\\\\', '/')\n\t\tself.entries.add(entry)\n\n\tdef remove(self):\n\t\t# type: () -> None\n\t\tlogger.debug('Removing pth entries from %s:', self.file)\n\n\t\t# If the file doesn't exist, log a warning and return\n\t\tif not os.path.isfile(self.file):\n\t\t\tlogger.warning(\n\t\t\t\t\"Cannot remove entries from nonexistent file %s\", self.file\n\t\t\t)\n\t\t\treturn\n\t\twith open(self.file, 'rb') as fh:\n\t\t\t# windows uses '\\r\\n' with py3k, but uses '\\n' with py2.x\n\t\t\tlines = fh.readlines()\n\t\t\tself._saved_lines = lines\n\t\tif any(b'\\r\\n' in line for line in lines):\n\t\t\tendline = '\\r\\n'\n\t\telse:\n\t\t\tendline = '\\n'\n\t\t# handle missing trailing newline\n\t\tif lines and not lines[-1].endswith(endline.encode(\"utf-8\")):\n\t\t\tlines[-1] = lines[-1] + endline.encode(\"utf-8\")\n\t\tfor entry in self.entries:\n\t\t\ttry:\n\t\t\t\tlogger.debug('Removing entry: %s', entry)\n\t\t\t\tlines.remove((entry + endline).encode(\"utf-8\"))\n\t\t\texcept ValueError:\n\t\t\t\tpass\n\t\twith open(self.file, 'wb') as fh:\n\t\t\tfh.writelines(lines)\n\n\tdef rollback(self):\n\t\t# type: () -> bool\n\t\tif self._saved_lines is None:\n\t\t\tlogger.error(\n\t\t\t\t'Cannot roll back changes to %s, none were made', self.file\n\t\t\t)\n\t\t\treturn False\n\t\tlogger.debug('Rolling %s back to previous state', self.file)\n\t\twith open(self.file, 'wb') as fh:\n\t\t\tfh.writelines(self._saved_lines)\n\t\treturn True\n", "description": null, "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}], [{"term": "def", "name": "_script_names", "data": "def _script_names(dist, script_name, is_gui):\n\t# type: (Distribution, str, bool) -> List[str]\n\t\"\"\"Create the fully qualified name of the files created by\n\t{console,gui}_scripts for the given ``dist``.\n\tReturns the list of file names\n\t\"\"\"\n\tif dist_in_usersite(dist):\n\t\tbin_dir = bin_user\n\telse:\n\t\tbin_dir = bin_py\n\texe_name = os.path.join(bin_dir, script_name)\n\tpaths_to_remove = [exe_name]\n\tif WINDOWS:\n\t\tpaths_to_remove.append(exe_name + '.exe')\n\t\tpaths_to_remove.append(exe_name + '.exe.manifest')\n\t\tif is_gui:\n\t\t\tpaths_to_remove.append(exe_name + '-script.pyw')\n\t\telse:\n\t\t\tpaths_to_remove.append(exe_name + '-script.py')\n\treturn paths_to_remove\n\n", "description": "Create the fully qualified name of the files created by\n\t{console,gui}_scripts for the given ``dist``.\n\tReturns the list of file names\n\t", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "_unique", "data": "def _unique(fn):\n\t# type: (Callable[..., Iterator[Any]]) -> Callable[..., Iterator[Any]]\n\t@functools.wraps(fn)\n\tdef unique(*args, **kw):\n\t\t# type: (Any, Any) -> Iterator[Any]\n\t\tseen = set()  # type: Set[Any]\n\t\tfor item in fn(*args, **kw):\n\t\t\tif item not in seen:\n\t\t\t\tseen.add(item)\n\t\t\t\tyield item\n\treturn unique\n\n", "description": null, "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "uninstallation_paths", "data": "def uninstallation_paths(dist):\n\t# type: (Distribution) -> Iterator[str]\n\t\"\"\"\n\tYield all the uninstallation paths for dist based on RECORD-without-.py[co]\n\n\tYield paths to all the files in RECORD. For each .py file in RECORD, add\n\tthe .pyc and .pyo in the same directory.\n\n\tUninstallPathSet.add() takes care of the __pycache__ .py[co].\n\t\"\"\"\n\tr = csv.reader(dist.get_metadata_lines('RECORD'))\n\tfor row in r:\n\t\tpath = os.path.join(dist.location, row[0])\n\t\tyield path\n\t\tif path.endswith('.py'):\n\t\t\tdn, fn = os.path.split(path)\n\t\t\tbase = fn[:-3]\n\t\t\tpath = os.path.join(dn, base + '.pyc')\n\t\t\tyield path\n\t\t\tpath = os.path.join(dn, base + '.pyo')\n\t\t\tyield path\n\n", "description": "\n\tYield all the uninstallation paths for dist based on RECORD-without-.py[co]\n\n\tYield paths to all the files in RECORD. For each .py file in RECORD, add\n\tthe .pyc and .pyo in the same directory.\n\n\tUninstallPathSet.add() takes care of the __pycache__ .py[co].\n\t", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "compact", "data": "def compact(paths):\n\t# type: (Iterable[str]) -> Set[str]\n\t\"\"\"Compact a path set to contain the minimal number of paths\n\tnecessary to contain all paths in the set. If /a/path/ and\n\t/a/path/to/a/file.txt are both in the set, leave only the\n\tshorter path.\"\"\"\n\n\tsep = os.path.sep\n\tshort_paths = set()  # type: Set[str]\n\tfor path in sorted(paths, key=len):\n\t\tshould_skip = any(\n\t\t\tpath.startswith(shortpath.rstrip(\"*\")) and\n\t\t\tpath[len(shortpath.rstrip(\"*\").rstrip(sep))] == sep\n\t\t\tfor shortpath in short_paths\n\t\t)\n\t\tif not should_skip:\n\t\t\tshort_paths.add(path)\n\treturn short_paths\n\n", "description": "Compact a path set to contain the minimal number of paths\n\tnecessary to contain all paths in the set. If /a/path/ and\n\t/a/path/to/a/file.txt are both in the set, leave only the\n\tshorter path.", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "compress_for_rename", "data": "def compress_for_rename(paths):\n\t# type: (Iterable[str]) -> Set[str]\n\t\"\"\"Returns a set containing the paths that need to be renamed.\n\n\tThis set may include directories when the original sequence of paths\n\tincluded every file on disk.\n\t\"\"\"\n\tcase_map = {os.path.normcase(p): p for p in paths}\n\tremaining = set(case_map)\n\tunchecked = sorted({os.path.split(p)[0] for p in case_map.values()}, key=len)\n\twildcards = set()  # type: Set[str]\n\n\tdef norm_join(*a):\n\t\t# type: (str) -> str\n\t\treturn os.path.normcase(os.path.join(*a))\n\n\tfor root in unchecked:\n\t\tif any(os.path.normcase(root).startswith(w)\n\t\t\t   for w in wildcards):\n\t\t\t# This directory has already been handled.\n\t\t\tcontinue\n\n\t\tall_files = set()  # type: Set[str]\n\t\tall_subdirs = set()  # type: Set[str]\n\t\tfor dirname, subdirs, files in os.walk(root):\n\t\t\tall_subdirs.update(norm_join(root, dirname, d)\n\t\t\t\t\t\t\t   for d in subdirs)\n\t\t\tall_files.update(norm_join(root, dirname, f)\n\t\t\t\t\t\t\t for f in files)\n\t\t# If all the files we found are in our remaining set of files to\n\t\t# remove, then remove them from the latter set and add a wildcard\n\t\t# for the directory.\n\t\tif not (all_files - remaining):\n\t\t\tremaining.difference_update(all_files)\n\t\t\twildcards.add(root + os.sep)\n\n\treturn set(map(case_map.__getitem__, remaining)) | wildcards\n\n", "description": "Returns a set containing the paths that need to be renamed.\n\n\tThis set may include directories when the original sequence of paths\n\tincluded every file on disk.\n\t", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "def", "name": "compress_for_output_listing", "data": "def compress_for_output_listing(paths):\n\t# type: (Iterable[str]) -> Tuple[Set[str], Set[str]]\n\t\"\"\"Returns a tuple of 2 sets of which paths to display to user\n\n\tThe first set contains paths that would be deleted. Files of a package\n\tare not added and the top-level directory of the package has a '*' added\n\tat the end - to signify that all it's contents are removed.\n\n\tThe second set contains files that would have been skipped in the above\n\tfolders.\n\t\"\"\"\n\n\twill_remove = set(paths)\n\twill_skip = set()\n\n\t# Determine folders and files\n\tfolders = set()\n\tfiles = set()\n\tfor path in will_remove:\n\t\tif path.endswith(\".pyc\"):\n\t\t\tcontinue\n\t\tif path.endswith(\"__init__.py\") or \".dist-info\" in path:\n\t\t\tfolders.add(os.path.dirname(path))\n\t\tfiles.add(path)\n\n\t# probably this one https://github.com/python/mypy/issues/390\n\t_normcased_files = set(map(os.path.normcase, files))  # type: ignore\n\n\tfolders = compact(folders)\n\n\t# This walks the tree using os.walk to not miss extra folders\n\t# that might get added.\n\tfor folder in folders:\n\t\tfor dirpath, _, dirfiles in os.walk(folder):\n\t\t\tfor fname in dirfiles:\n\t\t\t\tif fname.endswith(\".pyc\"):\n\t\t\t\t\tcontinue\n\n\t\t\t\tfile_ = os.path.join(dirpath, fname)\n\t\t\t\tif (os.path.isfile(file_) and\n\t\t\t\t\t\tos.path.normcase(file_) not in _normcased_files):\n\t\t\t\t\t# We are skipping this file. Add it to the set.\n\t\t\t\t\twill_skip.add(file_)\n\n\twill_remove = files | {\n\t\tos.path.join(folder, \"*\") for folder in folders\n\t}\n\n\treturn will_remove, will_skip\n\n", "description": "Returns a tuple of 2 sets of which paths to display to user\n\n\tThe first set contains paths that would be deleted. Files of a package\n\tare not added and the top-level directory of the package has a '*' added\n\tat the end - to signify that all it's contents are removed.\n\n\tThe second set contains files that would have been skipped in the above\n\tfolders.\n\t", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "class", "name": "classStashedUninstallPathSet:", "data": "class StashedUninstallPathSet:\n\t\"\"\"A set of file rename operations to stash files while\n\ttentatively uninstalling them.\"\"\"\n\tdef __init__(self):\n\t\t# type: () -> None\n\t\t# Mapping from source file root to [Adjacent]TempDirectory\n\t\t# for files under that directory.\n\t\tself._save_dirs = {}  # type: Dict[str, TempDirectory]\n\t\t# (old path, new path) tuples for each move that may need\n\t\t# to be undone.\n\t\tself._moves = []  # type: List[Tuple[str, str]]\n\n\tdef _get_directory_stash(self, path):\n\t\t# type: (str) -> str\n\t\t\"\"\"Stashes a directory.\n\n\t\tDirectories are stashed adjacent to their original location if\n\t\tpossible, or else moved/copied into the user's temp dir.\"\"\"\n\n\t\ttry:\n\t\t\tsave_dir = AdjacentTempDirectory(path)  # type: TempDirectory\n\t\texcept OSError:\n\t\t\tsave_dir = TempDirectory(kind=\"uninstall\")\n\t\tself._save_dirs[os.path.normcase(path)] = save_dir\n\n\t\treturn save_dir.path\n\n\tdef _get_file_stash(self, path):\n\t\t# type: (str) -> str\n\t\t\"\"\"Stashes a file.\n\n\t\tIf no root has been provided, one will be created for the directory\n\t\tin the user's temp directory.\"\"\"\n\t\tpath = os.path.normcase(path)\n\t\thead, old_head = os.path.dirname(path), None\n\t\tsave_dir = None\n\n\t\twhile head != old_head:\n\t\t\ttry:\n\t\t\t\tsave_dir = self._save_dirs[head]\n\t\t\t\tbreak\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\thead, old_head = os.path.dirname(head), head\n\t\telse:\n\t\t\t# Did not find any suitable root\n\t\t\thead = os.path.dirname(path)\n\t\t\tsave_dir = TempDirectory(kind='uninstall')\n\t\t\tself._save_dirs[head] = save_dir\n\n\t\trelpath = os.path.relpath(path, head)\n\t\tif relpath and relpath != os.path.curdir:\n\t\t\treturn os.path.join(save_dir.path, relpath)\n\t\treturn save_dir.path\n\n\tdef stash(self, path):\n\t\t# type: (str) -> str\n\t\t\"\"\"Stashes the directory or file and returns its new location.\n\t\tHandle symlinks as files to avoid modifying the symlink targets.\n\t\t\"\"\"\n\t\tpath_is_dir = os.path.isdir(path) and not os.path.islink(path)\n\t\tif path_is_dir:\n\t\t\tnew_path = self._get_directory_stash(path)\n\t\telse:\n\t\t\tnew_path = self._get_file_stash(path)\n\n\t\tself._moves.append((path, new_path))\n\t\tif (path_is_dir and os.path.isdir(new_path)):\n\t\t\t# If we're moving a directory, we need to\n\t\t\t# remove the destination first or else it will be\n\t\t\t# moved to inside the existing directory.\n\t\t\t# We just created new_path ourselves, so it will\n\t\t\t# be removable.\n\t\t\tos.rmdir(new_path)\n\t\trenames(path, new_path)\n\t\treturn new_path\n\n\tdef commit(self):\n\t\t# type: () -> None\n\t\t\"\"\"Commits the uninstall by removing stashed files.\"\"\"\n\t\tfor _, save_dir in self._save_dirs.items():\n\t\t\tsave_dir.cleanup()\n\t\tself._moves = []\n\t\tself._save_dirs = {}\n\n\tdef rollback(self):\n\t\t# type: () -> None\n\t\t\"\"\"Undoes the uninstall by moving stashed files back.\"\"\"\n\t\tfor p in self._moves:\n\t\t\tlogger.info(\"Moving to %s\\n from %s\", *p)\n\n\t\tfor new_path, path in self._moves:\n\t\t\ttry:\n\t\t\t\tlogger.debug('Replacing %s from %s', new_path, path)\n\t\t\t\tif os.path.isfile(new_path) or os.path.islink(new_path):\n\t\t\t\t\tos.unlink(new_path)\n\t\t\t\telif os.path.isdir(new_path):\n\t\t\t\t\trmtree(new_path)\n\t\t\t\trenames(path, new_path)\n\t\t\texcept OSError as ex:\n\t\t\t\tlogger.error(\"Failed to restore %s\", new_path)\n\t\t\t\tlogger.debug(\"Exception: %s\", ex)\n\n\t\tself.commit()\n\n\t@property\n\tdef can_rollback(self):\n\t\t# type: () -> bool\n\t\treturn bool(self._moves)\n\n", "description": "A set of file rename operations to stash files while\n\ttentatively uninstalling them.", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "class", "name": "classUninstallPathSet:", "data": "class UninstallPathSet:\n\t\"\"\"A set of file paths to be removed in the uninstallation of a\n\trequirement.\"\"\"\n\tdef __init__(self, dist):\n\t\t# type: (Distribution) -> None\n\t\tself.paths = set()  # type: Set[str]\n\t\tself._refuse = set()  # type: Set[str]\n\t\tself.pth = {}  # type: Dict[str, UninstallPthEntries]\n\t\tself.dist = dist\n\t\tself._moved_paths = StashedUninstallPathSet()\n\n\tdef _permitted(self, path):\n\t\t# type: (str) -> bool\n\t\t\"\"\"\n\t\tReturn True if the given path is one we are permitted to\n\t\tremove/modify, False otherwise.\n\n\t\t\"\"\"\n\t\treturn is_local(path)\n\n\tdef add(self, path):\n\t\t# type: (str) -> None\n\t\thead, tail = os.path.split(path)\n\n\t\t# we normalize the head to resolve parent directory symlinks, but not\n\t\t# the tail, since we only want to uninstall symlinks, not their targets\n\t\tpath = os.path.join(normalize_path(head), os.path.normcase(tail))\n\n\t\tif not os.path.exists(path):\n\t\t\treturn\n\t\tif self._permitted(path):\n\t\t\tself.paths.add(path)\n\t\telse:\n\t\t\tself._refuse.add(path)\n\n\t\t# __pycache__ files can show up after 'installed-files.txt' is created,\n\t\t# due to imports\n\t\tif os.path.splitext(path)[1] == '.py':\n\t\t\tself.add(cache_from_source(path))\n\n\tdef add_pth(self, pth_file, entry):\n\t\t# type: (str, str) -> None\n\t\tpth_file = normalize_path(pth_file)\n\t\tif self._permitted(pth_file):\n\t\t\tif pth_file not in self.pth:\n\t\t\t\tself.pth[pth_file] = UninstallPthEntries(pth_file)\n\t\t\tself.pth[pth_file].add(entry)\n\t\telse:\n\t\t\tself._refuse.add(pth_file)\n\n\tdef remove(self, auto_confirm=False, verbose=False):\n\t\t# type: (bool, bool) -> None\n\t\t\"\"\"Remove paths in ``self.paths`` with confirmation (unless\n\t\t``auto_confirm`` is True).\"\"\"\n\n\t\tif not self.paths:\n\t\t\tlogger.info(\n\t\t\t\t\"Can't uninstall '%s'. No files were found to uninstall.\",\n\t\t\t\tself.dist.project_name,\n\t\t\t)\n\t\t\treturn\n\n\t\tdist_name_version = (\n\t\t\tself.dist.project_name + \"-\" + self.dist.version\n\t\t)\n\t\tlogger.info('Uninstalling %s:', dist_name_version)\n\n\t\twith indent_log():\n\t\t\tif auto_confirm or self._allowed_to_proceed(verbose):\n\t\t\t\tmoved = self._moved_paths\n\n\t\t\t\tfor_rename = compress_for_rename(self.paths)\n\n\t\t\t\tfor path in sorted(compact(for_rename)):\n\t\t\t\t\tmoved.stash(path)\n\t\t\t\t\tlogger.debug('Removing file or directory %s', path)\n\n\t\t\t\tfor pth in self.pth.values():\n\t\t\t\t\tpth.remove()\n\n\t\t\t\tlogger.info('Successfully uninstalled %s', dist_name_version)\n\n\tdef _allowed_to_proceed(self, verbose):\n\t\t# type: (bool) -> bool\n\t\t\"\"\"Display which files would be deleted and prompt for confirmation\n\t\t\"\"\"\n\n\t\tdef _display(msg, paths):\n\t\t\t# type: (str, Iterable[str]) -> None\n\t\t\tif not paths:\n\t\t\t\treturn\n\n\t\t\tlogger.info(msg)\n\t\t\twith indent_log():\n\t\t\t\tfor path in sorted(compact(paths)):\n\t\t\t\t\tlogger.info(path)\n\n\t\tif not verbose:\n\t\t\twill_remove, will_skip = compress_for_output_listing(self.paths)\n\t\telse:\n\t\t\t# In verbose mode, display all the files that are going to be\n\t\t\t# deleted.\n\t\t\twill_remove = set(self.paths)\n\t\t\twill_skip = set()\n\n\t\t_display('Would remove:', will_remove)\n\t\t_display('Would not remove (might be manually added):', will_skip)\n\t\t_display('Would not remove (outside of prefix):', self._refuse)\n\t\tif verbose:\n\t\t\t_display('Will actually move:', compress_for_rename(self.paths))\n\n\t\treturn ask('Proceed (y/n)? ', ('y', 'n')) == 'y'\n\n\tdef rollback(self):\n\t\t# type: () -> None\n\t\t\"\"\"Rollback the changes previously made by remove().\"\"\"\n\t\tif not self._moved_paths.can_rollback:\n\t\t\tlogger.error(\n\t\t\t\t\"Can't roll back %s; was not uninstalled\",\n\t\t\t\tself.dist.project_name,\n\t\t\t)\n\t\t\treturn\n\t\tlogger.info('Rolling back uninstall of %s', self.dist.project_name)\n\t\tself._moved_paths.rollback()\n\t\tfor pth in self.pth.values():\n\t\t\tpth.rollback()\n\n\tdef commit(self):\n\t\t# type: () -> None\n\t\t\"\"\"Remove temporary save dir: rollback will no longer be possible.\"\"\"\n\t\tself._moved_paths.commit()\n\n\t@classmethod\n\tdef from_dist(cls, dist):\n\t\t# type: (Distribution) -> UninstallPathSet\n\t\tdist_path = normalize_path(dist.location)\n\t\tif not dist_is_local(dist):\n\t\t\tlogger.info(\n\t\t\t\t\"Not uninstalling %s at %s, outside environment %s\",\n\t\t\t\tdist.key,\n\t\t\t\tdist_path,\n\t\t\t\tsys.prefix,\n\t\t\t)\n\t\t\treturn cls(dist)\n\n\t\tif dist_path in {p for p in {sysconfig.get_path(\"stdlib\"),\n\t\t\t\t\t\t\t\t\t sysconfig.get_path(\"platstdlib\")}\n\t\t\t\t\t\t if p}:\n\t\t\tlogger.info(\n\t\t\t\t\"Not uninstalling %s at %s, as it is in the standard library.\",\n\t\t\t\tdist.key,\n\t\t\t\tdist_path,\n\t\t\t)\n\t\t\treturn cls(dist)\n\n\t\tpaths_to_remove = cls(dist)\n\t\tdevelop_egg_link = egg_link_path(dist)\n\t\tdevelop_egg_link_egg_info = '{}.egg-info'.format(\n\t\t\tpkg_resources.to_filename(dist.project_name))\n\t\tegg_info_exists = dist.egg_info and os.path.exists(dist.egg_info)\n\t\t# Special case for distutils installed package\n\t\tdistutils_egg_info = getattr(dist._provider, 'path', None)\n\n\t\t# Uninstall cases order do matter as in the case of 2 installs of the\n\t\t# same package, pip needs to uninstall the currently detected version\n\t\tif (egg_info_exists and dist.egg_info.endswith('.egg-info') and\n\t\t\t\tnot dist.egg_info.endswith(develop_egg_link_egg_info)):\n\t\t\t# if dist.egg_info.endswith(develop_egg_link_egg_info), we\n\t\t\t# are in fact in the develop_egg_link case\n\t\t\tpaths_to_remove.add(dist.egg_info)\n\t\t\tif dist.has_metadata('installed-files.txt'):\n\t\t\t\tfor installed_file in dist.get_metadata(\n\t\t\t\t\t\t'installed-files.txt').splitlines():\n\t\t\t\t\tpath = os.path.normpath(\n\t\t\t\t\t\tos.path.join(dist.egg_info, installed_file)\n\t\t\t\t\t)\n\t\t\t\t\tpaths_to_remove.add(path)\n\t\t\t# FIXME: need a test for this elif block\n\t\t\t# occurs with --single-version-externally-managed/--record outside\n\t\t\t# of pip\n\t\t\telif dist.has_metadata('top_level.txt'):\n\t\t\t\tif dist.has_metadata('namespace_packages.txt'):\n\t\t\t\t\tnamespaces = dist.get_metadata('namespace_packages.txt')\n\t\t\t\telse:\n\t\t\t\t\tnamespaces = []\n\t\t\t\tfor top_level_pkg in [\n\t\t\t\t\t\tp for p\n\t\t\t\t\t\tin dist.get_metadata('top_level.txt').splitlines()\n\t\t\t\t\t\tif p and p not in namespaces]:\n\t\t\t\t\tpath = os.path.join(dist.location, top_level_pkg)\n\t\t\t\t\tpaths_to_remove.add(path)\n\t\t\t\t\tpaths_to_remove.add(path + '.py')\n\t\t\t\t\tpaths_to_remove.add(path + '.pyc')\n\t\t\t\t\tpaths_to_remove.add(path + '.pyo')\n\n\t\telif distutils_egg_info:\n\t\t\traise UninstallationError(\n\t\t\t\t\"Cannot uninstall {!r}. It is a distutils installed project \"\n\t\t\t\t\"and thus we cannot accurately determine which files belong \"\n\t\t\t\t\"to it which would lead to only a partial uninstall.\".format(\n\t\t\t\t\tdist.project_name,\n\t\t\t\t)\n\t\t\t)\n\n\t\telif dist.location.endswith('.egg'):\n\t\t\t# package installed by easy_install\n\t\t\t# We cannot match on dist.egg_name because it can slightly vary\n\t\t\t# i.e. setuptools-0.6c11-py2.6.egg vs setuptools-0.6rc11-py2.6.egg\n\t\t\tpaths_to_remove.add(dist.location)\n\t\t\teasy_install_egg = os.path.split(dist.location)[1]\n\t\t\teasy_install_pth = os.path.join(os.path.dirname(dist.location),\n\t\t\t\t\t\t\t\t\t\t\t'easy-install.pth')\n\t\t\tpaths_to_remove.add_pth(easy_install_pth, './' + easy_install_egg)\n\n\t\telif egg_info_exists and dist.egg_info.endswith('.dist-info'):\n\t\t\tfor path in uninstallation_paths(dist):\n\t\t\t\tpaths_to_remove.add(path)\n\n\t\telif develop_egg_link:\n\t\t\t# develop egg\n\t\t\twith open(develop_egg_link, 'r') as fh:\n\t\t\t\tlink_pointer = os.path.normcase(fh.readline().strip())\n\t\t\tassert (link_pointer == dist.location), (\n\t\t\t\t'Egg-link {} does not match installed location of {} '\n\t\t\t\t'(at {})'.format(\n\t\t\t\t\tlink_pointer, dist.project_name, dist.location)\n\t\t\t)\n\t\t\tpaths_to_remove.add(develop_egg_link)\n\t\t\teasy_install_pth = os.path.join(os.path.dirname(develop_egg_link),\n\t\t\t\t\t\t\t\t\t\t\t'easy-install.pth')\n\t\t\tpaths_to_remove.add_pth(easy_install_pth, dist.location)\n\n\t\telse:\n\t\t\tlogger.debug(\n\t\t\t\t'Not sure how to uninstall: %s - Check: %s',\n\t\t\t\tdist, dist.location,\n\t\t\t)\n\n\t\t# find distutils scripts= scripts\n\t\tif dist.has_metadata('scripts') and dist.metadata_isdir('scripts'):\n\t\t\tfor script in dist.metadata_listdir('scripts'):\n\t\t\t\tif dist_in_usersite(dist):\n\t\t\t\t\tbin_dir = bin_user\n\t\t\t\telse:\n\t\t\t\t\tbin_dir = bin_py\n\t\t\t\tpaths_to_remove.add(os.path.join(bin_dir, script))\n\t\t\t\tif WINDOWS:\n\t\t\t\t\tpaths_to_remove.add(os.path.join(bin_dir, script) + '.bat')\n\n\t\t# find console_scripts\n\t\t_scripts_to_remove = []\n\t\tconsole_scripts = dist.get_entry_map(group='console_scripts')\n\t\tfor name in console_scripts.keys():\n\t\t\t_scripts_to_remove.extend(_script_names(dist, name, False))\n\t\t# find gui_scripts\n\t\tgui_scripts = dist.get_entry_map(group='gui_scripts')\n\t\tfor name in gui_scripts.keys():\n\t\t\t_scripts_to_remove.extend(_script_names(dist, name, True))\n\n\t\tfor s in _scripts_to_remove:\n\t\t\tpaths_to_remove.add(s)\n\n\t\treturn paths_to_remove\n\n", "description": "A set of file paths to be removed in the uninstallation of a\n\trequirement.", "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}, {"term": "class", "name": "classUninstallPthEntries:", "data": "class UninstallPthEntries:\n\tdef __init__(self, pth_file):\n\t\t# type: (str) -> None\n\t\tself.file = pth_file\n\t\tself.entries = set()  # type: Set[str]\n\t\tself._saved_lines = None  # type: Optional[List[bytes]]\n\n\tdef add(self, entry):\n\t\t# type: (str) -> None\n\t\tentry = os.path.normcase(entry)\n\t\t# On Windows, os.path.normcase converts the entry to use\n\t\t# backslashes.  This is correct for entries that describe absolute\n\t\t# paths outside of site-packages, but all the others use forward\n\t\t# slashes.\n\t\t# os.path.splitdrive is used instead of os.path.isabs because isabs\n\t\t# treats non-absolute paths with drive letter markings like c:foo\\bar\n\t\t# as absolute paths. It also does not recognize UNC paths if they don't\n\t\t# have more than \"\\\\sever\\share\". Valid examples: \"\\\\server\\share\\\" or\n\t\t# \"\\\\server\\share\\folder\".\n\t\tif WINDOWS and not os.path.splitdrive(entry)[0]:\n\t\t\tentry = entry.replace('\\\\', '/')\n\t\tself.entries.add(entry)\n\n\tdef remove(self):\n\t\t# type: () -> None\n\t\tlogger.debug('Removing pth entries from %s:', self.file)\n\n\t\t# If the file doesn't exist, log a warning and return\n\t\tif not os.path.isfile(self.file):\n\t\t\tlogger.warning(\n\t\t\t\t\"Cannot remove entries from nonexistent file %s\", self.file\n\t\t\t)\n\t\t\treturn\n\t\twith open(self.file, 'rb') as fh:\n\t\t\t# windows uses '\\r\\n' with py3k, but uses '\\n' with py2.x\n\t\t\tlines = fh.readlines()\n\t\t\tself._saved_lines = lines\n\t\tif any(b'\\r\\n' in line for line in lines):\n\t\t\tendline = '\\r\\n'\n\t\telse:\n\t\t\tendline = '\\n'\n\t\t# handle missing trailing newline\n\t\tif lines and not lines[-1].endswith(endline.encode(\"utf-8\")):\n\t\t\tlines[-1] = lines[-1] + endline.encode(\"utf-8\")\n\t\tfor entry in self.entries:\n\t\t\ttry:\n\t\t\t\tlogger.debug('Removing entry: %s', entry)\n\t\t\t\tlines.remove((entry + endline).encode(\"utf-8\"))\n\t\t\texcept ValueError:\n\t\t\t\tpass\n\t\twith open(self.file, 'wb') as fh:\n\t\t\tfh.writelines(lines)\n\n\tdef rollback(self):\n\t\t# type: () -> bool\n\t\tif self._saved_lines is None:\n\t\t\tlogger.error(\n\t\t\t\t'Cannot roll back changes to %s, none were made', self.file\n\t\t\t)\n\t\t\treturn False\n\t\tlogger.debug('Rolling %s back to previous state', self.file)\n\t\twith open(self.file, 'wb') as fh:\n\t\t\tfh.writelines(self._saved_lines)\n\t\treturn True\n", "description": null, "category": "remove", "imports": ["import csv", "import functools", "import logging", "import os", "import sys", "import sysconfig", "from importlib.util import cache_from_source", "from pip._vendor import pkg_resources", "from pip._internal.exceptions import UninstallationError", "from pip._internal.locations import bin_py, bin_user", "from pip._internal.utils.compat import WINDOWS", "from pip._internal.utils.logging import indent_log", "from pip._internal.utils.misc import (", "from pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory", "from pip._internal.utils.typing import MYPY_CHECK_RUNNING", "\tfrom typing import (", "\tfrom pip._vendor.pkg_resources import Distribution", "\t\t# due to imports"]}], [], [], [{"term": "def", "name": "remove_vowels1", "data": "def remove_vowels1(text) -> str:\n\tvowels = 'aeiouyAEIOUY'\n\ttext = str(text)\n\tfor char in text:\n\t\tif char in vowels:\n\t\t\ttext = text.replace(char, '')\n\treturn text\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_vowels2", "data": "def remove_vowels2(text) -> str:\n\tvowels = 'aeiouyAEIOUY'\n\tfor char in str(text):\n\t\tif char in vowels:\n\t\t\ttext = text.replace(char, '')\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_vowels3", "data": "def remove_vowels3(text) -> str:\n\tvowels = 'aeiouyAEIOUY'\n\t#  return ''.join([text.replace(char, '') for char in str(text) if char in vowels])\n\t# the list comprehension doesn't work\n\treturn ''.join([char for char in str(text) if char not in vowels])\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_vowels4", "data": "def remove_vowels4(text: str) -> str:\n\tvowels = 'aeiouyAEIOUY'\n\tresult = ''\n\tfor char in str(text):\n\t\tif char not in vowels:\n\t\t\tresult += char\n\treturn result\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_vowels5", "data": "def remove_vowels5(text: str) -> str:\n\tvowels = 'aeiouyAEIOUY'\n\treturn ''.join([char for char in str(text) if char not in vowels])\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "test_remove_vowels1", "data": "def test_remove_vowels1():\n\tassert remove_vowels1('this is Sparta') == 'ths s Sprt'\n\tassert remove_vowels1('Arkadius Ursa') == 'rkds rs'\n\tassert remove_vowels1('') == ''\n\tassert remove_vowels1(12345) == '12345'\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "test_remove_vowels2", "data": "def test_remove_vowels2():\n\tassert remove_vowels2('this is Sparta') == 'ths s Sprt'\n\tassert remove_vowels2('Arkadius Ursa') == 'rkds rs'\n\tassert remove_vowels2('') == ''\n\tassert remove_vowels2(12345) == '12345'\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "test_remove_vowels3", "data": "def test_remove_vowels3():\n\tassert remove_vowels3('this is Sparta') == 'ths s Sprt'\n\tassert remove_vowels3('Arkadius Ursa') == 'rkds rs'\n\tassert remove_vowels3('') == ''\n\tassert remove_vowels3(12345) == '12345'\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "test_remove_vowels4", "data": "def test_remove_vowels4():\n\tassert remove_vowels4('this is Sparta') == 'ths s Sprt'\n\tassert remove_vowels4('Arkadius Ursa') == 'rkds rs'\n\tassert remove_vowels4('') == ''\n\tassert remove_vowels4(12345) == '12345'\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "test_remove_vowels5", "data": "def test_remove_vowels5():\n\tassert remove_vowels5('this is Sparta') == 'ths s Sprt'\n\tassert remove_vowels5('Arkadius Ursa') == 'rkds rs'\n\tassert remove_vowels5('') == ''\n\tassert remove_vowels5(12345) == '12345'\n", "description": null, "category": "remove", "imports": []}], [{"term": "def", "name": "ncdef_", "data": "async def _(event):\n\tHELP_STR = \"`.remove.bg` as reply to a media, or give a link as an argument to this command\"\n\tif event.fwd_from:\n\t\treturn\n\tif Config.REM_BG_API_KEY is None:\n\t\tawait event.edit(\"You need API token from remove.bg to use this plugin.\\nor Try @Remove_BGBot \")\n\t\treturn False\n\tinput_str = event.pattern_match.group(1)\n\tstart = datetime.now()\n\tmessage_id = event.message.id\n\tif event.reply_to_msg_id:\n\t\tmessage_id = event.reply_to_msg_id\n\t\treply_message = await event.get_reply_message()\n\t\t# check if media message\n\t\tawait event.edit(\"Downloading this media ...\")\n\t\ttry:\n\t\t\tdownloaded_file_name = await borg.download_media(\n\t\t\t\treply_message,\n\t\t\t\tConfig.TMP_DOWNLOAD_DIRECTORY\n\t\t\t)\n\t\texcept Exception as e:\n\t\t\tawait event.edit(str(e))\n\t\t\treturn\n\t\telse:\n\t\t\tawait event.edit(\"sending to ReMove.BG\")\n\t\t\toutput_file_name = ReTrieveFile(downloaded_file_name)\n\t\t\tos.remove(downloaded_file_name)\n\telif input_str:\n\t\tawait event.edit(\"sending to ReMove.BG\")\n\t\toutput_file_name = ReTrieveURL(input_str)\n\telse:\n\t\tawait event.edit(HELP_STR)\n\t\treturn\n\tcontentType = output_file_name.headers.get(\"content-type\")\n\tif \"image\" in contentType:\n\t\twith io.BytesIO(output_file_name.content) as remove_bg_image:\n\t\t\tremove_bg_image.name = \"@UniBorg_ReMove.png\"\n\t\t\tawait borg.send_file(\n\t\t\t\tevent.chat_id,\n\t\t\t\tremove_bg_image,\n\t\t\t\tforce_document=True,\n\t\t\t\tsupports_streaming=False,\n\t\t\t\tallow_cache=False,\n\t\t\t\treply_to=message_id\n\t\t\t)\n\t\tend = datetime.now()\n\t\tms = (end - start).seconds\n\t\tawait event.edit(\"Background Removed in {} seconds using ReMove.BG API, powered by @UniBorg\".format(ms))\n\telse:\n\t\tawait event.edit(\"ReMove.BG API returned Errors. Please report to @UniBorg\\n`{}\".format(output_file_name.content.decode(\"UTF-8\")))\n\n", "description": null, "category": "remove", "imports": ["import asyncio", "from datetime import datetime", "import io", "import os", "import requests", "from telethon import events", "from uniborg.util import progress, admin_cmd"]}, {"term": "def", "name": "ReTrieveFile", "data": "def ReTrieveFile(input_file_name):\n\theaders = {\n\t\t\"X-API-Key\": Config.REM_BG_API_KEY,\n\t}\n\tfiles = {\n\t\t\"image_file\": (input_file_name, open(input_file_name, \"rb\")),\n\t}\n\tr = requests.post(\n\t\t\"https://api.remove.bg/v1.0/removebg\",\n\t\theaders=headers,\n\t\tfiles=files,\n\t\tallow_redirects=True,\n\t\tstream=True\n\t)\n\treturn r\n\n", "description": null, "category": "remove", "imports": ["import asyncio", "from datetime import datetime", "import io", "import os", "import requests", "from telethon import events", "from uniborg.util import progress, admin_cmd"]}, {"term": "def", "name": "ReTrieveURL", "data": "def ReTrieveURL(input_url):\n\theaders = {\n\t\t\"X-API-Key\": Config.REM_BG_API_KEY,\n\t}\n\tdata = {\n\t  \"image_url\": input_url\n\t}\n\tr = requests.post(\n\t\t\"https://api.remove.bg/v1.0/removebg\",\n\t\theaders=headers,\n\t\tdata=data,\n\t\tallow_redirects=True,\n\t\tstream=True\n\t)\n\treturn r\n", "description": null, "category": "remove", "imports": ["import asyncio", "from datetime import datetime", "import io", "import os", "import requests", "from telethon import events", "from uniborg.util import progress, admin_cmd"]}], [{"term": "class", "name": "TestDisconnect", "data": "class TestDisconnect(unittest.TestCase):\n\n\tdef setUp(self):\n\t\tkw = Keywords()\n\t\tself.disc = DisconnectPlot(kw)\n\t\tself.disc.InitialiseMin()\n\t\tself.disc.InitialiseTS()\n\t\n\tdef testCountMin1(self):\n\t\tn = 4447\n\t\tself.disc.CountMin()\n\t\tself.assertEqual(self.disc.minima_index['Size'], n)\n\n\tdef testMinList1(self):\n\t\ttest_list = {}\n\t\tminima_list = {}\n\n\t\tfor lines in open('minima_energy'):\n\t\t\tindice = int(lines.split()[0])\n\t\t\tenergy = float(lines.split()[1])\n\t\t\ttest_list[indice] = energy\n\n\t\tfor i in self.disc.minima_index['Index']:\n\t\t\tminima_list[i] = self.disc.minima_index['Index'][i]['Energy']\n\n\t\tself.assertDictEqual(minima_list, test_list)\n\n\tdef testCountTS1(self):\n\t\tn = 3526\n\n\t\tself.disc.CountTS()\n\t\tself.assertEqual(self.disc.ts_index['Size'], n)\n\n\tdef testTSListEnergy1(self):\n\t\t#self.disc.RemoveDegenerateTS()\n\t\ttest_list = {}\n\t\tts_list = {}\n\t\tfor lines in open('ts_energy'):\n\t\t\tindice = int(lines.split()[0])\n\t\t\tenergy = float(lines.split()[1])\n\t\t\ttest_list[indice] = energy\n\n\t\tfor i in self.disc.ts_index['Index']:\n\t\t\tts_list[i] = self.disc.ts_index['Index'][i]['Energy']\n\n\t\tself.assertDictEqual(ts_list, test_list)\n\n\tdef testMinList2(self):\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tself.disc.RemoveThreshold()\n\t\tfor lines in open('minima_threshold_energy'):\n\t\t\tindice = int(lines.split()[0])\n\t\t\tenergy = float(lines.split()[1])\n\t\t\ttest_list[indice] = energy\n\n\t\tfor i in self.disc.minima_index['Index']:\n\t\t\tminima_list[i] = self.disc.minima_index['Index'][i]['Energy']\n\n\t\tself.assertDictEqual(minima_list, test_list)\n\n\n\tdef testTSListEnergy2(self):\n\t\t#self.disc.RemoveDegenerateTS()\n\t\ttest_list = {}\n\t\tts_list = {}\n\t\tself.disc.RemoveThreshold()\n\t\tfor lines in open('ts_threshold_energy'):\n\t\t\tindice = int(lines.split()[0])\n\t\t\tenergy = float(lines.split()[1])\n\t\t\ttest_list[indice] = energy\n\n\t\tfor i in self.disc.ts_index['Index']:\n\t\t\tts_list[i] = self.disc.ts_index['Index'][i]['Energy']\n\n\t\tself.assertDictEqual(ts_list, test_list)\n\n\t\t\n\tdef testHighestTS(self):\n\t\tx = 19.1133463797\n\t\tself.disc.HighTS()\n\t\tself.assertEqual(self.disc.ts_index['HighTS']['Energy'], x)\n\n\tdef testLowestTS(self):\n\t\tx = -52.5702347522\n\t\tself.disc.LowTS()\n\t\tself.assertEqual(self.disc.ts_index['LowTS']['Energy'], x)\n\n\t\n\tdef testMinimaDegree(self):\n\t\t'''\n\t\tTests if node degree calculations are correct. Note, only\n\t\tcompares minima from PyConnect which are present in \n\t\tFORTRAN after using GETNCONN call.\n\t\t'''\n\t\t\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.CalcDegree()\n\t\tfor lines in open('minima_degree'):\n\t\t\tindice = int(lines.split()[0])\n\t\t\tdegree = int(lines.split()[1])\n\t\t\ttest_list[indice] = degree\n\t\t\n\t\tfor i in test_list:\n\t\t\tminima_list[i] = (self.disc.minima_index\n\t\t\t\t\t\t\t  ['Index'][i]['Degree'])\n\t\t\n\t\tself.assertDictEqual(minima_list, test_list)\n\n\tdef testFindGM(self):\n\t\t'''\n\t\tTests if the global minimum can be located\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestMinimaDegreeRemove", "data": "\tdef testMinimaDegreeRemove(self):\n\t\t'''\n\t\tTests if minima are correctly removed based on \n\t\ttheir degree\n\t\t'''\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tfor lines in open('minima_degree'):\n\t\t\tindice = int(lines.split()[0])\n\t\t\tdegree = int(lines.split()[1])\n\t\t\ttest_list[indice] = degree\n\t\t\t\n\t\tfor i in test_list:\n\t\t\tminima_list[i] = (self.disc.minima_index\n\t\t\t\t\t\t\t  ['Index'][i]['Degree'])\n\t\t\n\t\tself.assertDictEqual(minima_list, test_list)\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestRemoveDisjointMin", "data": "\tdef testRemoveDisjointMin(self):\n\t\t'''\n\t\tTests if disjoint minima are correctly removed\n\t\tfrom the database\n\t\t'''\n\t\ttest_list = []\n\t\tminima_list = []\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tself.disc.RemoveDisjoint()\n\t\tfor lines in open('minima_disjoint_removed'):\n\t\t\tindice = int(lines.split()[0])\n\t\t\t\n\t\t\ttest_list.append(indice)\n\t\t\t\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestRemoveDisjointTS", "data": "\tdef testRemoveDisjointTS(self):\n\t\t'''\n\t\tTests if disjoint minima are correctly removed\n\t\tfrom the database\n\t\t'''\n\t\ttest_list = []\n\t\tminima_list = []\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tself.disc.RemoveDisjoint()\n\t\tfor lines in open('ts_disjoint_removed'):\n\t\t\tindice = int(lines.split()[0])\n\t\t\t\n\t\t\ttest_list.append(indice)\n\t\t\t\n\t\tminima_list = self.disc.ts_index['Index'].keys()\n\t\t\n\t\t\n\t\tself.assertEqual(minima_list, test_list)\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestBasinAssignment", "data": "\tdef testBasinAssignment(self):\n\t\t'''\n\t\tTests if disjoint minima are correctly removed\n\t\tfrom the database\n\t\t'''\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestRenumberBasin", "data": "\tdef testRenumberBasin(self):\n\t\t'''\n\t\tTests if disjoint minima are correctly removed\n\t\tfrom the database\n\t\t'''\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tself.disc.RemoveDisjoint()\n\t\t#print self.disc.minima_index['GM']\n\t\tself.disc.InitialiseBasin()\n\t\tself.disc.AssignBasins()\n\t\tself.disc.PruneBasins()\n\t\t\n\t\tself.disc.ReNumberBasins()\n\t\tfor lines in open('min_in_basin_renumber'):\n\t\t\tlevel = int(lines.split()[0])\n\t\t\tnum = int(lines.split()[1])\n\t\t\ttest_list[level] = num\n\t\t\t\n\t\tfor l in self.disc.basin_index['Level']:\n\t\t\tminima_list[l] = self.disc.basin_index['Level'][l]['No. of Basins']\n\t\t#print 'min', self.disc.minima_index['Index'][3]\n\t\t#print 'min', self.disc.minima_index['Index'][4]\n\t\t#print self.disc.basin_index\n\t\tself.assertDictEqual(minima_list, test_list)\n\n\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestBasinAssignmentNum", "data": "\tdef testBasinAssignmentNum(self):\n\t\t'''\n\t\tTests if disjoint minima are correctly removed\n\t\tfrom the database\n\t\t'''\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tself.disc.RemoveDisjoint()\n\t\t#print self.disc.minima_index['GM']\n\t\tself.disc.InitialiseBasin()\n\t\tself.disc.AssignBasins()\n\t\t\n\t\tfor lines in open('min_in_basin'):\n\t\t\tm = int(lines.split()[0])\n\t\t\tl = int(lines.split()[1])\n\t\t\tb = int(lines.split()[2])\n\t\t\tif b != 0:\n\t\t\t\ttry:\n\t\t\t\t\ttest_list[l][b].append(m)\n\t\t\t\texcept KeyError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\ttest_list[l] = {}\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\t\t\n\t\tfor m in self.disc.minima_index['Index']:\n\t\t\tfor l in self.disc.basin_index['Level']:\n\t\t\t\t\n\t\t\t\tb = self.disc.minima_index['Index'][m]['Basin']['Level'][l]\n\t\t #\t   print m, l, b\n\t\t\t\tif b:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tminima_list[l][b].append(m)\n\t\t  #\t\t\t  print 'try', m,l,b,minima_list\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t#minima_list[l] = {}\n\t\t\t\t\t\t\tminima_list[l][b] = [m]\n\t\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t\tminima_list[l] = {}\n\t\t\t\t\t\t\tminima_list[l][b] = [m]\n\t\t   #\t\t\t print 'except', m,l,b,minima_list\n\t\t#print 'min', self.disc.minima_index['Index'][3]\n\t\t#print 'min', self.disc.minima_index['Index'][4]\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestRenumberBasinNum", "data": "\tdef testRenumberBasinNum(self):\n\t\t'''\n\t\tTests if disjoint minima are correctly removed\n\t\tfrom the database\n\t\t'''\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tself.disc.RemoveDisjoint()\n\t\t#print self.disc.minima_index['GM']\n\t\tself.disc.InitialiseBasin()\n\t\tself.disc.AssignBasins()\n\t\tself.disc.PruneBasins()\n\t\tself.disc.ReNumberBasins()\n\t\tfor lines in open('min_in_basin_parent'):\n\t\t\tm = int(lines.split()[0])\n\t\t\tl = int(lines.split()[1])\n\t\t\tb = int(lines.split()[2])\n\t\t\tif b != 0:\n\t\t\t\ttry:\n\t\t\t\t\ttest_list[l][b].append(m)\n\t\t\t\texcept KeyError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\ttest_list[l] = {}\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\tfor l in self.disc.basin_index['Level']:\n\t\t\tminima_list[l] = {}\n\t\t\tfor b in self.disc.basin_index['Level'][l]['Basin']:\n\t\t\t\t\n\t\t\t\tminima_list[l][b] = self.disc.basin_index['Level'][l]['Basin'][b]['Min']\n\t\t#print minima_list\n\t\t#print len(minima_list), len(test_list), 'len'\n\t\t#print test_list\n\t\t\n\t\t\n\t\tself.assertDictEqual(minima_list, test_list)\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestRenumberBasinMin", "data": "\tdef testRenumberBasinMin(self):\n\t\t'''\n\t\tTests if  minima are in correct basins\n\t\tafter renumbering\n\t\t'''\n\t\t#self.maxDiff = None\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tref = {}\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tself.disc.RemoveDisjoint()\n\t\t#print self.disc.minima_index['GM']\n\t\tself.disc.InitialiseBasin()\n\t\tself.disc.AssignBasins()\n\t\tself.disc.PruneBasins()\n\t\tself.disc.ReNumberBasins()\n\t\tfor lines in open('min_in_basin_parent'):\n\t\t\tm = int(lines.split()[0])\n\t\t\tl = int(lines.split()[1])\n\t\t\tb = int(lines.split()[2])\n\t\t\tif b != 0:\n\t\t\t\ttry:\n\t\t\t\t\ttest_list[l][m] = b\n\t\t\t\texcept KeyError:\n\t\t\t\t\ttest_list[l] = {}\n\t\t\t\t\ttest_list[l][m] = b\n\t\tfor l in self.disc.basin_index['Level']:\n\t\t\tminima_list[l] = {}\n\t\tfor m in self.disc.minima_index['Index']:\n\t\t\tfor l in self.disc.basin_index['Level']:\n\t\t\t\tif self.disc.minima_index['Index'][m]['Basin']['Level'][l]:\n\t\t\t\t\tminima_list[l][m] = self.disc.minima_index['Index'][m]['Basin']['Level'][l]\n\t\t\t\t\n\t\t\t\t#minima_list[l][b] = self.disc.basin_index['Level'][l]['Basin'][b]['Min']\n\t\tfor l in self.disc.basin_index['Level']:\n\t\t\tref[l] = {}\n\t\t\tfor m in test_list[l]:\n\t\t\t\tref[l][m] = {}\n\t\t\t\tb = test_list[l][m]\n\t\t\t\tif minima_list[l][m]:\n\t\t\t\t\tref[l][m] = {b:minima_list[l][m]}\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestBasinContent", "data": "\tdef testBasinContent(self):\n\t\t'''\n\t\tTests if object basin_dict['Level'][l]['Node'][n]\n\t\tcontains the correct list of minima.\n\t\t'''\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tself.disc.RemoveDisjoint()\n\t\t#print self.disc.minima_index['GM']\n\t\tself.disc.InitialiseBasin()\n\t\tself.disc.AssignBasins()\n\t\tself.disc.PruneBasins()\n\t\t#self.disc.ReNumberBasins()\n\t\tfor lines in open('min_in_basin'):\n\t\t\tm = int(lines.split()[0])\n\t\t\tl = int(lines.split()[1])\n\t\t\tb = int(lines.split()[2])\n\t\t\tif b != 0:\n\t\t\t\ttry:\n\t\t\t\t\ttest_list[l][b].append(m)\n\t\t\t\texcept KeyError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\ttest_list[l] = {}\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\t\tfor l in self.disc.basin_index['Level']:\n\t\t\t\tminima_list[l] = {}\n\t\t\t\tfor b in self.disc.basin_index['Level'][l]['Basin']:\n\t\t\t\t\n\t\t\t\t\tminima_list[l][b] = self.disc.basin_index['Level'][l]['Basin'][b]['Min']\n\t\t\n\t\tself.assertDictEqual(minima_list, test_list)\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestParents", "data": "\tdef testParents(self):\n\t\tself.maxDiff = None\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tparent_list = {}\n\t\tpmin_list = {}\n\t\tref = {}\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tself.disc.RemoveDisjoint()\n\t\tself.disc.InitialiseBasin()\n\t\tself.disc.AssignBasins()\n\t\tself.disc.PruneBasins()\n\t\tself.disc.ReNumberBasins()\n\t\tself.disc.GetParentsAndChildren()\n\t\tfor lines in open('min_in_basin_parent'):\n\t\t\tm = int(lines.split()[0])\n\t\t\tl = int(lines.split()[1])\n\t\t\tb = int(lines.split()[2])\n\t\t\tp = int(lines.split()[3])\n\t\t\t#c = int(lines.split()[4])\n\t\t\tif b != 0:\n\t\t\t\ttry:\n\t\t\t\t\ttest_list[l][b].append(m)\n\t\t\t\texcept KeyError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\ttest_list[l] = {}\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\t\t\t\t#test_list[l][b]\n\t\t\tif l != 1 and b != 0:\n\t\t\t\ttry:\n\t\t\t\t\tparent_list[l][b] = p\n\t\t\t\texcept KeyError:\n\t\t\t\t\tparent_list[l] = {}\n\t\t\t\t\tparent_list[l][b] = p\n\t\t\t\n\t\tfor l in self.disc.basin_index['Level']:\n\t\t\tminima_list[l] = {}\n\t\t\tfor b in self.disc.basin_index['Level'][l]['Basin']:\n\t\t\t\t\n\t\t\t\tminima_list[l][b] = self.disc.basin_index['Level'][l]['Basin'][b]['Min']\n\t\t\tif l !=1:\n\t\t\t\tpmin_list[l] = {}\n\t\t\t\tfor b in self.disc.basin_index['Level'][l]['Basin']:\n\t\t\t\t\tpmin_list[l][b] = self.disc.basin_index['Level'][l]['Basin'][b]['Parents']\n\t\t# Re-number test list to correspond to minima_list\n\t\t\n\t\t#print self.disc.basin_index['Level']\n\t\t#print self.disc.basin_index['Level'][2]['Basin'][13]\n\t\t#print 'python'\n\t\t#print minima_list[2]\n\t\t#print 'FORTRAN'\n\t\t#print test_list[2]\n\t\t#\n\t\t#print 'python'\n\t\t#print minima_list[3]\n\t\t#print 'FORTRAN'\n\t\t#print test_list[3]\n\n\t\t#print parent_list\n\t\t#print pmin_list\n\t\tself.assertDictEqual(pmin_list, parent_list)\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}, {"term": "def", "name": "ftestChildren", "data": "\tdef testChildren(self):\n\t\tself.maxDiff = None\n\t\ttest_list = {}\n\t\tminima_list = {}\n\t\tchildren_list = {}\n\t\tpmin_list = {}\n\t\tref = {}\n\t\tself.disc.CountMin()\n\t\tself.disc.CountTS()\n\t\tself.disc.RemoveThreshold()\n\t\tself.disc.RemoveUnderConnect()\n\t\tself.disc.RemoveDisjoint()\n\t\tself.disc.InitialiseBasin()\n\t\tself.disc.AssignBasins()\n\t\tself.disc.PruneBasins()\n\t\tself.disc.ReNumberBasins()\n\t\tself.disc.GgetParentsAndChildren()\n\t\tfor lines in open('min_in_basin_parent'):\n\t\t\tm = int(lines.split()[0])\n\t\t\tl = int(lines.split()[1])\n\t\t\tb = int(lines.split()[2])\n\t\t\tp = int(lines.split()[3])\n\t\t\t#c = int(lines.split()[4])\n\t\t\tif b != 0:\n\t\t\t\ttry:\n\t\t\t\t\ttest_list[l][b].append(m)\n\t\t\t\texcept KeyError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\ttest_list[l] = {}\n\t\t\t\t\t\ttest_list[l][b] = [m]\n\t\t\t\t\t#test_list[l][b]\n\t\t\tif l != 1 and b != 0:\n\t\t\t\ttry:\n\t\t\t\t\tchildren_list[l-1][p].append(b)\n\t\t\t\texcept KeyError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tchildren_list[l-1][p] = [b]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tchildren_list[l-1] = {}\n\t\t\t\t\t\tchildren_list[l-1][p] = [b]\n\t\t# Remove duplicates from childre_list\n\t\tfor l in children_list:\n\t\t\tfor p in children_list[l]:\n\t\t\t\t#print 'hello',children_list[l][p]\n\t\t\t\tchildren_list[l][p] = list(set(children_list[l][p]))\n\t\t\t\t#print 'hello again', children_list[l][p]\n\t\t\t\t#list(children_list[l][p])\n\t\t\t\t#print 'hello hello again', children_list[l][p]\n\t\tfor l in self.disc.basin_index['Level']:\n\t\t\tminima_list[l] = {}\n\t\t\tfor b in self.disc.basin_index['Level'][l]['Basin']:\n\t\t\t\t\n\t\t\t\tminima_list[l][b] = self.disc.basin_index['Level'][l]['Basin'][b]['Min']\n\t\t\tif l !=self.disc.kw.levels['n']:\n\t\t\t\tpmin_list[l] = {}\n\t\t\t\tfor b in self.disc.basin_index['Level'][l]['Basin']:\n\t\t\t\t\tc = self.disc.basin_index['Level'][l]['Basin'][b]['Children']\n\t\t\t\t\tif c:\n\t\t\t\t\t\tpmin_list[l][b] = c\n\n\t\tself.assertDictEqual(pmin_list, children_list)\n\n\n", "description": null, "category": "remove", "imports": ["import unittest", "from pyconnect.disconnectplot import DisconnectPlot", "from pyconnect.keywords import Keywords"]}], [], [], [], [{"term": "def", "name": "preprocess", "data": "def preprocess(x: str, remove_stars=False, remove_java_doc_vars=False, remove_html_tags=False, remove_comments=False,\n\t\t\t   remove_start_and_end_quotes=False, lower=False, to_edinburgh_format=False, remove_whitespace=False) -> str:\n\tif to_edinburgh_format:\n\t\tif x.endswith('\\n'):\n\t\t\tx = x[:-len('\\n')]\n\t\tx = x.replace('\\n', ' DCNL ')\n\t\tx = x.replace('\t', ' DCSP ')\n\t\tx = x.replace('\\t', ' DCSP ')\n\tif remove_java_doc_vars:\n\t\tx = re.sub(r'(?<![{])(@[\\s\\S]*)', ' ', x)\n\tif remove_comments:\n\t\tx = re.sub(r'(?<![:\\\"])(//.*?(?:\\n|\\\\n))', ' ', x)\n\tif remove_html_tags:\n\t\tx = re.sub(r'', ' ', x)\n\tif remove_whitespace:\n\t\tx = x.replace('\\\\n', ' ').replace('\\n', ' ')\n\t\tx = x.replace('\\\\t', ' ').replace('\\t', ' ')\n\tif remove_stars:\n\t\tx = x.replace('/*', ' ').replace('*/', ' ').replace('*', ' ')\n\tif remove_start_and_end_quotes:\n\t\tx = x.strip()\n\t\tif x.startswith(\"'\"):\n\t\t\tx = x[len(\"'\"):]\n\t\tif x.endswith(\"'\"):\n\t\t\tx = x[:-len(\"'\")]\n\t\tif x.startswith('\"'):\n\t\t\tx = x[len('\"'):]\n\t\tif x.endswith('\"'):\n\t\t\tx = x[:-len('\"')]\n\tx = x.strip()\n\tx = re.sub(r'(\\s\\s+)', ' ', x)\n\tif lower:\n\t\tx = x.lower()\n\treturn x\n\n", "description": null, "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "preprocess_csharp_or_java", "data": "def preprocess_csharp_or_java(x: str) -> str:\n\treturn preprocess(x, remove_comments=True, remove_start_and_end_quotes=True, remove_whitespace=True)\n\n", "description": null, "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "preprocess_javadoc", "data": "def preprocess_javadoc(x: str) -> str:\n\treturn preprocess(x, remove_stars=True, remove_java_doc_vars=True, remove_html_tags=True,\n\t\t\t\t\t  remove_start_and_end_quotes=True)\n", "description": null, "category": "remove", "imports": ["import re"]}], [], [], [], [], [{"term": "def", "name": "remove_one", "data": "def remove_one(x,xs): #\uc7ac\uadc0\ud568\uc218\n\tif xs != []:\n\t\tif x == xs[0]:\n\t\t\treturn xs[1:]\n\t\telse:\n\t\t\treturn [xs[0]] + remove_one(x,xs[1:])\n\telse:\n\t\treturn xs\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_one", "data": "def remove_one(x,xs): #\uaf2c\ub9ac\uc7ac\uadc0\n\tdef loop(xs,head):\n\t\tif xs != []:\n\t\t\tif x == xs[0]:\n\t\t\t\treturn head + xs[1:]\n\t\t\telse:\n\t\t\t\thead.append(xs[0])\n\t\t\t\treturn loop(xs[1:],head)\n\t\telse:\n\t\t\treturn head + xs\n\treturn loop (xs,[])\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_one", "data": "def remove_one(x,xs): #while \ub8e8\ud504\n\thead = []\n\twhile xs != []:\n\t\tif x == xs[0]:\n\t\t\thead += xs[1:]\n\t\t\txs = []\n\t\telse:\n\t\t\thead.append(xs[0])\n\t\t\txs = xs[1:]\n\treturn head + xs\n\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_all", "data": "def remove_all(x,xs): #\uc7ac\uadc0\ud568\uc218\n\tif xs != []:\n\t\tif x == xs[0]:\n\t\t\treturn remove_all(x,xs[1:])\n\t\telse:\n\t\t\treturn [xs[0]] + remove_all(x,xs[1:])\n\telse:\n\t\treturn xs\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_all", "data": "def remove_all(x,xs): #\uaf2c\ub9ac\uc7ac\uadc0\n\tdef loop(head,xs):\n\t\tif xs != []:\n\t\t\tif x == xs[0]:\n\t\t\t\treturn loop(head, xs[1:])\n\t\t\telse:\n\t\t\t\thead.append(xs[0])\n\t\t\t\treturn loop(head, xs[1:])\n\t\telse:\n\t\t\treturn head\n\treturn loop([],xs)\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_all", "data": "def remove_all(x,xs): #while \ub8e8\ud504\n\thead = []\n\twhile xs != []:\n\t\tif x == xs[0]:\n\t\t\txs = xs[1:]\n\t\telse:\n\t\t\thead.append(xs[0])\n\t\t\txs = xs[1:]\n\treturn head\n\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_duplicates", "data": "def remove_duplicates(xs): #\uc7ac\uadc0\ud568\uc218\n\tif len(xs) >= 2:\n\t\thead = xs[0]\n\t\treturn [head] + remove_duplicates(remove_all(head,xs[1:]))\n\telse:\n\t\treturn xs\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_duplicates", "data": "def remove_duplicates(xs): #\uaf2c\ub9ac \uc7ac\uadc0\n\tdef loop(head,xs):\n\t\tif len(xs) >= 2:\n\t\t\thead.append(xs[0])\n\t\t\treturn loop(head, remove_all(head[-1],xs[1:]))\n\t\telse:\n\t\t\treturn head + xs\n\treturn loop([],xs)\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_duplicates", "data": "def remove_duplicates(xs): #while loop\n\thead = []\n\twhile len(xs) >= 2:\n\t\thead.append(xs[0])\n\t\txs = remove_all(head[-1],xs[1:])\n\treturn head + xs\n\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "union", "data": "def union(xs,ys) : #\uc7ac\uadc0\ud568\uc218\n\tif xs != []:\n\t\tif xs[0] in ys:\n\t\t\treturn union(xs[1:],ys)\n\t\telse:\n\t\t\treturn [xs[0]] + union(xs[1:], ys)\n\telse:\n\t\treturn ys\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "union", "data": "def union(xs,ys): #\uaf2c\ub9ac\uc7ac\uadc0\n\tdef loop(head, xs, ys):\n\t\tif xs != []:\n\t\t\tif xs[0] in ys:\n\t\t\t\treturn loop(head, xs[1:], ys)\n\t\t\telse:\n\t\t\t\thead.append(xs[0])\n\t\t\t\treturn loop(head, xs[1:], ys)\n\t\telse:\n\t\t\treturn head + ys\n\treturn loop([], xs, ys)\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "union", "data": "def union(xs,ys): #while loop\n\thead = []\n\twhile xs != []:\n\t\tif xs[0] in ys:\n\t\t\tdel xs[0]\n\t\telse:\n\t\t\thead.append(xs[0])\n\t\t\tdel xs[0]\n\treturn head + ys\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "union", "data": "def union(xs,ys): #for loop\n\thead = []\n\tfor x in xs:\n\t\tif x in ys:\n\t\t\tNone\n\t\telse:\n\t\t\thead.append(x)\n\treturn head + ys\n\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "intersection", "data": "def intersection(xs, ys) : #\uc7ac\uadc0\ud568\uc218\n\tif xs != []:\n\t\tif xs[0] in ys:\n\t\t\treturn [xs[0]] + intersection(xs[1:], ys)\n\t\telse:\n\t\t\treturn intersection(xs[1:], ys)\n\telse:\n\t\treturn []\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "intersection", "data": "def intersection(xs, ys): #\uaf2c\ub9ac\uc7ac\uadc0\n\tdef loop(acc, xs, ys):\n\t\tif xs != []:\n\t\t\tif xs[0] in ys:\n\t\t\t\tacc.append(xs[0])\n\t\t\t\treturn loop(acc, xs[1:], ys)\n\t\t\telse:\n\t\t\t\treturn loop(acc, xs[1:], ys)\n\t\telse:\n\t\t\treturn acc\n\treturn loop([], xs, ys)\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "intersection", "data": "def intersection(xs, ys): #while loop\n\tacc = []\n\twhile xs != []:\n\t\tif xs[0] in ys:\n\t\t\tacc.append(xs[0])\n\t\t\tdel xs[0]\n\t\telse:\n\t\t\tdel xs[0]\n\treturn acc\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "intersection", "data": "def intersection(xs, ys): #for loop\n\tacc = []\n\tfor x in xs:\n\t\tif x in ys:\n\t\t\tacc.append(x)\n\t\telse:\n\t\t\tNone\n\treturn acc\n\n\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "difference", "data": "def difference(xs,ys) : #\uc7ac\uadc0\ud568\uc218\n\tif xs != []:\n\t\tif xs[0] in ys:\n\t\t\treturn difference(xs[1:], ys)\n\t\telse:\n\t\t\treturn [xs[0]] + difference(xs[1:], ys)\n\telse:\n\t\treturn []\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "difference", "data": "def difference(xs, ys): #\uaf2c\ub9ac\uc7ac\uadc0\n\tdef loop(acc, xs, ys):\n\t\tif xs != []:\n\t\t\tif xs[0] in ys:\n\t\t\t\treturn loop(acc, xs[1:], ys)\n\t\t\telse:\n\t\t\t\tacc.append(xs[0])\n\t\t\t\treturn loop(acc, xs[1:], ys)\n\t\telse:\n\t\t\treturn acc\n\treturn loop([], xs, ys)\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "difference", "data": "def difference(xs, ys): #while loop\n\tacc = []\n\twhile xs != []:\n\t\tif xs[0] in ys:\n\t\t\tdel xs[0]\n\t\telse:\n\t\t\tacc.append(xs[0])\n\t\t\tdel xs[0]\n\treturn acc\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "difference", "data": "def difference(xs, ys): #for loop\n\tacc = []\n\tfor x in xs:\n\t\tif not x in ys:\n\t\t\tacc.append(x)\n", "description": null, "category": "remove", "imports": []}], [], [], [], [], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('order', '0007_auto_20220615_0758'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='color',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='create_date',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='description',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='gender',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='is_open',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='part_number',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='price',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='quantity',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='quotation_id',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='receiver_name',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='receiver_tel',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='rep_id',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='req_image',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='ship_to_city',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='ship_to_state',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='ship_to_zipcode',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='size',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='sleeve',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ordermaster',\n\t\t\tname='update_date',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from django.db import migrations"]}], [], [], [{"term": "class", "name": "HoverButton", "data": "class HoverButton(Button):\r\n\tdef __init__(self, master, **kw):\r\n\t\tButton.__init__(self,master=master,**kw)\r\n\t\tself.defaultBackground = self[\"background\"]\r\n\t\tself.bind(\"\", self.on_enter)\r\n\t\tself.bind(\"\", self.on_leave)\r\n\r\n\tdef on_enter(self, e):\r\n\t\tself[\"background\"] = self[\"activebackground\"]\r\n\r\n\tdef on_leave(self, e):\r\n\t\tself[\"background\"] = self.defaultBackground\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "et_conf", "data": "def et_conf(x, y = 20, c =0):\r\n\tx.config(\r\n\t\tfg = color1,\r\n\t\tbg = color3,\r\n\t\tfont = (fontF,y),\r\n\t\tpadx = c\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "buttons_conf", "data": "def buttons_conf(x, y = 10, z = 10):\r\n\tx.config(\r\n\t\tfg = color3,\r\n\t\tbg = color1,\r\n\t\tpady=10,\r\n\t\tpadx=z,\r\n\t\tfont = (fontF, y),\r\n\t\tactivebackground= color2,\r\n\t\tactiveforeground= color3,\r\n\t)\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "validar", "data": "def validar():\r\n\tif in_entry.get()==\"123456\":\r\n\t\tprint(\"[+ Ingreso a apps]\")\r\n\t\tapps()\r\n\telse:\r\n\t\tin_entry.delete(\"0\",\"end\")\r\n\t\tmessagebox.showinfo(title =\"Error\", message = \"Clave incorrecta\")\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "a\u5358adir", "data": "def a\u5358adir():\r\n\tif len(co_entry1.get()) > 1 and len(co_entry2.get()) > 1:\r\n\t\ta = f\"{co_entry1.get()} : {co_entry2.get()}\" \r\n\t\tprint(a)\r\n\t\tif l1.cget(\"text\") == \"Vacio\":\r\n\t\t\tl1.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\telif l2.cget(\"text\") == \"Vacio\":\r\n\t\t\tl2.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\telif l3.cget(\"text\") == \"Vacio\":\r\n\t\t\tl3.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\telif l4.cget(\"text\") == \"Vacio\":\r\n\t\t\tl4.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\telif l5.cget(\"text\") == \"Vacio\":\r\n\t\t\tl5.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\telif l6.cget(\"text\") == \"Vacio\":\r\n\t\t\tl6.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\telif l7.cget(\"text\") == \"Vacio\":\r\n\t\t\tl7.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\telif l8.cget(\"text\") == \"Vacio\":\r\n\t\t\tl8.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\telif l9.cget(\"text\") == \"Vacio\":\r\n\t\t\tl9.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\telif l10.cget(\"text\") == \"Vacio\":\r\n\t\t\tl10.config(\r\n\t\t\t\ttext= a\r\n\t\t\t)\r\n\t\tco_entry1.delete(\"0\",\"end\")\r\n\t\tco_entry2.delete(\"0\",\"end\")\r\n\telse:\r\n\t\tmessagebox.showinfo(title=\"Error\",message=\"Los datos no son validos\")\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "Inicio", "data": "def Inicio():\r\n\ttry:\r\n\t\tventana.iconbitmap(bitmap= \"./permanente 3b/Telefono.ico\")\r\n\texcept :\r\n\t\tpass\r\n\tet_conf(in_label,title_size,60)\r\n\tet_conf(in_label2)\r\n\tet_conf(in_label3)\r\n\tet_conf(in_label4)\r\n\tin_entry.config(\r\n\t\tfg = color1,\r\n\t\tshow= \"*\",\r\n\t\tbg = color3,\r\n\t\tfont= (fontF, 24),\r\n\t\tjustify= \"center\"\r\n\t)\r\n\tbuttons_conf(in_button,20,30)\r\n\r\n\r\n\tin_label.grid(row=0, column=1, pady=15)\r\n\tin_label2.grid(row=1, column=1)\r\n\tin_label3.grid(row=2, column=1)\r\n\tin_label4.grid(row=3, column=1)\r\n\tin_entry.grid(row = 4, column=1,pady=40)\r\n\tin_button.grid(row=5, column=1)\r\n\r\n\t\r\n\r\n\tca_label.grid_remove()\r\n\tco_label.grid_remove()\r\n\tap_label.grid_remove()\r\n\tb1.grid_remove()\r\n\tb2.grid_remove()\r\n\tb3.grid_remove()\r\n\tb4.grid_remove()\r\n\tb5.grid_remove()\r\n\tb6.grid_remove()\r\n\tb7.grid_remove()\r\n\tb8.grid_remove()\r\n\tb9.grid_remove()\r\n\tb10.grid_remove()\r\n\tb11.grid_remove()\r\n\tb12.grid_remove()\r\n\tb13.grid_remove()\r\n\tb14.grid_remove()\r\n\tb15.grid_remove()\r\n\tb16.grid_remove()\r\n\tb17.grid_remove()\r\n\tb18.grid_remove()\r\n\tbutton_return.grid_remove()\r\n\tbutton_return2.grid_remove()\r\n\tca_entry.grid_remove()\r\n\tl1.grid_remove()\r\n\tl2.grid_remove()\r\n\tl3.grid_remove()\r\n\tl4.grid_remove()\r\n\tl5.grid_remove()\r\n\tl6.grid_remove()\r\n\tl7.grid_remove()\r\n\tl8.grid_remove()\r\n\tl9.grid_remove()\r\n\tl10.grid_remove()\r\n\tco_label3.grid_remove()\r\n\tco_entry2.grid_remove()\r\n\tco_label2.grid_remove()\r\n\tco_entry1.grid_remove()\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "Calculadora", "data": "def Calculadora():\r\n\ttry:\r\n\t\tventana.iconbitmap(bitmap= \"./permanente 3b/Calculadora.ico\")\r\n\texcept :\r\n\t\tpass\r\n\tventana.title(\"Calculadora\")\r\n\tet_conf(ca_label,title_size + 10, 0)\r\n\tca_entry.config(\r\n\t\tfont=(\"Arial\", 27), \r\n\t\tfg = color3,\r\n\t\tbg = color2,\r\n\t\tjustify= \"right\",\r\n\t)\r\n\r\n\tca_label.grid(row=0, column=1,columnspan=4)\r\n\tca_entry.grid(row=1, column=1,columnspan=4)\r\n\tb1.grid( column= 1 ,row=2,pady =10)\r\n\tb2.grid( column= 2 ,row=2,pady =10)\r\n\tb3.grid( column= 3 ,row=2,pady =10)\r\n\tb4.grid( column= 4 ,row=2,pady =10)\r\n\tb5.grid( column= 1 ,row=3,pady =10)\r\n\tb6.grid( column= 2 ,row=3,pady =10)\r\n\tb7.grid( column= 3 ,row=3,pady =10)\r\n\tb8.grid( column= 4 ,row=3,pady =10)\r\n\tb9.grid( column= 1 ,row=4,pady =10)\r\n\tb10.grid( column= 2 ,row=4,pady =5)\r\n\tb11.grid( column= 3 ,row=4,pady =5)\r\n\tb12.grid( column= 4 ,row=4,pady =5)\r\n\tb13.grid( column= 1 ,row=5,pady =5)\r\n\tb14.grid( column= 2 ,row=5,pady =5)\r\n\tb15.grid( column= 3 ,row=5,pady =5)\r\n\tb16.grid( column= 4 ,row=5,pady =5)\r\n\tb17.grid( column= 1 ,row=6,pady =5,columnspan=2)\r\n\tb18.grid( column= 3 ,row=6,pady =5,columnspan=2)\r\n\tbutton_return.grid(column= 1 ,row=7,columnspan=4)\r\n\r\n\tin_label.grid_remove()\r\n\tin_label2.grid_remove()\r\n\tin_label3.grid_remove()\r\n\tin_label4.grid_remove()\r\n\tco_label.grid_remove()\r\n\tap_label.grid_remove()\r\n\tin_entry.grid_remove()\r\n\tin_button.grid_remove()\r\n\tap_button1.grid_remove()\r\n\tap_button2.grid_remove()\r\n\tbutton_return2.grid_remove()\r\n\tap_button3.grid_remove()\r\n\tl1.grid_remove()\r\n\tl2.grid_remove()\r\n\tl3.grid_remove()\r\n\tl4.grid_remove()\r\n\tl5.grid_remove()\r\n\tl6.grid_remove()\r\n\tl7.grid_remove()\r\n\tl8.grid_remove()\r\n\tl9.grid_remove()\r\n\tl10.grid_remove()\r\n\tco_label3.grid_remove()\r\n\tco_entry2.grid_remove()\r\n\tco_label2.grid_remove()\r\n\tco_entry1.grid_remove()\r\n\tco_button.grid_remove()\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "apps", "data": "def apps():\r\n\ttry:\r\n\t\tventana.iconbitmap(bitmap= \"./permanente 3b/Telefono.ico\")\r\n\texcept:\r\n\t\tpass\r\n\tet_conf(ap_label,title_size,35)\r\n\tbuttons_conf(ap_button1,20,20)\r\n\tbuttons_conf(ap_button2,20,40)\r\n\tbuttons_conf(ap_button3,20, 60)\r\n\r\n\tap_label.grid(row=0, column=1,pady=20)\r\n\tap_button1.grid(row=1,column=1, pady = 20)\r\n\tap_button2.grid(row=2,column=1, pady = 20)\r\n\tap_button3.grid(row=3,column=1, pady = 20)\r\n\r\n\r\n\tin_label.grid_remove()\r\n\tin_label2.grid_remove()\r\n\tin_label3.grid_remove()\r\n\tin_label4.grid_remove()\r\n\tin_entry.grid_remove()\r\n\tin_button.grid_remove()\r\n\tca_label.grid_remove()\r\n\r\n\tco_label.grid_remove()\r\n\r\n\tb1.grid_remove()\r\n\tb2.grid_remove()\r\n\tb3.grid_remove()\r\n\tb4.grid_remove()\r\n\tb5.grid_remove()\r\n\tb6.grid_remove()\r\n\tb7.grid_remove()\r\n\tb8.grid_remove()\r\n\tb9.grid_remove()\r\n\tb10.grid_remove()\r\n\tb11.grid_remove()\r\n\tb12.grid_remove()\r\n\tb13.grid_remove()\r\n\tb14.grid_remove()\r\n\tb15.grid_remove()\r\n\tb16.grid_remove()\r\n\tb17.grid_remove()\r\n\tb18.grid_remove()\r\n\tca_entry.grid_remove()\r\n\tbutton_return.grid_remove()\r\n\tbutton_return2.grid_remove()\r\n\tl1.grid_remove()\r\n\tl2.grid_remove()\r\n\tl3.grid_remove()\r\n\tl4.grid_remove()\r\n\tl5.grid_remove()\r\n\tl6.grid_remove()\r\n\tl7.grid_remove()\r\n\tl8.grid_remove()\r\n\tl9.grid_remove()\r\n\tl10.grid_remove()\r\n\tco_label3.grid_remove()\r\n\tco_entry2.grid_remove()\r\n\tco_label2.grid_remove()\r\n\tco_entry1.grid_remove()\r\n\tco_button.grid_remove()\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "Contactos", "data": "def Contactos():\r\n\ttry:\r\n\t\tventana.iconbitmap(bitmap= \"./permanente 3b/Contactos.ico\")\r\n\texcept :\r\n\t\tpass\r\n\tco_label.config(\r\n\t\tfg = color1,\r\n\t\tbg = color3,\r\n\t\tfont = (fontF,title_size),\r\n\t\tpadx=80\r\n\t)\r\n\tco_entry1.config(\r\n\t\tfont=(\"Arial\", 10), \r\n\t\tfg = color3,\r\n\t\tbg = color2,\r\n\t\tjustify= \"center\",\r\n\t)\r\n\tco_entry2.config(\r\n\t\tfont=(\"Arial\", 10), \r\n\t\tfg = color3,\r\n\t\tbg = color2,\r\n\t\tjustify= \"center\",\r\n\t)\r\n\tet_conf(co_label2,10)\r\n\tet_conf(co_label3,10)\r\n\tbuttons_conf(co_button)\r\n\tet_conf(l1,10)\r\n\tet_conf(l2,10)\r\n\tet_conf(l3,10)\r\n\tet_conf(l4,10)\r\n\tet_conf(l5,10)\r\n\tet_conf(l6,10)\r\n\tet_conf(l7,10)\r\n\tet_conf(l8,10)\r\n\tet_conf(l9,10)\r\n\tet_conf(l10,10)\r\n\r\n\tco_label.grid(row=0, column=0, columnspan=2)\r\n\tco_label2.grid(row=1, column=0, pady=10)\r\n\tco_entry1.grid(row=1, column=1, pady=10)\r\n\tco_label3.grid(row=2, column=0, pady=10)\r\n\tco_entry2.grid(row=2, column=1, pady=10)\r\n\tco_button.grid(row=3,column=0, columnspan=2,pady=15)\r\n\tl1.grid(row=4, column=0)\r\n\tl2.grid(row=5, column=0)\r\n\tl3.grid(row=6, column=0)\r\n\tl4.grid(row=7, column=0)\r\n\tl5.grid(row=8, column=0)\r\n\tl6.grid(row=9, column=0)\r\n\tl7.grid(row=10, column=0)\r\n\tl8.grid(row=11, column=0)\r\n\tl9.grid(row=12, column=0)\r\n\tl10.grid(row=13, column=0)\r\n\r\n\r\n\tbutton_return.grid(column= 0 ,row=14, columnspan=2,pady=20)\r\n\r\n\tin_button.grid_remove()\r\n\tin_entry.grid_remove()\r\n\tin_label.grid_remove()\r\n\tin_label2.grid_remove()\r\n\tin_label3.grid_remove()\r\n\tin_label4.grid_remove()\r\n\tca_label.grid_remove()\r\n\tap_label.grid_remove()\t\r\n\tb1.grid_remove()\r\n\tb2.grid_remove()\r\n\tb3.grid_remove()\r\n\tb4.grid_remove()\r\n\tb5.grid_remove()\r\n\tb6.grid_remove()\r\n\tb7.grid_remove()\r\n\tb8.grid_remove()\r\n\tb9.grid_remove()\r\n\tb10.grid_remove()\r\n\tb11.grid_remove()\r\n\tb12.grid_remove()\r\n\tb13.grid_remove()\r\n\tb14.grid_remove()\r\n\tb15.grid_remove()\r\n\tb16.grid_remove()\r\n\tb17.grid_remove()\r\n\tb18.grid_remove()\r\n\tca_entry.grid_remove()\r\n\tap_button1.grid_remove()\r\n\tap_button2.grid_remove()\r\n\tap_button3.grid_remove()\r\n\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "obtener", "data": "def obtener(dato):\r\n\tglobal i\r\n\ti+=1\r\n\tca_entry.insert(i, dato)\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "operacion", "data": "def operacion():\r\n\tglobal i\r\n\r\n\tecuacion = ca_entry.get()\r\n\tif i !=0:\t\t\r\n\t\ttry:\r\n\t\t\tresult = str(eval(ecuacion))\r\n\t\t\tca_entry.delete(0,END)\r\n\t\t\tca_entry.insert(0,result)\r\n\t\t\tlongitud = len(result)\r\n\t\t\ti = longitud\r\n\r\n\t\texcept:\r\n\t\t\tresult = 'ERROR'\r\n\t\t\tca_entry.delete(0,END)\r\n\t\t\tca_entry.insert(0,result)\r\n\telse:\r\n\t\tpass\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "borrar_uno", "data": "def borrar_uno():\r\n\tglobal i \r\n\tif i==-1:\r\n\t\tpass\r\n\telse:\r\n\t\tca_entry.delete(i,last =None)\r\n\t\ti-=1\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}, {"term": "def", "name": "borrar_todo", "data": "def borrar_todo():\r\n\tca_entry.delete(0, END)\t\r\n\ti=0\r\n\r\n\r\n", "description": null, "category": "remove", "imports": ["from email.mime import message\r", "from itertools import count\r", "from tkinter import *\r", "from tkinter import messagebox\r", "from turtle import right\r"]}], [], [], [], [], [], [], [{"term": "class", "name": "classNode:", "data": "class Node:\n\tdef __init__ (self, value = None, next = None):\n\t\tself.value = value\n\t\tself.next = next\n", "description": null, "category": "remove", "imports": ["from util.Empty import Empty", "from util.Outbound import Outbound"]}, {"term": "class", "name": "classLinkedList:", "data": "class LinkedList:\n\t\n\tdef __init__(self):\n\t\tself.head = Node()\n\t\tself.length = 0\n\n\tdef peek(self):\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\treturn self.head.next\n\n\tdef get_first(self):\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\treturn self.head.next\n\t\t\n\tdef get_last(self):\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tnode = self.head\n\t\twhile node.next != None:\n\t\t\tnode = node.next\n\t\treturn node\n\t\n\tdef get(self, index):\n\t\tif (index < 0 or index >= self.length):\n\t\t\traise Outbound( 'index is out of bound' );\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tnode = self.head.next\n\t\tfor i in range(index):\n\t\t\tnode = node.next\n\t\treturn node\n\t\t\t\t\n\tdef add_first(self, value):\n\t\tnode = Node(value, None)\n\t\tnode.next = self.head.next\n\t\tself.head.next = node\n\t\tself.length += 1   \n\t\t\n\tdef add_last(self, value):\n\t\tnew_node = Node(value)\n\t\tnode = self.head\n\t\twhile node.next != None:\n\t\t\tnode = node.next\n\t\tnode.next = new_node\n\t\tself.length += 1\n\n\tdef add(self, index, value):\n\t\tif (index < 0 or index > self.length):\n\t\t\traise Outbound( 'index is out of bound' )\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tnew_node = Node(value)\n\t\tnode = self.head\n\t\tfor i in range(index):\n\t\t\tnode = node.next\n\t\tnew_node.next = node.next;\n\t\tnode.next = new_node;\n\t\tself.length += 1\t \n\t\t\n\tdef remove_first(self):\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tvalue = self.head.next\n\t\tself.head.next = self.head.next.next\n\t\tself.length -= 1\n\t\treturn value\t\n\t\t\n\tdef remove_last(self):\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tnode = self.head.next\n\t\tprev = self.head\n\t\twhile node.next != None:\n\t\t\tprev = node\n\t\t\tnode = node.next\n\t\tprev.next = None\n\t\treturn node.value\n\n\tdef remove(self, index):\n\t\tif (index < 0 or index >= self.length):\n\t\t\traise Outbound( 'index is out of bound' );\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tnode = self.head\n\t\tfor i in range(index):\n\t\t\tnode = node.next\n\t\tresult = node.next;\n\t\tnode.next = node.next.next;\n\t\tself.length += 1\t \n\t\treturn result;\t  \n\t\t\n\tdef printlist(self):\n\t\tnode = self.head.next\n\t\tcount = 0\n\t\twhile node and count<20:\n\t\t\tprint(node.value, end = \" \")\n\t\t\tnode = node.next\n\t\t\tcount = count + 1\n\t\tprint('')\n", "description": null, "category": "remove", "imports": ["from util.Empty import Empty", "from util.Outbound import Outbound"]}], [{"term": "class", "name": "classNode:", "data": "class Node:\n\tdef __init__ (self, value = None, next = None):\n\t\tself.value = value\n\t\tself.next = next\n", "description": null, "category": "remove", "imports": []}, {"term": "class", "name": "classLinkedList:", "data": "class LinkedList:\n\t\n\tdef __init__(self):\n\t\tself.head = Node()\n\t\tself.length = 0\n\n\tdef peek(self):\n\t\tif not self.head.next:\n\t\t\traise ValueError( 'LinkedList is empty' )\n\t\treturn self.head.next\n\n\tdef get_first(self):\n\t\tif not self.head.next:\n\t\t\traise ValueError( 'LinkedList is empty' )\n\t\treturn self.head.next\n\t\t\n\tdef get_last(self):\n\t\tif not self.head.next:\n\t\t\traise ValueError( 'LinkedList is empty' )\n\t\tnode = self.head\n\t\twhile node.next != None:\n\t\t\tnode = node.next\n\t\treturn node\n\t\n\tdef get(self, index):\n\t\tif (index < 0 or index >= self.length):\n\t\t\traise ValueError( 'index is out of bound' );\n\t\tif not self.head.next:\n\t\t\traise ValueError( 'LinkedList is empty' )\n\t\tnode = self.head.next\n\t\tfor i in range(index):\n\t\t\tnode = node.next\n\t\treturn node\n\t\t\t\t\n\tdef add_first(self, value):\n\t\tnode = Node(value, None)\n\t\tnode.next = self.head.next\n\t\tself.head.next = node\n\t\tself.length += 1   \n\t\t\n\tdef add_last(self, value):\n\t\tnew_node = Node(value)\n\t\tnode = self.head\n\t\twhile node.next != None:\n\t\t\tnode = node.next\n\t\tnode.next = new_node\n\t\tself.length += 1\n\n\tdef add(self, index, value):\n\t\tif (index < 0 or index > self.length):\n\t\t\traise Outbound( 'index is out of bound' )\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tnew_node = Node(value)\n\t\tnode = self.head\n\t\tfor i in range(index):\n\t\t\tnode = node.next\n\t\tnew_node.next = node.next;\n\t\tnode.next = new_node;\n\t\tself.length += 1\t \n\t\t\n\tdef remove_first(self):\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tvalue = self.head.next\n\t\tself.head.next = self.head.next.next\n\t\tself.length -= 1\n\t\treturn value\t\n\t\t\n\tdef remove_last(self):\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tnode = self.head.next\n\t\tprev = self.head\n\t\twhile node.next != None:\n\t\t\tprev = node\n\t\t\tnode = node.next\n\t\tprev.next = None\n\t\treturn node.value\n\n\tdef remove(self, index):\n\t\tif (index < 0 or index >= self.length):\n\t\t\traise Outbound( 'index is out of bound' );\n\t\tif not self.head.next:\n\t\t\traise Empty( 'LinkedList is empty' )\n\t\tnode = self.head\n\t\tfor i in range(index):\n\t\t\tnode = node.next\n\t\tresult = node.next;\n\t\tnode.next = node.next.next;\n\t\tself.length += 1\t \n\t\treturn result;\t  \n\t\t\n\tdef printlist(self):\n\t\tnode = self.head.next\n\t\tcount = 0\n\t\twhile node and count<20:\n\t\t\tprint(node.value, end = \" \")\n\t\t\tnode = node.next\n\t\t\tcount = count + 1\n\t\tprint('')\n", "description": null, "category": "remove", "imports": []}], [], [], [], [], [{"term": "def", "name": "count_enclosed_functions", "data": "def count_enclosed_functions (source):\n\tfunc_count = 0\n\topen_brace = 0\n\tclose_brace = 0\n\tfor ch in source:\n\t\tif ch == '{':\n\t\t\topen_brace += 1\n\t\telif ch == '}':\n\t\t\tclose_brace += 1\n\t\t\tif open_brace == close_brace:\n\t\t\t\tfunc_count += 1\n\t\tif open_brace < close_brace:\n\t\t\tprint \"count_enclosed_functions : open_brace < close_brace\"\n\t\t\treturn -1\n\treturn func_count\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_function_prototype", "data": "def find_function_prototype (source, proto_name):\n\tproto_re = \"(^[a-zA-Z_ \\t]+\\s+%s[^a-zA-Z0-9_]\\s*\\([^\\)]+\\)\\s+;\\n)\" % (proto_name)\n\tproto_result = re.search (proto_re, source, re.MULTILINE | re.DOTALL)\n\tif not proto_result:\n\t\treturn None\n\tproto_text = proto_result.groups ()[0]\n\treturn proto_text\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_function_definition", "data": "def find_function_definition (source, func_name):\n\tfunc_re = \"(\\n[a-zA-Z_ \\t]+\\n%s[^a-zA-Z0-9_].* /\\* %s \\*/\\n)\" % (func_name, func_name)\n\tfunc_result = re.search (func_re, source, re.MULTILINE | re.DOTALL)\n\tif not func_result:\n\t\tsys.exit (1)\n\t\treturn None\n\tfunc_text = func_result.groups ()[0]\n\n\t# Now to check that we only have one enclosing function.\n\tfunc_count = count_enclosed_functions (func_text)\n\tif func_count != 1:\n\t\treturn None\n\treturn func_text\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_include", "data": "def find_include (source, inc_name):\n\tinc_re = \"(^#include\\s+[\\<\\\"]%s[\\\"\\>]\\s*)\" % inc_name\n\tinc_result = re.search (inc_re, source, re.MULTILINE | re.DOTALL)\n\tif not inc_result:\n\t\treturn None\n\tinc_text = inc_result.groups ()[0]\n\treturn inc_text\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_assign_statement", "data": "def find_assign_statement (source, var_name):\n\tvar_re = \"(^\\s+%s\\s*=[^;]+;)\" % var_name\n\tvar_result = re.search (var_re, source, re.MULTILINE | re.DOTALL)\n\tif not var_result:\n\t\treturn None\n\tassign_text = var_result.groups ()[0]\n\treturn assign_text\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_include", "data": "def remove_include (source, inc_name):\n\tinc_text = find_include (source, inc_name)\n\tif not inc_text:\n\t\tprint \"remove_include : include '%s' not found. Exiting.\" % inc_name\n\t\tsys.exit (1)\n\n\tsource = string.replace (source, inc_text, \"\")\n\treturn source\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_assign", "data": "def remove_assign (source, assign_name):\n\tassign_text = find_assign (source, inc_name)\n\tif not inc_text:\n\t\tprint \"remove_include : include '%s' not found. Exiting.\" % inc_name\n\t\tsys.exit (1)\n\n\tsource = string.replace (source, inc_text, \"\")\n\treturn source\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_prototype", "data": "def remove_prototype (source, proto_name):\n\tproto_text = find_function_prototype (source, proto_name)\n\tif not proto_text:\n\t\tprint \"remove_prototype : prototype '%s' not found. Exiting.\" % proto_name\n\t\tsys.exit (1)\n\n\tsource = string.replace (source, proto_text, \"\")\n\treturn source\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_function", "data": "def remove_function (source, func_name):\n\tfunc_text = find_function_definition (source, func_name)\n\tif not func_text:\n\t\tprint \"remove_function : function '%s' not found. Exiting.\" % func_name\n\t\tsys.exit (1)\n\n\tsource = string.replace (source, func_text, \"/* Function %s() removed here. */\\n\" % func_name)\n\treturn source\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_all_assignments", "data": "def remove_all_assignments (source, var):\n\tcount = 0\n\twhile 1:\n\t\tassign_text = find_assign_statement (source, var)\n\t\tif not assign_text:\n\t\t\tif count != 0:\n\t\t\t\tbreak\n\t\t\tprint \"remove_all_assignments : variable '%s' not found. Exiting.\" % var\n\t\t\tsys.exit (1)\n\n\t\tsource = string.replace (source, assign_text, \"\")\n\t\tcount += 1\n\treturn source\n\n\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_funcs_and_protos_from_file", "data": "def remove_funcs_and_protos_from_file (filename, func_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor func in func_list:\n\t\tsource_code = remove_prototype (source_code, func) ;\n\t\tsource_code = remove_function (source_code, func) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_funcs_from_file", "data": "def remove_funcs_from_file (filename, func_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor func in func_list:\n\t\tsource_code = remove_function (source_code, func) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_protos_from_file", "data": "def remove_protos_from_file (filename, func_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor func in func_list:\n\t\tsource_code = remove_prototype (source_code, func) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_includes_from_file", "data": "def remove_includes_from_file (filename, inc_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor inc in inc_list:\n\t\tsource_code = remove_include (source_code, inc) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_all_assignments_from_file", "data": "def remove_all_assignments_from_file (filename, var_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor var in var_list:\n\t\tsource_code = remove_all_assignments (source_code, var) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_comment_start_end", "data": "def remove_comment_start_end (filename, start_comment, end_comment):\n\tsource_code = open (filename, 'r').read ()\n\n\twhile 1:\n\t\tstart_index = string.find (source_code, start_comment)\n\t\tend_index = string.find (source_code, end_comment)\n\t\tif start_index < 0 or end_index < start_index:\n\t\t\tbreak\n\t\tend_index += len (end_comment)\n\t\tsource_code = source_code [:start_index-1] + source_code [end_index:] ;\n\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_strings_from_file", "data": "def remove_strings_from_file (filename, str_list):\n\tfile_text = open (filename, 'r').read ()\n\tfor current_str in str_list:\n\t\tfile_text = string.replace (file_text, current_str, '')\n\topen (filename, 'w').write (file_text)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "string_replace_in_file", "data": "def string_replace_in_file (filename, from_str, to_str):\n\tfile_text = open (filename, 'r').read ()\n\tfile_text = string.replace (file_text, from_str, to_str)\n\topen (filename, 'w').write (file_text)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_regex_from_file", "data": "def remove_regex_from_file (filename, regex_list):\n\tfile_text = open (filename, 'r').read ()\n\tfor regex in regex_list:\n\t\tfile_text = re.sub (regex, '', file_text, re.MULTILINE | re.DOTALL)\n\topen (filename, 'w').write (file_text)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_configure_version", "data": "def find_configure_version (filename):\n\t# AM_INIT_AUTOMAKE(libsndfile,0.0.21pre6)\n\tfile = open (filename)\n\twhile 1:\n\t\tline = file.readline ()\n\t\tif re.search (\"AC_INIT\", line):\n\t\t\tx = re.sub (\"[^\\(]+\\(\", \"\", line)\n\t\t\tx = re.sub (\"\\).*\\n\", \"\", x)\n\t\t\tx = string.split (x, \",\")\n\t\t\tpackage = x [0]\n\t\t\tversion = x [1]\n\t\t\tbreak\n\tfile.close ()\n\t# version = re.escape (version)\n\treturn package, version\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "fix_configure_ac_file", "data": "def fix_configure_ac_file (filename):\n\tdata = open (filename, 'r').read ()\n\tdata = string.replace (data, \"AM_INIT_AUTOMAKE(libsndfile,\", \"AM_INIT_AUTOMAKE(libsndfile_lite,\", 1)\n\n\tfile = open (filename, 'w')\n\tfile.write (data)\n\tfile.close ()\n\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "make_dist_file", "data": "def make_dist_file (package, version):\n\tprint \"Making dist file.\"\n\ttar_gz_file = \"%s-%s.tar.gz\" % (package, version)\n\tif os.path.exists (tar_gz_file):\n\t\treturn\n\tif os.system (\"make dist\"):\n\t\tsys.exit (1)\n\treturn\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "delete_files", "data": "def delete_files (file_list):\n\tfor file_name in file_list:\n\t\tos.remove (file_name)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}], [{"term": "class", "name": "TestTrie", "data": "class TestTrie(object):\t   \n\n\tdef test_trie(self):\n\t\ttrie = Trie()\n\n\t\tprint('Test: Insert')\n\t\twords = ['a', 'at', 'has', 'hat', 'he',\n\t\t\t\t 'me', 'men', 'mens', 'met']\n\t\tfor word in words:\n\t\t\ttrie.insert(word)\n\t\tfor word in trie.list_words():\n\t\t\tassert_true(trie.find(word) is not None)\n\t\t\t\n\t\tprint('Test: Remove me')\n\t\ttrie.remove('me')\n\t\twords_removed = ['me']\n\t\twords = ['a', 'at', 'has', 'hat', 'he',\n\t\t\t\t 'men', 'mens', 'met']\n\t\tfor word in words:\n\t\t\tassert_true(trie.find(word) is not None)\n\t\tfor word in words_removed:\n\t\t\tassert_true(trie.find(word) is None)\n\n\t\tprint('Test: Remove mens')\n\t\ttrie.remove('mens')\n\t\twords_removed = ['me', 'mens']\n\t\twords = ['a', 'at', 'has', 'hat', 'he',\n\t\t\t\t 'men', 'met']\n\t\tfor word in words:\n\t\t\tassert_true(trie.find(word) is not None)\n\t\tfor word in words_removed:\n\t\t\tassert_true(trie.find(word) is None)\n\n\t\tprint('Test: Remove a')\n\t\ttrie.remove('a')\n\t\twords_removed = ['a', 'me', 'mens']\n\t\twords = ['at', 'has', 'hat', 'he',\n\t\t\t\t 'men', 'met']\n\t\tfor word in words:\n\t\t\tassert_true(trie.find(word) is not None)\n\t\tfor word in words_removed:\n\t\t\tassert_true(trie.find(word) is None)\n\n\t\tprint('Test: Remove has')\n\t\ttrie.remove('has')\n\t\twords_removed = ['a', 'has', 'me', 'mens']\n\t\twords = ['at', 'hat', 'he',\n\t\t\t\t 'men', 'met']\n\t\tfor word in words:\n\t\t\tassert_true(trie.find(word) is not None)\n\t\tfor word in words_removed:\n\t\t\tassert_true(trie.find(word) is None)\n\n\t\tprint('Success: test_trie')\n\n\t@raises(Exception)\n\tdef test_trie_remove_invalid(self):\n\t\tprint('Test: Remove from empty trie')\n\t\ttrie = Trie()\n\t\tassert_true(trie.remove('foo') is None) \n\n", "description": null, "category": "remove", "imports": ["from nose.tools import assert_true", "from nose.tools import raises"]}, {"term": "def", "name": "main", "data": "def main():\n\ttest = TestTrie()\n\ttest.test_trie()\n\ttest.test_trie_remove_invalid()\n\n", "description": null, "category": "remove", "imports": ["from nose.tools import assert_true", "from nose.tools import raises"]}], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\n\tdef __init__(self, data=None, name=\"\", file=None, **attr):\n\t\tsuper().__init__(data=data, name=name, **attr)\n\t\tif file is None:\n\t\t\timport sys\n\n\t\t\tself.fh = sys.stdout\n\t\telse:\n\t\t\tself.fh = open(file, \"w\")\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tsuper().add_node(n, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add node: {n}\\n\")\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self, n):\n\t\tsuper().remove_node(n)\n\t\tself.fh.write(f\"Remove node: {n}\\n\")\n\n\tdef remove_nodes_from(self, nodes):\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tsuper().add_edge(u, v, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add edge: {u}-{v}\\n\")\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.add_edge(u, v, attr_dict=attr_dict, **attr)\n\n\tdef remove_edge(self, u, v):\n\t\tsuper().remove_edge(u, v)\n\t\tself.fh.write(f\"Remove edge: {u}-{v}\\n\")\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.remove_edge(u, v)\n\n\tdef clear(self):\n\t\tsuper().clear()\n\t\tself.fh.write(\"Clear graph\\n\")\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["import matplotlib.pyplot as plt", "import networkx as nx", "from networkx import Graph", "\t\t\timport sys"]}], [{"term": "class", "name": "RemoveSectionPreprocessor", "data": "class RemoveSectionPreprocessor(Preprocessor):\n\t\"\"\"Remove arbitrary Markdown sections.\"\"\"\n\n\tdef __init__(self, config: dict, md: markdown.Markdown) -> None:\n\t\t\"\"\"Initialize.\"\"\"\n\t\t# Example use of config. See __init__ for RemoveSectionExtension\n\t\t# below for initialization.\n\t\t# self.encoding = config.get(\"encoding\")\n\t\tsuper().__init__()\n\n\tdef remove_sections(self, md: str) -> list[str]:\n\t\t\"\"\"Remove various markdown sections.\"\"\"\n\t\tfor section in settings.MARKDOWN_SECTIONS_TO_REMOVE:\n\t\t\tmd = self.remove_md_section(md, section)\n\t\treturn md.split(\"\\n\")\n\n\tdef remove_md_section(self, md: str, section_name: str) -> str:\n\t\t\"\"\"\n\t\tGiven markdown and a section name, removes the section header and the\n\t\ttext contained in the section.\n\t\t\"\"\"\n\t\theader_regex = re.compile(\"^#.*$\")\n\t\tsection_regex = re.compile(\"^#+ {}\".format(section_name))\n\t\tout_md = \"\"\n\t\tin_section = False\n\t\tfor line in md.splitlines():\n\t\t\tif in_section:\n\t\t\t\tif header_regex.match(line):\n\t\t\t\t\t# We found a header.  The section is over.\n\t\t\t\t\tout_md += line + \"\\n\"\n\t\t\t\t\tin_section = False\n\t\t\telse:\n\t\t\t\tif section_regex.match(line):\n\t\t\t\t\t# We found the section header.\n\t\t\t\t\tin_section = True\n\t\t\t\telse:\n\t\t\t\t\tout_md = \"{}{}\\n\".format(out_md, line)\n\t\treturn out_md\n\n\tdef run(self, lines: list[str]) -> list[str]:\n\t\t\"\"\"Entrypoint.\"\"\"\n\t\tsource = \"\\n\".join(lines)\n\t\treturn self.remove_sections(source)\n\n", "description": "Remove arbitrary Markdown sections.", "category": "remove", "imports": ["import re", "from typing import Any", "import markdown", "from markdown import Extension", "from markdown.preprocessors import Preprocessor", "from document.config import settings"]}, {"term": "class", "name": "RemoveSectionExtension", "data": "class RemoveSectionExtension(Extension):\n\t\"\"\"Wikilink to Markdown link conversion extension.\"\"\"\n\n\tdef __init__(self, *args: Any, **kwargs: Any) -> None:\n\t\t\"\"\"Initialize.\"\"\"\n\t\tself.config = {\n\t\t\t# Example config entry from the snippets extension that\n\t\t\t# ships with Python-Markdown library.\n\t\t\t# \"encoding\": [\"utf-8\", 'Encoding of snippets - Default: \"utf-8\"'],\n\t\t}\n\t\tsuper().__init__(*args, **kwargs)\n\n\tdef extendMarkdown(self, md: markdown.Markdown) -> None:\n\t\t\"\"\"Register the extension.\"\"\"\n\t\tself.md = md\n\t\tmd.registerExtension(self)\n\t\tconfig = self.getConfigs()\n\t\tremovesection = RemoveSectionPreprocessor(config, md)\n\t\tmd.preprocessors.register(removesection, \"remove_section\", 32)\n\n", "description": "Wikilink to Markdown link conversion extension.", "category": "remove", "imports": ["import re", "from typing import Any", "import markdown", "from markdown import Extension", "from markdown.preprocessors import Preprocessor", "from document.config import settings"]}, {"term": "def", "name": "makeExtension", "data": "def makeExtension(*args: Any, **kwargs: Any) -> RemoveSectionExtension:\n\t\"\"\"Return extension.\"\"\"\n\treturn RemoveSectionExtension(*args, **kwargs)\n", "description": "Return extension.", "category": "remove", "imports": ["import re", "from typing import Any", "import markdown", "from markdown import Extension", "from markdown.preprocessors import Preprocessor", "from document.config import settings"]}], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\n\tdef __init__(self, data=None, name=\"\", file=None, **attr):\n\t\tsuper().__init__(data=data, name=name, **attr)\n\t\tif file is None:\n\t\t\timport sys\n\n\t\t\tself.fh = sys.stdout\n\t\telse:\n\t\t\tself.fh = open(file, \"w\")\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tsuper().add_node(n, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add node: {n}\\n\")\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self, n):\n\t\tsuper().remove_node(n)\n\t\tself.fh.write(f\"Remove node: {n}\\n\")\n\n\tdef remove_nodes_from(self, nodes):\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tsuper().add_edge(u, v, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add edge: {u}-{v}\\n\")\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.add_edge(u, v, attr_dict=attr_dict, **attr)\n\n\tdef remove_edge(self, u, v):\n\t\tsuper().remove_edge(u, v)\n\t\tself.fh.write(f\"Remove edge: {u}-{v}\\n\")\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.remove_edge(u, v)\n\n\tdef clear(self):\n\t\tsuper().clear()\n\t\tself.fh.write(\"Clear graph\\n\")\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["import matplotlib.pyplot as plt", "import networkx as nx", "from networkx import Graph", "\t\t\timport sys"]}], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [{"term": "class", "name": "TestDataImportFixtures", "data": "class TestDataImportFixtures(unittest.TestCase):\n\tdef setUp(self):\n\t\tpass\n\n\t#start test for Custom Script\n\tdef test_Custom_Script_fixture_simple(self):\n\t\tfixture = \"Custom Script\"\n\t\tpath = frappe.scrub(fixture) + \"_original_style.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Script_fixture_simple_name_equal_default(self):\n\t\tfixture = [\"Custom Script\", {\"name\":[\"Item-Client\"]}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_simple_name_equal_default.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Script_fixture_simple_name_equal(self):\n\t\tfixture = [\"Custom Script\", {\"name\":[\"Item-Client\"],\"op\":\"=\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_simple_name_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Script_fixture_simple_name_not_equal(self):\n\t\tfixture = [\"Custom Script\", {\"name\":[\"Item-Client\"],\"op\":\"!=\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_simple_name_not_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\t#without [] around the name...\n\tdef test_Custom_Script_fixture_simple_name_at_least_equal(self):\n\t\tfixture = [\"Custom Script\", {\"name\":\"Item-Cli\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_simple_name_at_least_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Script_fixture_multi_name_equal(self):\n\t\tfixture = [\"Custom Script\", {\"name\":[\"Item-Client\", \"Customer-Client\"],\"op\":\"=\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_multi_name_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Script_fixture_multi_name_not_equal(self):\n\t\tfixture = [\"Custom Script\", {\"name\":[\"Item-Client\", \"Customer-Client\"],\"op\":\"!=\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_multi_name_not_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Script_fixture_empty_object(self):\n\t\tfixture = [\"Custom Script\", {}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_empty_object_should_be_all.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Script_fixture_just_list(self):\n\t\tfixture = [\"Custom Script\"]\n\t\tpath = frappe.scrub(fixture[0]) + \"_just_list_should_be_all.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\t# Custom Script regular expression\n\tdef test_Custom_Script_fixture_rex_no_flags(self):\n\t\tfixture = [\"Custom Script\", {\"name\":r\"^[i|A]\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_rex_no_flags.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Script_fixture_rex_with_flags(self):\n\t\tfixture = [\"Custom Script\", {\"name\":r\"^[i|A]\", \"flags\":\"L,M\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_rex_with_flags.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\t#start test for Custom Field\n\tdef test_Custom_Field_fixture_simple(self):\n\t\tfixture = \"Custom Field\"\n\t\tpath = frappe.scrub(fixture) + \"_original_style.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Field_fixture_simple_name_equal_default(self):\n\t\tfixture = [\"Custom Field\", {\"name\":[\"Item-vat\"]}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_simple_name_equal_default.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Field_fixture_simple_name_equal(self):\n\t\tfixture = [\"Custom Field\", {\"name\":[\"Item-vat\"],\"op\":\"=\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_simple_name_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Field_fixture_simple_name_not_equal(self):\n\t\tfixture = [\"Custom Field\", {\"name\":[\"Item-vat\"],\"op\":\"!=\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_simple_name_not_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\t#without [] around the name...\n\tdef test_Custom_Field_fixture_simple_name_at_least_equal(self):\n\t\tfixture = [\"Custom Field\", {\"name\":\"Item-va\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_simple_name_at_least_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Field_fixture_multi_name_equal(self):\n\t\tfixture = [\"Custom Field\", {\"name\":[\"Item-vat\", \"Bin-vat\"],\"op\":\"=\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_multi_name_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Field_fixture_multi_name_not_equal(self):\n\t\tfixture = [\"Custom Field\", {\"name\":[\"Item-vat\", \"Bin-vat\"],\"op\":\"!=\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_multi_name_not_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Field_fixture_empty_object(self):\n\t\tfixture = [\"Custom Field\", {}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_empty_object_should_be_all.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Field_fixture_just_list(self):\n\t\tfixture = [\"Custom Field\"]\n\t\tpath = frappe.scrub(fixture[0]) + \"_just_list_should_be_all.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\t# Custom Field regular expression\n\tdef test_Custom_Field_fixture_rex_no_flags(self):\n\t\tfixture = [\"Custom Field\", {\"name\":r\"^[r|L]\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_rex_no_flags.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Custom_Field_fixture_rex_with_flags(self):\n\t\tfixture = [\"Custom Field\", {\"name\":r\"^[i|A]\", \"flags\":\"L,M\"}]\n\t\tpath = frappe.scrub(fixture[0]) + \"_rex_with_flags.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\n\t#start test for Doctype\n\tdef test_Doctype_fixture_simple(self):\n\t\tfixture = \"ToDo\"\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture) + \"_original_style_should_be_all.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Doctype_fixture_simple_name_equal_default(self):\n\t\tfixture = [\"ToDo\", {\"name\":[\"TDI00000008\"]}]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_simple_name_equal_default.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Doctype_fixture_simple_name_equal(self):\n\t\tfixture = [\"ToDo\", {\"name\":[\"TDI00000002\"],\"op\":\"=\"}]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_simple_name_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Doctype_simple_name_not_equal(self):\n\t\tfixture = [\"ToDo\", {\"name\":[\"TDI00000002\"],\"op\":\"!=\"}]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_simple_name_not_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\t#without [] around the name...\n\tdef test_Doctype_fixture_simple_name_at_least_equal(self):\n\t\tfixture = [\"ToDo\", {\"name\":\"TDI\"}]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_simple_name_at_least_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Doctype_multi_name_equal(self):\n\t\tfixture = [\"ToDo\", {\"name\":[\"TDI00000002\", \"TDI00000008\"],\"op\":\"=\"}]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_multi_name_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Doctype_multi_name_not_equal(self):\n\t\tfixture = [\"ToDo\", {\"name\":[\"TDI00000002\", \"TDI00000008\"],\"op\":\"!=\"}]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_multi_name_not_equal.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Doctype_fixture_empty_object(self):\n\t\tfixture = [\"ToDo\", {}]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_empty_object_should_be_all.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Doctype_fixture_just_list(self):\n\t\tfixture = [\"ToDo\"]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_just_list_should_be_all.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\t# Doctype regular expression\n\tdef test_Doctype_fixture_rex_no_flags(self):\n\t\tfixture = [\"ToDo\", {\"name\":r\"^TDi\"}]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_rex_no_flags_should_be_all.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n\tdef test_Doctype_fixture_rex_with_flags(self):\n\t\tfixture = [\"ToDo\", {\"name\":r\"^TDi\", \"flags\":\"L,M\"}]\n\t\tpath = \"Doctype_\" + frappe.scrub(fixture[0]) + \"_rex_with_flags_should_be_none.csv\"\n\n\t\texport_csv(fixture, path)\n\t\tself.assertTrue(True)\n\t\tos.remove(path)\n\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "import frappe", "import frappe.defaults", "from frappe.core.doctype.data_import.data_import import export_csv", "import unittest", "import os"]}], [{"term": "class", "name": "classBuffer:", "data": "class Buffer:\n\t\"\"\"A Buffer provides a way of accessing a sequence one at a time.\n\tIts constructor takes a sequence, called the \"source\".\n\tThe Buffer supplies elements from source one at a time through its remove_front()\n\tmethod. In addition, Buffer provides a current() method to look at the\n\tnext item to be supplied, without moving past it.\n\t>>> buf = Buffer(['(', '+', 15, 12, ')'])\n\t>>> buf.remove_front()\n\t'('\n\t>>> buf.remove_front()\n\t'+'\n\t>>> buf.current()\n\t15\n\t>>> buf.remove_front()\n\t15\n\t>>> buf.current()\n\t12\n\t>>> buf.remove_front()\n\t12\n\t>>> buf.remove_front()\n\t')'\n\t>>> buf.remove_front()  # returns None\n\t\"\"\"\n\tdef __init__(self, source):\n\t\tself.index = 0\n\t\tself.source = source\n\n\tdef remove_front(self):\n\t\t\"\"\"Remove the next item from self and return it. If self has\n\t\texhausted its source, returns None.\"\"\"\n\t\tcurrent = self.current()\n\t\tself.index += 1\n\t\treturn current\n\n\tdef current(self):\n\t\t\"\"\"Return the current element, or None if none exists.\"\"\"\n\t\tif self.index >= len(self.source):\n\t\t\treturn None\n\t\telse:\n\t\t\treturn self.source[self.index]\n\n\tdef expect(self, expected):\n\t\tactual = self.remove_front()\n\t\tif expected != actual:\n\t\t\traise SyntaxError(\"expected '{}' but got '{}'\".format(expected, actual))\n\t\telse:\n\t\t\treturn actual\n\n\tdef __str__(self):\n", "description": "A Buffer provides a way of accessing a sequence one at a time.\n\tIts constructor takes a sequence, called the \"source\".\n\tThe Buffer supplies elements from source one at a time through its remove_front()\n\tmethod. In addition, Buffer provides a current() method to look at the\n\tnext item to be supplied, without moving past it.\n\t>>> buf = Buffer(['(', '+', 15, 12, ')'])\n\t>>> buf.remove_front()\n\t'('\n\t>>> buf.remove_front()\n\t'+'\n\t>>> buf.current()\n\t15\n\t>>> buf.remove_front()\n\t15\n\t>>> buf.current()\n\t12\n\t>>> buf.remove_front()\n\t12\n\t>>> buf.remove_front()\n\t')'\n\t>>> buf.remove_front()  # returns None\n\t", "category": "remove", "imports": []}], [], [], [], [], [], [], [{"term": "def", "name": "apply_weight_norm", "data": "def apply_weight_norm(module, name='', dim=0, hook_child=True):\r\n\tr\"\"\"\r\n\tApplies weight normalization to a parameter in the given module.\r\n\tIf no parameter is provided, applies weight normalization to all\r\n\tparameters in model (except 1-d vectors and scalars).\r\n\r\n\t.. math::\r\n\t\t \\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\r\n\r\n\tWeight normalization is a reparameterization that decouples the magnitude\r\n\tof a weight tensor from its direction. This replaces the parameter specified\r\n\tby `name` (e.g. \"weight\") with two parameters: one specifying the magnitude\r\n\t(e.g. \"weight_g\") and one specifying the direction (e.g. \"weight_v\").\r\n\tWeight normalization is implemented via a hook that recomputes the weight\r\n\ttensor from the magnitude and direction before every :meth:`~Module.forward`\r\n\tcall.\r\n\r\n\tBy default, with `dim=0`, the norm is computed independently per output\r\n\tchannel/plane. To compute a norm over the entire weight tensor, use\r\n\t`dim=None`.\r\n\r\n\tSee https://arxiv.org/abs/1602.07868\r\n\r\n\tArgs:\r\n\t\tmodule (nn.Module): containing module\r\n\t\tname (str, optional): name of weight parameter\r\n\t\tdim (int, optional): dimension over which to compute the norm\r\n\t\thook_child (boolean, optional): adds reparameterization hook to direct parent of the \r\n\t\t\tparameters. If False, it's added to `module` instead. Default: True\r\n\r\n\tReturns:\r\n\t\tThe original module with the weight norm hook\r\n\r\n\tExample::\r\n\r\n\t\t>>> m = apply_weight_norm(nn.Linear(20, 40), name='weight')\r\n\t\tLinear (20 -> 40)\r\n\t\t>>> m.weight_g.size()\r\n\t\ttorch.Size([40, 1])\r\n\t\t>>> m.weight_v.size()\r\n\t\ttorch.Size([40, 20])\r\n\r\n\t\"\"\"\r\n\treturn apply_reparameterization(module, reparameterization=WeightNorm, hook_child=hook_child,\r\n\t\t\t\t\t\t\t\t\tname=name, dim=dim)\r\n", "description": "\r\n\tApplies weight normalization to a parameter in the given module.\r\n\tIf no parameter is provided, applies weight normalization to all\r\n\tparameters in model (except 1-d vectors and scalars).\r\n\r\n\t.. math::\r\n\t\t \\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\r\n\r\n\tWeight normalization is a reparameterization that decouples the magnitude\r\n\tof a weight tensor from its direction. This replaces the parameter specified\r\n\tby `name` (e.g. \"weight\") with two parameters: one specifying the magnitude\r\n\t(e.g. \"weight_g\") and one specifying the direction (e.g. \"weight_v\").\r\n\tWeight normalization is implemented via a hook that recomputes the weight\r\n\ttensor from the magnitude and direction before every :meth:`~Module.forward`\r\n\tcall.\r\n\r\n\tBy default, with `dim=0`, the norm is computed independently per output\r\n\tchannel/plane. To compute a norm over the entire weight tensor, use\r\n\t`dim=None`.\r\n\r\n\tSee https://arxiv.org/abs/1602.07868\r\n\r\n\tArgs:\r\n\t\tmodule (nn.Module): containing module\r\n\t\tname (str, optional): name of weight parameter\r\n\t\tdim (int, optional): dimension over which to compute the norm\r\n\t\thook_child (boolean, optional): adds reparameterization hook to direct parent of the \r\n\t\t\tparameters. If False, it's added to `module` instead. Default: True\r\n\r\n\tReturns:\r\n\t\tThe original module with the weight norm hook\r\n\r\n\tExample::\r\n\r\n\t\t>>> m = apply_weight_norm(nn.Linear(20, 40), name='weight')\r\n\t\tLinear (20 -> 40)\r\n\t\t>>> m.weight_g.size()\r\n\t\ttorch.Size([40, 1])\r\n\t\t>>> m.weight_v.size()\r\n\t\ttorch.Size([40, 20])\r\n\r\n\t", "category": "remove", "imports": ["from .weight_norm import WeightNorm\r", "from .reparameterization import Reparameterization\r"]}, {"term": "def", "name": "remove_weight_norm", "data": "def remove_weight_norm(module, name='', remove_all=False):\r\n\t\"\"\"\r\n\tRemoves the weight normalization reparameterization of a parameter from a module.\r\n\tIf no parameter is supplied then all weight norm parameterizations are removed.\r\n\tArgs:\r\n\t\tmodule (nn.Module): containing module\r\n\t\tname (str, optional): name of weight parameter\r\n\tExample:\r\n\t\t>>> m = apply_weight_norm(nn.Linear(20, 40))\r\n\t\t>>> remove_weight_norm(m)\r\n\t\"\"\"\r\n\treturn remove_reparameterization(module, reparameterization=WeightNorm,\r\n\t\t\t\t\t\t\t\t\tname=name, remove_all=remove_all)\r\n", "description": "\r\n\tRemoves the weight normalization reparameterization of a parameter from a module.\r\n\tIf no parameter is supplied then all weight norm parameterizations are removed.\r\n\tArgs:\r\n\t\tmodule (nn.Module): containing module\r\n\t\tname (str, optional): name of weight parameter\r\n\tExample:\r\n\t\t>>> m = apply_weight_norm(nn.Linear(20, 40))\r\n\t\t>>> remove_weight_norm(m)\r\n\t", "category": "remove", "imports": ["from .weight_norm import WeightNorm\r", "from .reparameterization import Reparameterization\r"]}, {"term": "def", "name": "apply_reparameterization", "data": "def apply_reparameterization(module, reparameterization=None, name='', dim=0, hook_child=True):\r\n\t\"\"\"\r\n\tApplies a given weight reparameterization (such as weight normalization) to\r\n\ta parameter in the given module. If no parameter is given, applies the reparameterization\r\n\tto all parameters in model (except 1-d vectors and scalars).\r\n\r\n\tArgs:\r\n\t\tmodule (nn.Module): containing module\r\n\t\treparameterization (Reparameterization): reparamaterization class to apply\r\n\t\tname (str, optional): name of weight parameter\r\n\t\tdim (int, optional): dimension over which to perform reparameterization op\r\n\t\thook_child (boolean, optional): adds reparameterization hook to direct parent of the \r\n\t\t\tparameters. If False, it's added to `module` instead. Default: True\r\n\r\n\tReturns:\r\n\t\tThe original module with the reparameterization hook\r\n\r\n\tExample::\r\n\r\n\t\t>>> m = apply_reparameterization(nn.Linear(20, 40), WeightNorm)\r\n\t\tLinear (20 -> 40)\r\n\r\n\t\"\"\"\r\n\tassert reparameterization is not None\r\n\tif name != '':\r\n\t\tReparameterization.apply(module, name, dim, reparameterization, hook_child)\r\n\telse:\r\n\t\tnames = list(module.state_dict().keys())\r\n\t\tfor name in names:\r\n\t\t\tapply_reparameterization(module, reparameterization, name, dim, hook_child)\r\n\treturn module\r\n", "description": "\r\n\tApplies a given weight reparameterization (such as weight normalization) to\r\n\ta parameter in the given module. If no parameter is given, applies the reparameterization\r\n\tto all parameters in model (except 1-d vectors and scalars).\r\n\r\n\tArgs:\r\n\t\tmodule (nn.Module): containing module\r\n\t\treparameterization (Reparameterization): reparamaterization class to apply\r\n\t\tname (str, optional): name of weight parameter\r\n\t\tdim (int, optional): dimension over which to perform reparameterization op\r\n\t\thook_child (boolean, optional): adds reparameterization hook to direct parent of the \r\n\t\t\tparameters. If False, it's added to `module` instead. Default: True\r\n\r\n\tReturns:\r\n\t\tThe original module with the reparameterization hook\r\n\r\n\tExample::\r\n\r\n\t\t>>> m = apply_reparameterization(nn.Linear(20, 40), WeightNorm)\r\n\t\tLinear (20 -> 40)\r\n\r\n\t", "category": "remove", "imports": ["from .weight_norm import WeightNorm\r", "from .reparameterization import Reparameterization\r"]}, {"term": "def", "name": "remove_reparameterization", "data": "def remove_reparameterization(module, reparameterization=Reparameterization,\r\n\t\t\t\t\t\t\t\tname='', remove_all=False):\r\n\t\"\"\"\r\n\tRemoves the given reparameterization of a parameter from a module.\r\n\tIf no parameter is supplied then all reparameterizations are removed.\r\n\tArgs:\r\n\t\tmodule (nn.Module): containing module\r\n\t\treparameterization (Reparameterization): reparamaterization class to apply\r\n\t\tname (str, optional): name of weight parameter\r\n\t\tremove_all (bool, optional): if True, remove all reparamaterizations of given type. Default: False\r\n\tExample:\r\n\t\t>>> m = apply_reparameterization(nn.Linear(20, 40),WeightNorm)\r\n\t\t>>> remove_reparameterization(m)\r\n\t\"\"\"\r\n\tif name != '' or remove_all:\r\n\t\tto_remove = []\r\n\t\tfor k, hook in module._forward_pre_hooks.items():\r\n\t\t\tif isinstance(hook, reparameterization) and (hook.name == name or remove_all):\r\n\t\t\t\thook.remove(module)\r\n\t\t\t\tto_remove.append(k)\r\n\t\tif len(to_remove) > 0:\r\n\t\t\tfor k in to_remove:\r\n\t\t\t\tdel module._forward_pre_hooks[k]\r\n\t\t\treturn module\r\n\t\tif not remove_all:\r\n\t\t\traise ValueError(\"reparameterization of '{}' not found in {}\"\r\n\t\t\t\t\t\t\t .format(name, module))\r\n\telse:\r\n\t\tmodules = [module]+[x for x in module.modules()]\r\n\t\tfor m in modules:\r\n\t\t\tremove_reparameterization(m, reparameterization=reparameterization, remove_all=True)\r\n\t\treturn module\r\n", "description": "\r\n\tRemoves the given reparameterization of a parameter from a module.\r\n\tIf no parameter is supplied then all reparameterizations are removed.\r\n\tArgs:\r\n\t\tmodule (nn.Module): containing module\r\n\t\treparameterization (Reparameterization): reparamaterization class to apply\r\n\t\tname (str, optional): name of weight parameter\r\n\t\tremove_all (bool, optional): if True, remove all reparamaterizations of given type. Default: False\r\n\tExample:\r\n\t\t>>> m = apply_reparameterization(nn.Linear(20, 40),WeightNorm)\r\n\t\t>>> remove_reparameterization(m)\r\n\t", "category": "remove", "imports": ["from .weight_norm import WeightNorm\r", "from .reparameterization import Reparameterization\r"]}], [], [{"term": "def", "name": "querystring_modify", "data": "def querystring_modify(\n\tcontext: Dict[str, Any],\n\tbase: Optional[str] = None,\n\tremove_blanks: bool = False,\n\tremove_utm: bool = True,\n", "description": null, "category": "remove", "imports": ["from typing import Any, Dict, Optional, Set, Union", "from django import template", "from django.db.models import Model", "from django.http.request import QueryDict"]}, {"term": "def", "name": "get_base_querydict", "data": "def get_base_querydict(\n", "description": null, "category": "remove", "imports": ["from typing import Any, Dict, Optional, Set, Union", "from django import template", "from django.db.models import Model", "from django.http.request import QueryDict"]}, {"term": "def", "name": "clean_querydict", "data": "def clean_querydict(\n", "description": null, "category": "remove", "imports": ["from typing import Any, Dict, Optional, Set, Union", "from django import template", "from django.db.models import Model", "from django.http.request import QueryDict"]}], [], [], [], [], [], [], [], [], [], [], [{"term": "class", "name": "DisplayManagersTest", "data": "class DisplayManagersTest(unittest.TestCase):\n\t'''Check that multiple dm configurations are handled'''\n\n\tdef setUp(self):\n\t\tsuper().setUp()\n\t\twith suppress(FileNotFoundError):\n\t\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\twith suppress(FileNotFoundError):\n\t\t\tos.remove(DDM_CONFIG_PATH)\n\t\tsubprocess.check_call('apt-get install -y --reinstall lightdm 2>&1', shell=True)\n\t\t# Remove all Conditional ExecStartPre= as we want to check systemd logic, not unit\n\t\tfor line in fileinput.input([LIGHTDM_SYSTEMD_UNIT_PATH], inplace=True):\n\t\t\tif not line.startswith('ExecStartPre='):\n\t\t\t\tprint(line)\n\t\tself.files_to_clean = []\n\n\tdef tearDown(self):\n\t\tfor f in self.files_to_clean:\n\t\t\tos.remove(f)\n\t\tsuper().tearDown()\n\n\tdef test_one_systemd(self):\n\t\t'''one systemd dm is started'''\n\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('lightdm'))\n\n\tdef test_multiple_systemd(self):\n\t\t'''only default systemd dm is started'''\n\n\t\tself.create_systemd_dm_unit(\"systemddm\")\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('lightdm'))\n\t\tself.assertFalse(self.is_active_unit('systemddm'))\n\n\tdef test_multiple_systemd_ddmconfig_match(self):\n\t\t'''display-manager symlink respect ddm config and starts right unit'''\n\n\t\t# lightdm was the default\n\t\tself.create_systemd_dm_unit(\"systemddm\", make_ddm_default=\"systemddm\")\n\t\tself.reload_state()\n\n\t\tself.assertFalse(self.is_active_unit('lightdm'))\n\t\tself.assertTrue(self.is_active_unit('systemddm'))\n\n\tdef test_multiple_systemd_ddmconfig_match_no_symlink(self):\n\t\t'''create a display-manager symlink to matching systemd unit ddm config'''\n\n\t\t# lightdm was the default\n\t\tself.create_systemd_dm_unit(\"systemddm\", make_ddm_default=\"systemddm\")\n\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\tself.reload_state()\n\n\t\tself.assertFalse(self.is_active_unit('lightdm'))\n\t\tself.assertTrue(self.is_active_unit('systemddm'))\n\n\tdef test_one_systemd_no_ddmconfig(self):\n\t\t'''without any ddm config, the default systemd unit via symlink is still the default'''\n\n\t\tos.remove(DDM_CONFIG_PATH)\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('lightdm'))\n\n\tdef test_one_systemd_masked_symlink_with_ddmconfig(self):\n\t\t'''a masked symlink will be updated to match systemd ddmconfig unit'''\n\n\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\tos.symlink(\"/dev/null\", SYSTEMD_DEFAULT_DM_PATH)\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('lightdm'))\n\n\tdef test_one_systemd_masked_symlink_no_ddmconfig(self):\n\t\t'''without any ddm config, a masked symlinked will stayed masked'''\n\n\t\tos.remove(DDM_CONFIG_PATH)\n\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\tos.symlink(\"/dev/null\", SYSTEMD_DEFAULT_DM_PATH)\n\t\tself.reload_state()\n\n\t\tself.assertFalse(self.is_active_unit('lightdm'))\n\n\tdef test_multiple_systemd_wrong_ddmconfig(self):\n\t\t'''ddm config matches no systemd unit, don't start any of them'''\n\n\t\tself.create_systemd_dm_unit(\"systemddm\", make_ddm_default=\"systemddm_doesnt_match\")\n\t\tself.reload_state()\n\n\t\tself.assertFalse(self.is_active_unit('lightdm'))\n\t\tself.assertFalse(self.is_active_unit('systemddm'))\n\n\tdef test_one_init(self):\n\t\t'''one init dm is started'''\n\n\t\t# fake removing lightdm (or we shoud remove all processes under lightdm)\n\t\tos.remove(DDM_CONFIG_PATH)\n\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\tos.remove(LIGHTDM_SYSTEMD_UNIT_PATH)\n\t\tself.create_init_dm(\"initdm\", make_ddm_default=\"initdm\")\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('initdm'))\n\n\tdef test_multiple_init(self):\n\t\t'''all init dms are enabled, regardless of ddm'''\n\n\t\t# this enable to keep previous init behavior,\n\t\t# especially when they don't support ddm config like nodm\n\t\tos.remove(DDM_CONFIG_PATH)\n\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\tos.remove(LIGHTDM_SYSTEMD_UNIT_PATH)\n\t\tself.create_init_dm(\"initdm\", make_ddm_default=\"initdm\")\n\t\tself.create_init_dm(\"otherinitdm\")\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('initdm'))\n\t\tself.assertTrue(self.is_active_unit('otherinitdm'))\n\n\tdef test_multiple_init_no_ddm(self):\n\t\t'''all init dms are enabled, without any ddm file'''\n\n\t\tos.remove(DDM_CONFIG_PATH)\n\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\tos.remove(LIGHTDM_SYSTEMD_UNIT_PATH)\n\t\tself.create_init_dm(\"initdm\")\n\t\tself.create_init_dm(\"otherinitdm\")\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('initdm'))\n\t\tself.assertTrue(self.is_active_unit('otherinitdm'))\n\n\tdef test_systemd_matches_ddm_and_init(self):\n\t\t'''default ddm config systemd is enabled as well as all inits'''\n\n\t\t# lightdm is the default\n\t\tself.create_init_dm(\"initdm\")\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('lightdm'))\n\t\tself.assertTrue(self.is_active_unit('initdm'))\n\n\tdef test_systemd_and_init_matches_ddm(self):\n\t\t'''default ddm init prevents systemd units to start'''\n\n\t\tself.create_init_dm(\"initdm\", make_ddm_default=\"initdm\")\n\t\tself.reload_state()\n\n\t\tself.assertFalse(self.is_active_unit('lightdm'))\n\t\tself.assertTrue(self.is_active_unit('initdm'))\n\n\tdef test_no_ddmconfig_multiple_systemd_and_init(self):\n\t\t'''no ddm config let default systemd and all init dms enabled'''\n\n\t\tos.remove(DDM_CONFIG_PATH)\n\t\tself.create_systemd_dm_unit(\"systemddm\")\n\t\tself.create_init_dm(\"initdm\")\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('lightdm'))\n\t\tself.assertTrue(self.is_active_unit('initdm'))\n\t\tself.assertFalse(self.is_active_unit('systemddm'))\n\n\tdef test_no_ddmconfig_no_default_systemd_and_init(self):\n\t\t'''no ddm config default systemd unit will only have init dms enabled'''\n\n\t\tos.remove(DDM_CONFIG_PATH)\n\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\tself.create_systemd_dm_unit(\"systemddm\")\n\t\tself.create_init_dm(\"initdm\")\n\t\tself.reload_state()\n\n\t\tself.assertFalse(self.is_active_unit('lightdm'))\n\t\tself.assertTrue(self.is_active_unit('initdm'))\n\t\tself.assertFalse(self.is_active_unit('systemddm'))\n\n\t# NOTE: I think init shouldn't start in that case\n\tdef test_one_systemd_one_init_masked_symlink_with_ddmconfig(self):\n\t\t'''a masked symlink will be updated to match systemd ddmconfig systemd unit and init is started'''\n\n\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\tos.symlink(\"/dev/null\", SYSTEMD_DEFAULT_DM_PATH)\n\t\tself.create_init_dm(\"initdm\")\n\t\tself.reload_state()\n\n\t\tself.assertTrue(self.is_active_unit('lightdm'))\n\t\tself.assertTrue(self.is_active_unit('initdm'))\n\n\t# NOTE: I think init shouldn't start in that case\n\tdef test_one_systemd_one_init_masked_symlink_no_ddmconfig(self):\n\t\t'''without any ddm config, a masked symlinked will stayed masked, but init will be started'''\n\n\t\tos.remove(DDM_CONFIG_PATH)\n\t\tos.remove(SYSTEMD_DEFAULT_DM_PATH)\n\t\tos.symlink(\"/dev/null\", SYSTEMD_DEFAULT_DM_PATH)\n\t\tself.create_init_dm(\"initdm\")\n\t\tself.reload_state()\n\n\t\tself.assertFalse(self.is_active_unit('lightdm'))\n\t\tself.assertTrue(self.is_active_unit('initdm'))\n\n\t# Helper methods\n\n\tdef create_systemd_dm_unit(self, name, make_ddm_default=None):\n\t\tdest_unit = \"{}.service\".format(os.path.join(SYSTEMD_SYSTEM_UNIT_DIR, name))\n\t\tshutil.copy(LIGHTDM_SYSTEMD_UNIT_PATH, dest_unit)\n\t\t# remove BusName to avoid conflicts\n\t\tfor line in fileinput.input([LIGHTDM_SYSTEMD_UNIT_PATH], inplace=True):\n\t\t\tif not line.startswith('BusName='):\n\t\t\t\tprint(line)\n\t\tself.files_to_clean.append(dest_unit)\n\n\t\tif make_ddm_default:\n\t\t\tself.make_ddm_default(make_ddm_default)\n\n\tdef create_init_dm(self, name, make_ddm_default=None):\n\t\tinit_script = \"/etc/init.d/{}\".format(name)\n\t\twith open(init_script, 'w') as f:\n", "description": null, "category": "remove", "imports": ["from contextlib import suppress", "import fileinput", "import os", "import subprocess", "import shutil", "import sys", "import unittest", "from time import sleep"]}, {"term": "def", "name": "fmake_ddm_default", "data": "\tdef make_ddm_default(self, binary_name):\n\t\twith open(DDM_CONFIG_PATH, 'w') as f:\n\t\t\tf.write(\"/usr/bin/{}\".format(binary_name))\n", "description": null, "category": "remove", "imports": ["from contextlib import suppress", "import fileinput", "import os", "import subprocess", "import shutil", "import sys", "import unittest", "from time import sleep"]}, {"term": "def", "name": "fis_active_unit", "data": "\tdef is_active_unit(self, unit):\n\t\t'''Check that given unit is active'''\n\n\t\tif subprocess.call(['systemctl', '-q', 'is-active', unit]) != 0:\n\t\t\treturn False\n\t\treturn True\n", "description": null, "category": "remove", "imports": ["from contextlib import suppress", "import fileinput", "import os", "import subprocess", "import shutil", "import sys", "import unittest", "from time import sleep"]}, {"term": "def", "name": "freload_state", "data": "\tdef reload_state(self):\n\t\tsubprocess.check_call(['systemctl', 'daemon-reload'])\n\t\tsubprocess.check_call(['systemctl', 'default'])\n\t\tsleep(2)  # a more robust way would be to loop over remaining jobs to process\n\n", "description": null, "category": "remove", "imports": ["from contextlib import suppress", "import fileinput", "import os", "import subprocess", "import shutil", "import sys", "import unittest", "from time import sleep"]}, {"term": "def", "name": "boot_with_systemd", "data": "def boot_with_systemd():\n\t'''Reboot with systemd as init\n\n\tIn case something else is currently running in the testbed\n\t'''\n\tif subprocess.call(['systemctl', 'status'], stdout=subprocess.PIPE,\n\t\t\t\t\t   stderr=subprocess.PIPE) != 0:\n\t\tprint('Installing systemd-sysv and rebooting...')\n\t\tsubprocess.check_call('apt-get -y install systemd-sysv 2>&1',\n\t\t\t\t\t\t\t  shell=True)\n\t\tsubprocess.check_call(['autopkgtest-reboot', 'boot-systemd'])\n\n", "description": null, "category": "remove", "imports": ["from contextlib import suppress", "import fileinput", "import os", "import subprocess", "import shutil", "import sys", "import unittest", "from time import sleep"]}], [], [], [{"term": "def", "name": "test_ap_change_ssid", "data": "def test_ap_change_ssid(dev, apdev):\n\t\"\"\"Dynamic SSID change with hostapd and WPA2-PSK\"\"\"\n\tparams = hostapd.wpa2_params(ssid=\"test-wpa2-psk-start\",\n\t\t\t\t\t\t\t\t passphrase=\"12345678\")\n\thapd = hostapd.add_ap(apdev[0], params)\n\tid = dev[0].connect(\"test-wpa2-psk-start\", psk=\"12345678\",\n\t\t\t\t\t\tscan_freq=\"2412\")\n\tdev[0].request(\"DISCONNECT\")\n\n\tlogger.info(\"Change SSID dynamically\")\n\tres = hapd.request(\"SET ssid test-wpa2-psk-new\")\n\tif \"OK\" not in res:\n\t\traise Exception(\"SET command failed\")\n\tres = hapd.request(\"RELOAD\")\n\tif \"OK\" not in res:\n\t\traise Exception(\"RELOAD command failed\")\n\n\tdev[0].set_network_quoted(id, \"ssid\", \"test-wpa2-psk-new\")\n\tdev[0].connect_network(id)\n", "description": "Dynamic SSID change with hostapd and WPA2-PSK", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "multi_check", "data": "def multi_check(dev, check, scan_opt=True):\n\tid = []\n\tnum_bss = len(check)\n\tfor i in range(0, num_bss):\n\t\tdev[i].request(\"BSS_FLUSH 0\")\n\t\tdev[i].dump_monitor()\n\tfor i in range(0, num_bss):\n\t\tif check[i]:\n\t\t\tcontinue\n\t\tid.append(dev[i].connect(\"bss-\" + str(i + 1), key_mgmt=\"NONE\",\n\t\t\t\t\t\t\t\t scan_freq=\"2412\", wait_connect=False))\n\tfor i in range(num_bss):\n\t\tif not check[i]:\n\t\t\tcontinue\n\t\tbssid = '02:00:00:00:03:0' + str(i)\n\t\tif scan_opt:\n\t\t\tdev[i].scan_for_bss(bssid, freq=2412)\n\t\tid.append(dev[i].connect(\"bss-\" + str(i + 1), key_mgmt=\"NONE\",\n\t\t\t\t\t\t\t\t scan_freq=\"2412\", wait_connect=True))\n\tfirst = True\n\tfor i in range(num_bss):\n\t\tif not check[i]:\n\t\t\ttimeout = 0.2 if first else 0.01\n\t\t\tfirst = False\n\t\t\tev = dev[i].wait_event([\"CTRL-EVENT-CONNECTED\"], timeout=timeout)\n\t\t\tif ev:\n\t\t\t\traise Exception(\"Unexpected connection\")\n\n\tfor i in range(0, num_bss):\n\t\tdev[i].remove_network(id[i])\n\tfor i in range(num_bss):\n\t\tif check[i]:\n\t\t\tdev[i].wait_disconnected(timeout=5)\n\n\tres = ''\n\tfor i in range(0, num_bss):\n\t\tres = res + dev[i].request(\"BSS RANGE=ALL MASK=0x2\")\n\n\tfor i in range(0, num_bss):\n\t\tif not check[i]:\n\t\t\tbssid = '02:00:00:00:03:0' + str(i)\n\t\t\tif bssid in res:\n\t\t\t\traise Exception(\"Unexpected BSS\" + str(i) + \" in scan results\")\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_remove", "data": "def test_ap_bss_add_remove(dev, apdev):\n\t\"\"\"Dynamic BSS add/remove operations with hostapd\"\"\"\n\ttry:\n\t\t_test_ap_bss_add_remove(dev, apdev)\n\tfinally:\n\t\tfor i in range(3):\n\t\t\tdev[i].request(\"SCAN_INTERVAL 5\")\n", "description": "Dynamic BSS add/remove operations with hostapd", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "_test_ap_bss_add_remove", "data": "def _test_ap_bss_add_remove(dev, apdev):\n\tfor i in range(3):\n\t\tdev[i].flush_scan_cache()\n\t\tdev[i].request(\"SCAN_INTERVAL 1\")\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\tifname3 = apdev[0]['ifname'] + '-3'\n\tlogger.info(\"Set up three BSSes one by one\")\n\thostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\tmulti_check(dev, [True, False, False])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tmulti_check(dev, [True, True, False])\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(dev, [True, True, True])\n\n\tlogger.info(\"Remove the last BSS and re-add it\")\n\thostapd.remove_bss(apdev[0], ifname3)\n\tmulti_check(dev, [True, True, False])\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(dev, [True, True, True])\n\n\tlogger.info(\"Remove the middle BSS and re-add it\")\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(dev, [True, False, True])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tmulti_check(dev, [True, True, True])\n\n\tlogger.info(\"Remove the first BSS and re-add it and other BSSs\")\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(dev, [False, False, False])\n\thostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(dev, [True, True, True])\n\n\tlogger.info(\"Remove two BSSes and re-add them\")\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(dev, [True, False, True])\n\thostapd.remove_bss(apdev[0], ifname3)\n\tmulti_check(dev, [True, False, False])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tmulti_check(dev, [True, True, False])\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(dev, [True, True, True])\n\n\tlogger.info(\"Remove three BSSes in and re-add them\")\n\thostapd.remove_bss(apdev[0], ifname3)\n\tmulti_check(dev, [True, True, False])\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(dev, [True, False, False])\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(dev, [False, False, False])\n\thostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\tmulti_check(dev, [True, False, False])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tmulti_check(dev, [True, True, False])\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(dev, [True, True, True])\n\n\tlogger.info(\"Test error handling if a duplicate ifname is tried\")\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf', ignore_error=True)\n\tmulti_check(dev, [True, True, True])\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_remove_during_ht_scan", "data": "def test_ap_bss_add_remove_during_ht_scan(dev, apdev):\n\t\"\"\"Dynamic BSS add during HT40 co-ex scan\"\"\"\n\tfor i in range(3):\n\t\tdev[i].flush_scan_cache()\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\thostapd.add_bss(apdev[0], ifname1, 'bss-ht40-1.conf')\n\thostapd.add_bss(apdev[0], ifname2, 'bss-ht40-2.conf')\n\tmulti_check(dev, [True, True], scan_opt=False)\n\thostapd.remove_bss(apdev[0], ifname2)\n\thostapd.remove_bss(apdev[0], ifname1)\n\n\thostapd.add_bss(apdev[0], ifname1, 'bss-ht40-1.conf')\n\thostapd.add_bss(apdev[0], ifname2, 'bss-ht40-2.conf')\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(dev, [True, False], scan_opt=False)\n\thostapd.remove_bss(apdev[0], ifname1)\n\n\thostapd.add_bss(apdev[0], ifname1, 'bss-ht40-1.conf')\n\thostapd.add_bss(apdev[0], ifname2, 'bss-ht40-2.conf')\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(dev, [False, False])\n", "description": "Dynamic BSS add during HT40 co-ex scan", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_multi_bss_config", "data": "def test_ap_multi_bss_config(dev, apdev):\n\t\"\"\"hostapd start with a multi-BSS configuration file\"\"\"\n\tfor i in range(3):\n\t\tdev[i].flush_scan_cache()\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\tifname3 = apdev[0]['ifname'] + '-3'\n\tlogger.info(\"Set up three BSSes with one configuration file\")\n\thapd = hostapd.add_iface(apdev[0], 'multi-bss.conf')\n\thapd.enable()\n\tmulti_check(dev, [True, True, True])\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(dev, [True, False, True])\n\thostapd.remove_bss(apdev[0], ifname3)\n\tmulti_check(dev, [True, False, False])\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(dev, [False, False, False])\n\n\thapd = hostapd.add_iface(apdev[0], 'multi-bss.conf')\n\thapd.enable()\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(dev, [False, False, False])\n", "description": "hostapd start with a multi-BSS configuration file", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "invalid_ap", "data": "def invalid_ap(ap):\n\tlogger.info(\"Trying to start AP \" + ap['ifname'] + \" with invalid configuration\")\n\thapd = hostapd.add_ap(ap, {}, no_enable=True)\n\thapd.set(\"ssid\", \"invalid-config\")\n\thapd.set(\"channel\", \"12345\")\n\ttry:\n\t\thapd.enable()\n\t\tstarted = True\n\texcept Exception as e:\n\t\tstarted = False\n\tif started:\n\t\traise Exception(\"ENABLE command succeeded unexpectedly\")\n\treturn hapd\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_invalid_config", "data": "def test_ap_invalid_config(dev, apdev):\n\t\"\"\"Try to start AP with invalid configuration and fix configuration\"\"\"\n\thapd = invalid_ap(apdev[0])\n\n\tlogger.info(\"Fix configuration and start AP again\")\n\thapd.set(\"channel\", \"1\")\n\thapd.enable()\n\tdev[0].connect(\"invalid-config\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n", "description": "Try to start AP with invalid configuration and fix configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_invalid_config2", "data": "def test_ap_invalid_config2(dev, apdev):\n\t\"\"\"Try to start AP with invalid configuration and remove interface\"\"\"\n\thapd = invalid_ap(apdev[0])\n\tlogger.info(\"Remove interface with failed configuration\")\n\thostapd.remove_bss(apdev[0])\n", "description": "Try to start AP with invalid configuration and remove interface", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_acs", "data": "def test_ap_remove_during_acs(dev, apdev):\n\t\"\"\"Remove interface during ACS\"\"\"\n\tforce_prev_ap_on_24g(apdev[0])\n\tparams = hostapd.wpa2_params(ssid=\"test-acs-remove\", passphrase=\"12345678\")\n\tparams['channel'] = '0'\n\thostapd.add_ap(apdev[0], params)\n\thostapd.remove_bss(apdev[0])\n", "description": "Remove interface during ACS", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_acs2", "data": "def test_ap_remove_during_acs2(dev, apdev):\n\t\"\"\"Remove BSS during ACS in multi-BSS configuration\"\"\"\n\tforce_prev_ap_on_24g(apdev[0])\n\tifname = apdev[0]['ifname']\n\tifname2 = ifname + \"-2\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"test-acs-remove\")\n\thapd.set(\"channel\", \"0\")\n\thapd.set(\"bss\", ifname2)\n\thapd.set(\"ssid\", \"test-acs-remove2\")\n\thapd.enable()\n\thostapd.remove_bss(apdev[0])\n", "description": "Remove BSS during ACS in multi-BSS configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_acs3", "data": "def test_ap_remove_during_acs3(dev, apdev):\n\t\"\"\"Remove second BSS during ACS in multi-BSS configuration\"\"\"\n\tforce_prev_ap_on_24g(apdev[0])\n\tifname = apdev[0]['ifname']\n\tifname2 = ifname + \"-2\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"test-acs-remove\")\n\thapd.set(\"channel\", \"0\")\n\thapd.set(\"bss\", ifname2)\n\thapd.set(\"ssid\", \"test-acs-remove2\")\n\thapd.enable()\n\thostapd.remove_bss(apdev[0], ifname2)\n", "description": "Remove second BSS during ACS in multi-BSS configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_ht_coex_scan", "data": "def test_ap_remove_during_ht_coex_scan(dev, apdev):\n\t\"\"\"Remove interface during HT co-ex scan\"\"\"\n\tparams = hostapd.wpa2_params(ssid=\"test-ht-remove\", passphrase=\"12345678\")\n\tparams['channel'] = '1'\n\tparams['ht_capab'] = \"[HT40+]\"\n\tifname = apdev[0]['ifname']\n\thostapd.add_ap(apdev[0], params)\n\thostapd.remove_bss(apdev[0])\n", "description": "Remove interface during HT co-ex scan", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_ht_coex_scan2", "data": "def test_ap_remove_during_ht_coex_scan2(dev, apdev):\n\t\"\"\"Remove BSS during HT co-ex scan in multi-BSS configuration\"\"\"\n\tifname = apdev[0]['ifname']\n\tifname2 = ifname + \"-2\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"test-ht-remove\")\n\thapd.set(\"channel\", \"1\")\n\thapd.set(\"ht_capab\", \"[HT40+]\")\n\thapd.set(\"bss\", ifname2)\n\thapd.set(\"ssid\", \"test-ht-remove2\")\n\thapd.enable()\n\thostapd.remove_bss(apdev[0])\n", "description": "Remove BSS during HT co-ex scan in multi-BSS configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_ht_coex_scan3", "data": "def test_ap_remove_during_ht_coex_scan3(dev, apdev):\n\t\"\"\"Remove second BSS during HT co-ex scan in multi-BSS configuration\"\"\"\n\tifname = apdev[0]['ifname']\n\tifname2 = ifname + \"-2\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"test-ht-remove\")\n\thapd.set(\"channel\", \"1\")\n\thapd.set(\"ht_capab\", \"[HT40+]\")\n\thapd.set(\"bss\", ifname2)\n\thapd.set(\"ssid\", \"test-ht-remove2\")\n\thapd.enable()\n\thostapd.remove_bss(apdev[0], ifname2)\n", "description": "Remove second BSS during HT co-ex scan in multi-BSS configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_enable_disable_reenable", "data": "def test_ap_enable_disable_reenable(dev, apdev):\n\t\"\"\"Enable, disable, re-enable AP\"\"\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"dynamic\")\n\thapd.enable()\n\tev = hapd.wait_event([\"AP-ENABLED\"], timeout=30)\n\tif ev is None:\n\t\traise Exception(\"AP startup timed out\")\n\tdev[0].connect(\"dynamic\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\thapd.disable()\n\tev = hapd.wait_event([\"AP-DISABLED\"], timeout=30)\n\tif ev is None:\n\t\traise Exception(\"AP disabling timed out\")\n\tdev[0].wait_disconnected(timeout=10)\n\thapd.enable()\n\tev = hapd.wait_event([\"AP-ENABLED\"], timeout=30)\n\tif ev is None:\n\t\traise Exception(\"AP startup timed out\")\n\tdev[1].connect(\"dynamic\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[0].wait_connected(timeout=10)\n", "description": "Enable, disable, re-enable AP", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_double_disable", "data": "def test_ap_double_disable(dev, apdev):\n\t\"\"\"Double DISABLE regression test\"\"\"\n\thapd = hostapd.add_bss(apdev[0], apdev[0]['ifname'], 'bss-1.conf')\n\thostapd.add_bss(apdev[0], apdev[0]['ifname'] + '-2', 'bss-2.conf')\n\thapd.disable()\n\tif \"FAIL\" not in hapd.request(\"DISABLE\"):\n\t\traise Exception(\"Second DISABLE accepted unexpectedly\")\n\thapd.enable()\n\thapd.disable()\n\tif \"FAIL\" not in hapd.request(\"DISABLE\"):\n\t\traise Exception(\"Second DISABLE accepted unexpectedly\")\n", "description": "Double DISABLE regression test", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_many", "data": "def test_ap_bss_add_many(dev, apdev):\n\t\"\"\"Large number of BSS add operations with hostapd\"\"\"\n\ttry:\n\t\t_test_ap_bss_add_many(dev, apdev)\n\tfinally:\n\t\tdev[0].request(\"SCAN_INTERVAL 5\")\n\t\tifname = apdev[0]['ifname']\n\t\thapd = hostapd.HostapdGlobal(apdev[0])\n\t\thapd.flush()\n\t\tfor i in range(16):\n\t\t\tifname2 = ifname + '-' + str(i)\n\t\t\thapd.remove(ifname2)\n\t\ttry:\n\t\t\tos.remove('/tmp/hwsim-bss.conf')\n\t\texcept:\n\t\t\tpass\n", "description": "Large number of BSS add operations with hostapd", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "_test_ap_bss_add_many", "data": "def _test_ap_bss_add_many(dev, apdev):\n\tifname = apdev[0]['ifname']\n\thostapd.add_bss(apdev[0], ifname, 'bss-1.conf')\n\tfname = '/tmp/hwsim-bss.conf'\n\tfor i in range(16):\n\t\tifname2 = ifname + '-' + str(i)\n\t\twith open(fname, 'w') as f:\n\t\t\tf.write(\"driver=nl80211\\n\")\n\t\t\tf.write(\"hw_mode=g\\n\")\n\t\t\tf.write(\"channel=1\\n\")\n\t\t\tf.write(\"ieee80211n=1\\n\")\n\t\t\tf.write(\"interface=%s\\n\" % ifname2)\n\t\t\tf.write(\"bssid=02:00:00:00:03:%02x\\n\" % (i + 1))\n\t\t\tf.write(\"ctrl_interface=/var/run/hostapd\\n\")\n\t\t\tf.write(\"ssid=test-%d\\n\" % i)\n\t\thostapd.add_bss(apdev[0], ifname2, fname)\n\t\tos.remove(fname)\n\n\tdev[0].request(\"SCAN_INTERVAL 1\")\n\tdev[0].connect(\"bss-1\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[0].request(\"DISCONNECT\")\n\tdev[0].wait_disconnected(timeout=5)\n\tfor i in range(16):\n\t\tdev[0].connect(\"test-%d\" % i, key_mgmt=\"NONE\", scan_freq=\"2412\")\n\t\tdev[0].request(\"DISCONNECT\")\n\t\tdev[0].wait_disconnected(timeout=5)\n\t\tifname2 = ifname + '-' + str(i)\n\t\thostapd.remove_bss(apdev[0], ifname2)\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_reuse_existing", "data": "def test_ap_bss_add_reuse_existing(dev, apdev):\n\t\"\"\"Dynamic BSS add operation reusing existing interface\"\"\"\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\thostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\tsubprocess.check_call([\"iw\", \"dev\", ifname1, \"interface\", \"add\", ifname2,\n\t\t\t\t\t\t   \"type\", \"__ap\"])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\thostapd.remove_bss(apdev[0], ifname2)\n\tsubprocess.check_call([\"iw\", \"dev\", ifname2, \"del\"])\n", "description": "Dynamic BSS add operation reusing existing interface", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "hapd_bss_out_of_mem", "data": "def hapd_bss_out_of_mem(hapd, phy, confname, count, func):\n\twith alloc_fail(hapd, count, func):\n\t\thapd_global = hostapd.HostapdGlobal()\n\t\tres = hapd_global.ctrl.request(\"ADD bss_config=\" + phy + \":\" + confname)\n\t\tif \"OK\" in res:\n\t\t\traise Exception(\"add_bss succeeded\")\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_out_of_memory", "data": "def test_ap_bss_add_out_of_memory(dev, apdev):\n\t\"\"\"Running out of memory while adding a BSS\"\"\"\n\thapd2 = hostapd.add_ap(apdev[1], {\"ssid\": \"open\"})\n\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\n\thapd_bss_out_of_mem(hapd2, 'phy3', 'bss-1.conf', 1, 'hostapd_add_iface')\n\tfor i in range(1, 3):\n\t\thapd_bss_out_of_mem(hapd2, 'phy3', 'bss-1.conf',\n\t\t\t\t\t\t\ti, 'hostapd_interface_init_bss')\n\thapd_bss_out_of_mem(hapd2, 'phy3', 'bss-1.conf',\n\t\t\t\t\t\t1, 'ieee802_11_build_ap_params')\n\n\thostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\n\thapd_bss_out_of_mem(hapd2, 'phy3', 'bss-2.conf',\n\t\t\t\t\t\t1, 'hostapd_interface_init_bss')\n\thapd_bss_out_of_mem(hapd2, 'phy3', 'bss-2.conf',\n\t\t\t\t\t\t1, 'ieee802_11_build_ap_params')\n\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\thostapd.remove_bss(apdev[0], ifname2)\n\thostapd.remove_bss(apdev[0], ifname1)\n", "description": "Running out of memory while adding a BSS", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_multi_bss", "data": "def test_ap_multi_bss(dev, apdev):\n\t\"\"\"Multiple BSSes with hostapd\"\"\"\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\thapd1 = hostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\thapd2 = hostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tdev[0].connect(\"bss-1\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[1].connect(\"bss-2\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\n\thwsim_utils.test_connectivity(dev[0], hapd1)\n\thwsim_utils.test_connectivity(dev[1], hapd2)\n\n\tsta0 = hapd1.get_sta(dev[0].own_addr())\n\tsta1 = hapd2.get_sta(dev[1].own_addr())\n\tif 'rx_packets' not in sta0 or int(sta0['rx_packets']) < 1:\n\t\traise Exception(\"sta0 did not report receiving packets\")\n\tif 'rx_packets' not in sta1 or int(sta1['rx_packets']) < 1:\n\t\traise Exception(\"sta1 did not report receiving packets\")\n", "description": "Multiple BSSes with hostapd", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_add_with_driver", "data": "def test_ap_add_with_driver(dev, apdev):\n\t\"\"\"Add hostapd interface with driver specified\"\"\"\n\tifname = apdev[0]['ifname']\n\ttry:\n\t   hostname = apdev[0]['hostname']\n\texcept:\n\t   hostname = None\n\thapd_global = hostapd.HostapdGlobal(apdev[0])\n\thapd_global.add(ifname, driver=\"nl80211\")\n\tport = hapd_global.get_ctrl_iface_port(ifname)\n\thapd = hostapd.Hostapd(ifname, hostname, port)\n\thapd.set_defaults()\n\thapd.set(\"ssid\", \"dynamic\")\n\thapd.enable()\n\tev = hapd.wait_event([\"AP-ENABLED\"], timeout=30)\n\tif ev is None:\n\t\traise Exception(\"AP startup timed out\")\n\tdev[0].connect(\"dynamic\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[0].request(\"DISCONNECT\")\n\tdev[0].wait_disconnected()\n\thapd.disable()\n", "description": "Add hostapd interface with driver specified", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_iapp", "data": "def test_ap_iapp(dev, apdev):\n\t\"\"\"IAPP and multiple BSSes\"\"\"\n\trequire_under_vm()\n\ttry:\n\t\t_test_ap_iapp(dev, apdev)\n\tfinally:\n\t\tsubprocess.call(['ifconfig', 'br-multicast', 'down'],\n\t\t\t\t\t\tstderr=open('/dev/null', 'w'))\n\t\tsubprocess.call(['brctl', 'delbr', 'br-multicast'],\n\t\t\t\t\t\tstderr=open('/dev/null', 'w'))\n", "description": "IAPP and multiple BSSes", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "_test_ap_iapp", "data": "def _test_ap_iapp(dev, apdev):\n\tbr_ifname = 'br-multicast'\n\tsubprocess.call(['brctl', 'addbr', br_ifname])\n\tsubprocess.call(['brctl', 'setfd', br_ifname, '0'])\n\tsubprocess.call(['ip', 'addr', 'add', '10.174.65.206/31', 'dev', br_ifname])\n\tsubprocess.call(['ip', 'link', 'set', 'dev', br_ifname, 'up'])\n\tsubprocess.call(['ip', 'route', 'add', '224.0.0.0/4', 'dev', br_ifname])\n\n\tparams = {\"ssid\": \"test-1\",\n\t\t\t  \"bridge\": br_ifname,\n\t\t\t  \"iapp_interface\": br_ifname}\n\thapd = hostapd.add_ap(apdev[0], params)\n\n\tdev[0].scan_for_bss(apdev[0]['bssid'], freq=2412)\n\tdev[0].connect(\"test-1\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[1].connect(\"test-1\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\n\thapd2 = hostapd.add_ap(apdev[1], params)\n\tdev[0].scan_for_bss(apdev[1]['bssid'], freq=2412)\n\tdev[0].roam(apdev[1]['bssid'])\n\tdev[0].roam(apdev[0]['bssid'])\n\n\tdev[0].request(\"DISCONNECT\")\n\tdev[1].request(\"DISCONNECT\")\n\tdev[0].wait_disconnected()\n\tdev[1].wait_disconnected()\n\n\thapd.disable()\n\thapd2.disable()\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_duplicate_bssid", "data": "def test_ap_duplicate_bssid(dev, apdev):\n\t\"\"\"Duplicate BSSID\"\"\"\n\tparams = {\"ssid\": \"test\"}\n\thapd = hostapd.add_ap(apdev[0], params, no_enable=True)\n\thapd.enable()\n\tifname2 = apdev[0]['ifname'] + '-2'\n\tifname3 = apdev[0]['ifname'] + '-3'\n\t# \"BSS 'wlan3-2' may not have BSSID set to the MAC address of the radio\"\n\ttry:\n\t\thostapd.add_bss(apdev[0], ifname2, 'bss-2-dup.conf')\n\t\traise Exception(\"BSS add succeeded unexpectedly\")\n\texcept Exception as e:\n\t\tif \"Could not add hostapd BSS\" in str(e):\n\t\t\tpass\n\t\telse:\n\t\t\traise\n\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\n\tdev[0].connect(\"test\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[0].request(\"DISCONNECT\")\n\tdev[0].wait_disconnected()\n\n\thapd.set(\"bssid\", \"02:00:00:00:03:02\")\n\thapd.disable()\n\t# \"Duplicate BSSID 02:00:00:00:03:02 on interface 'wlan3-3' and 'wlan3'.\"\n\tif \"FAIL\" not in hapd.request(\"ENABLE\"):\n\t\traise Exception(\"ENABLE with duplicate BSSID succeeded unexpectedly\")\n", "description": "Duplicate BSSID", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_config_file", "data": "def test_ap_bss_config_file(dev, apdev, params):\n\t\"\"\"hostapd BSS config file\"\"\"\n\tpidfile = os.path.join(params['logdir'], \"ap_bss_config_file-hostapd.pid\")\n\tlogfile = os.path.join(params['logdir'], \"ap_bss_config_file-hostapd-log\")\n\tprg = os.path.join(params['logdir'], 'alt-hostapd/hostapd/hostapd')\n\tif not os.path.exists(prg):\n\t\tprg = '../../hostapd/hostapd'\n\tphy = get_phy(apdev[0])\n\tcmd = [prg, '-B', '-dddt', '-P', pidfile, '-f', logfile, '-S', '-T',\n\t\t   '-b', phy + ':bss-1.conf', '-b', phy + ':bss-2.conf',\n\t\t   '-b', phy + ':bss-3.conf']\n\tres = subprocess.check_call(cmd)\n\tif res != 0:\n\t\traise Exception(\"Could not start hostapd: %s\" % str(res))\n\tmulti_check(dev, [True, True, True])\n\tfor i in range(0, 3):\n\t\tdev[i].request(\"DISCONNECT\")\n\n\thapd = hostapd.Hostapd(apdev[0]['ifname'])\n\thapd.ping()\n\tif \"OK\" not in hapd.request(\"TERMINATE\"):\n\t\traise Exception(\"Failed to terminate hostapd process\")\n\tev = hapd.wait_event([\"CTRL-EVENT-TERMINATING\"], timeout=15)\n\tif ev is None:\n\t\traise Exception(\"CTRL-EVENT-TERMINATING not seen\")\n\tfor i in range(30):\n\t\ttime.sleep(0.1)\n\t\tif not os.path.exists(pidfile):\n\t\t\tbreak\n\tif os.path.exists(pidfile):\n\t\traise Exception(\"PID file exits after process termination\")\n", "description": "hostapd BSS config file", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import alloc_fail, require_under_vm, get_phy", "from test_ap_acs import force_prev_ap_on_24g"]}], [{"term": "def", "name": "process_manifest", "data": "def process_manifest(destdir, paths, track=None,\n\t\tremove_unaccounted=True,\n\t\tremove_all_directory_symlinks=True,\n\t\tremove_empty_directories=True,\n\t\tno_symlinks=False,\n\t\tdefines={}):\n\n\tif track:\n\t\tif os.path.exists(track):\n\t\t\t# We use the same format as install manifests for the tracking\n\t\t\t# data.\n\t\t\tmanifest = InstallManifest(path=track)\n\t\t\tremove_unaccounted = FileRegistry()\n\t\t\tdummy_file = BaseFile()\n\n\t\t\tfinder = FileFinder(destdir, find_dotfiles=True)\n\t\t\tfor dest in manifest._dests:\n\t\t\t\tfor p, f in finder.find(dest):\n\t\t\t\t\tremove_unaccounted.add(p, dummy_file)\n\n\t\telse:\n\t\t\t# If tracking is enabled and there is no file, we don't want to\n\t\t\t# be removing anything.\n\t\t\tremove_unaccounted=False\n\t\t\tremove_empty_directories=False\n\t\t\tremove_all_directory_symlinks=False\n\n\tmanifest = InstallManifest()\n\tfor path in paths:\n\t\tmanifest |= InstallManifest(path=path)\n\n\tcopier = FileCopier()\n\tlink_policy = \"copy\" if no_symlinks else \"symlink\"\n\tmanifest.populate_registry(\n\t\tcopier, defines_override=defines, link_policy=link_policy\n\t)\n\tresult = copier.copy(destdir,\n\t\tremove_unaccounted=remove_unaccounted,\n\t\tremove_all_directory_symlinks=remove_all_directory_symlinks,\n\t\tremove_empty_directories=remove_empty_directories)\n\n\tif track:\n\t\tmanifest.write(path=track)\n\n\treturn result\n\n", "description": null, "category": "remove", "imports": ["from __future__ import absolute_import, print_function, unicode_literals", "import argparse", "import os", "import sys", "import time", "from mozpack.copier import (", "from mozpack.files import (", "from mozpack.manifests import (", "from mozbuild.util import DefinesAction"]}, {"term": "def", "name": "main", "data": "def main(argv):\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Process install manifest files.')\n\n\tparser.add_argument('destdir', help='Destination directory.')\n\tparser.add_argument('manifests', nargs='+', help='Path to manifest file(s).')\n\tparser.add_argument('--no-remove', action='store_true',\n\t\thelp='Do not remove unaccounted files from destination.')\n\tparser.add_argument('--no-remove-all-directory-symlinks', action='store_true',\n\t\thelp='Do not remove all directory symlinks from destination.')\n\tparser.add_argument('--no-remove-empty-directories', action='store_true',\n\t\thelp='Do not remove empty directories from destination.')\n\tparser.add_argument('--no-symlinks', action='store_true',\n\t\thelp='Do not install symbolic links. Always copy files')\n\tparser.add_argument('--track', metavar=\"PATH\",\n\t\thelp='Use installed files tracking information from the given path.')\n\tparser.add_argument('-D', action=DefinesAction,\n\t\tdest='defines', metavar=\"VAR[=VAL]\",\n\t\thelp='Define a variable to override what is specified in the manifest')\n\n\targs = parser.parse_args(argv)\n\n\tstart = time.time()\n\n\tresult = process_manifest(args.destdir, args.manifests,\n\t\ttrack=args.track, remove_unaccounted=not args.no_remove,\n\t\tremove_all_directory_symlinks=not args.no_remove_all_directory_symlinks,\n\t\tremove_empty_directories=not args.no_remove_empty_directories,\n\t\tno_symlinks=args.no_symlinks,\n\t\tdefines=args.defines)\n\n\telapsed = time.time() - start\n\n\tprint(COMPLETE.format(\n\t\telapsed=elapsed,\n\t\tdest=args.destdir,\n\t\texisting=result.existing_files_count,\n\t\tupdated=result.updated_files_count,\n\t\trm_files=result.removed_files_count,\n\t\trm_dirs=result.removed_directories_count))\n", "description": null, "category": "remove", "imports": ["from __future__ import absolute_import, print_function, unicode_literals", "import argparse", "import os", "import sys", "import time", "from mozpack.copier import (", "from mozpack.files import (", "from mozpack.manifests import (", "from mozbuild.util import DefinesAction"]}], [], [], [], [], [], [], [], [], [], [], [{"term": "class", "name": "SolutionValidator", "data": "class SolutionValidator(object):\n\tdef validate_node_value(self, value: int) -> None:\n\t\tif MINIMUM_NODE_VALUE > value or value > MAXIMUM_NODE_VALUE:\n\t\t\traise ValueError('Invalid node value')\n\n\tdef validate_node_list_length(self, node_list_len: int) -> None:\n\t\tif node_list_len > MAXIMUM_NODE_LIST_LENGTH:\n\t\t\traise ValueError('Invalid node list length')\n\n\tdef validate_min_n_value(self, n: int) -> None:\n\t\tif MINIMUM_N_VALUE > n:\n\t\t\traise ValueError('Invalid n value')\n\n\tdef validate_max_n_value(self, n: int, node_list_len: int) -> None:\n\t\tif n > node_list_len:\n\t\t\traise ValueError('Invalid n value')\n\n", "description": null, "category": "remove", "imports": ["from typing import List, Optional", "import unittest"]}, {"term": "class", "name": "classListNode:", "data": "class ListNode:\n\tdef __init__(self, val=0, next=None):\n\t\tself.val = val\n\t\tself.next = next\n\n", "description": null, "category": "remove", "imports": ["from typing import List, Optional", "import unittest"]}, {"term": "class", "name": "classSolution:", "data": "class Solution:\n\tdef __init__(self):\n\t\t\"\"\" Given the head of a linked list, \n\t\t\tremove the nth node from the end of the list \n\t\t\tand return its head.\n\n\t\t\tFollow up: Could you do this in one pass?\n\t\t\"\"\"\n\t\tself.validator = SolutionValidator()\n\n\tdef removeNthFromEnd_two_pass(self, head: Optional[ListNode], n: int) -> Optional[ListNode]:\n\t\t\"\"\" Two pass solution\n\t\t\"\"\"\n\t\t# Basic validations\n\t\tif not head or type(head) != ListNode:\n\t\t\traise ValueError('Invalid head')\n\n\t\tself.validator.validate_min_n_value(n)\n\n\t\tnode_list_len = 0\n\t\thead_pointer = head\n\t\tn_pointer = None\n\t\twhile head_pointer:\n\t\t\tnode_list_len += 1\n\t\t\tself.validator.validate_node_value(head_pointer.val)\n\t\t\thead_pointer = head_pointer.next\n\n\t\tself.validator.validate_max_n_value(n, node_list_len)\n\n\t\tif node_list_len == 1:\n\t\t\treturn None\n\n\t\thead_pointer = head\n\t\tif node_list_len == n:\n\t\t\t# Remove first node\n\t\t\thead = head.next\n\t\t\treturn head\n\n\t\twhile node_list_len - 1 > n:\n\t\t\tnode_list_len -= 1\n\t\t\thead_pointer = head_pointer.next\n\n\t\tif head_pointer.next:\n\t\t\thead_pointer.next = head_pointer.next.next\n\n\t\treturn head\n\n\tdef removeNthFromEnd(self, head: Optional[ListNode], n: int) -> Optional[ListNode]:\n\t\t\"\"\" One pass solution\n\t\t\"\"\"\n\t\t# Basic validations\n\t\tif not head or type(head) != ListNode:\n\t\t\traise ValueError('Invalid head')\n\n\t\tself.validator.validate_min_n_value(n)\n\n\t\t# Keep all the pointer like a breadcrumb flag\n\t\tpointer_memory = {}\n\t\tnode_list_len = 1\n\t\thead_pointer = head\n\n\t\twhile head_pointer.next:\n\t\t\t# Move pointers through the node list\n\t\t\tself.validator.validate_node_value(head_pointer.val)\n\t\t\tpointer_memory[node_list_len] = head_pointer\n\t\t\thead_pointer, node_list_len = head_pointer.next, node_list_len + 1\n\n\t\tself.validator.validate_max_n_value(n, node_list_len)\n\n\t\tto_remove_index = node_list_len - n\n\t\tif to_remove_index == 0:\n\t\t\t# Remove first node\n\t\t\thead = head.next\n\t\t\treturn head\n\n\t\tif to_remove_index in pointer_memory:\n\t\t\tpointer_memory[to_remove_index].next = pointer_memory[to_remove_index].next.next\n\t\t\treturn head\n\n", "description": " Given the head of a linked list, \n\t\t\tremove the nth node from the end of the list \n\t\t\tand return its head.\n\n\t\t\tFollow up: Could you do this in one pass?\n\t\t", "category": "remove", "imports": ["from typing import List, Optional", "import unittest"]}, {"term": "def", "name": "make_typing_list", "data": "def make_typing_list(values: []) -> Optional[ListNode]:\n\tvalues.reverse()\n\thead = None\n\tlast = None\n\tfor i in range(len(values)):\n\t\thead = ListNode(values[i], last)\n\t\tlast = head\n\n\treturn head\n", "description": null, "category": "remove", "imports": ["from typing import List, Optional", "import unittest"]}, {"term": "def", "name": "extract_list", "data": "def extract_list(head: Optional[ListNode]) -> []:\n\toutput = []\n\twhile head:\n\t\toutput.append(head.val)\n\t\thead = head.next\n\n\treturn output\n", "description": null, "category": "remove", "imports": ["from typing import List, Optional", "import unittest"]}, {"term": "def", "name": "main", "data": "def main():\n\ttc = unittest.TestCase()\n\tsol = Solution()\n\n\tprint('Example 1')\n\ttc.assertEqual(extract_list(sol.removeNthFromEnd(make_typing_list([1,2,3,4,5]), 2)), [1,2,3,5])\n\n\tprint('Example 2')\n\ttc.assertEqual(extract_list(sol.removeNthFromEnd(make_typing_list([1]), 1)), [])\n\n\tprint('Example 3')\n\ttc.assertEqual(extract_list(sol.removeNthFromEnd(make_typing_list([1,2]), 1)), [1])\n\n\tprint('Example 4')\n\ttc.assertEqual(extract_list(sol.removeNthFromEnd(make_typing_list([2,1]), 2)), [1])\n\n\tprint('Example 5')\n\ttc.assertEqual(extract_list(sol.removeNthFromEnd(make_typing_list([1,2]), 2)), [2])\n\n\tprint('Example 6')\n\ttc.assertEqual(extract_list(sol.removeNthFromEnd(make_typing_list([1,2,3,4]), 4)), [2,3,4])\n\n\tprint('Example 7')\n\ttc.assertEqual(extract_list(sol.removeNthFromEnd(make_typing_list([1,2,3,4]), 1)), [1,2,3])\n\n", "description": null, "category": "remove", "imports": ["from typing import List, Optional", "import unittest"]}], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('topo', '0019_auto_20210115_1342'),\n\t]\n\n\toperations = [\n\t\tmigrations.CreateModel(\n\t\t\tname='LineInfo',\n\t\t\tfields=[\n\t\t\t\t('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n\t\t\t\t('incontournable', models.IntegerField(choices=[(0, 'Aucun'), (1, 'Secteur'), (2, 'Site')], default=0)),\n\t\t\t\t('code_topo', models.CharField(blank=True, max_length=10)),\n\t\t\t\t('da', models.BooleanField(default=False)),\n\t\t\t\t('da_cota', models.CharField(blank=True, max_length=10)),\n\t\t\t\t('hauteur', models.FloatField(default=0)),\n\t\t\t\t('inclin', models.IntegerField(default=0, validators=[django.core.validators.MinValueValidator(0), django.core.validators.MaxValueValidator(5)])),\n\t\t\t\t('orientation', models.CharField(blank=True, choices=[('n', 'Nord'), ('s', 'Sud'), ('e', 'Est'), ('o', 'Ouest'), ('ne', 'Nord-Est'), ('se', 'Sud_Est'), ('no', 'Nord-Ouest'), ('so', 'Sud-Ouest')], max_length=2)),\n\t\t\t\t('expo', models.IntegerField(default=0, validators=[django.core.validators.MinValueValidator(0), django.core.validators.MaxValueValidator(3)])),\n\t\t\t\t('trav', models.CharField(blank=True, max_length=3)),\n\t\t\t\t('descente', models.BooleanField(default=False)),\n\t\t\t\t('morfo', models.BooleanField(default=False)),\n\t\t\t\t('reglette', models.BooleanField(default=False)),\n\t\t\t\t('plans', models.BooleanField(default=False)),\n\t\t\t\t('reta', models.BooleanField(default=False)),\n\t\t\t\t('compression', models.BooleanField(default=False)),\n\t\t\t\t('jette', models.BooleanField(default=False)),\n\t\t\t\t('technique', models.BooleanField(default=False)),\n\t\t\t\t('physique', models.BooleanField(default=False)),\n\t\t\t\t('touche', models.BooleanField(default=False)),\n\t\t\t\t('elim', models.BooleanField(default=False)),\n\t\t\t\t('top', models.BooleanField(default=False)),\n\t\t\t\t('obs_fr', models.CharField(blank=True, max_length=1000)),\n\t\t\t\t('obs_es', models.CharField(blank=True, max_length=1000)),\n\t\t\t\t('obs_en', models.CharField(blank=True, max_length=1000)),\n\t\t\t\t('obs_cat', models.CharField(blank=True, max_length=1000)),\n\t\t\t\t('da_nom', models.CharField(blank=True, max_length=50)),\n\t\t\t\t('ext', models.CharField(blank=True, max_length=5)),\n\t\t\t\t('var', models.CharField(blank=True, max_length=5)),\n\t\t\t\t('avis_1', models.CharField(blank=True, max_length=5)),\n\t\t\t\t('avis_2', models.CharField(blank=True, max_length=5)),\n\t\t\t\t('annee', models.CharField(blank=True, max_length=5)),\n\t\t\t\t('ouvert', models.CharField(blank=True, max_length=50)),\n\t\t\t\t('first_ascent', models.CharField(blank=True, max_length=50)),\n\t\t\t],\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='annee',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='avis_1',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='avis_2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='code_topo',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='compression',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='da',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='da_cota',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='da_nom',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='descente',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='elim',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='expo',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='ext',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='first_ascent',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='hauteur',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='inclin',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='incontournable',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='jette',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='morfo',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='obs_cat',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='obs_en',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='obs_es',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='obs_fr',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='orientation',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='ouvert',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='physique',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='plans',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='reglette',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='reta',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='technique',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='top',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='touche',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='trav',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='line',\n\t\t\tname='var',\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='line',\n\t\t\tname='line_info',\n\t\t\tfield=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, to='topo.lineinfo'),\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["import django.core.validators", "from django.db import migrations, models", "import django.db.models.deletion"]}], [], [], [], [], [], [], [{"term": "class", "name": "Test_Deque", "data": "class Test_Deque(unittest.TestCase):\n\n\tdef test_add_front(self):\n\t\tdeque = Deque()\n\t\tdeque.addFront(1)\n\t\tdeque.addFront(2)\n\t\tdeque.addFront(3)\n\t\tself.assertEqual(3, deque.size())\n\t\tself.assertEqual(3, deque.removeFront())\n\t\tself.assertEqual(2, deque.removeFront())\n\t\tself.assertEqual(1, deque.removeFront())\n\t#\n\tdef test_add_tail(self):\n\t\tdeque = Deque()\n\t\tdeque.addTail(1)\n\t\tdeque.addTail(2)\n\t\tdeque.addTail(3)\n\t\tself.assertEqual(3, deque.size())\n\t\tself.assertEqual(3, deque.removeTail())\n\t\tself.assertEqual(2, deque.removeTail())\n\t\tself.assertEqual(1, deque.removeTail())\n\t#\n\tdef test_add_mix(self):\n\t\tdeque = Deque()\n\t\tdeque.addFront('A')\n\t\tdeque.addFront('B')\n\t\tdeque.addFront('C')\n\t\tdeque.addTail(1)\n\t\tdeque.addTail(2)\n\t\tdeque.addTail(3)\n\t\tself.assertEqual(6, deque.size())\n\t\tself.assertEqual(3, deque.removeTail())\n\t\tself.assertEqual(2, deque.removeTail())\n\t\tself.assertEqual(1, deque.removeTail())\n\t\tself.assertEqual('A', deque.removeTail())\n\t\tself.assertEqual('B', deque.removeTail())\n\t\tself.assertEqual('C', deque.removeTail())\n\t#\n\tdef test_remove_front_empty(self):\n\t\tdeque = Deque()\n\t\tself.assertEqual(None, deque.removeFront())\n\t\tself.assertEqual(0, deque.size())\n\t#\n\tdef test_remove_front_one(self):\n\t\tdeque = Deque()\n\t\tdeque.addTail('A')\n\t\tself.assertEqual('A', deque.removeFront())\n\t\tself.assertEqual(0, deque.size())\n\t#\n\tdef test_remove_front(self):\n\t\tdeque = Deque()\n\t\tdeque.addFront('A')\n\t\tdeque.addFront('B')\n\t\tdeque.addFront('C')\n\t\tdeque.addTail(1)\n\t\tdeque.addTail(2)\n\t\tdeque.addTail(3)\n\t\tself.assertEqual(3, deque.removeTail())\n\t\tself.assertEqual(5, deque.size())\n\t\tself.assertEqual(2, deque.removeTail())\n\t\tself.assertEqual(1, deque.removeTail())\n\t\tself.assertEqual('C', deque.removeFront())\n\t\tself.assertEqual('B', deque.removeFront())\n\t\tself.assertEqual('A', deque.removeFront())\n\t#\n\tdef test_remove_tail_empty(self):\n\t\tdeque = Deque()\n\t\tself.assertEqual(None, deque.removeTail())\n\t\tself.assertEqual(0, deque.size())\n\n\tdef test_remove_tail_one(self):\n\t\tdeque = Deque()\n\t\tdeque.addFront(1)\n\t\tself.assertEqual(1, deque.removeTail())\n\t\tself.assertEqual(0, deque.size())\n\n\tdef test_remove_tail(self):\n\t\tdeque = Deque()\n\t\tdeque.addFront('A')\n\t\tdeque.addFront('B')\n\t\tdeque.addFront('C')\n\t\tdeque.addTail(1)\n\t\tdeque.addTail(2)\n\t\tdeque.addTail(3)\n\t\tself.assertEqual('C', deque.removeFront())\n\t\tself.assertEqual(5, deque.size())\n\t\tself.assertEqual(3, deque.removeTail())\n\t\tself.assertEqual(2, deque.removeTail())\n\t\tself.assertEqual(1, deque.removeTail())\n\t\tself.assertEqual('B', deque.removeFront())\n\t\tself.assertEqual('A', deque.removeFront())\n", "description": null, "category": "remove", "imports": ["import unittest", "from Deque import Deque"]}], [], [], [{"term": "class", "name": "RemoveGroupsNotificationProtocolEntity", "data": "class RemoveGroupsNotificationProtocolEntity(GroupsNotificationProtocolEntity):\n\t'''\n\n\n\n\n\n\n\t'''\n\tTYPE_PARTICIPANT_ADMIN = \"admin\"\n\tdef __init__(self, _id, _from, timestamp, notify, participant, offline,\n\t\t\t\t subject,\n\t\t\t\t participants):\n\t\tsuper(RemoveGroupsNotificationProtocolEntity, self).__init__(_id, _from, timestamp, notify, participant, offline)\n\t\tself.setGroupProps(subject, participants)\n\n\tdef setGroupProps(self,\n\t\t\t\t\t  subject,\n\t\t\t\t\t  participants):\n\n\t\tassert type(participants) is dict, \"Participants must be a dict {jid => type?}\"\n\n\t\tself.subject = subject\n\t\tself.participants = participants\n\n\tdef getParticipants(self):\n\t\treturn self.participants\n\n\tdef getSubject(self):\n\t\treturn self.subject\n\n\tdef toProtocolTreeNode(self):\n\t\tnode = super(RemoveGroupsNotificationProtocolEntity, self).toProtocolTreeNode()\n\t\tremoveNode = ProtocolTreeNode(\"remove\", {\"subject\": self.subject})\n\t\tparticipants = []\n\t\tfor jid in self.getParticipants():\n\t\t\tpnode = ProtocolTreeNode(\"participant\", {\"jid\": jid})\n\t\t\tparticipants.append(pnode)\n\n\t\tremoveNode.addChildren(participants)\n\t\tnode.addChild(removeNode)\n\n\t\treturn node\n\n\t@staticmethod\n\tdef fromProtocolTreeNode(node):\n\t\tremoveNode = node.getChild(\"remove\")\n\t\tparticipants = {}\n\t\tfor p in removeNode.getAllChildren(\"participant\"):\n\t\t\tparticipants[p[\"jid\"]] = p[\"type\"]\n\n\t\treturn RemoveGroupsNotificationProtocolEntity(\n\t\t\tnode[\"id\"], node[\"from\"], node[\"t\"], node[\"notify\"], node[\"participant\"], node[\"offline\"],\n\t\t\tremoveNode[\"subject\"], participants\n\t\t)\n", "description": null, "category": "remove", "imports": ["from .notification_groups import GroupsNotificationProtocolEntity", "from yowsup.structs import ProtocolTreeNode"]}], [], [{"term": "def", "name": "test_stem_remove", "data": "def test_stem_remove():\n\tax = plt.gca()\n\tst = ax.stem([1, 2], [1, 2])\n\tst.remove()\n\n", "description": null, "category": "remove", "imports": ["import matplotlib.pyplot as plt"]}, {"term": "def", "name": "test_errorbar_remove", "data": "def test_errorbar_remove():\n\n\t# Regression test for a bug that caused remove to fail when using\n\t# fmt='none'\n\n\tax = plt.gca()\n\n\teb = ax.errorbar([1], [1])\n\teb.remove()\n\n\teb = ax.errorbar([1], [1], xerr=1)\n\teb.remove()\n\n\teb = ax.errorbar([1], [1], yerr=2)\n\teb.remove()\n\n\teb = ax.errorbar([1], [1], xerr=[2], yerr=2)\n\teb.remove()\n\n\teb = ax.errorbar([1], [1], fmt='none')\n\teb.remove()\n", "description": null, "category": "remove", "imports": ["import matplotlib.pyplot as plt"]}], [{"term": "def", "name": "test_stem_remove", "data": "def test_stem_remove():\n\tax = plt.gca()\n\tst = ax.stem([1, 2], [1, 2])\n\tst.remove()\n\n", "description": null, "category": "remove", "imports": ["import matplotlib.pyplot as plt"]}, {"term": "def", "name": "test_errorbar_remove", "data": "def test_errorbar_remove():\n\n\t# Regression test for a bug that caused remove to fail when using\n\t# fmt='none'\n\n\tax = plt.gca()\n\n\teb = ax.errorbar([1], [1])\n\teb.remove()\n\n\teb = ax.errorbar([1], [1], xerr=1)\n\teb.remove()\n\n\teb = ax.errorbar([1], [1], yerr=2)\n\teb.remove()\n\n\teb = ax.errorbar([1], [1], xerr=[2], yerr=2)\n\teb.remove()\n\n\teb = ax.errorbar([1], [1], fmt='none')\n\teb.remove()\n", "description": null, "category": "remove", "imports": ["import matplotlib.pyplot as plt"]}], [], [], [], [{"term": "class", "name": "BaseMultiGraphTester", "data": "class BaseMultiGraphTester(BaseAttrGraphTester):\n\tdef test_has_edge(self):\n\t\tG=self.K3\n\t\tassert_equal(G.has_edge(0,1),True)\n\t\tassert_equal(G.has_edge(0,-1),False)\n\t\tassert_equal(G.has_edge(0,1,0),True)\n\t\tassert_equal(G.has_edge(0,1,1),False)\n\n\tdef test_get_edge_data(self):\n\t\tG=self.K3\n\t\tassert_equal(G.get_edge_data(0,1),{0:{}})\n\t\tassert_equal(G[0][1],{0:{}})\n\t\tassert_equal(G[0][1][0],{})\n\t\tassert_equal(G.get_edge_data(10,20),None)\n\t\tassert_equal(G.get_edge_data(0,1,0),{})\n\t\t\n\n\tdef test_adjacency(self):\n\t\tG=self.K3\n\t\tassert_equal(dict(G.adjacency()),\n\t\t\t\t\t\t  {0: {1: {0:{}}, 2: {0:{}}},\n\t\t\t\t\t\t   1: {0: {0:{}}, 2: {0:{}}},\n\t\t\t\t\t\t   2: {0: {0:{}}, 1: {0:{}}}})\n\n\tdef deepcopy_edge_attr(self,H,G):\n\t\tassert_equal(G[1][2][0]['foo'],H[1][2][0]['foo'])\n\t\tG[1][2][0]['foo'].append(1)\n\t\tassert_not_equal(G[1][2][0]['foo'],H[1][2][0]['foo'])\n\n\tdef shallow_copy_edge_attr(self,H,G):\n\t\tassert_equal(G[1][2][0]['foo'],H[1][2][0]['foo'])\n\t\tG[1][2][0]['foo'].append(1)\n\t\tassert_equal(G[1][2][0]['foo'],H[1][2][0]['foo'])\n\n\tdef same_attrdict(self, H, G):\n\t\t# same attrdict in the edgedata\n\t\told_foo=H[1][2][0]['foo']\n\t\tH.add_edge(1,2,0,foo='baz')\n\t\tassert_equal(G.edge,H.edge)\n\t\tH.add_edge(1,2,0,foo=old_foo)\n\t\tassert_equal(G.edge,H.edge)\n\t\t# but not same edgedata dict\n\t\tH.add_edge(1,2,foo='baz')\n\t\tassert_not_equal(G.edge,H.edge)\n\n\t\told_foo=H.node[0]['foo']\n\t\tH.node[0]['foo']='baz'\n\t\tassert_equal(G.node,H.node)\n\t\tH.node[0]['foo']=old_foo\n\t\tassert_equal(G.node,H.node)\n\n\tdef different_attrdict(self, H, G):\n\t\t# used by graph_equal_but_different\n\t\told_foo=H[1][2][0]['foo']\n\t\tH.add_edge(1,2,0,foo='baz')\n\t\tassert_not_equal(G.edge,H.edge)\n\t\tH.add_edge(1,2,0,foo=old_foo)\n\t\tassert_equal(G.edge,H.edge)\n\t\tHH=H.copy()\n\t\tH.add_edge(1,2,foo='baz')\n\t\tassert_not_equal(G.edge,H.edge)\n\t\tH=HH\n\t\told_foo=H.node[0]['foo']\n\t\tH.node[0]['foo']='baz'\n\t\tassert_not_equal(G.node,H.node)\n\t\tH.node[0]['foo']=old_foo\n\t\tassert_equal(G.node,H.node)\n\n\tdef test_to_undirected(self):\n\t\tG=self.K3\n\t\tself.add_attributes(G)\n\t\tH=nx.MultiGraph(G)\n\t\tself.is_shallow_copy(H,G)\n\t\tH=G.to_undirected()\n\t\tself.is_deepcopy(H,G)\n\n\tdef test_to_directed(self):\n\t\tG=self.K3\n\t\tself.add_attributes(G)\n\t\tH=nx.MultiDiGraph(G)\n\t\tself.is_shallow_copy(H,G)\n\t\tH=G.to_directed()\n\t\tself.is_deepcopy(H,G)\n\n\tdef test_selfloops(self):\n\t\tG=self.K3\n\t\tG.add_edge(0,0)\n\t\tassert_nodes_equal(G.nodes_with_selfloops(), [0])\n\t\tassert_edges_equal(G.selfloop_edges(), [(0, 0)])\n\t\tassert_edges_equal(G.selfloop_edges(data=True), [(0, 0, {})])\n\t\tassert_equal(G.number_of_selfloops(),1)\n\n\tdef test_selfloops2(self):\n\t\tG=self.K3\n\t\tG.add_edge(0,0)\n\t\tG.add_edge(0,0)\n\t\tG.add_edge(0,0,key='parallel edge')\n\t\tG.remove_edge(0,0,key='parallel edge')\n\t\tassert_equal(G.number_of_edges(0,0),2)\n\t\tG.remove_edge(0,0)\n\t\tassert_equal(G.number_of_edges(0,0),1)\n\n\tdef test_edge_attr4(self):\n\t\tG=self.Graph()\n\t\tG.add_edge(1,2,key=0,data=7,spam='bar',bar='foo')\n\t\tassert_edges_equal(G.edges(data=True),\n\t\t\t\t\t  [(1,2,{'data':7,'spam':'bar','bar':'foo'})])\n\t\tG[1][2][0]['data']=10 # OK to set data like this\n\t\tassert_edges_equal(G.edges(data=True),\n\t\t\t\t\t [(1,2,{'data':10,'spam':'bar','bar':'foo'})])\n\n\t\tG.edge[1][2][0]['data']=20 # another spelling, \"edge\"\n\t\tassert_edges_equal(G.edges(data=True),\n\t\t\t\t\t  [(1,2,{'data':20,'spam':'bar','bar':'foo'})])\n\t\tG.edge[1][2][0]['listdata']=[20,200]\n\t\tG.edge[1][2][0]['weight']=20\n\t\tassert_edges_equal(G.edges(data=True),\n\t\t\t\t\t [(1,2,{'data':20,'spam':'bar',\n\t\t\t\t\t\t\t'bar':'foo','listdata':[20,200],'weight':20})])\n\n", "description": null, "category": "remove", "imports": ["from nose.tools import assert_equal", "from nose.tools import assert_is", "from nose.tools import assert_not_equal", "from nose.tools import assert_raises", "import networkx as nx", "from networkx.testing.utils import *", "from test_graph import BaseAttrGraphTester, TestGraph"]}, {"term": "class", "name": "TestMultiGraph", "data": "class TestMultiGraph(BaseMultiGraphTester,TestGraph):\n\tdef setUp(self):\n\t\tself.Graph=nx.MultiGraph\n\t\t# build K3\n\t\ted1,ed2,ed3 = ({0:{}},{0:{}},{0:{}})\n\t\tself.k3adj={0: {1: ed1, 2: ed2},\n\t\t\t\t\t1: {0: ed1, 2: ed3},\n\t\t\t\t\t2: {0: ed2, 1: ed3}}\n\t\tself.k3edges=[(0, 1), (0, 2), (1, 2)]\n\t\tself.k3nodes=[0, 1, 2]\n\t\tself.K3=self.Graph()\n\t\tself.K3.adj = self.K3.edge = self.k3adj\n\t\tself.K3.node={}\n\t\tself.K3.node[0]={}\n\t\tself.K3.node[1]={}\n\t\tself.K3.node[2]={}\n\n\tdef test_data_input(self):\n\t\tG=self.Graph(data={1:[2],2:[1]}, name=\"test\")\n\t\tassert_equal(G.name,\"test\")\n\t\tassert_equal(sorted(G.adj.items()),[(1, {2: {0:{}}}), (2, {1: {0:{}}})])\n\n\tdef test_getitem(self):\n\t\tG=self.K3\n\t\tassert_equal(G[0],{1: {0:{}}, 2: {0:{}}})\n\t\tassert_raises(KeyError, G.__getitem__, 'j')\n\t\tassert_raises((TypeError,nx.NetworkXError), G.__getitem__, ['A'])\n\n\tdef test_remove_node(self):\n\t\tG=self.K3\n\t\tG.remove_node(0)\n\t\tassert_equal(G.adj,{1:{2:{0:{}}},2:{1:{0:{}}}})\n\t\tassert_raises((KeyError,nx.NetworkXError), G.remove_node,-1)\n\n\tdef test_add_edge(self):\n\t\tG=self.Graph()\n\t\tG.add_edge(0,1)\n\t\tassert_equal(G.adj,{0: {1: {0:{}}}, 1: {0: {0:{}}}})\n\t\tG=self.Graph()\n\t\tG.add_edge(*(0,1))\n\t\tassert_equal(G.adj,{0: {1: {0:{}}}, 1: {0: {0:{}}}})\n\t\t\n\tdef test_add_edge_conflicting_key(self):\n\t\tG=self.Graph()\n\t\tG.add_edge(0,1,key=1)\n\t\tG.add_edge(0,1)\n\t\tassert_equal(G.number_of_edges(),2)\n\t\tG=self.Graph()\n\t\tG.add_edges_from([(0,1,1,{})])\n\t\tG.add_edges_from([(0,1)])\n\t\tassert_equal(G.number_of_edges(),2)\n\n\tdef test_add_edges_from(self):\n\t\tG=self.Graph()\n\t\tG.add_edges_from([(0,1),(0,1,{'weight':3})])\n\t\tassert_equal(G.adj,{0: {1: {0:{},1:{'weight':3}}},\n\t\t\t\t\t\t\t1: {0: {0:{},1:{'weight':3}}}})\n\t\tG.add_edges_from([(0,1),(0,1,{'weight':3})],weight=2)\n\t\tassert_equal(G.adj,{0: {1: {0:{},1:{'weight':3},\n\t\t\t\t\t\t\t\t\t2:{'weight':2},3:{'weight':3}}},\n\t\t\t\t\t\t\t1: {0: {0:{},1:{'weight':3},\n\t\t\t\t\t\t\t\t\t2:{'weight':2},3:{'weight':3}}}})\n\n\t\t# too few in tuple\n\t\tassert_raises(nx.NetworkXError, G.add_edges_from,[(0,)])\n\t\t# too many in tuple\n\t\tassert_raises(nx.NetworkXError, G.add_edges_from,[(0,1,2,3,4)])\n\t\tassert_raises(TypeError, G.add_edges_from,[0])  # not a tuple\n\n\tdef test_remove_edge(self):\n\t\tG=self.K3\n\t\tG.remove_edge(0,1)\n\t\tassert_equal(G.adj,{0: {2: {0: {}}},\n\t\t\t\t\t\t\t1: {2: {0: {}}},\n\t\t\t\t\t\t\t2: {0: {0: {}},\n\t\t\t\t\t\t\t\t1: {0: {}}}})\n\n\t\tassert_raises((KeyError,nx.NetworkXError), G.remove_edge,-1,0)\n\t\tassert_raises((KeyError,nx.NetworkXError), G.remove_edge,0,2,\n\t\t\t\t\t  key=1)\n\t\t\n\tdef test_remove_edges_from(self):\n\t\tG=self.K3.copy()\n\t\tG.remove_edges_from([(0,1)])\n\t\tassert_equal(G.adj,{0:{2:{0:{}}},1:{2:{0:{}}},2:{0:{0:{}},1:{0:{}}}})\n\t\tG.remove_edges_from([(0,0)]) # silent fail\n\t\tself.K3.add_edge(0,1)\n\t\tG=self.K3.copy()\n\t\tG.remove_edges_from(list(G.edges(data=True,keys=True)))\n\t\tassert_equal(G.adj,{0:{},1:{},2:{}})\n\t\tG=self.K3.copy()\n\t\tG.remove_edges_from(list(G.edges(data=False,keys=True)))\n\t\tassert_equal(G.adj,{0:{},1:{},2:{}})\n\t\tG=self.K3.copy()\n\t\tG.remove_edges_from(list(G.edges(data=False,keys=False)))\n\t\tassert_equal(G.adj,{0:{},1:{},2:{}})\n\t\tG=self.K3.copy()\n\t\tG.remove_edges_from([(0,1,0),(0,2,0,{}),(1,2)])\n\t\tassert_equal(G.adj,{0:{1:{1:{}}},1:{0:{1:{}}},2:{}})\n\n\tdef test_remove_multiedge(self):\n\t\tG=self.K3\n\t\tG.add_edge(0,1,key='parallel edge')\n\t\tG.remove_edge(0,1,key='parallel edge')\n\t\tassert_equal(G.adj,{0: {1: {0:{}}, 2: {0:{}}},\n\t\t\t\t\t\t   1: {0: {0:{}}, 2: {0:{}}},\n\t\t\t\t\t\t   2: {0: {0:{}}, 1: {0:{}}}})\n\t\tG.remove_edge(0,1)\n\t\tassert_equal(G.adj,{0:{2:{0:{}}},1:{2:{0:{}}},2:{0:{0:{}},1:{0:{}}}})\n\t\tassert_raises((KeyError,nx.NetworkXError), G.remove_edge,-1,0)\n\n", "description": null, "category": "remove", "imports": ["from nose.tools import assert_equal", "from nose.tools import assert_is", "from nose.tools import assert_not_equal", "from nose.tools import assert_raises", "import networkx as nx", "from networkx.testing.utils import *", "from test_graph import BaseAttrGraphTester, TestGraph"]}, {"term": "class", "name": "TestEdgeSubgraph", "data": "class TestEdgeSubgraph(object):\n\t\"\"\"Unit tests for the :meth:`MultiGraph.edge_subgraph` method.\"\"\"\n\n\tdef setup(self):\n\t\t# Create a doubly-linked path graph on five nodes.\n\t\tG = nx.MultiGraph()\n\t\tnx.add_path(G, range(5))\n\t\tnx.add_path(G, range(5))\n\t\t# Add some node, edge, and graph attributes.\n\t\tfor i in range(5):\n\t\t\tG.node[i]['name'] = 'node{}'.format(i)\n\t\tG.edge[0][1][0]['name'] = 'edge010'\n\t\tG.edge[0][1][1]['name'] = 'edge011'\n\t\tG.edge[3][4][0]['name'] = 'edge340'\n\t\tG.edge[3][4][1]['name'] = 'edge341'\n\t\tG.graph['name'] = 'graph'\n\t\t# Get the subgraph induced by one of the first edges and one of\n\t\t# the last edges.\n\t\tself.G = G\n\t\tself.H = G.edge_subgraph([(0, 1, 0), (3, 4, 1)])\n\n\tdef test_correct_nodes(self):\n\t\t\"\"\"Tests that the subgraph has the correct nodes.\"\"\"\n\t\tassert_equal([0, 1, 3, 4], sorted(self.H.nodes()))\n\n\tdef test_correct_edges(self):\n\t\t\"\"\"Tests that the subgraph has the correct edges.\"\"\"\n\t\tassert_equal([(0, 1, 0, 'edge010'), (3, 4, 1, 'edge341')],\n\t\t\t\t\t sorted(self.H.edges(keys=True, data='name')))\n\n\tdef test_add_node(self):\n\t\t\"\"\"Tests that adding a node to the original graph does not\n\t\taffect the nodes of the subgraph.\n\n\t\t\"\"\"\n\t\tself.G.add_node(5)\n\t\tassert_equal([0, 1, 3, 4], sorted(self.H.nodes()))\n\n\tdef test_remove_node(self):\n\t\t\"\"\"Tests that removing a node in the original graph does not\n\t\taffect the nodes of the subgraph.\n\n\t\t\"\"\"\n\t\tself.G.remove_node(0)\n\t\tassert_equal([0, 1, 3, 4], sorted(self.H.nodes()))\n\n\tdef test_node_attr_dict(self):\n\t\t\"\"\"Tests that the node attribute dictionary of the two graphs is\n\t\tthe same object.\n\n\t\t\"\"\"\n\t\tfor v in self.H:\n\t\t\tassert_equal(self.G.node[v], self.H.node[v])\n\t\t# Making a change to G should make a change in H and vice versa.\n\t\tself.G.node[0]['name'] = 'foo'\n\t\tassert_equal(self.G.node[0], self.H.node[0])\n\t\tself.H.node[1]['name'] = 'bar'\n\t\tassert_equal(self.G.node[1], self.H.node[1])\n\n\tdef test_edge_attr_dict(self):\n\t\t\"\"\"Tests that the edge attribute dictionary of the two graphs is\n\t\tthe same object.\n\n\t\t\"\"\"\n\t\tfor u, v, k in self.H.edges(keys=True):\n\t\t\tassert_equal(self.G.edge[u][v][k], self.H.edge[u][v][k])\n\t\t# Making a change to G should make a change in H and vice versa.\n\t\tself.G.edge[0][1][0]['name'] = 'foo'\n\t\tassert_equal(self.G.edge[0][1][0]['name'],\n\t\t\t\t\t self.H.edge[0][1][0]['name'])\n\t\tself.H.edge[3][4][1]['name'] = 'bar'\n\t\tassert_equal(self.G.edge[3][4][1]['name'],\n\t\t\t\t\t self.H.edge[3][4][1]['name'])\n\n\tdef test_graph_attr_dict(self):\n\t\t\"\"\"Tests that the graph attribute dictionary of the two graphs\n\t\tis the same object.\n\n\t\t\"\"\"\n\t\tassert_is(self.G.graph, self.H.graph)\n", "description": "Unit tests for the :meth:`MultiGraph.edge_subgraph` method.", "category": "remove", "imports": ["from nose.tools import assert_equal", "from nose.tools import assert_is", "from nose.tools import assert_not_equal", "from nose.tools import assert_raises", "import networkx as nx", "from networkx.testing.utils import *", "from test_graph import BaseAttrGraphTester, TestGraph"]}], [{"term": "def", "name": "RunCommand", "data": "  def RunCommand(self):\n\t# self.recursion_requested initialized in command.py (so can be checked\n\t# in parent class for all commands).\n\tself.continue_on_error = False\n\tself.all_versions = False\n\tif self.sub_opts:\n\t  for o, unused_a in self.sub_opts:\n\t\tif o == '-a':\n\t\t  self.all_versions = True\n\t\telif o == '-f':\n\t\t  self.continue_on_error = True\n\t\telif o == '-r' or o == '-R':\n\t\t  self.recursion_requested = True\n\t\telif o == '-v':\n\t\t  self.THREADED_LOGGER.info('WARNING: The %s -v option is no longer'\n\t\t\t\t\t\t\t\t\t' needed, and will eventually be removed.\\n'\n\t\t\t\t\t\t\t\t\t% self.command_name)\n\n\t# Used to track if any files failed to be removed.\n\tself.everything_removed_okay = True\n\n\t# Tracks if any URIs matched the given args.\n\n\tremove_func = self._MkRemoveFunc()\n\texception_handler = self._MkRemoveExceptionHandler()\n\n\ttry:\n\t  # Expand wildcards, dirs, buckets, and bucket subdirs in URIs.\n\t  name_expansion_iterator = NameExpansionIterator(\n\t\t  self.command_name, self.proj_id_handler, self.headers, self.debug,\n\t\t  self.bucket_storage_uri_class, self.args, self.recursion_requested,\n\t\t  flat=self.recursion_requested, all_versions=self.all_versions)\n\n\t  # Perform remove requests in parallel (-m) mode, if requested, using\n\t  # configured number of parallel processes and threads. Otherwise,\n\t  # perform requests with sequential function calls in current process.\n\t  self.Apply(remove_func, name_expansion_iterator, exception_handler)\n\n\t# Assuming the bucket has versioning enabled, uri's that don't map to\n\t# objects should throw an error even with all_versions, since the prior\n\t# round of deletes only sends objects to a history table.\n\t# This assumption that rm -a is only called for versioned buckets should be\n\t# corrected, but the fix is non-trivial.\n\texcept CommandException as e:\n\t  if not self.continue_on_error:\n\t\traise\n\texcept GSResponseError, e:\n\t  if not self.continue_on_error:\n\t\traise\n\n\tif not self.everything_removed_okay and not self.continue_on_error:\n\t  raise CommandException('Some files could not be removed.')\n\n\t# If this was a gsutil rm -r command covering any bucket subdirs,\n\t# remove any dir_$folder$ objects (which are created by various web UI\n\t# tools to simulate folders).\n\tif self.recursion_requested:\n\t  folder_object_wildcards = []\n\t  for uri_str in self.args:\n\t\turi = self.suri_builder.StorageUri(uri_str)\n\t\tif uri.names_object:\n\t\t  folder_object_wildcards.append('%s**_$folder$' % uri)\n\t  if len(folder_object_wildcards):\n\t\tself.continue_on_error = True\n\t\ttry:\n\t\t  name_expansion_iterator = NameExpansionIterator(\n\t\t\t  self.command_name, self.proj_id_handler, self.headers, self.debug,\n\t\t\t  self.bucket_storage_uri_class, folder_object_wildcards,\n\t\t\t  self.recursion_requested, flat=True,\n\t\t\t  all_versions=self.all_versions)\n\t\t  self.Apply(remove_func, name_expansion_iterator, exception_handler)\n\t\texcept CommandException as e:\n\t\t  # Ignore exception from name expansion due to an absent folder file.\n\t\t  if not e.reason.startswith('No URIs matched:'):\n\t\t\traise\n\n\treturn 0\n", "description": null, "category": "remove", "imports": ["import boto", "from boto.exception import GSResponseError", "from gslib.command import Command", "from gslib.command import COMMAND_NAME", "from gslib.command import COMMAND_NAME_ALIASES", "from gslib.command import CONFIG_REQUIRED", "from gslib.command import FILE_URIS_OK", "from gslib.command import MAX_ARGS", "from gslib.command import MIN_ARGS", "from gslib.command import PROVIDER_URIS_OK", "from gslib.command import SUPPORTED_SUB_ARGS", "from gslib.command import URIS_START_ARG", "from gslib.exception import CommandException", "from gslib.help_provider import HELP_NAME", "from gslib.help_provider import HELP_NAME_ALIASES", "from gslib.help_provider import HELP_ONE_LINE_SUMMARY", "from gslib.help_provider import HELP_TEXT", "from gslib.help_provider import HelpType", "from gslib.help_provider import HELP_TYPE", "from gslib.name_expansion import NameExpansionIterator", "from gslib.util import NO_MAX"]}, {"term": "def", "name": "_MkRemoveExceptionHandler", "data": "  def _MkRemoveExceptionHandler(self):\n\tdef RemoveExceptionHandler(e):\n\t  \"\"\"Simple exception handler to allow post-completion status.\"\"\"\n\t  self.THREADED_LOGGER.error(str(e))\n\t  self.everything_removed_okay = False\n\treturn RemoveExceptionHandler\n", "description": "Simple exception handler to allow post-completion status.", "category": "remove", "imports": ["import boto", "from boto.exception import GSResponseError", "from gslib.command import Command", "from gslib.command import COMMAND_NAME", "from gslib.command import COMMAND_NAME_ALIASES", "from gslib.command import CONFIG_REQUIRED", "from gslib.command import FILE_URIS_OK", "from gslib.command import MAX_ARGS", "from gslib.command import MIN_ARGS", "from gslib.command import PROVIDER_URIS_OK", "from gslib.command import SUPPORTED_SUB_ARGS", "from gslib.command import URIS_START_ARG", "from gslib.exception import CommandException", "from gslib.help_provider import HELP_NAME", "from gslib.help_provider import HELP_NAME_ALIASES", "from gslib.help_provider import HELP_ONE_LINE_SUMMARY", "from gslib.help_provider import HELP_TEXT", "from gslib.help_provider import HelpType", "from gslib.help_provider import HELP_TYPE", "from gslib.name_expansion import NameExpansionIterator", "from gslib.util import NO_MAX"]}, {"term": "def", "name": "_MkRemoveFunc", "data": "  def _MkRemoveFunc(self):\n\tdef RemoveFunc(name_expansion_result):\n\t  exp_src_uri = self.suri_builder.StorageUri(\n\t\t  name_expansion_result.GetExpandedUriStr(),\n\t\t  is_latest=name_expansion_result.is_latest)\n\t  if exp_src_uri.names_container():\n\t\tif exp_src_uri.is_cloud_uri():\n\t\t  # Before offering advice about how to do rm + rb, ensure those\n\t\t  # commands won't fail because of bucket naming problems.\n\t\t  boto.s3.connection.check_lowercase_bucketname(exp_src_uri.bucket_name)\n\t\turi_str = exp_src_uri.object_name.rstrip('/')\n\t\traise CommandException('\"rm\" command will not remove buckets. To '\n\t\t\t\t\t\t\t   'delete this/these bucket(s) do:\\n\\tgsutil rm '\n\t\t\t\t\t\t\t   '%s/*\\n\\tgsutil rb %s' % (uri_str, uri_str))\n\n\t  # Perform delete.\n\t  self.THREADED_LOGGER.info('Removing %s...',\n\t\t\t\t\t\t\t\tname_expansion_result.expanded_uri_str)\n\t  try:\n\t\texp_src_uri.delete_key(validate=False, headers=self.headers)\n\n\t  except:\n\t\tif self.continue_on_error:\n\t\t  self.everything_removed_okay = False\n\t\telse:\n\t\t  raise\n\treturn RemoveFunc\n", "description": null, "category": "remove", "imports": ["import boto", "from boto.exception import GSResponseError", "from gslib.command import Command", "from gslib.command import COMMAND_NAME", "from gslib.command import COMMAND_NAME_ALIASES", "from gslib.command import CONFIG_REQUIRED", "from gslib.command import FILE_URIS_OK", "from gslib.command import MAX_ARGS", "from gslib.command import MIN_ARGS", "from gslib.command import PROVIDER_URIS_OK", "from gslib.command import SUPPORTED_SUB_ARGS", "from gslib.command import URIS_START_ARG", "from gslib.exception import CommandException", "from gslib.help_provider import HELP_NAME", "from gslib.help_provider import HELP_NAME_ALIASES", "from gslib.help_provider import HELP_ONE_LINE_SUMMARY", "from gslib.help_provider import HELP_TEXT", "from gslib.help_provider import HelpType", "from gslib.help_provider import HELP_TYPE", "from gslib.name_expansion import NameExpansionIterator", "from gslib.util import NO_MAX"]}], [], [], [{"term": "class", "name": "AvatarFriendsManager", "data": "class AvatarFriendsManager(DistributedObjectGlobal):\n\tnotify = directNotify.newCategory('AvatarFriendsManager')\n\n\tdef __init__(self, cr):\n\t\tDistributedObjectGlobal.__init__(self, cr)\n\t\tself.reset()\n\n\tdef reset(self):\n\t\tself.avatarFriendsList = set()\n\t\tself.avatarId2Info = {}\n\t\tself.invitedAvatarsList = []\n\t\tself.ignoredAvatarList = []\n\n\tdef addIgnore(self, avId):\n\t\tif avId not in self.ignoredAvatarList:\n\t\t\tself.ignoredAvatarList.append(avId)\n\t\t\tbase.cr.centralLogger.writeClientEvent(\n\t\t\t\t'addIgnore', sender=base.localAvatar.doId, receiver=avId)\n\t\tmessenger.send('AvatarIgnoreChange')\n\n\tdef removeIgnore(self, avId):\n\t\tif avId in self.ignoredAvatarList:\n\t\t\tself.ignoredAvatarList.remove(avId)\n\t\t\tbase.cr.centralLogger.writeClientEvent(\n\t\t\t\t'removeIgnore', sender=base.localAvatar.doId, receiver=avId)\n\t\tmessenger.send('AvatarIgnoreChange')\n\n\tdef checkIgnored(self, avId):\n\t\treturn avId and avId in self.ignoredAvatarList\n\n\tdef sendRequestInvite(self, avId):\n\t\tself.notify.debugCall()\n\t\tself.sendUpdate('requestInvite', [avId])\n\t\tself.invitedAvatarsList.append(avId)\n\n\tdef sendRequestRemove(self, avId):\n\t\tself.notify.debugCall()\n\t\tself.sendUpdate('requestRemove', [avId])\n\t\tif avId in self.invitedAvatarsList:\n\t\t\tself.invitedAvatarsList.remove(avId)\n\n\tdef friendConsidering(self, avId):\n\t\tself.notify.debugCall()\n\t\tmessenger.send(OTPGlobals.AvatarFriendConsideringEvent, [1, avId])\n\n\tdef invitationFrom(self, avId, avatarName):\n\t\tself.notify.debugCall()\n\t\tmessenger.send(OTPGlobals.AvatarFriendInvitationEvent, [avId, avatarName])\n\n\tdef retractInvite(self, avId):\n\t\tself.notify.debugCall()\n\t\tmessenger.send(OTPGlobals.AvatarFriendRetractInviteEvent, [avId])\n\t\tif avId in self.invitedAvatarsList:\n\t\t\tself.invitedAvatarsList.remove(avId)\n\n\tdef rejectInvite(self, avId, reason):\n\t\tself.notify.debugCall()\n\t\tmessenger.send(OTPGlobals.AvatarFriendRejectInviteEvent, [avId, reason])\n\t\tif avId in self.invitedAvatarsList:\n\t\t\tself.invitedAvatarsList.remove(avId)\n\n\tdef rejectRemove(self, avId, reason):\n\t\tself.notify.debugCall()\n\t\tmessenger.send(OTPGlobals.AvatarFriendRejectRemoveEvent, [avId, reason])\n\n\tdef updateAvatarFriend(self, avId, info):\n\t\tif hasattr(info, 'avatarId') and not info.avatarId and avId:\n\t\t\tinfo.avatarId = avId\n\t\tif avId not in self.avatarFriendsList:\n\t\t\tself.avatarFriendsList.add(avId)\n\t\t\tself.avatarId2Info[avId] = info\n\t\t\tmessenger.send(OTPGlobals.AvatarFriendAddEvent, [avId, info])\n\t\tif self.avatarId2Info[avId].onlineYesNo != info.onlineYesNo:\n\t\t\tbase.talkAssistant.receiveFriendUpdate(avId, info.getName(), info.onlineYesNo)\n\t\tself.avatarId2Info[avId] = info\n\t\tmessenger.send(OTPGlobals.AvatarFriendUpdateEvent, [avId, info])\n\t\tif avId in self.invitedAvatarsList:\n\t\t\tself.invitedAvatarsList.remove(avId)\n\t\t\tmessenger.send(OTPGlobals.AvatarNewFriendAddEvent, [avId])\n\n\tdef removeAvatarFriend(self, avId):\n\t\tself.avatarFriendsList.remove(avId)\n\t\tself.avatarId2Info.pop(avId, None)\n\t\tmessenger.send(OTPGlobals.AvatarFriendRemoveEvent, [avId])\n\t\treturn\n\n\tdef setFriends(self, avatarIds):\n\t\tself.notify.debugCall()\n\n\tdef isFriend(self, avId):\n\t\treturn self.isAvatarFriend(avId)\n\n\tdef isAvatarFriend(self, avId):\n\t\treturn avId in self.avatarFriendsList\n\n\tdef getFriendInfo(self, avId):\n\t\treturn self.avatarId2Info.get(avId)\n\n\tdef countTrueFriends(self):\n\t\tcount = 0\n\t\tfor id in self.avatarId2Info:\n\t\t\tif self.avatarId2Info[id].openChatFriendshipYesNo:\n\t\t\t\tcount += 1\n\n\t\treturn count\n", "description": null, "category": "remove", "imports": ["from direct.distributed.DistributedObjectGlobal import DistributedObjectGlobal", "from direct.directnotify.DirectNotifyGlobal import directNotify", "from otp.uberdog.RejectCode import RejectCode", "from otp.otpbase import OTPGlobals", "from otp.otpbase import OTPLocalizer"]}], [{"term": "def", "name": "uploadUsreReview", "data": "def uploadUsreReview():\n\tglobal response\n\tif(request.method=='POST'):\n\t\trequest_data = request.data\n\t\trequest_data = json.loads(request_data.decode('utf-8'))\n\t\treview = request_data['review']\n\t\t# tokenization\n\t\tlower = review.lower()\n\t\twords = word_tokenize(lower, \"english\")\n\t\t# remove punctuation\n\t\t# def isCountry(text):\n\t\t#  count = 0\n\t\t#  flag = 0\n\t\t#  for letter in text:\n\t\t#   count+=1\n\t\t#   if( count == 1 and letter == \".\"):\n\t\t#\tprint(text)\n\t\t#\tflag=1\n\t\t#  return flag\n\n\t\ttext_after_remove_punc = []\n\n\t\tdef removePunc(words):\n\t\t\tremoved = []\n\t\t\tfor word in words:\n\t\t\t\tfor letter in word:\n\t\t\t\t\tif letter in string.punctuation:\n\t\t\t\t\t\tword = word.replace(letter, \"\")\n\t\t\t\tremoved.append(word)\n\t\t\treturn removed\n\n\t\ttext_after_remove_punc = removePunc(words)\n\t\tprint(text_after_remove_punc)\n\t\tvalueToBeRemoved = \"\"\n\t\t# remove all empty string\n\t\ttry:\n\t\t\twhile True:\n\t\t\t\ttext_after_remove_punc.remove(valueToBeRemoved)\n\t\texcept ValueError:\n\t\t\tpass\n\n\t\t# remove stop words\n\t\ttext_after_stop_words = []\n\t\tfor word in text_after_remove_punc:\n\t\t\tif word not in set(stopwords.words('english')):\n\t\t\t\ttext_after_stop_words.append(word)\n\n\t\tprint(text_after_stop_words)\n\t\t# convert to stemming text\n\t\tps = PorterStemmer()\n\t\tstem_text = []\n\t\tfor word in text_after_stop_words:\n\t\t\tstem_text.append(ps.stem(word))\n\t\tprint(stem_text)\n\n\t\t################################\n\t\t# convert to lemmatization\n\t\t# Function to convert\n\t\tdef listToString(s):\n\t\t\tstr1 = \"\"\n\t\t\tfor ele in s:\n\t\t\t\tstr1 += ele + ' '\n\t\t\treturn str1\n\n\t\ttext_lemma = []\n\t\tdoc1 = nlp(listToString(text_after_stop_words))\n\t\tfor token in doc1:\n\t\t\ttext_lemma.append(token.lemma_)\n\n\t\t# sentiment analysis\n\t\tfinalVal = \"\"\n\n\t\tresult=sentimentAnalyze(finalVal,text_after_remove_punc)\n\n\t\treturn jsonify(result)\n", "description": null, "category": "remove", "imports": ["from flask import Flask,request,jsonify", "import json", "import string", "from nltk.corpus import stopwords", "from nltk.tokenize import word_tokenize", "from nltk.stem import PorterStemmer", "from nltk.sentiment.vader import SentimentIntensityAnalyzer", "import spacy"]}, {"term": "def", "name": "listToString", "data": "def listToString(s):\n\tstr1 = \"\"\n\tfor ele in s:\n\t\tstr1 += ele + ' '\n", "description": null, "category": "remove", "imports": ["from flask import Flask,request,jsonify", "import json", "import string", "from nltk.corpus import stopwords", "from nltk.tokenize import word_tokenize", "from nltk.stem import PorterStemmer", "from nltk.sentiment.vader import SentimentIntensityAnalyzer", "import spacy"]}, {"term": "def", "name": "sentimentAnalyze", "data": "def sentimentAnalyze(finalVal,text_after_remove_punc):\n\tscore = SentimentIntensityAnalyzer().polarity_scores(listToString(text_after_remove_punc))\n\tprint(score)\n\tif score['pos'] > score['neg']:\n\t\tfinalVal = 'Positive'\n\t\tprint('Positive')\n\telif score['pos'] < score['neg']:\n\t\tfinalVal = 'Negative'\n\t\tprint('negative')\n\telse:\n\t\tfinalVal = 'Equal Article'\n\t\tprint('equal article')\n\treturn {'positive':score['pos'],\n\t\t\t'negative':score['neg']\n\t\t\t}\n\n", "description": null, "category": "remove", "imports": ["from flask import Flask,request,jsonify", "import json", "import string", "from nltk.corpus import stopwords", "from nltk.tokenize import word_tokenize", "from nltk.stem import PorterStemmer", "from nltk.sentiment.vader import SentimentIntensityAnalyzer", "import spacy"]}], [], [], [], [], [], [], [], [], [], [{"term": "def", "name": "compute_api_add_fixed_ip", "data": "def compute_api_add_fixed_ip(self, context, instance, network_id):\n\tglobal last_add_fixed_ip\n\n\tlast_add_fixed_ip = (instance['uuid'], network_id)\n\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova.api.openstack.compute import multinic as multinic_v21", "from nova import compute", "from nova import exception", "from nova import objects", "from nova import test", "from nova.tests.unit.api.openstack import fakes"]}, {"term": "def", "name": "compute_api_remove_fixed_ip", "data": "def compute_api_remove_fixed_ip(self, context, instance, address):\n\tglobal last_remove_fixed_ip\n\n\tlast_remove_fixed_ip = (instance['uuid'], address)\n\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova.api.openstack.compute import multinic as multinic_v21", "from nova import compute", "from nova import exception", "from nova import objects", "from nova import test", "from nova.tests.unit.api.openstack import fakes"]}, {"term": "def", "name": "compute_api_get", "data": "def compute_api_get(self, context, instance_id, expected_attrs=None,\n\t\t\t\t\tcell_down_support=False):\n\tinstance = objects.Instance()\n\tinstance.uuid = instance_id\n\tinstance.id = 1\n\tinstance.vm_state = 'fake'\n\tinstance.task_state = 'fake'\n\tinstance.obj_reset_changes()\n\treturn instance\n\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova.api.openstack.compute import multinic as multinic_v21", "from nova import compute", "from nova import exception", "from nova import objects", "from nova import test", "from nova.tests.unit.api.openstack import fakes"]}, {"term": "class", "name": "FixedIpTestV21", "data": "class FixedIpTestV21(test.NoDBTestCase):\n\tcontroller_class = multinic_v21\n\tvalidation_error = exception.ValidationError\n\n\tdef setUp(self):\n\t\tsuper(FixedIpTestV21, self).setUp()\n\t\tfakes.stub_out_networking(self)\n\t\tself.stub_out('nova.compute.api.API.add_fixed_ip',\n\t\t\t\t\t  compute_api_add_fixed_ip)\n\t\tself.stub_out('nova.compute.api.API.remove_fixed_ip',\n\t\t\t\t\t  compute_api_remove_fixed_ip)\n\t\tself.stub_out('nova.compute.api.API.get', compute_api_get)\n\t\tself.controller = self.controller_class.MultinicController()\n\t\tself.fake_req = fakes.HTTPRequest.blank('')\n\n\tdef test_add_fixed_ip(self):\n\t\tglobal last_add_fixed_ip\n\t\tlast_add_fixed_ip = (None, None)\n\n\t\tbody = dict(addFixedIp=dict(networkId='test_net'))\n\t\tself.controller._add_fixed_ip(self.fake_req, UUID, body=body)\n\t\tself.assertEqual(last_add_fixed_ip, (UUID, 'test_net'))\n\n\tdef _test_add_fixed_ip_bad_request(self, body):\n\t\tself.assertRaises(self.validation_error,\n\t\t\t\t\t\t  self.controller._add_fixed_ip,\n\t\t\t\t\t\t  self.fake_req,\n\t\t\t\t\t\t  UUID, body=body)\n\n\tdef test_add_fixed_ip_empty_network_id(self):\n\t\tbody = {'addFixedIp': {'network_id': ''}}\n\t\tself._test_add_fixed_ip_bad_request(body)\n\n\tdef test_add_fixed_ip_network_id_bigger_than_36(self):\n\t\tbody = {'addFixedIp': {'network_id': 'a' * 37}}\n\t\tself._test_add_fixed_ip_bad_request(body)\n\n\tdef test_add_fixed_ip_no_network(self):\n\t\tglobal last_add_fixed_ip\n\t\tlast_add_fixed_ip = (None, None)\n\n\t\tbody = dict(addFixedIp=dict())\n\t\tself._test_add_fixed_ip_bad_request(body)\n\t\tself.assertEqual(last_add_fixed_ip, (None, None))\n\n\t@mock.patch.object(compute.api.API, 'add_fixed_ip')\n\tdef test_add_fixed_ip_no_more_ips_available(self, mock_add_fixed_ip):\n\t\tmock_add_fixed_ip.side_effect = exception.NoMoreFixedIps(net='netid')\n\n\t\tbody = dict(addFixedIp=dict(networkId='test_net'))\n\t\tself.assertRaises(webob.exc.HTTPBadRequest,\n\t\t\t\t\t\t  self.controller._add_fixed_ip,\n\t\t\t\t\t\t  self.fake_req,\n\t\t\t\t\t\t  UUID, body=body)\n\n\tdef test_remove_fixed_ip(self):\n\t\tglobal last_remove_fixed_ip\n\t\tlast_remove_fixed_ip = (None, None)\n\n\t\tbody = dict(removeFixedIp=dict(address='10.10.10.1'))\n\t\tself.controller._remove_fixed_ip(self.fake_req, UUID, body=body)\n\t\tself.assertEqual(last_remove_fixed_ip, (UUID, '10.10.10.1'))\n\n\tdef test_remove_fixed_ip_no_address(self):\n\t\tglobal last_remove_fixed_ip\n\t\tlast_remove_fixed_ip = (None, None)\n\n\t\tbody = dict(removeFixedIp=dict())\n\t\tself.assertRaises(self.validation_error,\n\t\t\t\t\t\t  self.controller._remove_fixed_ip,\n\t\t\t\t\t\t  self.fake_req,\n\t\t\t\t\t\t  UUID, body=body)\n\t\tself.assertEqual(last_remove_fixed_ip, (None, None))\n\n\tdef test_remove_fixed_ip_invalid_address(self):\n\t\tbody = {'removeFixedIp': {'address': ''}}\n\t\tself.assertRaises(self.validation_error,\n\t\t\t\t\t\t  self.controller._remove_fixed_ip,\n\t\t\t\t\t\t  self.fake_req,\n\t\t\t\t\t\t  UUID, body=body)\n\n\t@mock.patch.object(compute.api.API, 'remove_fixed_ip',\n\t\tside_effect=exception.FixedIpNotFoundForInstance(\n\t\t\tinstance_uuid=UUID, ip='10.10.10.1'))\n\tdef test_remove_fixed_ip_not_found(self, _remove_fixed_ip):\n\n\t\tbody = {'removeFixedIp': {'address': '10.10.10.1'}}\n\t\tself.assertRaises(webob.exc.HTTPBadRequest,\n\t\t\t\t\t\t  self.controller._remove_fixed_ip,\n\t\t\t\t\t\t  self.fake_req,\n\t\t\t\t\t\t  UUID, body=body)\n\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova.api.openstack.compute import multinic as multinic_v21", "from nova import compute", "from nova import exception", "from nova import objects", "from nova import test", "from nova.tests.unit.api.openstack import fakes"]}, {"term": "class", "name": "MultinicPolicyEnforcementV21", "data": "class MultinicPolicyEnforcementV21(test.NoDBTestCase):\n\n\tdef setUp(self):\n\t\tsuper(MultinicPolicyEnforcementV21, self).setUp()\n\t\tself.controller = multinic_v21.MultinicController()\n\t\tself.req = fakes.HTTPRequest.blank('')\n\n\tdef test_add_fixed_ip_policy_failed(self):\n\t\trule_name = \"os_compute_api:os-multinic\"\n\t\tself.policy.set_rules({rule_name: \"project:non_fake\"})\n\t\texc = self.assertRaises(\n\t\t\texception.PolicyNotAuthorized,\n\t\t\tself.controller._add_fixed_ip, self.req, fakes.FAKE_UUID,\n\t\t\tbody={'addFixedIp': {'networkId': fakes.FAKE_UUID}})\n\t\tself.assertEqual(\n\t\t\t\"Policy doesn't allow %s to be performed.\" % rule_name,\n\t\t\texc.format_message())\n\n\tdef test_remove_fixed_ip_policy_failed(self):\n\t\trule_name = \"os_compute_api:os-multinic\"\n\t\tself.policy.set_rules({rule_name: \"project:non_fake\"})\n\t\texc = self.assertRaises(\n\t\t\texception.PolicyNotAuthorized,\n\t\t\tself.controller._remove_fixed_ip, self.req, fakes.FAKE_UUID,\n\t\t\tbody={'removeFixedIp': {'address': \"10.0.0.1\"}})\n\t\tself.assertEqual(\n\t\t\t\"Policy doesn't allow %s to be performed.\" % rule_name,\n\t\t\texc.format_message())\n\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova.api.openstack.compute import multinic as multinic_v21", "from nova import compute", "from nova import exception", "from nova import objects", "from nova import test", "from nova.tests.unit.api.openstack import fakes"]}, {"term": "class", "name": "MultinicAPIDeprecationTest", "data": "class MultinicAPIDeprecationTest(test.NoDBTestCase):\n\n\tdef setUp(self):\n\t\tsuper(MultinicAPIDeprecationTest, self).setUp()\n\t\tself.controller = multinic_v21.MultinicController()\n\t\tself.req = fakes.HTTPRequest.blank('', version='2.44')\n\n\tdef test_add_fixed_ip_not_found(self):\n\t\tbody = dict(addFixedIp=dict(networkId='test_net'))\n\t\tself.assertRaises(exception.VersionNotFoundForAPIMethod,\n\t\t\tself.controller._add_fixed_ip, self.req, UUID, body=body)\n\n\tdef test_remove_fixed_ip__not_found(self):\n\t\tbody = dict(removeFixedIp=dict(address='10.10.10.1'))\n\t\tself.assertRaises(exception.VersionNotFoundForAPIMethod,\n\t\t\tself.controller._remove_fixed_ip, self.req, UUID, body=body)\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova.api.openstack.compute import multinic as multinic_v21", "from nova import compute", "from nova import exception", "from nova import objects", "from nova import test", "from nova.tests.unit.api.openstack import fakes"]}], [], [], [], [], [], [], [], [], [{"term": "def", "name": "_process_add_video", "data": "def _process_add_video(provider, context, re_match):\n\tplaylist_id = context.get_param('playlist_id', '')\n\tif not playlist_id:\n\t\traise kodion.KodionException('Playlist/Remove: missing playlist_id')\n\tvideo_id = context.get_param('video_id', '')\n\tif not video_id:\n\t\traise kodion.KodionException('Playlist/Remove: missing video_id')\n\n\tjson_data = provider.get_client(context).add_video_to_playlist(playlist_id=playlist_id, video_id=video_id)\n\tif not v3.handle_error(provider, context, json_data):\n\t\treturn False\n\n\treturn True\n\n", "description": null, "category": "remove", "imports": ["from resources.lib.kodion.utils.function_cache import FunctionCache", "from resources.lib import kodion", "from resources.lib.youtube.helper import v3"]}, {"term": "def", "name": "_process_remove_video", "data": "def _process_remove_video(provider, context, re_match):\n\tplaylist_id = context.get_param('playlist_id', '')\n\tif not playlist_id:\n\t\traise kodion.KodionException('Playlist/Remove: missing playlist_id')\n\n\tvideo_id = context.get_param('video_id', '')\n\tif not video_id:\n\t\traise kodion.KodionException('Playlist/Remove: missing video_id')\n\n\tvideo_name = context.get_param('video_name', '')\n\tif not video_name:\n\t\traise kodion.KodionException('Playlist/Remove: missing video_name')\n\n\tif context.get_ui().on_remove_content(video_name):\n\t\tjson_data = provider.get_client(context).remove_video_from_playlist(playlist_id=playlist_id,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tplaylist_item_id=video_id)\n\t\tif not v3.handle_error(provider, context, json_data):\n\t\t\treturn False\n\n\t\tcontext.get_ui().refresh_container()\n\t\tpass\n\treturn True\n\n", "description": null, "category": "remove", "imports": ["from resources.lib.kodion.utils.function_cache import FunctionCache", "from resources.lib import kodion", "from resources.lib.youtube.helper import v3"]}, {"term": "def", "name": "_process_remove_playlist", "data": "def _process_remove_playlist(provider, context, re_match):\n\tplaylist_id = context.get_param('playlist_id', '')\n\tif not playlist_id:\n\t\traise kodion.KodionException('Playlist/Remove: missing playlist_id')\n\n\tplaylist_name = context.get_param('playlist_name', '')\n\tif not playlist_name:\n\t\traise kodion.KodionException('Playlist/Remove: missing playlist_name')\n\n\tif context.get_ui().on_delete_content(playlist_name):\n\t\tjson_data = provider.get_client(context).remove_playlist(playlist_id=playlist_id)\n\t\tif not v3.handle_error(provider, context, json_data):\n\t\t\treturn False\n\n\t\tcontext.get_ui().refresh_container()\n\t\tpass\n\treturn True\n\n", "description": null, "category": "remove", "imports": ["from resources.lib.kodion.utils.function_cache import FunctionCache", "from resources.lib import kodion", "from resources.lib.youtube.helper import v3"]}, {"term": "def", "name": "_process_select_playlist", "data": "def _process_select_playlist(provider, context, re_match):\n\tjson_data = context.get_function_cache().get(FunctionCache.ONE_MINUTE / 3,\n\t\t\t\t\t\t\t\t\t\t\t\t provider.get_client(context).get_playlists_of_channel,\n\t\t\t\t\t\t\t\t\t\t\t\t channel_id='mine')\n\tplaylists = json_data.get('items', [])\n\n\titems = []\n\t# create playlist\n\titems.append(('[B]' + context.localize(provider.LOCAL_MAP['youtube.playlist.create']) + '[/B]', 'playlist.create'))\n\n\t# add the 'Watch Later' playlist\n\tresource_manager = provider.get_resource_manager(context)\n\tmy_playlists = resource_manager.get_related_playlists(channel_id='mine')\n\tif 'watchLater' in my_playlists:\n\t\twatch_later_playlist_id = my_playlists.get('watchLater', '')\n\t\titems.append(\n\t\t\t('[B]' + context.localize(provider.LOCAL_MAP['youtube.watch_later']) + '[/B]', watch_later_playlist_id))\n\t\tpass\n\n\tfor playlist in playlists:\n\t\tsnippet = playlist.get('snippet', {})\n\t\ttitle = snippet.get('title', '')\n\t\tplaylist_id = playlist.get('id', '')\n\t\tif title and playlist_id:\n\t\t\titems.append((title, playlist_id))\n\t\t\tpass\n\t\tpass\n\n\tresult = context.get_ui().on_select(context.localize(provider.LOCAL_MAP['youtube.playlist.select']), items)\n\tif result == 'playlist.create':\n\t\tresult, text = context.get_ui().on_keyboard_input(\n\t\t\tcontext.localize(provider.LOCAL_MAP['youtube.playlist.create']))\n\t\tif result and text:\n\t\t\tjson_data = provider.get_client(context).create_playlist(title=text)\n\t\t\tif not v3.handle_error(provider, context, json_data):\n\t\t\t\treturn\n\n\t\t\tplaylist_id = json_data.get('id', '')\n\t\t\tif playlist:\n\t\t\t\tnew_params = {}\n\t\t\t\tnew_params.update(context.get_params())\n\t\t\t\tnew_params['playlist_id'] = playlist_id\n\t\t\t\tnew_context = context.clone(new_params=new_params)\n\t\t\t\t_process_add_video(provider, new_context, re_match)\n\t\t\t\tpass\n\t\t\tpass\n\t\tpass\n\telif result != -1:\n\t\tnew_params = {}\n\t\tnew_params.update(context.get_params())\n\t\tnew_params['playlist_id'] = result\n\t\tnew_context = context.clone(new_params=new_params)\n\t\t_process_add_video(provider, new_context, re_match)\n\t\tpass\n\tpass\n\n", "description": null, "category": "remove", "imports": ["from resources.lib.kodion.utils.function_cache import FunctionCache", "from resources.lib import kodion", "from resources.lib.youtube.helper import v3"]}, {"term": "def", "name": "_process_rename_playlist", "data": "def _process_rename_playlist(provider, context, re_match):\n\tplaylist_id = context.get_param('playlist_id', '')\n\tif not playlist_id:\n\t\traise kodion.KodionException('playlist/rename: missing playlist_id')\n\n\tcurrent_playlist_name = context.get_param('playlist_name', '')\n\tresult, text = context.get_ui().on_keyboard_input(context.localize(provider.LOCAL_MAP['youtube.rename']),\n\t\t\t\t\t\t\t\t\t\t\t\t\t  default=current_playlist_name)\n\tif result and text:\n\t\tjson_data = provider.get_client(context).rename_playlist(playlist_id=playlist_id, new_title=text)\n\t\tif not v3.handle_error(provider, context, json_data):\n\t\t\treturn\n\n\t\tcontext.get_ui().refresh_container()\n\t\tpass\n\tpass\n\n", "description": null, "category": "remove", "imports": ["from resources.lib.kodion.utils.function_cache import FunctionCache", "from resources.lib import kodion", "from resources.lib.youtube.helper import v3"]}, {"term": "def", "name": "process", "data": "def process(method, category, provider, context, re_match):\n\tif method == 'add' and category == 'video':\n\t\treturn _process_add_video(provider, context, re_match)\n\telif method == 'remove' and category == 'video':\n\t\treturn _process_remove_video(provider, context, re_match)\n\telif method == 'remove' and category == 'playlist':\n\t\treturn _process_remove_playlist(provider, context, re_match)\n\telif method == 'select' and category == 'playlist':\n\t\treturn _process_select_playlist(provider, context, re_match)\n\telif method == 'rename' and category == 'playlist':\n\t\treturn _process_rename_playlist(provider, context, re_match)\n\telse:\n\t\traise kodion.KodionException(\"Unknown category '%s' or method '%s'\" % (category, method))\n\n\treturn True\n", "description": null, "category": "remove", "imports": ["from resources.lib.kodion.utils.function_cache import FunctionCache", "from resources.lib import kodion", "from resources.lib.youtube.helper import v3"]}], [], [], [], [], [], [], [], [], [], [{"term": "def", "name": "RemoveUnsupportedTests", "data": "  def RemoveUnsupportedTests(self, test_output):\n\tif not SUPPORTS_DEATH_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'DeathTest')\n\tif not SUPPORTS_TYPED_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'TypedTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypedDeathTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypeParamDeathTest')\n\tif not SUPPORTS_THREADS:\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ExpectFailureWithThreadsTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ScopedFakeTestPartResultReporterTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'WorksConcurrently')\n\tif not SUPPORTS_STACK_TRACES:\n\t  test_output = RemoveStackTraces(test_output)\n\n\treturn test_output\n", "description": null, "category": "remove", "imports": ["import difflib", "import os", "import re", "import sys", "import gtest_test_utils"]}, {"term": "def", "name": "testOutput", "data": "  def testOutput(self):\n\toutput = GetOutputOfAllCommands()\n\n\tgolden_file = open(GOLDEN_PATH, 'rb')\n\t# A mis-configured source control system can cause \\r appear in EOL\n\t# sequences when we read the golden file irrespective of an operating\n\t# system used. Therefore, we need to strip those \\r's from newlines\n\t# unconditionally.\n\tgolden = ToUnixLineEnding(golden_file.read().decode())\n\tgolden_file.close()\n\n\t# We want the test to pass regardless of certain features being\n\t# supported or not.\n\n\t# We still have to remove type name specifics in all cases.\n\tnormalized_actual = RemoveTypeInfoDetails(output)\n\tnormalized_golden = RemoveTypeInfoDetails(golden)\n\n\tif CAN_GENERATE_GOLDEN_FILE:\n\t  self.assertEqual(normalized_golden, normalized_actual,\n\t\t\t\t\t   '\\n'.join(difflib.unified_diff(\n\t\t\t\t\t\t   normalized_golden.split('\\n'),\n\t\t\t\t\t\t   normalized_actual.split('\\n'),\n\t\t\t\t\t\t   'golden', 'actual')))\n\telse:\n\t  normalized_actual = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(normalized_actual))\n\t  normalized_golden = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(self.RemoveUnsupportedTests(normalized_golden)))\n\n\t  # This code is very handy when debugging golden file differences:\n\t  if os.getenv('DEBUG_GTEST_OUTPUT_TEST'):\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_googletest-output-test_normalized_actual.txt'), 'wb').write(\n\t\t\t\tnormalized_actual)\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_googletest-output-test_normalized_golden.txt'), 'wb').write(\n\t\t\t\tnormalized_golden)\n\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\n", "description": null, "category": "remove", "imports": ["import difflib", "import os", "import re", "import sys", "import gtest_test_utils"]}], [{"term": "def", "name": "RemoveUnsupportedTests", "data": "  def RemoveUnsupportedTests(self, test_output):\n\tif not SUPPORTS_DEATH_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'DeathTest')\n\tif not SUPPORTS_TYPED_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'TypedTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypedDeathTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypeParamDeathTest')\n\tif not SUPPORTS_THREADS:\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ExpectFailureWithThreadsTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ScopedFakeTestPartResultReporterTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'WorksConcurrently')\n\tif not SUPPORTS_STACK_TRACES:\n\t  test_output = RemoveStackTraces(test_output)\n\n\treturn test_output\n", "description": null, "category": "remove", "imports": ["import os", "import re", "import sys", "import gtest_test_utils"]}, {"term": "def", "name": "testOutput", "data": "  def testOutput(self):\n\toutput = GetOutputOfAllCommands()\n\n\tgolden_file = open(GOLDEN_PATH, 'rb')\n\t# A mis-configured source control system can cause \\r appear in EOL\n\t# sequences when we read the golden file irrespective of an operating\n\t# system used. Therefore, we need to strip those \\r's from newlines\n\t# unconditionally.\n\tgolden = ToUnixLineEnding(golden_file.read())\n\tgolden_file.close()\n\n\t# We want the test to pass regardless of certain features being\n\t# supported or not.\n\n\t# We still have to remove type name specifics in all cases.\n\tnormalized_actual = RemoveTypeInfoDetails(output)\n\tnormalized_golden = RemoveTypeInfoDetails(golden)\n\n\tif CAN_GENERATE_GOLDEN_FILE:\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\telse:\n\t  normalized_actual = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(normalized_actual))\n\t  normalized_golden = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(self.RemoveUnsupportedTests(normalized_golden)))\n\n\t  # This code is very handy when debugging golden file differences:\n\t  if os.getenv('DEBUG_GTEST_OUTPUT_TEST'):\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_actual.txt'), 'wb').write(\n\t\t\t\tnormalized_actual)\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_golden.txt'), 'wb').write(\n\t\t\t\tnormalized_golden)\n\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\n", "description": null, "category": "remove", "imports": ["import os", "import re", "import sys", "import gtest_test_utils"]}], [{"term": "def", "name": "RemoveUnsupportedTests", "data": "  def RemoveUnsupportedTests(self, test_output):\n\tif not SUPPORTS_DEATH_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'DeathTest')\n\tif not SUPPORTS_TYPED_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'TypedTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypedDeathTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypeParamDeathTest')\n\tif not SUPPORTS_THREADS:\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ExpectFailureWithThreadsTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ScopedFakeTestPartResultReporterTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'WorksConcurrently')\n\tif not SUPPORTS_STACK_TRACES:\n\t  test_output = RemoveStackTraces(test_output)\n\n\treturn test_output\n", "description": null, "category": "remove", "imports": ["import os", "import re", "import sys", "import gtest_test_utils"]}, {"term": "def", "name": "testOutput", "data": "  def testOutput(self):\n\toutput = GetOutputOfAllCommands()\n\n\tgolden_file = open(GOLDEN_PATH, 'rb')\n\t# A mis-configured source control system can cause \\r appear in EOL\n\t# sequences when we read the golden file irrespective of an operating\n\t# system used. Therefore, we need to strip those \\r's from newlines\n\t# unconditionally.\n\tgolden = ToUnixLineEnding(golden_file.read())\n\tgolden_file.close()\n\n\t# We want the test to pass regardless of certain features being\n\t# supported or not.\n\n\t# We still have to remove type name specifics in all cases.\n\tnormalized_actual = RemoveTypeInfoDetails(output)\n\tnormalized_golden = RemoveTypeInfoDetails(golden)\n\n\tif CAN_GENERATE_GOLDEN_FILE:\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\telse:\n\t  normalized_actual = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(normalized_actual))\n\t  normalized_golden = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(self.RemoveUnsupportedTests(normalized_golden)))\n\n\t  # This code is very handy when debugging golden file differences:\n\t  if os.getenv('DEBUG_GTEST_OUTPUT_TEST'):\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_actual.txt'), 'wb').write(\n\t\t\t\tnormalized_actual)\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_golden.txt'), 'wb').write(\n\t\t\t\tnormalized_golden)\n\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\n", "description": null, "category": "remove", "imports": ["import os", "import re", "import sys", "import gtest_test_utils"]}], [{"term": "def", "name": "RemoveUnsupportedTests", "data": "  def RemoveUnsupportedTests(self, test_output):\n\tif not SUPPORTS_DEATH_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'DeathTest')\n\tif not SUPPORTS_TYPED_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'TypedTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypedDeathTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypeParamDeathTest')\n\tif not SUPPORTS_THREADS:\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ExpectFailureWithThreadsTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ScopedFakeTestPartResultReporterTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'WorksConcurrently')\n\tif not SUPPORTS_STACK_TRACES:\n\t  test_output = RemoveStackTraces(test_output)\n\n\treturn test_output\n", "description": null, "category": "remove", "imports": ["import os", "import re", "import sys", "import gtest_test_utils"]}, {"term": "def", "name": "testOutput", "data": "  def testOutput(self):\n\toutput = GetOutputOfAllCommands()\n\n\tgolden_file = open(GOLDEN_PATH, 'rb')\n\t# A mis-configured source control system can cause \\r appear in EOL\n\t# sequences when we read the golden file irrespective of an operating\n\t# system used. Therefore, we need to strip those \\r's from newlines\n\t# unconditionally.\n\tgolden = ToUnixLineEnding(golden_file.read())\n\tgolden_file.close()\n\n\t# We want the test to pass regardless of certain features being\n\t# supported or not.\n\n\t# We still have to remove type name specifics in all cases.\n\tnormalized_actual = RemoveTypeInfoDetails(output)\n\tnormalized_golden = RemoveTypeInfoDetails(golden)\n\n\tif CAN_GENERATE_GOLDEN_FILE:\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\telse:\n\t  normalized_actual = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(normalized_actual))\n\t  normalized_golden = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(self.RemoveUnsupportedTests(normalized_golden)))\n\n\t  # This code is very handy when debugging golden file differences:\n\t  if os.getenv('DEBUG_GTEST_OUTPUT_TEST'):\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_actual.txt'), 'wb').write(\n\t\t\t\tnormalized_actual)\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_golden.txt'), 'wb').write(\n\t\t\t\tnormalized_golden)\n\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\n", "description": null, "category": "remove", "imports": ["import os", "import re", "import sys", "import gtest_test_utils"]}], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\n\tdef __init__(self, data=None, name=\"\", file=None, **attr):\n\t\tGraph.__init__(self, data=data, name=name, **attr)\n\t\tif file is None:\n\t\t\timport sys\n\n\t\t\tself.fh = sys.stdout\n\t\telse:\n\t\t\tself.fh = open(file, \"w\")\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tGraph.add_node(self, n, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add node: {n}\\n\")\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self, n):\n\t\tGraph.remove_node(self, n)\n\t\tself.fh.write(f\"Remove node: {n}\\n\")\n\n\tdef remove_nodes_from(self, nodes):\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tGraph.add_edge(self, u, v, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add edge: {u}-{v}\\n\")\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.add_edge(u, v, attr_dict=attr_dict, **attr)\n\n\tdef remove_edge(self, u, v):\n\t\tGraph.remove_edge(self, u, v)\n\t\tself.fh.write(f\"Remove edge: {u}-{v}\\n\")\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.remove_edge(u, v)\n\n\tdef clear(self):\n\t\tGraph.clear(self)\n\t\tself.fh.write(\"Clear graph\\n\")\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["import matplotlib.pyplot as plt", "import networkx as nx", "from networkx import Graph", "\t\t\timport sys"]}], [], [], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\n\tdef __init__(self, data=None, name=\"\", file=None, **attr):\n\t\tsuper().__init__(data=data, name=name, **attr)\n\t\tif file is None:\n\t\t\timport sys\n\n\t\t\tself.fh = sys.stdout\n\t\telse:\n\t\t\tself.fh = open(file, \"w\")\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tsuper().add_node(n, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add node: {n}\\n\")\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self, n):\n\t\tsuper().remove_node(n)\n\t\tself.fh.write(f\"Remove node: {n}\\n\")\n\n\tdef remove_nodes_from(self, nodes):\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tsuper().add_edge(u, v, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add edge: {u}-{v}\\n\")\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.add_edge(u, v, attr_dict=attr_dict, **attr)\n\n\tdef remove_edge(self, u, v):\n\t\tsuper().remove_edge(u, v)\n\t\tself.fh.write(f\"Remove edge: {u}-{v}\\n\")\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.remove_edge(u, v)\n\n\tdef clear(self):\n\t\tsuper().clear()\n\t\tself.fh.write(\"Clear graph\\n\")\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["import matplotlib.pyplot as plt", "import networkx as nx", "from networkx import Graph", "\t\t\timport sys"]}], [{"term": "def", "name": "remove_one", "data": "def remove_one(x,xs): #\uc7ac\uadc0\ud568\uc218\n\tif xs != []:\n\t\tif x == xs[0]:\n\t\t\treturn xs[1:]\n\t\telse:\n\t\t\treturn [xs[0]] + remove_one(x,xs[1:])\n\telse:\n\t\treturn xs\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_one", "data": "def remove_one(x,xs): #\uaf2c\ub9ac\uc7ac\uadc0\n\tdef loop(xs,head):\n\t\tif xs != []:\n\t\t\tif x == xs[0]:\n\t\t\t\treturn head + xs[1:]\n\t\t\telse:\n\t\t\t\thead.append(xs[0])\n\t\t\t\treturn loop(xs[1:],head)\n\t\telse:\n\t\t\treturn head + xs\n\treturn loop (xs,[])\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "remove_one", "data": "def remove_one(x,xs): #while \ub8e8\ud504\n\thead = []\n\twhile xs != []:\n\t\tif x == xs[0]:\n\t\t\thead += xs[1:]\n\t\t\txs = []\n\t\telse:\n\t\t\thead.append(xs[0])\n\t\t\txs = xs[1:]\n\treturn head + xs\n\n", "description": null, "category": "remove", "imports": []}], [], [], [], [], [{"term": "def", "name": "get_text", "data": "def get_text(tag):\n\t\"\"\"return text from hashtags\n\n\tArgs:\n\t\ttag (object): object with hashtags\n\n\tReturns:\n\t\tstring: the text from a hashtag\n\t\"\"\"\n\treturn tag.get('text').lower()\n\n", "description": "return text from hashtags\n\n\tArgs:\n\t\ttag (object): object with hashtags\n\n\tReturns:\n\t\tstring: the text from a hashtag\n\t", "category": "remove", "imports": ["import sys", "import string", "import pickle", "import json", "import matplotlib.dates as mdates", "import pandas as pd", "import numpy as np", "import many_stop_words", "import matplotlib.pyplot as plt", "import matplotlib.dates as mdates", "from collections import Counter, defaultdict", "from nltk.tokenize import TweetTokenizer", "from nltk.corpus import stopwords", "from datetime import datetime", "from datetime import datetime"]}, {"term": "def", "name": "get_hashtags", "data": "def get_hashtags(tweet):\n\t\"\"\"return hashtags from a given tweet\n\n\tArgs:\n\t\ttweet (object): an object representing a tweet\n\n\tReturns:\n\t\tlist: list of hastags in a tweet\n\t\"\"\"\n\tentities = tweet.get('entities', {})\n\thashtags = entities.get('hashtags', [])\n\treturn [get_text(tag) for tag in hashtags if get_text(\n\t\ttag) not in ['rdc', 'drc', 'rdcongo', 'drcongo']]\n\n", "description": "return hashtags from a given tweet\n\n\tArgs:\n\t\ttweet (object): an object representing a tweet\n\n\tReturns:\n\t\tlist: list of hastags in a tweet\n\t", "category": "remove", "imports": ["import sys", "import string", "import pickle", "import json", "import matplotlib.dates as mdates", "import pandas as pd", "import numpy as np", "import many_stop_words", "import matplotlib.pyplot as plt", "import matplotlib.dates as mdates", "from collections import Counter, defaultdict", "from nltk.tokenize import TweetTokenizer", "from nltk.corpus import stopwords", "from datetime import datetime", "from datetime import datetime"]}, {"term": "def", "name": "read_tweets_file", "data": "def read_tweets_file(path):\n\t\"\"\" function which read a file with tweets\n\n\tArgs:\n\t\tpath (string): path for the file of tweets\n\n\tReturns:\n\t\titerator: an iterator of tweets obejcts\n\t\"\"\"\n\twith open(path, 'r') as f:\n\t\tfor line in f:\n\t\t\ttweet = json.loads(line)\n\t\t\tyield tweet\n\n", "description": " function which read a file with tweets\n\n\tArgs:\n\t\tpath (string): path for the file of tweets\n\n\tReturns:\n\t\titerator: an iterator of tweets obejcts\n\t", "category": "remove", "imports": ["import sys", "import string", "import pickle", "import json", "import matplotlib.dates as mdates", "import pandas as pd", "import numpy as np", "import many_stop_words", "import matplotlib.pyplot as plt", "import matplotlib.dates as mdates", "from collections import Counter, defaultdict", "from nltk.tokenize import TweetTokenizer", "from nltk.corpus import stopwords", "from datetime import datetime", "from datetime import datetime"]}, {"term": "def", "name": "get_most_common_hashtags", "data": "def get_most_common_hashtags(path, number):\n\t\"\"\"\n\tfind the most common hash tags in a corpus of tweets\n\n\tArgs:\n\t\tpath (string): path of the tweet files\n\t\tnumber (int): number of most frequent hashtags to return\n\tReturns :\n\t\tan iterator of most tags and their count\n\t\"\"\"\n\thastags = Counter()\n\tfor tweet in read_tweets_file(TWEETS_PATH):\n\t\thashtags_in_tweet = get_hashtags(tweet)\n\t\thastags.update(hashtags_in_tweet)\n\tfor tag, count in hastags.most_common(number):\n\t\tyield tag, count\n\n", "description": "\n\tfind the most common hash tags in a corpus of tweets\n\n\tArgs:\n\t\tpath (string): path of the tweet files\n\t\tnumber (int): number of most frequent hashtags to return\n\tReturns :\n\t\tan iterator of most tags and their count\n\t", "category": "remove", "imports": ["import sys", "import string", "import pickle", "import json", "import matplotlib.dates as mdates", "import pandas as pd", "import numpy as np", "import many_stop_words", "import matplotlib.pyplot as plt", "import matplotlib.dates as mdates", "from collections import Counter, defaultdict", "from nltk.tokenize import TweetTokenizer", "from nltk.corpus import stopwords", "from datetime import datetime", "from datetime import datetime"]}, {"term": "def", "name": "process_text", "data": "def process_text(text, tokenizer=TweetTokenizer(), words_to_remove=[]):\n\t\"\"\"\n\tclean a tweet text\n\n\tArgs:\n\t\ttext (string): text of a tweet\n\t\ttokenizer (TweetTokenizer, optional): tokenizer to use from NLTK. Defaults to TweetTokenizer().\n\t\twords_to_remove (list, optional): list of words to remove. Defaults to [].\n\n\tReturns:\n\t\tlist: tokens from a tweet\n\t\"\"\"\n\ttext = text.lower()\n\ttokens = tokenizer.tokenize(text)\n\treturn [\n\t\ttoken for token in tokens if token not in words_to_remove and not token.isdigit()]\n\n", "description": "\n\tclean a tweet text\n\n\tArgs:\n\t\ttext (string): text of a tweet\n\t\ttokenizer (TweetTokenizer, optional): tokenizer to use from NLTK. Defaults to TweetTokenizer().\n\t\twords_to_remove (list, optional): list of words to remove. Defaults to [].\n\n\tReturns:\n\t\tlist: tokens from a tweet\n\t", "category": "remove", "imports": ["import sys", "import string", "import pickle", "import json", "import matplotlib.dates as mdates", "import pandas as pd", "import numpy as np", "import many_stop_words", "import matplotlib.pyplot as plt", "import matplotlib.dates as mdates", "from collections import Counter, defaultdict", "from nltk.tokenize import TweetTokenizer", "from nltk.corpus import stopwords", "from datetime import datetime", "from datetime import datetime"]}, {"term": "def", "name": "get_words_to_remove", "data": "def get_words_to_remove():\n\t\"\"\"\n\tgenerate a list of words to remove for a better cleaning of tweets\n\tReturns:\n\t\tset : an array of words to remove\n\t\"\"\"\n\tpunctuation = list(string.punctuation)\n\tstop_word_list_english = stopwords.words('english')\n\tstop_word_list_french = stopwords.words('french')\n\tothers_words = ['rt', 'via', '...', '\u2026', '\u00bb:', '\u00ab:', '\u2019:', 'les', '-', ]\n\twords_to_remove = punctuation + stop_word_list_english + \\\n\t\tstop_word_list_french + others_words\n\tcongo_words = {\n\t\t'congo',\n\t\t'congolais',\n\t\t'rdc',\n\t\t'drc',\n\t\t'-',\n\t\t'https',\n\t\t'rdcongo',\n\t\t'drc',\n\t\t'drcongo', }\n\twords_to_remove = set(words_to_remove).union(congo_words)\n\twords_to_remove = words_to_remove.union(\n\t\tset(many_stop_words.get_stop_words('fr')))\n\treturn words_to_remove\n\n", "description": "\n\tgenerate a list of words to remove for a better cleaning of tweets\n\tReturns:\n\t\tset : an array of words to remove\n\t", "category": "remove", "imports": ["import sys", "import string", "import pickle", "import json", "import matplotlib.dates as mdates", "import pandas as pd", "import numpy as np", "import many_stop_words", "import matplotlib.pyplot as plt", "import matplotlib.dates as mdates", "from collections import Counter, defaultdict", "from nltk.tokenize import TweetTokenizer", "from nltk.corpus import stopwords", "from datetime import datetime", "from datetime import datetime"]}, {"term": "def", "name": "get_most_common_words", "data": "def get_most_common_words(number, words_to_remove, path=TWEETS_PATH):\n\t\"\"\"\n\tfind the most common words in a corpus of tweets\n\tArgs:\n\t\tpath (string): path of the tweet files\n\t\tnumber (int): number of most frequent hashtags to return\n\t\tword_to_remove: list of words to remove\n\tReturns :\n\t\tan iterator most frequent words  and their count\n\t\"\"\"\n\tterm_counts = Counter()\n\tfor tweet in read_tweets_file(TWEETS_PATH):\n\t\ttokens = process_text(\n\t\t\ttext=tweet.get('text'),\n\t\t\twords_to_remove=words_to_remove)\n\t\tterm_counts.update(tokens)\n\tfor tag, count in term_counts.most_common(number):\n\t\tyield tag, count\n", "description": "\n\tfind the most common words in a corpus of tweets\n\tArgs:\n\t\tpath (string): path of the tweet files\n\t\tnumber (int): number of most frequent hashtags to return\n\t\tword_to_remove: list of words to remove\n\tReturns :\n\t\tan iterator most frequent words  and their count\n\t", "category": "remove", "imports": ["import sys", "import string", "import pickle", "import json", "import matplotlib.dates as mdates", "import pandas as pd", "import numpy as np", "import many_stop_words", "import matplotlib.pyplot as plt", "import matplotlib.dates as mdates", "from collections import Counter, defaultdict", "from nltk.tokenize import TweetTokenizer", "from nltk.corpus import stopwords", "from datetime import datetime", "from datetime import datetime"]}], [{"term": "def", "name": "simple_tokenizer", "data": "def simple_tokenizer(text):\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "is_punctuation", "data": "def is_punctuation(word):\n\tpunkts = string.punctuation\n\ttested_chars = [i for i in word if i in punkts]\n\treturn len(word) == len(tested_chars)\n\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "tokenizer", "data": "def tokenizer(text, lang, remove_punkt=True):\n\t'''\n\ttext : any string in lang\n\tlang : langauge of the string (english, turkish..) \n\tremove_punkt: if true, remove the punctuation tokens\n\t'''\n\n\t# @TODO if lang not found, use english\n\t#tokens = nltktokenizer.word_tokenize(text, language=lang)\n\ttokens = nltktokenizer.wordpunct_tokenize(text)\n\t# t = Text(text, hint_language_code=\"tr\")\n\t# tokens = list(t.tokens)\n\t\n\tif remove_punkt:\n\t\ttokens = [token for token in tokens if not is_punctuation(token)]\n\t\n\ttokens = eliminate_empty_strings(tokens)\n\tif lang not in [\"ar\"]:\n\t\ttokens = [token for token in tokens if token.isalnum()]\t# this is already eliminating the punctuation!\n\treturn tokens\n\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "stem_words", "data": "def stem_words(words, lang):\n\troots = []\n\tif lang in [\"tr\", \"turkish\"]:\n\t\troots = tr_stemmer.stem_words(words)\n\t\n\tif lang in [\"en\", \"english\"]:\n\t\troots = [en_stemmer.stem1(word) for word in words]\n\t\n\tif lang in [\"ar\", \"arabic\", \"arab\"]:\n\t\troots = [ar_stemmer.stem(word) for word in words]\n\treturn roots\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "deasciify_words", "data": "def deasciify_words(words, lang):\n\t\n\tif lang in [\"tr\", \"turkish\"]:\n\t\treturn [Deasciifier(token).convert_to_turkish() for token in words]\n\telse:\n\t\treturn words\n\t'''\n\tif lang in [\"en\", \"english\", \"ar\", \"arab\", \"arabic\"]:  # not applicable for english\n\t\treturn words\n\t'''\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "spellcheck_words", "data": "def spellcheck_words(words, lang):\n\t\n\tif lang in [\"en\", \"english\"]:\n\t\treturn [en_spellchecker.spellcheck(token) for token in words]\n\tif lang in [\"tr\", \"turkish\"]:  # not yet for turkish\n\t\treturn words\n\t\t\n\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "eliminate_empty_strings", "data": "def eliminate_empty_strings(wordlist):\n\tl = [w.strip() for w in wordlist]\n\tl = [w for w in l if len(w) > 0]\n\treturn l\n\n\n\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "language_map", "data": "def language_map(lang_shortcut):\n\tlangmap = { \"tr\" : \"turkish\",\n\t\t\t\t\"en\" : \"english\",\n\t\t\t\t\"eng\" : \"english\",\n\t\t\t\t\"ar\" : \"arabic\",\n\t\t\t\t\"arab\" : \"arabic\"\n\t\t\t  }\n\t\n\treturn langmap[lang_shortcut]\n\n\n\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "original_to_preprocessed_map", "data": "def original_to_preprocessed_map(preprocessor, text):\n\t\n\twords = text.split()\n\twords_prep = []\n\tfor word in words:\n\t\tprepword = preprocessor.tokenize(word)\n\t\tif prepword:\n\t\t\tprepword = prepword[0]\n\t\telse:\n\t\t\tprepword = \"\"\n\t\t\n\t\twords_prep.append((prepword, word))\n\t\n\tprep_word_map = {}\n\tfor x, y in words_prep:\n\t\tprep_word_map.setdefault(x, []).append(y)\n\t\n\treturn prep_word_map\n\n\n\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "class", "name": "Preprocessor2", "data": "class Preprocessor2(BaseEstimator, TransformerMixin):\n\t\"\"\"\n\tTransforms input data by using tokenization and\n\tother normalization and filtering techniques.\n\t\"\"\"\n\tlang = \"\"\n\tstopword = \"\"\n\tspellcheck = \"\"\n\tstemming = \"\"\n\tremove_numbers = \"\"\n\tdeasciify = \"\"\n\tremove_punkt = \"\"\n\tlowercase = \"\"\n\tdef __init__(self, \n\t\t\t\t lang,\n\t\t\t\t stopword=False, \n\t\t\t\t spellcheck=False,\n\t\t\t\t stemming=False,\n\t\t\t\t remove_numbers=False,\n\t\t\t\t deasciify=False,\n\t\t\t\t remove_punkt=True,\n\t\t\t\t lowercase=True):\n\t\t\n\t\t\n\t\tself.lang = lang\n\t\tself.stopword = stopword\n\t\tself.spellcheck = spellcheck\n\t\tself.stemming = stemming\n\t\tself.remove_numbers = remove_numbers\n\t\tself.deasciify = deasciify\n\t\tself.remove_punkt = remove_punkt\n\t\tself.lowercase = lowercase\n\t\t\n\n\n\tdef fit(self, X, y=None):\n\t\t\"\"\"\n\t\tFit simply returns self, no other information is needed.\n\t\t\"\"\"\n\t\t\n\t\treturn self\n\n\tdef inverse_transform(self, X):\n\t\t\"\"\"\n\t\tNo inverse transformation\n\t\t\"\"\"\n\t\treturn X\n\n\tdef transform(self, X):\n\t\t\"\"\"\n\t\tActually runs the preprocessing on each document.\n\t\t\"\"\"\n\t\treturn X\n\t\n\n", "description": "\n\tTransforms input data by using tokenization and\n\tother normalization and filtering techniques.\n\t", "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "class", "name": "Preprocessor", "data": "class Preprocessor(BaseEstimator, TransformerMixin):\n\t\"\"\"\n\tTransforms input data by using tokenization and\n\tother normalization and filtering techniques.\n\t\"\"\"\n\tdef __init__(self, lang,\n\t\t\t\t stopword=True, more_stopwords=None,\n\t\t\t\t spellcheck=False,\n\t\t\t\t stemming=False,\n\t\t\t\t remove_numbers=False,\n\t\t\t\t deasciify=False,\n\t\t\t\t remove_punkt=True,\n\t\t\t\t lowercase=True):\n\t\t# these paramater names should be same as the class variable names (like lang -> lang). otherwise, the param. values cannot be transferred to the sk.pipeline.\n\t\tself.lang = lang\n\t\tself.stopword = stopword\n\t\tself.more_stopwords = more_stopwords\n\t\tself.spellcheck = spellcheck\n\t\tself.stemming = stemming\n\t\tself.remove_numbers = remove_numbers\n\t\tself.deasciify = deasciify\n\t\tself.remove_punkt = remove_punkt\n\t\tself.lowercase = lowercase\n\n\t\tself.notfound = 0\n\t\tself.surface_root_pairs = {}   # {word : root} for faster stemming, we stem all the words in all docs once - not to call tr_morph via os multiple times; then, retrieve the root form from this dict upon tokenizing - if stemming is True.\n\t\t \n\t\t\n\n\tdef fit(self, X, y=None):\n\t\t\"\"\"\n\t\tFit simply returns self, no other information is needed.\n\t\t\"\"\"\n\t\t\n\t\treturn self\n\n\tdef inverse_transform(self, X):\n\t\t\"\"\"\n\t\tNo inverse transformation\n\t\t\"\"\"\n\t\treturn X\n\n\tdef transform(self, X):\n\t\t\"\"\"\n\t\tActually runs the preprocessing on each document.\n\t\t\"\"\"\n\t\t\n\t\tif self.stemming:\n\t\t\twords = []\n\t\t\tfor doc in X:\n\t\t\t\tdoc_tokens = tokenizer(doc, lang=language_map(self.lang), remove_punkt=self.remove_punkt)\n\t\t\t\twords.extend(doc_tokens)\n\t\t\t\n\t\t\tif self.lowercase and self.lang not in [\"ar\", \"arab\", \"arabic\"]:\n\t\t\t\twords = [token.lower() for token in words]\n\t\t\n\t\t\tif self.deasciify:\n\t\t\t\twords = deasciify_words(words, self.lang)\n\t\t\t\t\n\t\t\tif self.spellcheck:\n\t\t\t\twords = spellcheck_words(words, self.lang)\n\t\t\t\n\t\t\troots = stem_words(words, lang=self.lang)\n\t\t\tl = [(word, root) for word, root in zip(words, roots)]\n\t\t\tl.append((\"\", \"\"))\n\t\t\tself.surface_root_pairs = dict(l)\n\t\t\n\t\treturn [self.tokenize(doc) for doc in X]\n\t\n \n\t   \n   \n\tdef tokenize(self, doc):\n\t\ttokens = tokenizer(doc, lang=language_map(self.lang), remove_punkt=self.remove_punkt) \n\t\t\t\t\t   \n\t\tif self.lowercase and self.lang not in [\"ar\", \"arab\", \"arabic\"]:\n\t\t\ttokens = [token.lower() for token in tokens]\n\t\t\n\t\t# problem: \"\u0130\" is lowercased to \"i\u0307\"\n\t\t# i = 'i\u0307'\n\t\t# tokens = [token.replace(i, \"i\") for token in tokens]\t\t\n\t\t\n\t\tif self.deasciify:\n\t\t\ttokens = deasciify_words(tokens, self.lang)\n\t\t\t\n\t\tif self.spellcheck:\n\t\t\ttokens = spellcheck_words(tokens, self.lang)\n\t\tif self.remove_numbers:\n\t\t\tnumber_pattern = \"[a-zA-z]{,3}\\d+\"  # d{6,}  # TODO real numbers & rational numbers\n\t\t\ttokens = [re.sub(number_pattern, \"\", token) for token in tokens]\n\t\t\n\t\tif self.stopword:\n\t\t\tstopwords = stopword_lists.get_stopwords(lang=self.lang)\t\t  \n\t\t\ttokens = [token for token in tokens if token not in stopwords]  \n\t\tif self.more_stopwords:  # len(self.more_stopwords) > 0:\t# re-organize not to have this list through memory but disc\n\t\t\ttokens = [token for token in tokens if token not in self.more_stopwords]\n\t\t\n\t\tif self.stemming:\n\t\t\t#tokens = stem_words(tokens, lang=self.lang)\n\t\t\ttokens_ = []\n\t\t\tfor word in tokens:\n\t\t\t\ttry:\n\t\t\t\t\troot = self.surface_root_pairs[word]\n\t\t\t\t\t#print(0, root, \"  \", word)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tself.notfound += 1\n\t\t\t\t\t#print(self.notfound, \"  \", word)\n\t\t\t\t\troot = tr_stemmer.stem(word)\n\t\t\t\t\tself.surface_root_pairs[word] = root\n\t\t\t\tfinally:\n\t\t\t\t\ttokens_.append(root)\n\t\t\ttokens = tokens_   \n\t\t\t#tokens = [self.surface_root_pairs[word] for word in tokens]\n\n\t\t'''\n\t\tif self.stopword:\n\t\t\tstopwords = stopword_lists.get_stopwords(lang=self.lang)\t\t\t\t\t  \n\t\t\ttokens = [token for token in tokens if token not in stopwords]  \n\t\t\t  \n\t\tif self.more_stopwords:  # len(self.more_stopwords) > 0:\t# re-organize not to have this list through memory but disc\n\t\t\ttokens = [token for token in tokens if token not in self.more_stopwords]\n\t\t'''\n\t\t\t\n\t\ttokens = eliminate_empty_strings(tokens)\n\t  \n\t\t#print(doc,\"  -> \", tokens)\n\t\t\n\t\treturn tokens   \n\n\n\n\n\n\n", "description": "\n\tTransforms input data by using tokenization and\n\tother normalization and filtering techniques.\n\t", "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "identity", "data": "def identity(arg):\n\t\"\"\"\n\tSimple identity function works as a passthrough.\n\t\"\"\"\n\treturn arg\n\n", "description": "\n\tSimple identity function works as a passthrough.\n\t", "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "run_prep", "data": "def run_prep():\n\t\n\t\n\tclassifier = sklinear.SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42) \n\n\tlang = \"tr\"\n\tstopword_choice = True\n\tmore_stopwords_list = None\n\tspellcheck_choice = False\n\tstemming_choice = False\n\tnumber_choice = False\n\tdeasc_choice = True\n\tpunct_choice = True\n\tcase_choice = True\n\t\n\tngramrange = (1, 2)  # tuple\n\tnmaxfeature = 10000  # int or None  \n\tnorm = \"l2\"\n\tuse_idf = True\n\t\t\t\t \n\tpreprocessor = Preprocessor(lang=lang,\n\t\t\t\t\t\t\t\t stopword=stopword_choice, more_stopwords=more_stopwords_list,\n\t\t\t\t\t\t\t\t spellcheck=spellcheck_choice,\n\t\t\t\t\t\t\t\t stemming=stemming_choice,\n\t\t\t\t\t\t\t\t remove_numbers=number_choice,\n\t\t\t\t\t\t\t\t deasciify=deasc_choice,\n\t\t\t\t\t\t\t\t remove_punkt=punct_choice,\n\t\t\t\t\t\t\t\t lowercase=case_choice\n\t\t\t\t\t\t\t\t)\n\ttfidfvect = TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False,\n\t\t\t\t\t\t\t\tuse_idf=use_idf, ngram_range=ngramrange, max_features=nmaxfeature)\n\n\t\n\tkeyword = \"ar\u0131za\"\n\tapipe = tbt.get_keyword_pipeline(keyword)\n\tkeyword2 = \"pstn\"\n\tpstnpipe = tbt.get_keyword_pipeline(keyword2)\n\tpolpipe1 = tbt.get_polylglot_polarity_count_pipe(lang)\n\tpolpipe2 = tbt.get_polylglot_polarity_value_pipe(lang)\n\tpolpipe3 = obt.get_lexicon_count_pipeline(tokenizer=identity)\n\t\n\ttokenizedpipe = skpipeline.Pipeline([('preprocessor', preprocessor),\n\t\t\t\t\t\t\t\t\t\t ('union1',\n\t\t\t\t\t\t\t\t\t\t  skpipeline.FeatureUnion(\n\t\t\t\t\t\t\t\t\t\t\t  transformer_list=[\n\t\t\t\t\t\t\t\t\t\t ('vect', tfidfvect),\n\t\t\t\t\t\t\t\t\t\t ('polarity3', polpipe3), ])), ]\n\t\t\t\t\t\t\t\t\t\t)\n\t\n\ttextbasedpipe = skpipeline.Pipeline([('union2', skpipeline.FeatureUnion([\n\t\t\t\t\t\t\t\t\t\t ('has_ariza', apipe),\n\t\t\t\t\t\t\t\t\t\t ('has_pstn', pstnpipe),\n\t\t\t\t\t\t\t\t\t\t ('polarity1', polpipe1),\n\t\t\t\t\t\t\t\t\t\t ('polarity2', polpipe2), ]),)])\n\t\n\tmodel = skpipeline.Pipeline([\n\t\t\n\t\t# ('preprocessor', preprocessor),\n\t\t\n\t\t(\"union\", skpipeline.FeatureUnion(transformer_list=[\n\t\t\t\n\t\t\t('tfidf', tokenizedpipe),\n\t\t\t\n\t\t\t('txtpipe', textbasedpipe),\n\t\t\t\n\t\t\t])\n\t\t ),\n\t\t\t\n\t\t('classifier', classifier),\n\t\t])\n\t\n\tt0 = time()\n\tprint(\"Read data\")\n\tinstances, labels = get_data.get_data()\n\t\n\tN = 100\n\tinstances, labels = corpus_io.select_N_instances(N, instances, labels)\n\t# instances_train, instances_test, ytrain, ytest = cv.train_test_split(instances, labels, test_size=0.30, random_state=20)\n\t\n\tprint(\"Start classification\\n..\")\n\tnfolds = 5\n\typred = cv.cross_val_predict(model, instances, labels, cv=nfolds)\n\ttc_utils.get_performance(labels, ypred, verbose=True)\n\tt1 = time()\n\tprint(\"Classification took \", round(t1 - t0, 2), \"sec.\")\n\n\n\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "tokenize", "data": "def tokenize(doc, lang=\"tr\", stopword=False, more_stopwords=None,\n\t\t\t\t spellcheck=False,\n\t\t\t\t stemming=True,\n\t\t\t\t remove_numbers=False,\n\t\t\t\t deasciify=False,\n\t\t\t\t remove_punkt=False,\n\t\t\t\t lowercase=False):\n\t#tokens = tokenizer(doc, lang=language_map(lang), remove_punkt=remove_punkt) \n\ttokens = doc.split() \n\t\t\t\t  \n\tif lowercase and lang not in [\"ar\", \"arab\", \"arabic\"]:\n\t\ttokens = [token.lower() for token in tokens]\n\t\n\t# problem: \"\u0130\" is lowercased to \"i\u0307\"\n\t# i = 'i\u0307'\n\t# tokens = [token.replace(i, \"i\") for token in tokens]\t\t\n\t\n\tif deasciify:\n\t\ttokens = deasciify_words(tokens, lang)\n\t\t\n\tif spellcheck:\n\t\ttokens = spellcheck_words(tokens, lang)\n\tif remove_numbers:\n\t\tnumber_pattern = \"[a-zA-z]{,3}\\d+\"  # d{6,}  # TODO real numbers & rational numbers\n\t\ttokens = [re.sub(number_pattern, \"\", token) for token in tokens]\n\t\n\tif stopword:\n\t\tstopwords = stopword_lists.get_stopwords(lang=lang)\t\t  \n\t\ttokens = [token for token in tokens if token not in stopwords]  \n\tif more_stopwords:  # len(more_stopwords) > 0:\t# re-organize not to have this list through memory but disc\n\t\ttokens = [token for token in tokens if token not in more_stopwords]\n\t\n\tif stemming:\n\t\t#tokens = stem_words(tokens, lang=lang)\n\t\ttokens = tr_stemmer.stem_words(tokens)\n\t'''\n\tif stopword:\n\t\tstopwords = stopword_lists.get_stopwords(lang=lang)\t\t\t\t\t  \n\t\ttokens = [token for token in tokens if token not in stopwords]  \n\t\t  \n\tif more_stopwords:  # len(more_stopwords) > 0:\t# re-organize not to have this list through memory but disc\n\t\ttokens = [token for token in tokens if token not in more_stopwords]\n\t'''\n\t\t\n\ttokens = eliminate_empty_strings(tokens)\n  \n\t#print(doc,\"  -> \", tokens)\n\t\n\treturn tokens   \n   \n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}, {"term": "def", "name": "main", "data": "def main():\t\n\t\t\n\tprint()\n\n\t\n\t# tweet polarity preprocessor\n\tx = Preprocessor(lang=\"tr\", stopword=True, more_stopwords=None,\n\t\t\t\t spellcheck=False,\n\t\t\t\t stemming=True,\n\t\t\t\t remove_numbers=True,\n\t\t\t\t deasciify=True,\n\t\t\t\t remove_punkt=True,\n\t\t\t\t lowercase=True)\n\n\t\n\t\n\t'''\n\tsentences = [\"film guzelmis.\",\n\t\t\t\t \"filmler g\u00fczelmi\u015f.\",\n\t\t\t\t\t  \"film g\u00fczelmi\u015f.\",\n\t\t\t\t\t  \"film g\u00fczelmi\u015f..\",\n\t\t\t\t\t  \"film g\u00fczelmi\u015f\",\n\t\t\t\t\t  \"hic begenmedim\",\n\t\t\t\t\t  \"hi\u00e7 be\u011fenmedim.\",\n\t\t\t\t\t  \"kapal\u0131\u0131\u0131\u0131\u0131\",\n\t\t\t\t\t  \"Turkcell 97 liraya veriyor...bo\u015f verin aveay\u0131 Turkcellden al\u0131n =)  \"]\n\t'''\n\tsentences = [\"Turkcell 97 liraya veriyor...bo\u015f verin aveay\u0131 Turkcellden al\u0131n =)  \"]\n\ts2 = [x.tokenize(s) for s in sentences]\n\tprint(s2)\n\tfor i in s2:\n\t\tprint([type(j) for j in i])\n\t\n\ts3 = [\" \".join(i) for i in s2]\n\tprint(list(set(s3)))\n\t\n\ta = ['t\u00fcrkcell', '', 'liraya', 'veriyor', 'bo\u015f', 'verin', 'aveay\u0131', 't\u00fcrkcellden', 'al\u0131n']\n\tprint(\"st1: \", stem_words(a, lang=\"tr\"))\n\tprint(\"st2: \", tr_stemmer.stem_words(a))\n\tprint(\"st3: \", [tr_stemmer.stem(word) for word in a])\n\n\n\n\n", "description": null, "category": "remove", "imports": ["import sys", "# from language_tools import  stopword_lists, tr_stemmer, en_stemmer", "# from language_tools.spellchecker import en_spellchecker ", "import string, re", "from time import time", "from sklearn.base import BaseEstimator, TransformerMixin", "from sklearn.feature_extraction.text import TfidfVectorizer", "from turkish.deasciifier import Deasciifier", "from modules.learning.language_tools import stopword_lists, tr_stemmer, en_stemmer, ar_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "import nltk.tokenize as nltktokenizer", "import sklearn.cross_validation as cv", "import sklearn.linear_model as sklinear", "import sklearn.pipeline as skpipeline", "from modules.learning.text_categorization.utils import tc_utils", "from modules.learning.dataset import corpus_io", "import modules.learning.text_categorization.prototypes.feature_extraction.text_based_transformers as tbt", "import modules.learning.text_categorization.prototypes.feature_extraction.token_based_transformers as obt", "\tfrom dataset import corpus_io", "\timport os"]}], [], [], [], [], [], [], [], [], [{"term": "def", "name": "get_combinations", "data": "def get_combinations():\n\tcombinations = [\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':True,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':True,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':2,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':3,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':True,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':True,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':True,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':True,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':True,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':True,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':2,\n\t\t'text_features':True\n\t},\n\t# dois ao mesmo tempo\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':True,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':False,\n\t\t'remove_punct':True,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':2,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':True,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':True,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':True,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':True,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':True,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':False,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':True,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':True,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':True,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':True,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':2,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':True,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':True,\n\t\t'remove_punct':False,\n\t\t'n_gram':2,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':True,\n\t\t'remove_punct':False,\n\t\t'n_gram':3,\n\t\t'tags':False,\n\t\t'pos':False,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':1,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':False,\n\t\t'stem':True,\n\t\t'remove_punct':False,\n\t\t'n_gram':1,\n\t\t'tags':True,\n\t\t'pos':True,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':False,\n\t\t'lex':False,\n\t\t'sentiment':False,\n\t\t'tag_ngram':2,\n\t\t'text_features':True\n\t},\n\t{\n\t\t'remove_stop_words':True,\n\t\t'stem':True,\n\t\t'remove_punct':True,\n\t\t'n_gram':2,\n\t\t'tags':True,\n\t\t'pos':True,\n\t\t'dep':False,\n\t\t'alpha':False,\n\t\t'ent':True,\n\t\t'lex':True,\n\t\t'sentiment':True,\n\t\t'tag_ngram':2,\n\t\t'text_features':True\n\t}\n\t]\n\treturn combinations\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "use_all_features", "data": "def use_all_features():\n\treturn [{\n\t\t'remove_stop_words':True,\n\t\t'stem':True,\n\t\t'remove_punct':True,\n\t\t'n_gram':2,\n\t\t'tags':True,\n\t\t'pos':True,\n\t\t'dep':True,\n\t\t'alpha':True,\n\t\t'ent':True,\n\t\t'sentiment': True,\n\t\t'lex': True,\n\t\t'tag_ngram': 2\n\t}]\n", "description": null, "category": "remove", "imports": []}, {"term": "def", "name": "use_custom", "data": "def use_custom():\n\treturn [\n\t\t{\n\t\t\t'remove_stop_words':True,\n\t\t\t'stem':False,\n\t\t\t'remove_punct':True,\n\t\t\t'n_gram':1,\n\t\t\t'tags':False,\n\t\t\t'pos':False,\n\t\t\t'dep':False,\n\t\t\t'alpha':False,\n\t\t\t'ent':True,\n\t\t\t'sentiment': True,\n\t\t\t'lex': True,\n\t\t\t'tag_ngram': 1\n\t\t},\n", "description": null, "category": "remove", "imports": []}], [], [{"term": "class", "name": "classLoadData:", "data": "class LoadData:\n\n\tdef __init__(self,\n\t\t\t\t path: str,\n\t\t\t\t indices_to_remove: List[int] = None,\n\t\t\t\t headers: bool = True,\n\t\t\t\t delete_points: bool = False,\n\t\t\t\t sheet_name=0\n\t\t\t\t ):\n\t\t\"\"\"\n\t\tsheet_namestr: int, list, or None, default 0\n\t\tStrings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets.\n\n\t\tAvailable cases:\n\n\t\t* Defaults to 0: 1st sheet as a DataFrame\n\n\t\t* 1: 2nd sheet as a DataFrame\n\n\t\t* \"Sheet1\": Load sheet with name \u201cSheet1\u201d\n\n\t\t* [0, 1, \"Sheet5\"]: Load first, second and sheet named \u201cSheet5\u201d as a dict of DataFrame\n\n\t\t* None: All worksheets.\n\t\t\"\"\"\n\n\t\tself.path = path\n\n\t\t# Transform input to a form which pandas accepts\n\t\tif headers:  # This only handles the case where the headers row is the first one\n\t\t\tself.headers = 0\n\t\telse:\n\t\t\tself.headers = None\n\n\t\tif delete_points:\n\t\t\tif isinstance(indices_to_remove, list):\n\t\t\t\tself.points_to_remove = indices_to_remove\n\t\t\telse:\n\t\t\t\traise TypeError(f'If delete_points is True you must provide a list of points to remove. {type(indices_to_remove)} type was provided.')\n\t\telse:\n\t\t\tself.points_to_remove = None\n\n\t\tif self.path.endswith('.csv'):\n\t\t\tself.data = pd.read_csv(self.path, header=self.headers, skiprows=self.points_to_remove).to_numpy()\n\t\telif self.path.endswith(('xlsx', 'xls', 'xlsm', 'xlsb', 'odf', 'ods', 'odt')):\n\t\t\tself.data = pd.read_excel(self.path, sheet_name=sheet_name, header=self.headers, skiprows=self.points_to_remove).to_numpy()\n\t\telse:\n\t\t\traise TypeError('File type not supported yet.')\n", "description": "\n\t\tsheet_namestr: int, list, or None, default 0\n\t\tStrings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets.\n\n\t\tAvailable cases:\n\n\t\t* Defaults to 0: 1st sheet as a DataFrame\n\n\t\t* 1: 2nd sheet as a DataFrame\n\n\t\t* \"Sheet1\": Load sheet with name \u201cSheet1\u201d\n\n\t\t* [0, 1, \"Sheet5\"]: Load first, second and sheet named \u201cSheet5\u201d as a dict of DataFrame\n\n\t\t* None: All worksheets.\n\t\t", "category": "remove", "imports": ["import pandas as pd", "from typing import List, Union"]}], [{"term": "def", "name": "remove_stopwords", "data": "def remove_stopwords(text, stopwords=STOP_WORDS):\n\t\"\"\"\n\tRemove stopwords from document.  Naively uses SpaCy Stop words\n\t\"\"\"\n\ttext = sp(text)\n\ttext = [token.text for token in text if token.text not in stopwords]\n\treturn ' '.join(text)\n", "description": "\n\tRemove stopwords from document.  Naively uses SpaCy Stop words\n\t", "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}, {"term": "def", "name": "remove_digits", "data": "def remove_digits(text):\n\t\"\"\"\n\tUses Regex to replace anything that is not an alphabetic character with an empty string\n\t\"\"\"\n\treturn re.sub(r'[^A-Za-z\\s]', ' ', text)\n", "description": "\n\tUses Regex to replace anything that is not an alphabetic character with an empty string\n\t", "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}, {"term": "def", "name": "lemma", "data": "def lemma(text, pos_to_avoid=[]):\n\t\"\"\"\n\tLemmatize Text.  Pass in a list of POS to avoid when lemmatizing\n\t\"\"\"\n\ttext = sp(text)\n\ttext = [token.lemma_ for token in text if token.lemma_ not in pos_to_avoid]\n\treturn ' '.join(text)\n", "description": "\n\tLemmatize Text.  Pass in a list of POS to avoid when lemmatizing\n\t", "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}, {"term": "def", "name": "remove_emojis", "data": "def remove_emojis(text):\n\t\"\"\"\n\tRemove emojis from text\n\t\"\"\"\n", "description": "\n\tRemove emojis from text\n\t", "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}, {"term": "def", "name": "remove_multiple_spaces", "data": "def remove_multiple_spaces(text):\n\t\"\"\"\n\tRemove multiple consecutive spaces from text\n\t\"\"\"\n\ttext = re.sub(r'\\s+', ' ', text.strip())\n\treturn text\n", "description": "\n\tRemove multiple consecutive spaces from text\n\t", "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}, {"term": "def", "name": "clean_doc", "data": "def clean_doc(text):\n\n\t# remove emojis\n\ttext = remove_emojis(text)\n\t# lemmatize and remove pronouns\n\ttext = lemma(text, ['-PRON-'])\n\t# remove digits and punctuation\n\ttext = remove_digits(text)\n\t# change all to lowercase\n\ttext = text.lower()\n\t# remove stopwords\n\ttext = remove_stopwords(text)\n\t# remove blank spaces\n\ttext = remove_multiple_spaces(text)\n\t\n\treturn text\n", "description": null, "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}, {"term": "def", "name": "prep_doc2vec", "data": "def prep_doc2vec(text):\n\t# remove emojis\n\ttext = remove_emojis(text)\n\t# lemmatize and remove pronouns\n\ttext = lemma(text, ['-PRON-'])\n\t# remove digits and punctuation\n\ttext = remove_digits(text)\n\t# change all to lowercase\n\ttext = text.lower()\n\t# remove blank spaces\n\ttext = remove_multiple_spaces(text)\n\treturn text\n", "description": null, "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}, {"term": "def", "name": "clean_subtitle", "data": "def clean_subtitle(df):\n\t\"\"\"\n\tReplace subtitle with None if Author's name appears in Subtitle\n\tSome subtitles scraped have the author's signature as well. \n\tEx: \"by Jose Marcial Portilla\"\n\n\targs: \n\t\tdf (DataFrame): Pandas Dataframe with the necessary columns\n\treturns: \n\t\tNone: Operation happens inplace.\n\t\"\"\"\n\tassert \"author\" in df.columns, 'Be sure to include the author in the columns'\n\tassert \"subtitle\" in df.columns, 'Be sure to include the subtitle in the columns'\n\n\t### Is there a faster way to \"map\" this function?\n\tfor idx, row in df.iterrows():\n\t\tif re.search(str(row['author']), str(row['subtitle'])):\n\t\t\tdf.iloc[idx]['subtitle'] = None\n", "description": "\n\tReplace subtitle with None if Author's name appears in Subtitle\n\tSome subtitles scraped have the author's signature as well. \n\tEx: \"by Jose Marcial Portilla\"\n\n\targs: \n\t\tdf (DataFrame): Pandas Dataframe with the necessary columns\n\treturns: \n\t\tNone: Operation happens inplace.\n\t", "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}, {"term": "def", "name": "sql_delete_duplicates", "data": "def sql_delete_duplicates(conn):\n\tquery = \"\"\"\n\tDELETE FROM towards_ds\n\tWHERE article_id NOT IN \n\t(\n\t\tSELECT MAX(article_id) \n\t\tFROM towards_ds \n\t\tGROUP BY body\n\t);\n\t\"\"\"\n\tcursor = conn.cursor()\n\tcursor.execute(\"BEGIN;\")\n\tcursor.execute(query)\n\tcursor.execute(\"commit;\")\n", "description": "\n\tDELETE FROM towards_ds\n\tWHERE article_id NOT IN \n\t(\n\t\tSELECT MAX(article_id) \n\t\tFROM towards_ds \n\t\tGROUP BY body\n\t);\n\t", "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}, {"term": "def", "name": "sql_to_csv", "data": "def sql_to_csv(filepath, conn):\n\tquery = f\"\"\"\n\tCOPY towards_ds \n\tto {filepath}\n\tDELIMITER ',' CSV HEADER;\n\t\"\"\"\n\tcursor = conn.cursor()\n\tcursor.execute(\"BEGIN;\")\n\tcursor.execute(query)\n\tcursor.execute(\"commit;\")\n\n\t\n", "description": "\n\tCOPY towards_ds \n\tto {filepath}\n\tDELIMITER ',' CSV HEADER;\n\t", "category": "remove", "imports": ["import spacy", "from spacy.lang.en.stop_words import STOP_WORDS", "import re", "import itertools", "\timport sqlalchemy", "\timport psycopg2", "\timport pyspark", "\timport pandas as pd"]}], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('payments', '0006_auto_20181115_1321'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='orderpayment',\n\t\t\tname='authorization_action',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='orderpayment',\n\t\t\tname='order',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='orderpayment',\n\t\t\tname='user',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='payment',\n\t\t\tname='order_payment',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='payment',\n\t\t\tname='polymorphic_ctype',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='transaction',\n\t\t\tname='payment',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='transaction',\n\t\t\tname='polymorphic_ctype',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='OrderPayment',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='OrderPaymentAction',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations"]}], [], [], [{"term": "class", "name": "TestBacktranslationDataset", "data": "class TestBacktranslationDataset(unittest.TestCase):\n\tdef setUp(self):\n\t\t(\n\t\t\tself.tgt_dict,\n\t\t\tself.w1,\n\t\t\tself.w2,\n\t\t\tself.src_tokens,\n\t\t\tself.src_lengths,\n\t\t\tself.model,\n\t\t) = test_utils.sequence_generator_setup()\n\n\t\tdummy_src_samples = self.src_tokens\n\n\t\tself.tgt_dataset = test_utils.TestDataset(data=dummy_src_samples)\n\t\tself.cuda = torch.cuda.is_available()\n\n\tdef _backtranslation_dataset_helper(\n\t\tself,\n\t\tremove_eos_from_input_src,\n\t\tremove_eos_from_output_src,\n\t):\n\t\ttgt_dataset = LanguagePairDataset(\n\t\t\tsrc=self.tgt_dataset,\n\t\t\tsrc_sizes=self.tgt_dataset.sizes,\n\t\t\tsrc_dict=self.tgt_dict,\n\t\t\ttgt=None,\n\t\t\ttgt_sizes=None,\n\t\t\ttgt_dict=None,\n\t\t)\n\n\t\tgenerator = SequenceGenerator(\n\t\t\t[self.model],\n\t\t\ttgt_dict=self.tgt_dict,\n\t\t\tmax_len_a=0,\n\t\t\tmax_len_b=200,\n\t\t\tbeam_size=2,\n\t\t\tunk_penalty=0,\n\t\t)\n\n\t\tbacktranslation_dataset = BacktranslationDataset(\n\t\t\ttgt_dataset=TransformEosDataset(\n\t\t\t\tdataset=tgt_dataset,\n\t\t\t\teos=self.tgt_dict.eos(),\n\t\t\t\t# remove eos from the input src\n\t\t\t\tremove_eos_from_src=remove_eos_from_input_src,\n\t\t\t),\n\t\t\tsrc_dict=self.tgt_dict,\n\t\t\tbacktranslation_fn=(\n\t\t\t\tlambda sample: generator.generate([self.model], sample)\n\t\t\t),\n\t\t\toutput_collater=TransformEosDataset(\n\t\t\t\tdataset=tgt_dataset,\n\t\t\t\teos=self.tgt_dict.eos(),\n\t\t\t\t# if we remove eos from the input src, then we need to add it\n\t\t\t\t# back to the output tgt\n\t\t\t\tappend_eos_to_tgt=remove_eos_from_input_src,\n\t\t\t\tremove_eos_from_src=remove_eos_from_output_src,\n\t\t\t).collater,\n\t\t\tcuda=self.cuda,\n\t\t)\n\t\tdataloader = torch.utils.data.DataLoader(\n\t\t\tbacktranslation_dataset,\n\t\t\tbatch_size=2,\n\t\t\tcollate_fn=backtranslation_dataset.collater,\n\t\t)\n\t\tbacktranslation_batch_result = next(iter(dataloader))\n\n\t\teos, pad, w1, w2 = self.tgt_dict.eos(), self.tgt_dict.pad(), self.w1, self.w2\n\n\t\t# Note that we sort by src_lengths and add left padding, so actually\n\t\t# ids will look like: [1, 0]\n\t\texpected_src = torch.LongTensor([[w1, w2, w1, eos], [pad, pad, w1, eos]])\n\t\tif remove_eos_from_output_src:\n\t\t\texpected_src = expected_src[:, :-1]\n\t\texpected_tgt = torch.LongTensor([[w1, w2, eos], [w1, w2, eos]])\n\t\tgenerated_src = backtranslation_batch_result[\"net_input\"][\"src_tokens\"]\n\t\ttgt_tokens = backtranslation_batch_result[\"target\"]\n\n\t\tself.assertTensorEqual(expected_src, generated_src)\n\t\tself.assertTensorEqual(expected_tgt, tgt_tokens)\n\n\tdef test_backtranslation_dataset_no_eos_in_output_src(self):\n\t\tself._backtranslation_dataset_helper(\n\t\t\tremove_eos_from_input_src=False,\n\t\t\tremove_eos_from_output_src=True,\n\t\t)\n\n\tdef test_backtranslation_dataset_with_eos_in_output_src(self):\n\t\tself._backtranslation_dataset_helper(\n\t\t\tremove_eos_from_input_src=False,\n\t\t\tremove_eos_from_output_src=False,\n\t\t)\n\n\tdef test_backtranslation_dataset_no_eos_in_input_src(self):\n\t\tself._backtranslation_dataset_helper(\n\t\t\tremove_eos_from_input_src=True,\n\t\t\tremove_eos_from_output_src=False,\n\t\t)\n\n\tdef assertTensorEqual(self, t1, t2):\n\t\tself.assertEqual(t1.size(), t2.size(), \"size mismatch\")\n\t\tself.assertEqual(t1.ne(t2).long().sum(), 0)\n\n", "description": null, "category": "remove", "imports": ["import unittest", "import tests.utils as test_utils", "import torch", "from fairseq.data import (", "from fairseq.sequence_generator import SequenceGenerator"]}], [{"term": "def", "name": "clear", "data": "def clear():\n\tos.system(\"cls\")\n", "description": null, "category": "remove", "imports": ["import re", "import os", "import sys ", "\tBecareful as this may remove important symbols necessary for folder readability\"\"\")"]}, {"term": "def", "name": "prompt", "data": "def prompt():\n\t\n\tproceed = input(\"\\n\tContinue? Y|N: \")\n\t\n\twhile not 'Y' or not 'N':\n\t\tproceed = input(\"\\n\tContinue? Y|N: \")\n\t\t\n\tif proceed == 'Y':\n\t\treturn True\n\n\telif proceed == 'N':\n\t\tprint(\"\tCautious are we?\")\n\t\tsys.exit(0)\n", "description": null, "category": "remove", "imports": ["import re", "import os", "import sys ", "\tBecareful as this may remove important symbols necessary for folder readability\"\"\")"]}, {"term": "def", "name": "fixPathName", "data": "def fixPathName(location):\n\tpattern = re.compile(r'\\s+')\n\tif \"\\\\\" in location:\n\t\tfolder_directory = re.sub(pattern, ' ', location.replace(\"\\\\\",\"/\"))\n\t\treturn folder_directory\n\telse:\n\t\tprint(\"\tthis is not a directory\")\n", "description": null, "category": "remove", "imports": ["import re", "import os", "import sys ", "\tBecareful as this may remove important symbols necessary for folder readability\"\"\")"]}, {"term": "def", "name": "remove_words", "data": "def remove_words(folders):\n\tclear()\n\tprint(\"\"\"\\n\n\tEnter the files you want to remove (separate each one with a space) ex: (C89) (C90) [English] ~ 1080p\")\n\tfor words in parenthesis ex: (English Sub)\n\tsimply type them as such => (English Sub) \n\n\tTo prevent errors, I advice you to do this on the notepad first \n\n\tShortcuts for copy and pasting in the terminal is ctrl + shift + key\\n\"\"\")\n\t\n\titems = {input(\"\tEnter here: \")}\n\titems_to_remove = \",\".join(items).split()\n\t\n\tfor name in range(len(folders)):\n\t\tresult = [word for word in folders[name].split() if word not in items_to_remove]\n\t\tos.rename(folders[name], \" \".join(result))\n", "description": "\\n\n\tEnter the files you want to remove (separate each one with a space) ex: (C89) (C90) [English] ~ 1080p\")\n\tfor words in parenthesis ex: (English Sub)\n\tsimply type them as such => (English Sub) \n\n\tTo prevent errors, I advice you to do this on the notepad first \n\n\tShortcuts for copy and pasting in the terminal is ctrl + shift + key\\n", "category": "remove", "imports": ["import re", "import os", "import sys ", "\tBecareful as this may remove important symbols necessary for folder readability\"\"\")"]}, {"term": "def", "name": "incrementFolders", "data": "def incrementFolders(folders):\n\tclear()\n\tprint(\"\"\"\\n\n", "description": "\\n\n", "category": "remove", "imports": ["import re", "import os", "import sys ", "\tBecareful as this may remove important symbols necessary for folder readability\"\"\")"]}, {"term": "def", "name": "remove_symbols", "data": "def remove_symbols(folders):\n\tclear()\n\tremoval_option = input(\"\"\"\n\tManually add symbols? M\\n\n\tRemove all symbols?   R\\n\n\tInput: \"\"\")\n\t\n\tif removal_option == 'M':\n\t\tclear()\n\t\tprint(\"\"\"\\n\n\tTo manually remove symbols simply type all symbols without any space\n\t\n\tex: ?&^$% \n\t\n\t(Please do not include any of these character ' \" ][ }{ () \\ / ) < > ? | * \"\"\")\n\t\t\n\t\tbanned =[\"'\",'\"',\"[\",\"]\",\"(\",\")\",\"}\",\"{\",\"\\\\\",\"/\",\"?\",\"*\",\"|\",\">\",\"<\"]\n\t\t\n\t\tsymbols = input(\"\tPlace symbols here: \")\n\t\t\n\t\twhile symbols in banned:\n\t\t\tsymbols = input(\"\tPlease mind the quotation marks: \")\n\t\t\tif symbols not in banned:\n\t\t\t\tcontinue\n\t\t\n\t\tif prompt():\n\t\t\tfor name in range(len(folders)):\n\t\t\t\tos.rename(folders[name], ''.join(words for words in folders[name] if words not in set(symbols)))\n\t\t\tprint(\"\\n\tAction Completed :)\\n\")   \n\t\n\telif removal_option == 'R':\n\t\tclear()\t\n\t\tprint(\"\"\"\\n\n\tthis will help remove all symbols in your files folders. \n\tBecareful as this may remove important symbols necessary for folder readability\"\"\")\n\t\tpattern = re.compile(r\"[^a-zA-Z0-9 ]\")\n\t\tif prompt():\n\t\t\tfor name in range(len(folders)):\n\t\t\t\tnew_folder_name = re.sub(pattern, \"\", folders[name])\n\t\t\t\tos.replace(folders[name], new_folder_name)\n", "description": "\n\tManually add symbols? M\\n\n\tRemove all symbols?   R\\n\n\tInput: ", "category": "remove", "imports": ["import re", "import os", "import sys ", "\tBecareful as this may remove important symbols necessary for folder readability\"\"\")"]}, {"term": "def", "name": "basic", "data": "def basic(folders):\n\tclear()\n\tprint(\":0\") \n", "description": null, "category": "remove", "imports": ["import re", "import os", "import sys ", "\tBecareful as this may remove important symbols necessary for folder readability\"\"\")"]}, {"term": "def", "name": "choose", "data": "def choose(location):\n\tclear()\n\tos.chdir(location)\n\tfolders = os.listdir(location)\n\t\n\tprint(\"\"\"\\nChoose from the following options\n\t\t  \n\t\t\t  Remove unnecessary details like: [English] [1080] 480p Engsub\t\t press 1\n\t\t\t  Add a number to sequence all your files\t\t\t\t\t\t\t   press 2\n\t\t\t  Remove all symbols from the folders name\t\t\t\t\t\t\t  press 3\n\t\t\t  Default (remove unnecessary details and sequence all folders)\t\t press 4\\n\"\"\")\n\t\n\toption = int(input(\"\tChoose: \"))\n\t\n\tif option == 1:\n\t\tremove_words(folders),\n\telif option == 2:\n\t\tincrementFolders(folders),\n\telif option == 3:\n\t\tremove_symbols(folders),\n\telif option == 4:\n\t\tbasic(folders),\n\telse:\n\t\tprint(\"\tInvalid input\")\n\t\t\t  \n", "description": "\\nChoose from the following options\n\t\t  \n\t\t\t  Remove unnecessary details like: [English] [1080] 480p Engsub\t\t press 1\n\t\t\t  Add a number to sequence all your files\t\t\t\t\t\t\t   press 2\n\t\t\t  Remove all symbols from the folders name\t\t\t\t\t\t\t  press 3\n\t\t\t  Default (remove unnecessary details and sequence all folders)\t\t press 4\\n", "category": "remove", "imports": ["import re", "import os", "import sys ", "\tBecareful as this may remove important symbols necessary for folder readability\"\"\")"]}, {"term": "def", "name": "main", "data": "def main():\n\tlocation = fixPathName(input(\"\"\"\n\tFolder Renaming System in python\n\t\n\tFeatures\n\t\tRemove unwanted folder details like \"[English], Eng Sub, 1080p\"\n\t\tRemove unwanted symbols like -*&^%\n\t\tAdd numbers to each folder name to sequentialize them\n\t\tCombine 1st and 2nd\n\t\t\n\tTo start, copy the path of the parent folder you want to manage\n\t\n\tex: D:\\Parent\\Child\n\t\n\tpath: \"\"\"))\n\t\n\tchoose(location)\n", "description": "\n\tFolder Renaming System in python\n\t\n\tFeatures\n\t\tRemove unwanted folder details like \"[English], Eng Sub, 1080p\"\n\t\tRemove unwanted symbols like -*&^%\n\t\tAdd numbers to each folder name to sequentialize them\n\t\tCombine 1st and 2nd\n\t\t\n\tTo start, copy the path of the parent folder you want to manage\n\t\n\tex: D:\\Parent\\Child\n\t\n\tpath: ", "category": "remove", "imports": ["import re", "import os", "import sys ", "\tBecareful as this may remove important symbols necessary for folder readability\"\"\")"]}], [], [], [{"term": "def", "name": "get_data_sell", "data": "def get_data_sell(datasell=datasell):\n\tfor row in datasell:\n\t\tif row['label'] == 'Items and Pricing':\n\t\t\tdatasell.remove(row)\t  #Remove Items and Pricing Section\n\t\tif row['label'] == 'Key Reports':\n\t\t\tfor rowitem in row['items']:\n\t\t\t\tif 'Customer Acquisition and Loyalty' in rowitem['name']:\n\t\t\t\t\trow['items'].remove(rowitem)\n\t\t\t\tif 'Inactive Customers' in rowitem['name']:\n\t\t\t\t\trow['items'].remove(rowitem)\n\t\t\t\tif 'Ordered Items To Be Delivered' in rowitem['name']:\n\t\t\t\t\trow['items'].remove(rowitem)\n\t\t\t\tif 'Sales Person-wise Transaction Summary' in rowitem['name']:\n\t\t\t\t\trow['items'].remove(rowitem)\n\t\t\t\tif 'Item-wise Sales History' in rowitem['name']:\n\t\t\t\t\trow['items'].remove(rowitem)\n\t\t\t\tif 'Quotation Trends' in rowitem['name']:\n\t\t\t\t\trow['items'].remove(rowitem)\n\t\t\t\tif 'Sales Order Trends' in rowitem['name']:\n\t\t\t\t\trow['items'].remove(rowitem)\n\t\t\t\n\t\tif row['label'] == 'Other Reports':\n\t\t\tdatasell.remove(row)\t  #Remove other reports Section\n\t\tif row['label'] == 'Sales':\n\t\t\tfor rowitem in row['items']:\n\t\t\t\tif 'Sales Partner' in rowitem['name']:\n\t\t\t\t\trow['items'].remove(rowitem)\n\t\t\t\tif 'Blanket Order' in rowitem['name']:\n\t\t\t\t\trow['items'].remove(rowitem)\n\t\t\t\n\n\t\t\t\t\trow['items'].append({\n\t\t\t\t\t\t\t\t\t   \"type\": \"doctype\",\n\t\t\t\t\t\t\t\t\t   \"name\": \"Estimation Sheet\",\n\t\t\t\t\t\t\t\t\t   \"description\": _(\"Estimation Sheet\"),\n\t\t\t\t\t\t\t\t\t   \"onboard\": 1,\n\t\t\t\t\t\t\t   })\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "import frappe", "from frappe import _", "from erpnext.config import buying,selling,projects,crm"]}, {"term": "def", "name": "get_data", "data": "def get_data(data=data):\n\tfor row in data:\n\t\tif row['label'] == 'Items and Pricing':\n\t\t   for rowitem in row['items']:\n\t\t\t   if rowitem['name'] == \"Product Bundle\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Item Group\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Promotional Scheme\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\tif row['label'] == 'Supplier Scorecard':\n\t\t\tdata.remove(row)\t  #Remove Supplier Scorecard\n\n\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "import frappe", "from frappe import _", "from erpnext.config import buying,selling,projects,crm"]}, {"term": "def", "name": "get_data_crm", "data": "def get_data_crm(datacrm = datacrm):\n\tfor row in datacrm:\n\t\tif row['label'] == 'Sales Pipeline':\n\t\t   for rowitem in row['items']:\n\t\t\t   if rowitem['name'] == \"Lead\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Communication\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Lead Source\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Contract\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Appointment\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Newsletter\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\tif row['label'] == 'Settings':\n\t\t   for rowitem in row['items']:\n\t\t\t   if rowitem['name'] == \"Sales Person\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Campaign\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Email Campaign\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"SMS Center\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"SMS Log\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"SMS Settings\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Email Group\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\tif row['label'] == 'Reports':\n\t\t\tdatacrm.remove(row)\t  #Remove Reports\n\t\tif row['label'] == 'Maintenance':\n\t\t\tdatacrm.remove(row)\t  #Remove Maintenance\n\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "import frappe", "from frappe import _", "from erpnext.config import buying,selling,projects,crm"]}, {"term": "def", "name": "get_data_proj", "data": "def get_data_proj(dataproj=dataproj):\n\tfor row in dataproj:\n\t\tif row['label'] == 'Projects':\n\t\t   for rowitem in row['items']:\n\t\t\t   if rowitem['name'] == \"Project Template\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Project Type\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Project Update\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t\t   row['items'].append({\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\"type\": \"doctype\",\n\t\t\t\t\t\t\t\t\t\t\"name\": \"Machinery and Other Cost\",\n\t\t\t\t\t\t\t\t\t\t\"description\": _(\"Machinery and Other Cost\"),\n\t\t\t\t\t\t\t\t\t\t\"onboard\": 1,\n\t\t\t\t\t\t\t\t})\n\t\t\t\t   row['items'].append({\n\t\t\t\t\t\t\t\t\t\t\"type\": \"doctype\",\n\t\t\t\t\t\t\t\t\t\t\"name\": \"Project Timesheet\",\n\t\t\t\t\t\t\t\t\t\t\"label\": _(\"Project Time Sheet\"),\n\t\t\t\t\t\t\t\t\t\t\"onboard\": 1,\n\t\t\t\t\t\t\t\t})\n\t\t\t\t   row['items'].append({\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\"type\": \"doctype\",\n\t\t\t\t\t\t\t\t\t\t\"name\": \"Employee Allocation\",\n\t\t\t\t\t\t\t\t\t\t\"label\": _(\"Employee Allocation\")\n\t\t\t\t\t \n\t\t\t\t\t\t\t\t})\n\t\t\t\t   row['items'].append({\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\"type\": \"doctype\",\n\t\t\t\t\t\t\t\t\t\t\"name\": \"Employee Allocation Tool\",\n\t\t\t\t\t\t\t\t\t\t\"label\": _(\"Employee Allocation Tool\")\n\t\t\t\t\t\t\t\t})\n\n\n\n\n\t\tif row['label'] == 'Reports':\n\t\t   for rowitem in row['items']:\n\t\t\t   if rowitem['name'] == \"Project wise Stock Tracking\":\n\t\t\t\t   row['items'].remove(rowitem)\n\t\t\t   if rowitem['name'] == \"Project Billing Summary\":\n\t\t\t\t   row['items'].remove(rowitem)\n\n\n\n\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "import frappe", "from frappe import _", "from erpnext.config import buying,selling,projects,crm"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('website', '0020_auto_20180927_1157'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnews',\n\t\t\tname='branch',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnews',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnews',\n\t\t\tname='last_modified_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnewsfile',\n\t\t\tname='file_post',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnewsfile',\n\t\t\tname='posted_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='event',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='event',\n\t\t\tname='last_modified_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='eventfile',\n\t\t\tname='file_post',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='eventfile',\n\t\t\tname='posted_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='imageadvert',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='news',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='news',\n\t\t\tname='last_modified_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='newsfile',\n\t\t\tname='file_post',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='newsfile',\n\t\t\tname='posted_by',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='BranchNews',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='BranchNewsFile',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='Event',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='EventFile',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ImageAdvert',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='News',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='NewsFile',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('website', '0020_auto_20180927_1157'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnews',\n\t\t\tname='branch',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnews',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnews',\n\t\t\tname='last_modified_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnewsfile',\n\t\t\tname='file_post',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnewsfile',\n\t\t\tname='posted_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='event',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='event',\n\t\t\tname='last_modified_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='eventfile',\n\t\t\tname='file_post',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='eventfile',\n\t\t\tname='posted_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='imageadvert',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='news',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='news',\n\t\t\tname='last_modified_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='newsfile',\n\t\t\tname='file_post',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='newsfile',\n\t\t\tname='posted_by',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='BranchNews',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='BranchNewsFile',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='Event',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='EventFile',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ImageAdvert',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='News',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='NewsFile',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('website', '0020_auto_20180927_1157'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnews',\n\t\t\tname='branch',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnews',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnews',\n\t\t\tname='last_modified_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnewsfile',\n\t\t\tname='file_post',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branchnewsfile',\n\t\t\tname='posted_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='event',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='event',\n\t\t\tname='last_modified_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='eventfile',\n\t\t\tname='file_post',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='eventfile',\n\t\t\tname='posted_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='imageadvert',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='news',\n\t\t\tname='created_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='news',\n\t\t\tname='last_modified_by',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='newsfile',\n\t\t\tname='file_post',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='newsfile',\n\t\t\tname='posted_by',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='BranchNews',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='BranchNewsFile',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='Event',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='EventFile',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ImageAdvert',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='News',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='NewsFile',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations"]}], [], [], [], [], [{"term": "class", "name": "RemoveListElementNode", "data": "class RemoveListElementNode(bpy.types.Node, AnimationNode):\n\tbl_idname = \"an_RemoveListElementNode\"\n\tbl_label = \"Remove List Element\"\n\n\tdef typeChanged(self, context):\n\t\tif self.isAllowedDataType(self.assignedType):\n\t\t\tself.refresh()\n\t\telse:\n\t\t\tself.reset_and_show_error()\n\n\tassignedType: ListTypeSelectorSocket.newProperty(default = \"Integer\")\n\n\tremoveType: EnumProperty(name = \"Remove Type\", default = \"FIRST_OCCURRENCE\",\n\t\titems = removeTypeItems, update = typeChanged)\n\n\tdef create(self):\n\t\tprop = (\"assignedType\", \"BASE\")\n\n\t\tif self.removeType in (\"FIRST_OCCURRENCE\", \"INDEX\"):\n\t\t\tself.newInput(ListTypeSelectorSocket(\n\t\t\t\t\"List\", \"inList\", \"LIST\", prop, dataIsModified = True))\n\t\telse:\n\t\t\tself.newInput(ListTypeSelectorSocket(\n\t\t\t\t\"List\", \"inList\", \"LIST\", prop))\n\n\t\tif self.removeType in (\"FIRST_OCCURRENCE\", \"ALL_OCCURRENCES\"):\n\t\t\tself.newInput(ListTypeSelectorSocket(\n\t\t\t\t\"Element\", \"element\", \"BASE\", prop, defaultDrawType = \"PREFER_PROPERTY\"))\n\t\telif self.removeType == \"INDEX\":\n\t\t\tself.newInput(\"Integer\", \"Index\", \"index\")\n\n\t\tself.newOutput(ListTypeSelectorSocket(\n\t\t\t\"List\", \"outList\", \"LIST\", prop))\n\n\tdef draw(self, layout):\n\t\tlayout.prop(self, \"removeType\", text = \"\")\n\n\tdef drawAdvanced(self, layout):\n\t\tself.invokeSelector(layout, \"DATA_TYPE\", \"assignListDataType\",\n\t\t\tdataTypes = \"LIST\", text = \"Change Type\", icon = \"TRIA_RIGHT\")\n\n\tdef getExecutionCode(self, required):\n\t\tyield \"outList = inList\"\n\t\tif self.removeType == \"FIRST_OCCURRENCE\":\n\t\t\tyield \"try: inList.remove(element)\"\n\t\t\tyield \"except ValueError: pass\"\n\t\telif self.removeType == \"ALL_OCCURRENCES\":\n\t\t\tyield \"outList = self.outputs[0].getDefaultValue()\"\n\t\t\tyield \"outList += [e for e in inList if e != element]\"\n\t\telif self.removeType == \"INDEX\":\n\t\t\tyield \"if 0 <= index < len(inList): del inList[index]\"\n\n\tdef assignListDataType(self, listDataType):\n\t\tself.assignType(toBaseDataType(listDataType))\n\n\tdef assignType(self, baseDataType):\n\t\tif not isBase(baseDataType): return\n\t\tif baseDataType == self.assignedType: return\n\t\tself.assignedType = baseDataType\n\t\tself.refresh()\n\n\tdef isAllowedDataType(self, dataType):\n\t\tif self.removeType in (\"FIRST_OCCURRENCE\", \"ALL_OCCURRENCES\"):\n\t\t\tif not isComparable(dataType):\n\t\t\t\treturn False\n\t\treturn True\n\n\tdef reset_and_show_error(self):\n\t\tself.show_type_error(self.assignedType)\n\t\tself.removeLinks()\n\t\tself.assignedType = \"Integer\"\n\n\tdef show_type_error(self, dataType):\n\t\ttext = \"This list type only supports element removal using an index: '{}'\".format(toListDataType(dataType))\n\t\tshowTextPopup(text = text, title = \"Error\", icon = \"ERROR\")\n", "description": null, "category": "remove", "imports": ["import bpy", "from bpy.props import *", "from ... ui.info_popups import showTextPopup", "from ... base_types import AnimationNode, ListTypeSelectorSocket", "from ... sockets.info import toBaseDataType, isBase, isComparable, toListDataType"]}], [], [], [{"term": "def", "name": "main", "data": "def main():\n\tparser = argparse.ArgumentParser(prog=\"dedupimg\", add_help=False)\n\tparser.add_argument(\"-p\", \"--path\", type=str, action=\"store\", required=True)\n\tparser.add_argument(\n\t\t\"-s\", \"--chunk-size\", type=int, action=\"store\", required=True\n\t)\n\n\targs = parser.parse_args()\n\tpath = args.path\n\tchunk_size = args.chunk_size\n\n\tfiles_to_remove = []\n\n\tfiles = os.listdir(path)\n\tchunked_files = [\n\t\tfiles[i : i + chunk_size] for i in range(0, len(files), chunk_size)\n\t]\n\tfor chunk in chunked_files:\n\t\tchunk_str = \" \".join((f\"'{file}'\" for file in chunk))\n\t\tto_remove = (\n\t\t\tos.popen(f\"sxiv -to -g 1920x1080 {chunk_str}\").read().split(\"\\n\")\n\t\t)\n\t\tto_remove = to_remove[:len(files_to_remove) - 1]\n\t\tfor f in to_remove:\n\t\t\tif os.path.isfile(f):\n\t\t\t\tos.remove(f)\n\t\t\telse:\n\t\t\t\tsys.stderr.write(f\"Error removing {f}\")\n\t\tfiles_to_remove.extend(to_remove)\n\n\toutstr = '\\n'.join(files_to_remove)\n\tsys.stdout.write(outstr)\n\n", "description": null, "category": "remove", "imports": ["import os", "import sys", "import argparse"]}], [{"term": "def", "name": "fix_html", "data": "def fix_html(text):\n\ttmp_ls = []\n\tfor e in listify(text):\n\t\te = e.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace('nbsp;', ' ').replace(\n\t\t\t'#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace('', \"\\n\").replace(\n\t\t\t'\\\\\"', '\"').replace('', ' ').replace(' @.@ ', '.').replace(' @-@ ', '-').replace('...', ' \u2026')\n\t\ttmp_ls.append(html.unescape(e))\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}, {"term": "def", "name": "remove_control_char", "data": "def remove_control_char(text):\n\ttmp_ls = []\n\tfor e in listify(text):\n\t\ttmp_ls.append(re.sub(control_char_regex, '.', e))\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}, {"term": "def", "name": "remove_remaining_control_chars", "data": "def remove_remaining_control_chars(text):\n\ttmp_ls = []\n\tfor e in listify(text):\n\t\ttmp_ls.append(\n\t\t\t''.join(ch for ch in e if unicodedata.category(ch)[0] != 'C'))\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}, {"term": "def", "name": "remove_unicode_symbols", "data": "def remove_unicode_symbols(text):\n\ttmp_ls = []\n\tfor e in listify(text):\n\t\ttmp_ls.append(\n\t\t\t''.join(ch for ch in e if unicodedata.category(ch)[0] != 'So'))\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}, {"term": "def", "name": "standardise_punc", "data": "def standardise_punc(text):\n\ttransl_table = dict([(ord(x), ord(y))\n\t\t\t\t\t\t for x, y in zip(u\"\u2018\u2019\u00b4\u201c\u201d\u2013-\",  u\"'''\\\"\\\"--\")])\n\ttmp_ls = []\n\tfor e in listify(text):\n\t\te = e.translate(transl_table)\n\t\ttmp_ls.append(e)\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}, {"term": "def", "name": "remove_news_tags", "data": "def remove_news_tags(text):\n\ttmp_ls = []\n\tfor e in listify(text):\n\t\te = re.sub(r\"(<[A-Z].+?>)|([A-Z].+?>)\", \"\", e)\n\t\ttmp_ls.append(e)\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}, {"term": "def", "name": "replace_urls", "data": "def replace_urls(text):\n\tfiller, tmp_ls = '', []\n\tfor e in listify(text):\n\t\te = re.sub(r\"()|()|()\", \"\", e)\n\t\te = re.sub(url_regex, filler, e)\n\t\ttmp_ls.append(e)\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}, {"term": "def", "name": "replace_usernames", "data": "def replace_usernames(text):\n\tfiller, tmp_ls = '', []\n\tfor e in listify(text):\n\t\tocc = e.count('@')\n\t\tfor _ in range(occ):\n\t\t\te = e.replace('@', f'{filler}')\n\t\t\t# replace other user handles by filler\n\t\t\te = re.sub(username_regex, filler, e)\n\t\ttmp_ls.append(e)\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}, {"term": "def", "name": "remove_duplicate_punctuation", "data": "def remove_duplicate_punctuation(text):\n\ttmp_ls = []\n\tfor e in listify(text):\n\t\te = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', e)\n\t\tpunc = set(punctuation)\n\t\tnewtext = []\n\t\tfor k, g in groupby(e):\n\t\t\tif k in punc:\n\t\t\t\tnewtext.append(k)\n\t\t\telse:\n\t\t\t\tnewtext.extend(g)\n\t\te = ''.join(newtext)\n\t\ttmp_ls.append(e)\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}, {"term": "def", "name": "remove_multi_space", "data": "def remove_multi_space(text):\n\ttmp_ls = []\n\tfor e in listify(text):\n\t\ttmp_ls.append(' '.join(e.split()))\n\n\ttext = tmp_ls\n\treturn text\n\n", "description": null, "category": "remove", "imports": ["from fastcore.basics import listify", "from fastcore.utils import compose", "import unicodedata", "from string import punctuation", "import html", "from itertools import groupby", "import re"]}], [], [], [], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('main', '0014_remove_test_answer1_remove_test_answer2_and_more'),\n\t]\n\n\toperations = [\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='test',\n\t\t\told_name='answer1_1',\n\t\t\tnew_name='answer1',\n\t\t),\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='test',\n\t\t\told_name='answer2_1',\n\t\t\tnew_name='answer2',\n\t\t),\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='test',\n\t\t\told_name='answer3_1',\n\t\t\tnew_name='answer3',\n\t\t),\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='test',\n\t\t\told_name='answer4_1',\n\t\t\tnew_name='answer4',\n\t\t),\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='test',\n\t\t\told_name='correct_1',\n\t\t\tnew_name='correct',\n\t\t),\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='test',\n\t\t\told_name='question_1',\n\t\t\tnew_name='question',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer1_2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer1_3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer1_4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer1_5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer2_2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer2_3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer2_4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer2_5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer3_2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer3_3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer3_4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer3_5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer4_2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer4_3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer4_4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='answer4_5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='article',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='correct_2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='correct_3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='correct_4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='correct_5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='question_2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='question_3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='question_4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='test',\n\t\t\tname='question_5',\n\t\t),\n\t\tmigrations.CreateModel(\n\t\t\tname='Comment',\n\t\t\tfields=[\n\t\t\t\t('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n\t\t\t\t('article', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='main.article')),\n\t\t\t],\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from django.db import migrations, models", "import django.db.models.deletion"]}], [], [], [{"term": "def", "name": "tokenize_and_lemmatize", "data": "def tokenize_and_lemmatize(tweet):\n\tresult = []\n\tfor token in gensim.utils.simple_preprocess(tweet):\n\t\tif token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n\t\t\tresult.append(WordNetLemmatizer().lemmatize(token, pos='v'))\n\treturn result\n\n", "description": null, "category": "remove", "imports": ["import re", "import gensim", "from nltk.corpus import stopwords, wordnet", "import nltk", "from nltk.stem import WordNetLemmatizer"]}, {"term": "def", "name": "remove_unrecognized_words", "data": "def remove_unrecognized_words(tokenized_tweets):\n\twpt = nltk.WordPunctTokenizer()\n\trecognized_words = []\n\n\tfor tweet in tokenized_tweets:\n\t\ttokens = wpt.tokenize(tweet)\n\t\tif tokens:\n\t\t\tfor token in tokens:\n\t\t\t\tif wordnet.synsets(token):\n\t\t\t\t\trecognized_words.append(token)\n\n\treturn recognized_words\n\n", "description": null, "category": "remove", "imports": ["import re", "import gensim", "from nltk.corpus import stopwords, wordnet", "import nltk", "from nltk.stem import WordNetLemmatizer"]}, {"term": "def", "name": "preprocess_tweet", "data": "def preprocess_tweet(tweet):\n\n\t# Remove Retweets\n\ttweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n\t#Remove Mentions\n\ttweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n\n\t# Remove http links\n\ttweet = re.sub(r'http\\S+', '', tweet)\n\t# Remove Bitly links\n\ttweet = re.sub(r'bit.ly/\\S+', '', tweet)\n\t# remove other links\n\ttweet = tweet.strip('[link]')\n\t# remove picture links\n\ttweet = re.sub(r'pic.twitter\\S+', '', tweet)\n\n\t# remove hashtags\n\ttweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n\n\t# Remove video\n\ttweet = re.sub('VIDEO:', '', tweet)\n\t# Remove audio\n\ttweet = re.sub('AUDIO:', '', tweet)\n\n\t# lower case\n\ttweet = tweet.lower()\n\n\t# Remove punctuation\n\ttweet = re.sub('[' + punctuation + ']+', ' ', tweet)\n\t# Remove double spacing\n\ttweet = re.sub('\\s+', ' ', tweet)\n\t# Remove numbers\n\ttweet = re.sub('([0-9]+)', '', tweet)\n\t# Lemmatization and tokenization\n\ttweet_token_list = tokenize_and_lemmatize(tweet)\n\t# Remove unrecognized words by wordnet\n\ttweet = remove_unrecognized_words(tweet_token_list)\n\treturn tweet\n\n", "description": null, "category": "remove", "imports": ["import re", "import gensim", "from nltk.corpus import stopwords, wordnet", "import nltk", "from nltk.stem import WordNetLemmatizer"]}, {"term": "def", "name": "tokenize_tweets", "data": "def tokenize_tweets(df):\n\tdf['tokens'] = df.text.apply(preprocess_tweet)\n", "description": null, "category": "remove", "imports": ["import re", "import gensim", "from nltk.corpus import stopwords, wordnet", "import nltk", "from nltk.stem import WordNetLemmatizer"]}], [{"term": "class", "name": "MyTestCase", "data": "class MyTestCase(unittest.TestCase):\n\tdef test_add_node(self):\n\t\tgraph = DiGraph()\n\t\tfor i in range(4):\n\t\t\tgraph.add_node(i)\n\t\tgraph.add_edge(0, 1, 1)\n\t\tgraph.add_edge(1, 2, 2)\n\t\tgraph.add_edge(2, 0, 3)\n\t\tgraph.add_edge(3, 0, 7)\n\t\tself.assertEqual(4, graph.v_size())\n\t\tself.assertEqual(8, graph.get_mc())\n\t\tgraph.add_edge(3, 0, 5)\n\t\tself.assertEqual(9, graph.get_mc())\n\t\tgraph.add_edge(3, 0, 5)\n\t\tself.assertEqual(9, graph.get_mc())\n\t\tself.assertEqual(4, graph.e_size())\n\t\tgraph.remove_node(0)\n\t\tself.assertEqual(3, graph.v_size())\n\t\tself.assertEqual(1, graph.e_size())\n\t\tself.assertTrue(graph.remove_edge(1, 2))\n\t\tself.assertEqual(0, graph.e_size())\n\t\tself.assertEqual(3, graph.v_size())\n\t\tself.assertFalse(graph.remove_edge(8, 9))\n\n\tdef test_add_node1(self):\n\t\tgraph = DiGraph()\n\t\tgraph.add_node(1)\n\t\tgraph.add_node(2)\n\t\tgraph.add_node(3)\n\t\tgraph.add_node(4)\n\t\tgraph.add_node(5)\n\t\tgraph.add_edge(1, 2, 3)\n\t\tgraph.add_edge(2, 3, 4)\n\t\tgraph.add_edge(3, 4, 10)\n\t\tgraph.add_edge(4, 3, 1)\n\t\tgraph.add_edge(3, 5, 7)\n\t\tgraph.add_edge(5, 1, 1)\n\t\tgraph.add_edge(1, 5, 2)\n\n\t\tself.assertEqual(12, graph.get_mc())\n\t\tself.assertEqual(5, graph.v_size())\n\t\tself.assertEqual(7, graph.e_size())\n\n\t\tkeys = [1, 2, 3, 4, 5]\n\t\tself.assertEqual(keys, list(graph.get_all_v().keys()))\n\t\tself.assertEqual({2: 4, 4: 1}, graph.all_in_edges_of_node(3))\n\t\tself.assertEqual({4: 10, 5: 7}, graph.all_out_edges_of_node(3))\n\n\t\tgraph.remove_edge(5, 1)\n\t\tself.assertEqual(6, graph.e_size())\n\t\tself.assertEqual(0, len(graph.all_out_edges_of_node(5)))\n\t\tself.assertEqual(13, graph.get_mc())\n\n\t\tgraph.remove_node(3)\n\t\tself.assertEqual(4, graph.v_size())\n\t\tself.assertEqual(2, graph.e_size())\n\n\t\tself.assertEqual(0, len(graph.all_out_edges_of_node(4)))\n\t\tself.assertEqual(14, graph.get_mc())\n\n\tdef test_add_node2(self):\n\t\tgraph = DiGraph()\n\t\tfor i in range(100):\n\t\t\tgraph.add_node(i)\n\t\tgraph.add_node(50)\n\t\tself.assertEqual(100, graph.v_size())\n\t\tself.assertTrue(graph.remove_node(6))\n\t\tself.assertEqual(99,graph.v_size())\n\t\tself.assertFalse( graph.remove_node(101))\n\n\tdef test_empty_graph(self):\n\t\tgraph = DiGraph()\n\t\tself.assertEqual(0, graph.v_size())\n\t\tself.assertFalse(graph.remove_edge(2, 5))\n\t\tself.assertFalse(graph.remove_node(2))\n\n\tdef test_add_edge(self):\n\t\tgraph = DiGraph()\n\t\tfor i in range(1, 11):\n\t\t\tgraph.add_node(i)\n\t\tgraph.add_edge(1, 2, 5)\n\t\tgraph.add_edge(1, 8, 4)\n\t\tgraph.add_edge(9, 10, 0)\n\t\tgraph.add_edge(5, 2, 3)\n\n\t\tself.assertEqual(4, graph.e_size())\n\t\tself.assertFalse(graph.add_edge(1, 100, 4))\n\t\tself.assertEqual(2, len(graph.all_out_edges_of_node(1)))\n\t\tself.assertEqual(0, len(graph.all_out_edges_of_node(4)))\n\t\tgraph.remove_edge(1, 8)\n\t\tself.assertEqual(3, graph.e_size())\n\t\tself.assertEqual(1, len(graph.all_out_edges_of_node(1)))\n\t\tgraph.remove_node(9)\n\t\tself.assertEqual(2, graph.e_size())\n\t\tself.assertEqual(0, len(graph.all_out_edges_of_node(10)))\n\t\tself.assertFalse(graph.add_edge(1, 2, -9))\n\t\tself.assertFalse(graph.add_edge(1, 4, -9))\n\t\tself.assertEqual(2, graph.e_size())# If the weight is negative then the number of ribs does not change.\n\t\tself.assertEqual(16, graph.get_mc())\n\n\tdef test_edges_Empty_Graph(self):\n\t\tgraph = DiGraph()\n\t\tself.assertFalse(graph.remove_edge(1,5))\n\t\tself.assertEqual(0, graph.v_size())\n\t\tself.assertEqual(0, graph.e_size())\n\t\tself.assertEqual(0, graph.get_mc())\n\n\tdef test_edges_number_On_Full_Graph(self):\n\t\tgraph = DiGraph()\n\t\tfor i in range(1, 6):\n\t\t\tgraph.add_node(i)\n\n\t\tfor i in range(1, 6):\n\t\t\tfor j in range(1, 6):\n\t\t\t\tresult = graph.add_edge(i, j, 5)\n\t\t\t\tif i == j:\n\t\t\t\t\tself.assertFalse(result)\n\t\t\t\telse:\n\t\t\t\t\tself.assertTrue(result)\n\t\tgraph.remove_edge(1, 5)\n\t\tself.assertEqual(19, graph.e_size())\n\t\tself.assertEqual(26, graph.get_mc())\n\n\n", "description": null, "category": "remove", "imports": ["import unittest", "import time", "from src.DiGraph import DiGraph"]}], [{"term": "def", "name": "RemoveUnsupportedTests", "data": "  def RemoveUnsupportedTests(self, test_output):\n\tif not SUPPORTS_DEATH_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'DeathTest')\n\tif not SUPPORTS_TYPED_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'TypedTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypedDeathTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypeParamDeathTest')\n\tif not SUPPORTS_THREADS:\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ExpectFailureWithThreadsTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ScopedFakeTestPartResultReporterTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'WorksConcurrently')\n\tif not SUPPORTS_STACK_TRACES:\n\t  test_output = RemoveStackTraces(test_output)\n\n\treturn test_output\n", "description": null, "category": "remove", "imports": ["import difflib", "import os", "import re", "import sys", "import gtest_test_utils"]}, {"term": "def", "name": "testOutput", "data": "  def testOutput(self):\n\toutput = GetOutputOfAllCommands()\n\n\tgolden_file = open(GOLDEN_PATH, 'r')\n\t# A mis-configured source control system can cause \\r appear in EOL\n\t# sequences when we read the golden file irrespective of an operating\n\t# system used. Therefore, we need to strip those \\r's from newlines\n\t# unconditionally.\n\tgolden = ToUnixLineEnding(golden_file.read())\n\tgolden_file.close()\n\n\t# We want the test to pass regardless of certain features being\n\t# supported or not.\n\n\t# We still have to remove type name specifics in all cases.\n\tnormalized_actual = RemoveTypeInfoDetails(output)\n\tnormalized_golden = RemoveTypeInfoDetails(golden)\n\n\tif CAN_GENERATE_GOLDEN_FILE:\n\t  self.assertEqual(normalized_golden, normalized_actual,\n\t\t\t\t\t   '\\n'.join(difflib.unified_diff(\n\t\t\t\t\t\t   normalized_golden.split('\\n'),\n\t\t\t\t\t\t   normalized_actual.split('\\n'),\n\t\t\t\t\t\t   'golden', 'actual')))\n\telse:\n\t  normalized_actual = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(normalized_actual))\n\t  normalized_golden = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(self.RemoveUnsupportedTests(normalized_golden)))\n\n\t  # This code is very handy when debugging golden file differences:\n\t  if os.getenv('DEBUG_GTEST_OUTPUT_TEST'):\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_actual.txt'), 'wb').write(\n\t\t\t\tnormalized_actual)\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_golden.txt'), 'wb').write(\n\t\t\t\tnormalized_golden)\n\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\n", "description": null, "category": "remove", "imports": ["import difflib", "import os", "import re", "import sys", "import gtest_test_utils"]}], [{"term": "def", "name": "RemoveUnsupportedTests", "data": "  def RemoveUnsupportedTests(self, test_output):\n\tif not SUPPORTS_DEATH_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'DeathTest')\n\tif not SUPPORTS_TYPED_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'TypedTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypedDeathTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypeParamDeathTest')\n\tif not SUPPORTS_THREADS:\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ExpectFailureWithThreadsTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ScopedFakeTestPartResultReporterTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'WorksConcurrently')\n\tif not SUPPORTS_STACK_TRACES:\n\t  test_output = RemoveStackTraces(test_output)\n\n\treturn test_output\n", "description": null, "category": "remove", "imports": ["import difflib", "import os", "import re", "import sys", "import gtest_test_utils"]}, {"term": "def", "name": "testOutput", "data": "  def testOutput(self):\n\toutput = GetOutputOfAllCommands()\n\n\tgolden_file = open(GOLDEN_PATH, 'rb')\n\t# A mis-configured source control system can cause \\r appear in EOL\n\t# sequences when we read the golden file irrespective of an operating\n\t# system used. Therefore, we need to strip those \\r's from newlines\n\t# unconditionally.\n\tgolden = ToUnixLineEnding(golden_file.read())\n\tgolden_file.close()\n\n\t# We want the test to pass regardless of certain features being\n\t# supported or not.\n\n\t# We still have to remove type name specifics in all cases.\n\tnormalized_actual = RemoveTypeInfoDetails(output)\n\tnormalized_golden = RemoveTypeInfoDetails(golden)\n\n\tif CAN_GENERATE_GOLDEN_FILE:\n\t  self.assertEqual(normalized_golden, normalized_actual,\n\t\t\t\t\t   '\\n'.join(difflib.unified_diff(\n\t\t\t\t\t\t   normalized_golden.split('\\n'),\n\t\t\t\t\t\t   normalized_actual.split('\\n'),\n\t\t\t\t\t\t   'golden', 'actual')))\n\telse:\n\t  normalized_actual = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(normalized_actual))\n\t  normalized_golden = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(self.RemoveUnsupportedTests(normalized_golden)))\n\n\t  # This code is very handy when debugging golden file differences:\n\t  if os.getenv('DEBUG_GTEST_OUTPUT_TEST'):\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_actual.txt'), 'wb').write(\n\t\t\t\tnormalized_actual)\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_gtest_output_test_normalized_golden.txt'), 'wb').write(\n\t\t\t\tnormalized_golden)\n\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\n", "description": null, "category": "remove", "imports": ["import difflib", "import os", "import re", "import sys", "import gtest_test_utils"]}], [{"term": "def", "name": "RemoveUnsupportedTests", "data": "  def RemoveUnsupportedTests(self, test_output):\n\tif not SUPPORTS_DEATH_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'DeathTest')\n\tif not SUPPORTS_TYPED_TESTS:\n\t  test_output = RemoveMatchingTests(test_output, 'TypedTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypedDeathTest')\n\t  test_output = RemoveMatchingTests(test_output, 'TypeParamDeathTest')\n\tif not SUPPORTS_THREADS:\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ExpectFailureWithThreadsTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'ScopedFakeTestPartResultReporterTest')\n\t  test_output = RemoveMatchingTests(test_output,\n\t\t\t\t\t\t\t\t\t\t'WorksConcurrently')\n\tif not SUPPORTS_STACK_TRACES:\n\t  test_output = RemoveStackTraces(test_output)\n\n\treturn test_output\n", "description": null, "category": "remove", "imports": ["import difflib", "import os", "import re", "import sys", "import gtest_test_utils"]}, {"term": "def", "name": "testOutput", "data": "  def testOutput(self):\n\toutput = GetOutputOfAllCommands()\n\n\tgolden_file = open(GOLDEN_PATH, 'rb')\n\t# A mis-configured source control system can cause \\r appear in EOL\n\t# sequences when we read the golden file irrespective of an operating\n\t# system used. Therefore, we need to strip those \\r's from newlines\n\t# unconditionally.\n\tgolden = ToUnixLineEnding(golden_file.read().decode())\n\tgolden_file.close()\n\n\t# We want the test to pass regardless of certain features being\n\t# supported or not.\n\n\t# We still have to remove type name specifics in all cases.\n\tnormalized_actual = RemoveTypeInfoDetails(output)\n\tnormalized_golden = RemoveTypeInfoDetails(golden)\n\n\tif CAN_GENERATE_GOLDEN_FILE:\n\t  self.assertEqual(normalized_golden, normalized_actual,\n\t\t\t\t\t   '\\n'.join(difflib.unified_diff(\n\t\t\t\t\t\t   normalized_golden.split('\\n'),\n\t\t\t\t\t\t   normalized_actual.split('\\n'),\n\t\t\t\t\t\t   'golden', 'actual')))\n\telse:\n\t  normalized_actual = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(normalized_actual))\n\t  normalized_golden = NormalizeToCurrentPlatform(\n\t\t  RemoveTestCounts(self.RemoveUnsupportedTests(normalized_golden)))\n\n\t  # This code is very handy when debugging golden file differences:\n\t  if os.getenv('DEBUG_GTEST_OUTPUT_TEST'):\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_googletest-output-test_normalized_actual.txt'), 'wb').write(\n\t\t\t\tnormalized_actual)\n\t\topen(os.path.join(\n\t\t\tgtest_test_utils.GetSourceDir(),\n\t\t\t'_googletest-output-test_normalized_golden.txt'), 'wb').write(\n\t\t\t\tnormalized_golden)\n\n\t  self.assertEqual(normalized_golden, normalized_actual)\n\n", "description": null, "category": "remove", "imports": ["import difflib", "import os", "import re", "import sys", "import gtest_test_utils"]}], [], [], [{"term": "def", "name": "read_auto", "data": "def read_auto(datapath):\n\t_, ext = os.path.splitext(datapath)\n\n\tif ext == \".csv\":\n\t\treturn pd.read_csv(datapath)\n\telif ext == \".tsv\":\n\t\treturn pd.read_csv(datapath, sep=\"\\t\")\n\telif ext in [\".parquet\", \".pq\", \".parq\"]:\n\t\treturn pd.read_parquet(datapath)\n\telif ext in [\".xlsx\", \".xls\"]:\n\t\treturn pd.read_excel(datapath)\n\telse:\n\t\traise Exception(\"Unsupported file type. Should be one of csv, tsv, parquet, or xlsx.\")\n\n", "description": null, "category": "remove", "imports": ["import argparse", "import os", "import pandas as pd", "import numpy as np"]}, {"term": "def", "name": "lambd", "data": "def lambd(x):\n\tto_add = np.setdiff1d([string.strip() for string in x[\"add\"].split(\",\")], [\"\"])\n\tto_remove = np.setdiff1d([string.strip() for string in x[\"remove\"].split(\",\")], [\"\"])\n\tcluster = disambiguation[disambiguation == x.inventor_id].index.values\n\tcluster = np.append(cluster, to_add)\n\tif len(to_remove) > 0:\n\t\tassert all(\n\t\t\tmention in cluster for mention in to_remove\n\t\t), f\"{to_remove[np.array([mention not in cluster for mention in to_remove])]}\"\n\t\tcluster = np.setdiff1d(cluster, to_remove)\n\n\tassert all(\n\t\tmention in disambiguation for mention in to_add\n\t), f\"{to_add[np.array([mention not in disambiguation for mention in to_add])]}\"\n\n\treturn cluster\n\n", "description": null, "category": "remove", "imports": ["import argparse", "import os", "import pandas as pd", "import numpy as np"]}, {"term": "def", "name": "lambd_remove_errors", "data": "def lambd_remove_errors(x):\n\tcluster = disambiguation[disambiguation == x.inventor_id].index.values\n\tto_remove = np.setdiff1d([string.strip() for string in x[\"remove\"].split(\",\")], [\"\"])\n\tI = np.array([mention not in cluster for mention in to_remove])\n\tif len(I > 0):\n\t\treturn to_remove[I]\n\telse:\n\t\treturn []\n\n", "description": null, "category": "remove", "imports": ["import argparse", "import os", "import pandas as pd", "import numpy as np"]}, {"term": "def", "name": "lambd_add_errors", "data": "def lambd_add_errors(x):\n\tto_add = np.setdiff1d([string.strip() for string in x[\"add\"].split(\",\")], [\"\"])\n\tI = np.array([mention not in disambiguation for mention in to_add])\n\tif len(to_add) > 0:\n\t\treturn to_add[I]\n\telse:\n\t\treturn []\n\n", "description": null, "category": "remove", "imports": ["import argparse", "import os", "import pandas as pd", "import numpy as np"]}], [], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('blog', '0002_questionnairesatu'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='keterangan_delapan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='keterangan_dua_belas',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='keterangan_enam',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='keterangan_lima',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='keterangan_sebelas',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='keterangan_sembilan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='keterangan_sepuluh',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='keterangan_tiga_belas',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='keterangan_tujuh',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_delapan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_dua',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_dua_belas',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_empat',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_enam',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_lima',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_satu',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_sebelas',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_sembilan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_sepuluh',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_tiga',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_tiga_belas',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='na_tujuh',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='nilai_delapan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='nilai_dua_belas',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='nilai_enam',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='nilai_lima',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='nilai_sebelas',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='nilai_sembilan',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='nilai_sepuluh',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='nilai_tiga_belas',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='nilai_tujuh',\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='rekomendasi_dua',\n\t\t\tfield=models.TextField(default='Rekomendasi'),\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='rekomendasi_empat',\n\t\t\tfield=models.TextField(default='Rekomendasi'),\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='rekomendasi_satu',\n\t\t\tfield=models.TextField(default='Rekomendasi'),\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='questionnairesatu',\n\t\t\tname='rekomendasi_tiga',\n\t\t\tfield=models.TextField(default='Rekomendasi'),\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from django.db import migrations, models"]}], [], [], [], [], [], [], [], [{"term": "def", "name": "simple_tokenizer", "data": "def simple_tokenizer(text):\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}, {"term": "def", "name": "is_punctuation", "data": "def is_punctuation(word):\n\tpunkts = string.punctuation\n\ttested_chars = [i for i in word if i in punkts]\n\treturn len(word) == len(tested_chars)\n\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}, {"term": "def", "name": "tokenizer", "data": "def tokenizer(text, lang, remove_punkt=True):\n\t'''\n\ttext : any string in lang\n\tlang : langauge of the string (english, turkish..) \n\tremove_punkt: if true, remove the punctuation tokens\n\t'''\n\n\t# @TODO if lang not found, use english\n\ttokens = nltktokenizer.word_tokenize(text, language=lang)\n\t#t = Text(text, hint_language_code=\"tr\")\n\t#tokens = list(t.tokens)\n\t\n\tif remove_punkt:\n\t\ttokens = [token for token in tokens if not is_punctuation(token)]\n\t\n\ttokens = eliminate_empty_strings(tokens)\n\ttokens = [token for token in tokens if token.isalnum()]\n\treturn tokens\n\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}, {"term": "def", "name": "stem_words", "data": "def stem_words(words, lang):\n\n\tif lang in [\"tr\", \"turkish\"]:\n\t\twords = [tr_stemmer.stem2(word) for word in words]\n\t\n\tif lang in [\"en\", \"english\"]:\n\t\twords = [en_stemmer.stem1(word) for word in words]\n\t\n\treturn words\n\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}, {"term": "def", "name": "eliminate_empty_strings", "data": "def eliminate_empty_strings(wordlist):\n\tl = [w.strip() for w in wordlist]\n\tl = [w for w in l if len(w) > 0]\n\treturn l\n\n\n\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}, {"term": "def", "name": "language_map", "data": "def language_map(lang_shortcut):\n\tlangmap = { \"tr\" : \"turkish\",\n\t\t\t\t\"en\" : \"english\",\n\t\t\t\t\"eng\" : \"english\"\n\t\t\t  }\n\t\n\treturn langmap[lang_shortcut]\n  \n\t\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}, {"term": "class", "name": "_TokenHandler", "data": "class _TokenHandler():\n\t__metaclass__ = ABCMeta\n\t\n\t\n\t\n\t@abstractmethod   \n\tdef __init__(self, language=None, \n\t\t\t\t stopword=False, more_stopwords=None, \n\t\t\t\t stemming=False, \n\t\t\t\t remove_numbers=False, deasciify=False, \n\t\t\t\t remove_punkt=True,\n\t\t\t\t lowercase=True):\n\t\tself.lang = language\n\t\tself.stopword = stopword\n\t\tself.more_stopwords = more_stopwords\n\t\tself.stemming = stemming\n\t\tself.remove_numbers = remove_numbers\n\t\tself.deasciify = deasciify\n\t\tself.remove_punkt = remove_punkt\n\t\tself.lowercase = lowercase\n\t\treturn\n\t\t\n\t\t\n\t@abstractmethod \n\tdef __call__(self, doc):\n\t\ttokens = tokenizer(doc, lang=language_map(self.lang), remove_punkt=self.remove_punkt) \n\t\n\t\tif self.lowercase:\n\t\t\ttokens = [token.lower() for token in tokens]\n\t\t\n\t\t# problem: \"\u0130\" is lowercased to \"i\u0307\"\n\t\t#i = 'i\u0307'\n\t\t#tokens = [token.replace(i, \"i\") for token in tokens]\t\t\n\t\t\n\t\tif self.remove_numbers:\n\t\t\tnumber_pattern = \"[a-zA-z]{,3}\\d+\"   #d{6,}  \n\t\t\ttokens = [re.sub(number_pattern, \"\", token) for token in tokens]\n\t\t\n\t\tif self.stopword:\n\t\t\tstopwords = stopword_lists.get_stopwords(lang=self.lang)\t\t  \n\t\t\ttokens = [token for token in tokens if token not in stopwords]  \n\t\t\t\n\t\t\tif self.more_stopwords: #len(self.more_stopwords) > 0:\t# re-organize not to have this list through memory but disc\n\t\t\t\ttokens = [token for token in tokens if token not in self.more_stopwords]\n\t\t\t\t\n\t\tif self.stemming:\n\t\t\ttokens = stem_words(tokens, lang=self.lang)\n\t\t\n\t\tif self.stopword:\n\t\t\tstopwords = stopword_lists.get_stopwords(lang=self.lang)\t\t  \n\t\t\ttokens = [token for token in tokens if token not in stopwords]  \n\t\t\t\n\t\t\tif self.more_stopwords: #len(self.more_stopwords) > 0:\t# re-organize not to have this list through memory but disc\n\t\t\t\ttokens = [token for token in tokens if token not in self.more_stopwords]\n\t\t\n\t\t\n\t\t'''\n\t\tif self.stemming:\n\t\t\ttokens = [tr_stemmer.stem2(token) for token in tokens]\t\n\t\t\n\t\tif self.deasciify:\n\t\t\ttokens = [Deasciifier(token).convert_to_turkish() for token in tokens]\n\t\t'''\n\t\t\n\t\t\n\t\ttokens = eliminate_empty_strings(tokens)\n\t\treturn tokens   \n\t\n\n\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}, {"term": "class", "name": "TrTokenHandler", "data": "class TrTokenHandler(_TokenHandler):\n\t\n\tdef __init__(self,  \n\t\t\t\t stopword=False, more_stopwords=None,\n\t\t\t\t stemming=False, \n\t\t\t\t remove_numbers=False, deasciify=False, \n\t\t\t\t remove_punkt=True,\n\t\t\t\t lowercase=True):\n\t\tself.lang = \"tr\"\n\t\tsuper().__init__(self.lang, stopword, more_stopwords, stemming, remove_numbers, deasciify, remove_punkt, lowercase)   \n\t\n\t\n\t@abstractmethod\n\tdef __call__(self, doc):\n\t\t\n\t\t#tokens = super.__call__(self, doc)\n\t\ttokens = _TokenHandler.__call__(self, doc) \n\t\t\n\t\t'''\n\t\tif self.stemming:\n\t\t\ttokens = [tr_stemmer.stem2(token) for token in tokens] \n\t\t'''\n\t\tif self.deasciify:\n\t\t\ttokens = [Deasciifier(token).convert_to_turkish() for token in tokens]\n\t\t\n\t\treturn tokens\n\n  \n\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}, {"term": "class", "name": "EnTokenHandler", "data": "class EnTokenHandler(_TokenHandler):\n\t\n\tspellcheck = False\n\t\n\tdef __init__(self, \n\t\t\t\t spellcheck=False,\n\t\t\t\t stopword=False, more_stopwords=None, \n\t\t\t\t stemming=False, \n\t\t\t\t remove_numbers=False, deasciify=False, \n\t\t\t\t remove_punkt=True,\n\t\t\t\t lowercase=True):\n\t\tself.lang = \"en\"\n\t\tsuper().__init__(self.lang, stopword, more_stopwords, stemming, remove_numbers, deasciify, remove_punkt, lowercase)   \n\t\tself.spellcheck = spellcheck\n\t\n\t\n\t\n\t@abstractmethod\n\tdef __call__(self, doc):\n\t\t#tokens = super.__call__(doc)\n\t\ttokens = _TokenHandler.__call__(self, doc) \n\t\t\n\t\tif self.spellcheck:\n\t\t\ttokens = [en_spellchecker.spellcheck(token) for token in tokens]\n\t\t'''\n\t\tif self.stemming:\n\t\t\ttokens = [en_stemmer.stem1(token) for token in tokens] \n\t\t'''\n\t\t# ascii is english..\n\t\t\t\n\t\t\n\t\treturn tokens\n\t\n\n\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}, {"term": "def", "name": "original_to_preprocessed_map", "data": "def original_to_preprocessed_map(preprocessor, text):\n\t\n\twords = text.split()\n\twords_prep = []\n\tfor word in words:\n\t\tprepword = preprocessor(word)\n\t\tif prepword:\n\t\t\tprepword = prepword[0]\n\t\telse:\n\t\t\tprepword = \"\"\n\t\t\n\t\twords_prep.append((prepword, word))\n\t\n\tprep_word_map = {}\n\tfor x, y in words_prep:\n\t\tprep_word_map.setdefault(x, []).append(y)\n\t\n\treturn prep_word_map\n\n\n", "description": null, "category": "remove", "imports": ["import re", "import string", "from turkish.deasciifier import Deasciifier", "import nltk.tokenize as nltktokenizer", "import nltk.stem as stemmer", "from polyglot.text import Text", "from modules.learning.language_tools import  stopword_lists, tr_stemmer, en_stemmer", "from modules.learning.language_tools.spellchecker import en_spellchecker ", "from abc import ABCMeta, abstractmethod"]}], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [{"term": "def", "name": "to_lowercase", "data": "def to_lowercase(text):\n\treturn text.lower()\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "remove_stopwords", "data": "def remove_stopwords(text, stopwords = pl_stopwords):\n\treturn ' '.join([word for word in text.split(\" \") if word not in stopwords])\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "remove_mentions", "data": "def remove_mentions(text):\n\treturn re.sub(r'\\B\\@([\\w\\-]+)','', text)\t\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "remove_hashtags", "data": "def remove_hashtags(text):\n\treturn re.sub(r'\\B\\#([\\w\\-]+)','', text)\t\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "remove_urls", "data": "def remove_urls(text):\n\treturn re.sub(r'http\\S+','', text)\t\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "remove_emojis", "data": "def remove_emojis(text):\n\tregrex_pattern = re.compile(pattern = \"[\"\n\tu\"\\U0001F600-\\U0001F64F\"  # emoticons\n\tu\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n\tu\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n\tu\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n\t\t\t\t\t   \"]+\", flags = re.UNICODE)\n\treturn regrex_pattern.sub(r'', text)\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "remove_punctuation", "data": "def remove_punctuation(text):\n\treturn re.sub(r'[^\\w\\s]','', text)\t\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "remove_numbers", "data": "def remove_numbers(text):\n\treturn re.sub(r' \\d+','', text)\t\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "remove_n_length_words", "data": "def remove_n_length_words(text, n = 1):\n\treturn ' '.join([word for word in text.split(\" \") if len(word) > n])\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "remove_duplicated_spaces", "data": "def remove_duplicated_spaces(text):\n\treturn \" \".join(text.split())\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}, {"term": "def", "name": "preprocess_text", "data": "def preprocess_text(text):\n\ttext = to_lowercase(text)\n\ttext = remove_stopwords(text)\n\ttext = remove_mentions(text)\n\ttext = remove_hashtags(text)\n\ttext = remove_urls(text)\n\ttext = remove_emojis(text)\n\ttext = remove_punctuation(text)\n\ttext = remove_numbers(text)\n\ttext = remove_n_length_words(text)\n\ttext = remove_duplicated_spaces(text)\n", "description": null, "category": "remove", "imports": ["import re", "# TODO in the future as emojis can deliver a lot of important informations"]}], [], [], [], [{"term": "class", "name": "TransformEosDataset", "data": "class TransformEosDataset(FairseqDataset):\n\t\"\"\"A :class:`~fairseq.data.FairseqDataset` wrapper that appends/prepends/strips EOS.\n\n\tNote that the transformation is applied in :func:`collater`.\n\n\tArgs:\n\t\tdataset (~fairseq.data.FairseqDataset): dataset to wrap\n\t\teos (int): index of the end-of-sentence symbol\n\t\tappend_eos_to_src (bool, optional): append EOS to the end of src\n\t\tremove_eos_from_src (bool, optional): remove EOS from the end of src\n\t\tappend_eos_to_tgt (bool, optional): append EOS to the end of tgt\n\t\tremove_eos_from_tgt (bool, optional): remove EOS from the end of tgt\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tdataset,\n\t\teos,\n\t\tappend_eos_to_src=False,\n\t\tremove_eos_from_src=False,\n\t\tappend_eos_to_tgt=False,\n\t\tremove_eos_from_tgt=False,\n\t\thas_target=True,\n\t):\n\t\tif not isinstance(dataset, FairseqDataset):\n\t\t\traise ValueError(\"dataset must be an instance of FairseqDataset\")\n\t\tif append_eos_to_src and remove_eos_from_src:\n\t\t\traise ValueError(\"cannot combine append_eos_to_src and remove_eos_from_src\")\n\t\tif append_eos_to_tgt and remove_eos_from_tgt:\n\t\t\traise ValueError(\"cannot combine append_eos_to_tgt and remove_eos_from_tgt\")\n\n\t\tself.dataset = dataset\n\t\tself.eos = torch.LongTensor([eos])\n\t\tself.append_eos_to_src = append_eos_to_src\n\t\tself.remove_eos_from_src = remove_eos_from_src\n\t\tself.append_eos_to_tgt = append_eos_to_tgt\n\t\tself.remove_eos_from_tgt = remove_eos_from_tgt\n\t\tself.has_target = has_target\n\n\t\t# precompute how we should adjust the reported sizes\n\t\tself._src_delta = 0\n\t\tself._src_delta += 1 if append_eos_to_src else 0\n\t\tself._src_delta -= 1 if remove_eos_from_src else 0\n\t\tself._tgt_delta = 0\n\t\tself._tgt_delta += 1 if append_eos_to_tgt else 0\n\t\tself._tgt_delta -= 1 if remove_eos_from_tgt else 0\n\n\t\tself._checked_src = False\n\t\tself._checked_tgt = False\n\n\tdef _check_src(self, src, expect_eos):\n\t\tif not self._checked_src:\n\t\t\tassert (src[-1] == self.eos[0]) == expect_eos\n\t\t\tself._checked_src = True\n\n\tdef _check_tgt(self, tgt, expect_eos):\n\t\tif self.has_target and not self._checked_tgt:\n\t\t\tassert (tgt[-1] == self.eos[0]) == expect_eos\n\t\t\tself._checked_tgt = True\n\n\tdef __getitem__(self, index):\n\t\treturn self.dataset[index]\n\n\tdef __len__(self):\n\t\treturn len(self.dataset)\n\n\tdef collater(self, samples):\n\t\tdef transform(item):\n\t\t\tif self.append_eos_to_src:\n\t\t\t\tself.eos = self.eos.to(device=item[\"source\"].device)\n\t\t\t\tself._check_src(item[\"source\"], expect_eos=False)\n\t\t\t\titem[\"source\"] = torch.cat([item[\"source\"], self.eos])\n\t\t\tif self.remove_eos_from_src:\n\t\t\t\tself.eos = self.eos.to(device=item[\"source\"].device)\n\t\t\t\tself._check_src(item[\"source\"], expect_eos=True)\n\t\t\t\titem[\"source\"] = item[\"source\"][:-1]\n\t\t\tif self.append_eos_to_tgt:\n\t\t\t\tself.eos = self.eos.to(device=item[\"target\"].device)\n\t\t\t\tself._check_tgt(item[\"target\"], expect_eos=False)\n\t\t\t\titem[\"target\"] = torch.cat([item[\"target\"], self.eos])\n\t\t\tif self.remove_eos_from_tgt:\n\t\t\t\tself.eos = self.eos.to(device=item[\"target\"].device)\n\t\t\t\tself._check_tgt(item[\"target\"], expect_eos=True)\n\t\t\t\titem[\"target\"] = item[\"target\"][:-1]\n\t\t\treturn item\n\n\t\tsamples = list(map(transform, samples))\n\t\treturn self.dataset.collater(samples)\n\n\tdef num_tokens(self, index):\n\t\treturn self.dataset.num_tokens(index)\n\n\tdef size(self, index):\n\t\tif self.has_target:\n\t\t\tsrc_len, tgt_len = self.dataset.size(index)\n\t\t\treturn (src_len + self._src_delta, tgt_len + self._tgt_delta)\n\t\telse:\n\t\t\treturn self.dataset.size(index)\n\n\tdef ordered_indices(self):\n\t\t# NOTE: we assume that the ordering does not change based on the\n\t\t# addition or removal of eos\n\t\treturn self.dataset.ordered_indices()\n\n\t@property\n\tdef supports_prefetch(self):\n\t\treturn getattr(self.dataset, \"supports_prefetch\", False)\n\n\tdef prefetch(self, indices):\n\t\treturn self.dataset.prefetch(indices)\n", "description": "A :class:`~fairseq.data.FairseqDataset` wrapper that appends/prepends/strips EOS.\n\n\tNote that the transformation is applied in :func:`collater`.\n\n\tArgs:\n\t\tdataset (~fairseq.data.FairseqDataset): dataset to wrap\n\t\teos (int): index of the end-of-sentence symbol\n\t\tappend_eos_to_src (bool, optional): append EOS to the end of src\n\t\tremove_eos_from_src (bool, optional): remove EOS from the end of src\n\t\tappend_eos_to_tgt (bool, optional): append EOS to the end of tgt\n\t\tremove_eos_from_tgt (bool, optional): remove EOS from the end of tgt\n\t", "category": "remove", "imports": ["import torch", "from . import FairseqDataset"]}], [{"term": "class", "name": "TransformEosDataset", "data": "class TransformEosDataset(FairseqDataset):\n\t\"\"\"A :class:`~fairseq.data.FairseqDataset` wrapper that appends/prepends/strips EOS.\n\n\tNote that the transformation is applied in :func:`collater`.\n\n\tArgs:\n\t\tdataset (~fairseq.data.FairseqDataset): dataset to wrap\n\t\teos (int): index of the end-of-sentence symbol\n\t\tappend_eos_to_src (bool, optional): append EOS to the end of src\n\t\tremove_eos_from_src (bool, optional): remove EOS from the end of src\n\t\tappend_eos_to_tgt (bool, optional): append EOS to the end of tgt\n\t\tremove_eos_from_tgt (bool, optional): remove EOS from the end of tgt\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tdataset,\n\t\teos,\n\t\tappend_eos_to_src=False,\n\t\tremove_eos_from_src=False,\n\t\tappend_eos_to_tgt=False,\n\t\tremove_eos_from_tgt=False,\n\t\thas_target=True,\n\t):\n\t\tif not isinstance(dataset, FairseqDataset):\n\t\t\traise ValueError(\"dataset must be an instance of FairseqDataset\")\n\t\tif append_eos_to_src and remove_eos_from_src:\n\t\t\traise ValueError(\"cannot combine append_eos_to_src and remove_eos_from_src\")\n\t\tif append_eos_to_tgt and remove_eos_from_tgt:\n\t\t\traise ValueError(\"cannot combine append_eos_to_tgt and remove_eos_from_tgt\")\n\n\t\tself.dataset = dataset\n\t\tself.eos = torch.LongTensor([eos])\n\t\tself.append_eos_to_src = append_eos_to_src\n\t\tself.remove_eos_from_src = remove_eos_from_src\n\t\tself.append_eos_to_tgt = append_eos_to_tgt\n\t\tself.remove_eos_from_tgt = remove_eos_from_tgt\n\t\tself.has_target = has_target\n\n\t\t# precompute how we should adjust the reported sizes\n\t\tself._src_delta = 0\n\t\tself._src_delta += 1 if append_eos_to_src else 0\n\t\tself._src_delta -= 1 if remove_eos_from_src else 0\n\t\tself._tgt_delta = 0\n\t\tself._tgt_delta += 1 if append_eos_to_tgt else 0\n\t\tself._tgt_delta -= 1 if remove_eos_from_tgt else 0\n\n\t\tself._checked_src = False\n\t\tself._checked_tgt = False\n\n\tdef _check_src(self, src, expect_eos):\n\t\tif not self._checked_src:\n\t\t\tassert (src[-1] == self.eos[0]) == expect_eos\n\t\t\tself._checked_src = True\n\n\tdef _check_tgt(self, tgt, expect_eos):\n\t\tif self.has_target and not self._checked_tgt:\n\t\t\tassert (tgt[-1] == self.eos[0]) == expect_eos\n\t\t\tself._checked_tgt = True\n\n\tdef __getitem__(self, index):\n\t\treturn self.dataset[index]\n\n\tdef __len__(self):\n\t\treturn len(self.dataset)\n\n\tdef collater(self, samples):\n\t\tdef transform(item):\n\t\t\tif self.append_eos_to_src:\n\t\t\t\tself.eos = self.eos.to(device=item[\"source\"].device)\n\t\t\t\tself._check_src(item[\"source\"], expect_eos=False)\n\t\t\t\titem[\"source\"] = torch.cat([item[\"source\"], self.eos])\n\t\t\tif self.remove_eos_from_src:\n\t\t\t\tself.eos = self.eos.to(device=item[\"source\"].device)\n\t\t\t\tself._check_src(item[\"source\"], expect_eos=True)\n\t\t\t\titem[\"source\"] = item[\"source\"][:-1]\n\t\t\tif self.append_eos_to_tgt:\n\t\t\t\tself.eos = self.eos.to(device=item[\"target\"].device)\n\t\t\t\tself._check_tgt(item[\"target\"], expect_eos=False)\n\t\t\t\titem[\"target\"] = torch.cat([item[\"target\"], self.eos])\n\t\t\tif self.remove_eos_from_tgt:\n\t\t\t\tself.eos = self.eos.to(device=item[\"target\"].device)\n\t\t\t\tself._check_tgt(item[\"target\"], expect_eos=True)\n\t\t\t\titem[\"target\"] = item[\"target\"][:-1]\n\t\t\treturn item\n\n\t\tsamples = list(map(transform, samples))\n\t\treturn self.dataset.collater(samples)\n\n\tdef num_tokens(self, index):\n\t\treturn self.dataset.num_tokens(index)\n\n\tdef size(self, index):\n\t\tif self.has_target:\n\t\t\tsrc_len, tgt_len = self.dataset.size(index)\n\t\t\treturn (src_len + self._src_delta, tgt_len + self._tgt_delta)\n\t\telse:\n\t\t\treturn self.dataset.size(index)\n\n\tdef ordered_indices(self):\n\t\t# NOTE: we assume that the ordering does not change based on the\n\t\t# addition or removal of eos\n\t\treturn self.dataset.ordered_indices()\n\n\t@property\n\tdef supports_prefetch(self):\n\t\treturn getattr(self.dataset, \"supports_prefetch\", False)\n\n\tdef prefetch(self, indices):\n\t\treturn self.dataset.prefetch(indices)\n", "description": "A :class:`~fairseq.data.FairseqDataset` wrapper that appends/prepends/strips EOS.\n\n\tNote that the transformation is applied in :func:`collater`.\n\n\tArgs:\n\t\tdataset (~fairseq.data.FairseqDataset): dataset to wrap\n\t\teos (int): index of the end-of-sentence symbol\n\t\tappend_eos_to_src (bool, optional): append EOS to the end of src\n\t\tremove_eos_from_src (bool, optional): remove EOS from the end of src\n\t\tappend_eos_to_tgt (bool, optional): append EOS to the end of tgt\n\t\tremove_eos_from_tgt (bool, optional): remove EOS from the end of tgt\n\t", "category": "remove", "imports": ["import torch", "from . import FairseqDataset"]}], [], [{"term": "class", "name": "PluginBrowserSummary", "data": "class PluginBrowserSummary(Screen):\n\tdef __init__(self, session, parent):\n\t\tScreen.__init__(self, session, parent = parent)\n\t\tself[\"entry\"] = StaticText(\"\")\n\t\tself[\"desc\"] = StaticText(\"\")\n\t\tself.onShow.append(self.addWatcher)\n\t\tself.onHide.append(self.removeWatcher)\n\n\tdef addWatcher(self):\n\t\tself.parent.onChangedEntry.append(self.selectionChanged)\n\t\tself.parent.selectionChanged()\n\n\tdef removeWatcher(self):\n\t\tself.parent.onChangedEntry.remove(self.selectionChanged)\n\n\tdef selectionChanged(self, name, desc):\n\t\tself[\"entry\"].text = name\n\t\tself[\"desc\"].text = desc\n\n", "description": null, "category": "remove", "imports": ["from Screen import Screen", "from Screens.ParentalControlSetup import ProtectedScreen", "from enigma import eConsoleAppContainer, eDVBDB", "from Components.ActionMap import ActionMap, NumberActionMap", "from Components.config import config, ConfigSubsection, ConfigText", "from Components.PluginComponent import plugins", "from Components.PluginList import *", "from Components.Label import Label", "from Components.Language import language", "from Components.Harddisk import harddiskmanager", "from Components.Sources.StaticText import StaticText", "from Components import Ipkg", "from Screens.MessageBox import MessageBox", "from Screens.ChoiceBox import ChoiceBox", "from Screens.Console import Console", "from Plugins.Plugin import PluginDescriptor", "from Tools.Directories import resolveFilename, SCOPE_PLUGINS, SCOPE_CURRENT_SKIN", "from Tools.LoadPixmap import LoadPixmap", "from time import time", "import os", "\t\t\t\tfrom Plugins.SystemPlugins.SoftwareManager.plugin import PluginManager", "\t\t\t\t\timport Components.Harddisk", "\t\t\t\t\t\tfrom Components.Renderer import Picon", "\t\t\tfrom Components import opkg"]}, {"term": "class", "name": "PluginBrowser", "data": "class PluginBrowser(Screen, ProtectedScreen):\n\tdef __init__(self, session):\n\t\tScreen.__init__(self, session)\n\t\tProtectedScreen.__init__(self)\n\n\t\tself.firsttime = True\n\n\t\tself[\"key_red\"] = self[\"red\"] = Label(_(\"Remove plugins\"))\n\t\tself[\"key_green\"] = self[\"green\"] = Label(_(\"Download plugins\"))\n\n\t\tself.list = []\n\t\tself[\"list\"] = PluginList(self.list)\n\n\t\tself[\"actions\"] = ActionMap([\"WizardActions\",\"MenuActions\"],\n\t\t{\n\t\t\t\"ok\": self.save,\n\t\t\t\"back\": self.close,\n\t\t\t\"menu\": self.exit,\n\t\t})\n\t\tself[\"PluginDownloadActions\"] = ActionMap([\"ColorActions\"],\n\t\t{\n\t\t\t\"red\": self.delete,\n\t\t\t\"green\": self.download\n\t\t})\n\t\tself[\"DirectionActions\"] = ActionMap([\"DirectionActions\"],\n\t\t{\n\t\t\t\"moveUp\": self.moveUp,\n\t\t\t\"moveDown\": self.moveDown\n\t\t})\n\t\tself[\"NumberActions\"] = NumberActionMap([\"NumberActions\"],\n\t\t{\n\t\t\t\"1\": self.keyNumberGlobal,\n\t\t\t\"2\": self.keyNumberGlobal,\n\t\t\t\"3\": self.keyNumberGlobal,\n\t\t\t\"4\": self.keyNumberGlobal,\n\t\t\t\"5\": self.keyNumberGlobal,\n\t\t\t\"6\": self.keyNumberGlobal,\n\t\t\t\"7\": self.keyNumberGlobal,\n\t\t\t\"8\": self.keyNumberGlobal,\n\t\t\t\"9\": self.keyNumberGlobal,\n\t\t\t\"0\": self.keyNumberGlobal\n\t\t})\n\n\t\tself.onFirstExecBegin.append(self.checkWarnings)\n\t\tself.onShown.append(self.updateList)\n\t\tself.onChangedEntry = []\n\t\tself[\"list\"].onSelectionChanged.append(self.selectionChanged)\n\t\tself.onLayoutFinish.append(self.saveListsize)\n\n\tdef isProtected(self):\n\t\treturn config.ParentalControl.setuppinactive.value and (not config.ParentalControl.config_sections.main_menu.value or hasattr(self.session, 'infobar') and self.session.infobar is None) and config.ParentalControl.config_sections.plugin_browser.value\n\n\tdef exit(self):\n\t\tself.close(True)\n\n\tdef saveListsize(self):\n\t\tlistsize = self[\"list\"].instance.size()\n\t\tself.listWidth = listsize.width()\n\t\tself.listHeight = listsize.height()\n\n\tdef createSummary(self):\n\t\treturn PluginBrowserSummary\n\n\tdef selectionChanged(self):\n\t\titem = self[\"list\"].getCurrent()\n\t\tif item:\n\t\t\tp = item[0]\n\t\t\tname = p.name\n\t\t\tdesc = p.description\n\t\telse:\n\t\t\tname = \"-\"\n\t\t\tdesc = \"\"\n\t\tfor cb in self.onChangedEntry:\n\t\t\tcb(name, desc)\n\n\tdef checkWarnings(self):\n\t\tif len(plugins.warnings):\n\t\t\ttext = _(\"Some plugins are not available:\\n\")\n\t\t\tfor (pluginname, error) in plugins.warnings:\n\t\t\t\ttext += _(\"%s (%s)\\n\") % (pluginname, error)\n\t\t\tplugins.resetWarnings()\n\t\t\tself.session.open(MessageBox, text = text, type = MessageBox.TYPE_WARNING)\n\n\tdef save(self):\n\t\tself.run()\n\n\tdef run(self):\n\t\tplugin = self[\"list\"].l.getCurrentSelection()[0]\n\t\tplugin(session=self.session)\n\n\tdef setDefaultList(self, answer):\n\t\tif answer:\n\t\t\tconfig.misc.pluginbrowser.plugin_order.value = \"\"\n\t\t\tconfig.misc.pluginbrowser.plugin_order.save()\n\t\t\tself.updateList()\n\n\tdef keyNumberGlobal(self, number):\n\t\tif number == 0:\n\t\t\tif len(self.list) > 0 and config.misc.pluginbrowser.plugin_order.value != \"\":\n\t\t\t\tself.session.openWithCallback(self.setDefaultList, MessageBox, _(\"Sort plugins list to default?\"), MessageBox.TYPE_YESNO)\n\t\telse:\n\t\t\treal_number = number - 1\n\t\t\tif real_number < len(self.list):\n\t\t\t\tself[\"list\"].moveToIndex(real_number)\n\t\t\t\tself.run()\n\n\tdef moveUp(self):\n\t\tself.move(-1)\n\n\tdef moveDown(self):\n\t\tself.move(1)\n\n\tdef move(self, direction):\n\t\tif len(self.list) > 1:\n\t\t\tcurrentIndex = self[\"list\"].getSelectionIndex()\n\t\t\tswapIndex = (currentIndex + direction) % len(self.list)\n\t\t\tif currentIndex == 0 and swapIndex != 1:\n\t\t\t\tself.list = self.list[1:] + [self.list[0]]\n\t\t\telif swapIndex == 0 and currentIndex != 1:\n\t\t\t\tself.list = [self.list[-1]] + self.list[:-1]\n\t\t\telse:\n\t\t\t\tself.list[currentIndex], self.list[swapIndex] = self.list[swapIndex], self.list[currentIndex]\n\t\t\tself[\"list\"].l.setList(self.list)\n\t\t\tif direction == 1:\n\t\t\t\tself[\"list\"].down()\n\t\t\telse:\n\t\t\t\tself[\"list\"].up()\n\t\t\tplugin_order = []\n\t\t\tfor x in self.list:\n\t\t\t\tplugin_order.append(x[0].path[24:])\n\t\t\tconfig.misc.pluginbrowser.plugin_order.value = \",\".join(plugin_order)\n\t\t\tconfig.misc.pluginbrowser.plugin_order.save()\n\n\tdef updateList(self):\n\t\tself.list = []\n\t\tpluginlist = plugins.getPlugins(PluginDescriptor.WHERE_PLUGINMENU)[:]\n\t\tfor x in config.misc.pluginbrowser.plugin_order.value.split(\",\"):\n\t\t\tplugin = list(plugin for plugin in pluginlist if plugin.path[24:] == x)\n\t\t\tif plugin:\n\t\t\t\tself.list.append(PluginEntryComponent(plugin[0], self.listWidth))\n\t\t\t\tpluginlist.remove(plugin[0])\n\t\tself.list = self.list + [PluginEntryComponent(plugin, self.listWidth) for plugin in pluginlist]\n\t\tself[\"list\"].l.setList(self.list)\n\n\tdef delete(self):\n\t\tself.session.openWithCallback(self.PluginDownloadBrowserClosed, PluginDownloadBrowser, PluginDownloadBrowser.REMOVE)\n\n\tdef download(self):\n\t\tself.session.openWithCallback(self.PluginDownloadBrowserClosed, PluginDownloadBrowser, PluginDownloadBrowser.DOWNLOAD, self.firsttime)\n\t\tself.firsttime = False\n\n\tdef PluginDownloadBrowserClosed(self):\n\t\tself.updateList()\n\t\tself.checkWarnings()\n\n\tdef openExtensionmanager(self):\n\t\tif fileExists(resolveFilename(SCOPE_PLUGINS, \"SystemPlugins/SoftwareManager/plugin.py\")):\n\t\t\ttry:\n\t\t\t\tfrom Plugins.SystemPlugins.SoftwareManager.plugin import PluginManager\n\t\t\texcept ImportError:\n\t\t\t\tself.session.open(MessageBox, _(\"The software management extension is not installed!\\nPlease install it.\"), type = MessageBox.TYPE_INFO,timeout = 10 )\n\t\t\telse:\n\t\t\t\tself.session.openWithCallback(self.PluginDownloadBrowserClosed, PluginManager)\n", "description": null, "category": "remove", "imports": ["from Screen import Screen", "from Screens.ParentalControlSetup import ProtectedScreen", "from enigma import eConsoleAppContainer, eDVBDB", "from Components.ActionMap import ActionMap, NumberActionMap", "from Components.config import config, ConfigSubsection, ConfigText", "from Components.PluginComponent import plugins", "from Components.PluginList import *", "from Components.Label import Label", "from Components.Language import language", "from Components.Harddisk import harddiskmanager", "from Components.Sources.StaticText import StaticText", "from Components import Ipkg", "from Screens.MessageBox import MessageBox", "from Screens.ChoiceBox import ChoiceBox", "from Screens.Console import Console", "from Plugins.Plugin import PluginDescriptor", "from Tools.Directories import resolveFilename, SCOPE_PLUGINS, SCOPE_CURRENT_SKIN", "from Tools.LoadPixmap import LoadPixmap", "from time import time", "import os", "\t\t\t\tfrom Plugins.SystemPlugins.SoftwareManager.plugin import PluginManager", "\t\t\t\t\timport Components.Harddisk", "\t\t\t\t\t\tfrom Components.Renderer import Picon", "\t\t\tfrom Components import opkg"]}, {"term": "class", "name": "PluginDownloadBrowser", "data": "class PluginDownloadBrowser(Screen):\n\tDOWNLOAD = 0\n\tREMOVE = 1\n\tPLUGIN_PREFIX = 'enigma2-plugin-'\n\tlastDownloadDate = None\n\n\tdef __init__(self, session, type = 0, needupdate = True):\n\t\tScreen.__init__(self, session)\n\n\t\tself.type = type\n\t\tself.needupdate = needupdate\n\n\t\tself.container = eConsoleAppContainer()\n\t\tself.container.appClosed.append(self.runFinished)\n\t\tself.container.dataAvail.append(self.dataAvail)\n\t\tself.onLayoutFinish.append(self.startRun)\n\t\tself.onShown.append(self.setWindowTitle)\n\n\t\tself.list = []\n\t\tself[\"list\"] = PluginList(self.list)\n\t\tself.pluginlist = []\n\t\tself.expanded = []\n\t\tself.installedplugins = []\n\t\tself.plugins_changed = False\n\t\tself.reload_settings = False\n\t\tself.check_settings = False\n\t\tself.install_settings_name = ''\n\t\tself.remove_settings_name = ''\n\n\t\tif self.type == self.DOWNLOAD:\n\t\t\tself[\"text\"] = Label(_(\"Downloading plugin information. Please wait...\"))\n\t\telif self.type == self.REMOVE:\n\t\t\tself[\"text\"] = Label(_(\"Getting plugin information. Please wait...\"))\n\n\t\tself.run = 0\n\t\tself.remainingdata = \"\"\n\t\tself[\"actions\"] = ActionMap([\"WizardActions\"],\n\t\t{\n\t\t\t\"ok\": self.go,\n\t\t\t\"back\": self.requestClose,\n\t\t})\n\t\tif os.path.isfile('/usr/bin/opkg'):\n\t\t\tself.ipkg = '/usr/bin/opkg'\n\t\t\tself.ipkg_install = self.ipkg + ' install'\n\t\t\tself.ipkg_remove =  self.ipkg + ' remove --autoremove'\n\t\telse:\n\t\t\tself.ipkg = 'ipkg'\n\t\t\tself.ipkg_install = 'ipkg install -force-defaults'\n\t\t\tself.ipkg_remove =  self.ipkg + ' remove'\n\n\tdef go(self):\n\t\tsel = self[\"list\"].l.getCurrentSelection()\n\n\t\tif sel is None:\n\t\t\treturn\n\n\t\tsel = sel[0]\n\t\tif isinstance(sel, str): # category\n\t\t\tif sel in self.expanded:\n\t\t\t\tself.expanded.remove(sel)\n\t\t\telse:\n\t\t\t\tself.expanded.append(sel)\n\t\t\tself.updateList()\n\t\telse:\n\t\t\tif self.type == self.DOWNLOAD:\n\t\t\t\tself.session.openWithCallback(self.runInstall, MessageBox, _(\"Do you really want to download\\nthe plugin \\\"%s\\\"?\") % sel.name)\n\t\t\telif self.type == self.REMOVE:\n\t\t\t\tself.session.openWithCallback(self.runInstall, MessageBox, _(\"Do you really want to remove\\nthe plugin \\\"%s\\\"?\") % sel.name)\n\n\tdef requestClose(self):\n\t\tif self.plugins_changed:\n\t\t\tplugins.readPluginList(resolveFilename(SCOPE_PLUGINS))\n\t\tif self.reload_settings:\n\t\t\tself[\"text\"].setText(_(\"Reloading bouquets and services...\"))\n\t\t\teDVBDB.getInstance().reloadBouquets()\n\t\t\teDVBDB.getInstance().reloadServicelist()\n\t\tplugins.readPluginList(resolveFilename(SCOPE_PLUGINS))\n\t\tself.container.appClosed.remove(self.runFinished)\n\t\tself.container.dataAvail.remove(self.dataAvail)\n\t\tself.close()\n\n\tdef resetPostInstall(self):\n\t\ttry:\n\t\t\tdel self.postInstallCall\n\t\texcept:\n\t\t\tpass\n\n\tdef installDestinationCallback(self, result):\n\t\tif result is not None:\n\t\t\tdest = result[1]\n\t\t\tif dest.startswith('/'):\n\t\t\t\t# Custom install path, add it to the list too\n\t\t\t\tdest = os.path.normpath(dest)\n\t\t\t\textra = '--add-dest %s:%s -d %s' % (dest,dest,dest)\n\t\t\t\tIpkg.opkgAddDestination(dest)\n\t\t\telse:\n\t\t\t\textra = '-d ' + dest\n\t\t\tself.doInstall(self.installFinished, self[\"list\"].l.getCurrentSelection()[0].name + ' ' + extra)\n\t\telse:\n\t\t\tself.resetPostInstall()\n\n\tdef runInstall(self, val):\n\t\tif val:\n\t\t\tif self.type == self.DOWNLOAD:\n\t\t\t\tif self[\"list\"].l.getCurrentSelection()[0].name.startswith(\"picons-\"):\n\t\t\t\t\tsupported_filesystems = frozenset(('ext4', 'ext3', 'ext2', 'reiser', 'reiser4', 'jffs2', 'ubifs', 'rootfs'))\n\t\t\t\t\tcandidates = []\n\t\t\t\t\timport Components.Harddisk\n\t\t\t\t\tmounts = Components.Harddisk.getProcMounts()\n\t\t\t\t\tfor partition in harddiskmanager.getMountedPartitions(False, mounts):\n\t\t\t\t\t\tif partition.filesystem(mounts) in supported_filesystems:\n\t\t\t\t\t\t\tcandidates.append((partition.description, partition.mountpoint))\n\t\t\t\t\tif candidates:\n\t\t\t\t\t\tfrom Components.Renderer import Picon\n\t\t\t\t\t\tself.postInstallCall = Picon.initPiconPaths\n\t\t\t\t\t\tself.session.openWithCallback(self.installDestinationCallback, ChoiceBox, title=_(\"Install picons on\"), list=candidates)\n\t\t\t\t\treturn\n\t\t\t\tself.install_settings_name = self[\"list\"].l.getCurrentSelection()[0].name\n\t\t\t\tif self[\"list\"].l.getCurrentSelection()[0].name.startswith('settings-'):\n\t\t\t\t\tself.check_settings = True\n\t\t\t\t\tself.startIpkgListInstalled(self.PLUGIN_PREFIX + 'settings-*')\n\t\t\t\telse:\n\t\t\t\t\tself.runSettingsInstall()\n\t\t\telif self.type == self.REMOVE:\n\t\t\t\tself.doRemove(self.installFinished, self[\"list\"].l.getCurrentSelection()[0].name)\n\n\tdef doRemove(self, callback, pkgname):\n\t\tself.session.openWithCallback(callback, Console, cmdlist = [self.ipkg_remove + Ipkg.opkgExtraDestinations() + \" \" + self.PLUGIN_PREFIX + pkgname, \"sync\"], closeOnSuccess = True)\n\n\tdef doInstall(self, callback, pkgname):\n\t\tself.session.openWithCallback(callback, Console, cmdlist = [self.ipkg_install + \" \" + self.PLUGIN_PREFIX + pkgname, \"sync\"], closeOnSuccess = True)\n\n\tdef runSettingsRemove(self, val):\n\t\tif val:\n\t\t\tself.doRemove(self.runSettingsInstall, self.remove_settings_name)\n\n\tdef runSettingsInstall(self):\n\t\tself.doInstall(self.installFinished, self.install_settings_name)\n\n\tdef setWindowTitle(self):\n\t\tif self.type == self.DOWNLOAD:\n\t\t\tself.setTitle(_(\"Downloadable new plugins\"))\n\t\telif self.type == self.REMOVE:\n\t\t\tself.setTitle(_(\"Remove plugins\"))\n\n\tdef startIpkgListInstalled(self, pkgname = PLUGIN_PREFIX + '*'):\n\t\tself.container.execute(self.ipkg + Ipkg.opkgExtraDestinations() + \" list_installed '%s'\" % pkgname)\n\n\tdef startIpkgListAvailable(self):\n\t\tself.container.execute(self.ipkg + Ipkg.opkgExtraDestinations() + \" list '\" + self.PLUGIN_PREFIX + \"*'\")\n\n\tdef startRun(self):\n\t\tlistsize = self[\"list\"].instance.size()\n\t\tself[\"list\"].instance.hide()\n\t\tself.listWidth = listsize.width()\n\t\tself.listHeight = listsize.height()\n\t\tif self.type == self.DOWNLOAD:\n\t\t\tif self.needupdate and not PluginDownloadBrowser.lastDownloadDate or (time() - PluginDownloadBrowser.lastDownloadDate) > 3600:\n\t\t\t\t# Only update from internet once per hour\n\t\t\t\tself.container.execute(self.ipkg + \" update\")\n\t\t\t\tPluginDownloadBrowser.lastDownloadDate = time()\n\t\t\telse:\n\t\t\t\tself.run = 1\n\t\t\t\tself.startIpkgListInstalled()\n\t\telif self.type == self.REMOVE:\n\t\t\tself.run = 1\n\t\t\tself.startIpkgListInstalled()\n\n\tdef installFinished(self):\n\t\tif hasattr(self, 'postInstallCall'):\n\t\t\ttry:\n\t\t\t\tself.postInstallCall()\n\t\t\texcept Exception, ex:\n\t\t\t\tprint \"[PluginBrowser] postInstallCall failed:\", ex\n\t\t\tself.resetPostInstall()\n\t\ttry:\n\t\t\tos.unlink('/tmp/opkg.conf')\n\t\texcept:\n\t\t\tpass\n\t\tfor plugin in self.pluginlist:\n\t\t\tif plugin[3] == self[\"list\"].l.getCurrentSelection()[0].name:\n\t\t\t\tself.pluginlist.remove(plugin)\n\t\t\t\tbreak\n\t\tself.plugins_changed = True\n\t\tif self[\"list\"].l.getCurrentSelection()[0].name.startswith(\"settings-\"):\n\t\t\tself.reload_settings = True\n\t\tself.expanded = []\n\t\tself.updateList()\n\t\tself[\"list\"].moveToIndex(0)\n\n\tdef runFinished(self, retval):\n\t\tif self.check_settings:\n\t\t\tself.check_settings = False\n\t\t\tself.runSettingsInstall()\n\t\t\treturn\n\t\tself.remainingdata = \"\"\n\t\tif self.run == 0:\n\t\t\tself.run = 1\n\t\t\tif self.type == self.DOWNLOAD:\n\t\t\t\tself.startIpkgListInstalled()\n\t\telif self.run == 1 and self.type == self.DOWNLOAD:\n\t\t\tself.run = 2\n\t\t\tfrom Components import opkg\n\t\t\tpluginlist = []\n\t\t\tself.pluginlist = pluginlist\n\t\t\tfor plugin in opkg.enumPlugins(self.PLUGIN_PREFIX):\n\t\t\t\tif plugin[0] not in self.installedplugins:\n\t\t\t\t\tpluginlist.append(plugin + (plugin[0][15:],))\n\t\t\tif pluginlist:\n\t\t\t\tpluginlist.sort()\n\t\t\t\tself.updateList()\n\t\t\t\tself[\"list\"].instance.show()\n\t\t\telse:\n\t\t\t\tself[\"text\"].setText(_(\"No new plugins found\"))\n\t\telse:\n\t\t\tif self.pluginlist:\n\t\t\t\tself.updateList()\n\t\t\t\tself[\"list\"].instance.show()\n\t\t\telse:\n\t\t\t\tself[\"text\"].setText(_(\"No new plugins found\"))\n\n\tdef dataAvail(self, str):\n\t\t#prepend any remaining data from the previous call\n\t\tstr = self.remainingdata + str\n\t\t#split in lines\n\t\tlines = str.split('\\n')\n\t\t#'str' should end with '\\n', so when splitting, the last line should be empty. If this is not the case, we received an incomplete line\n\t\tif len(lines[-1]):\n\t\t\t#remember this data for next time\n\t\t\tself.remainingdata = lines[-1]\n\t\t\tlines = lines[0:-1]\n\t\telse:\n\t\t\tself.remainingdata = \"\"\n\n\t\tif self.check_settings:\n\t\t\tself.check_settings = False\n\t\t\tself.remove_settings_name = str.split(' - ')[0].replace(self.PLUGIN_PREFIX, '')\n\t\t\tself.session.openWithCallback(self.runSettingsRemove, MessageBox, _('You already have a channel list installed,\\nwould you like to remove\\n\"%s\"?') % self.remove_settings_name)\n\t\t\treturn\n\n\t\tif self.run == 1:\n\t\t\tfor x in lines:\n\t\t\t\tplugin = x.split(\" - \", 2)\n\t\t\t\t# 'opkg list_installed' only returns name + version, no description field\n\t\t\t\tif len(plugin) >= 2:\n\t\t\t\t\tif not plugin[0].endswith('-dev') and not plugin[0].endswith('-staticdev') and not plugin[0].endswith('-dbg') and not plugin[0].endswith('-doc') and not plugin[0].endswith('-src'):\n\t\t\t\t\t\tif plugin[0] not in self.installedplugins:\n\t\t\t\t\t\t\tif self.type == self.DOWNLOAD:\n\t\t\t\t\t\t\t\tself.installedplugins.append(plugin[0])\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tif len(plugin) == 2:\n\t\t\t\t\t\t\t\t\tplugin.append('')\n\t\t\t\t\t\t\t\tplugin.append(plugin[0][15:])\n\t\t\t\t\t\t\t\tself.pluginlist.append(plugin)\n\n\tdef updateList(self):\n\t\tlist = []\n\t\texpandableIcon = LoadPixmap(resolveFilename(SCOPE_CURRENT_SKIN, \"skin_default/expandable-plugins.png\"))\n\t\texpandedIcon = LoadPixmap(resolveFilename(SCOPE_CURRENT_SKIN, \"skin_default/expanded-plugins.png\"))\n\t\tverticallineIcon = LoadPixmap(resolveFilename(SCOPE_CURRENT_SKIN, \"skin_default/verticalline-plugins.png\"))\n\n\t\tself.plugins = {}\n\t\tfor x in self.pluginlist:\n\t\t\tsplit = x[3].split('-', 1)\n\t\t\tif len(split) < 2:\n\t\t\t\tcontinue\n\t\t\tif not self.plugins.has_key(split[0]):\n\t\t\t\tself.plugins[split[0]] = []\n\n\t\t\tself.plugins[split[0]].append((PluginDescriptor(name = x[3], description = x[2], icon = verticallineIcon), split[1], x[1]))\n\n\t\tfor x in self.plugins.keys():\n\t\t\tif x in self.expanded:\n\t\t\t\tlist.append(PluginCategoryComponent(x, expandedIcon, self.listWidth))\n\t\t\t\tlist.extend([PluginDownloadComponent(plugin[0], plugin[1], plugin[2], self.listWidth) for plugin in self.plugins[x]])\n\t\t\telse:\n\t\t\t\tlist.append(PluginCategoryComponent(x, expandableIcon, self.listWidth))\n\t\tself.list = list\n\t\tself[\"list\"].l.setList(list)\n", "description": null, "category": "remove", "imports": ["from Screen import Screen", "from Screens.ParentalControlSetup import ProtectedScreen", "from enigma import eConsoleAppContainer, eDVBDB", "from Components.ActionMap import ActionMap, NumberActionMap", "from Components.config import config, ConfigSubsection, ConfigText", "from Components.PluginComponent import plugins", "from Components.PluginList import *", "from Components.Label import Label", "from Components.Language import language", "from Components.Harddisk import harddiskmanager", "from Components.Sources.StaticText import StaticText", "from Components import Ipkg", "from Screens.MessageBox import MessageBox", "from Screens.ChoiceBox import ChoiceBox", "from Screens.Console import Console", "from Plugins.Plugin import PluginDescriptor", "from Tools.Directories import resolveFilename, SCOPE_PLUGINS, SCOPE_CURRENT_SKIN", "from Tools.LoadPixmap import LoadPixmap", "from time import time", "import os", "\t\t\t\tfrom Plugins.SystemPlugins.SoftwareManager.plugin import PluginManager", "\t\t\t\t\timport Components.Harddisk", "\t\t\t\t\t\tfrom Components.Renderer import Picon", "\t\t\tfrom Components import opkg"]}], [{"term": "def", "name": "solution", "data": "def solution(n, losts, reserves):\n\treserve = set(reserves) - set(losts) # \uc9c4\uc9dc \uc5ec\ubc8c \uc788\ub294 \ud559\uc0dd\ub4e4\ub9cc \ub0a8\uae30\uae30\n\tlost = set(losts) - set(reserves)\n\t\n\tfor r in reserve:\n\t\tif r-1 in lost: # \uc783\uc5b4\ubc84\ub9b0 \uc0ac\ub78c\uc774\uba74....\n\t\t\tlost.remove(r-1)\n\t\telif r+1 in lost:\n\t\t\tlost.remove(r+1)\n\t\n\treturn n - len(lost)\n\n", "description": null, "category": "remove", "imports": []}], [{"term": "class", "name": "ObjectPropertyGroup", "data": "class ObjectPropertyGroup(bpy.types.PropertyGroup):\n\tbl_idname = \"an_ObjectPropertyGroup\"\n\tobject: PointerProperty(type = bpy.types.Object, name = \"Object\")\n", "description": null, "category": "remove", "imports": ["import bpy", "from bpy.props import *", "from ... events import propertyChanged", "from ... base_types import AnimationNode", "from ... utils.names import getRandomString", "from ... utils.blender_ui import iterActiveSpacesByType", "from ... utils.data_blocks import removeNotUsedDataBlock", "from ... nodes.container_provider import getMainObjectContainer", "from ... utils.names import (getPossibleMeshName,"]}, {"term": "class", "name": "ObjectInstancerNode", "data": "class ObjectInstancerNode(bpy.types.Node, AnimationNode):\n\tbl_idname = \"an_ObjectInstancerNode\"\n\tbl_label = \"Object Instancer\"\n\tbl_width_default = 160\n\toptions = {\"NOT_IN_SUBPROGRAM\"}\n\n\tdef copyFromSourceChanged(self, context):\n\t\tself.refresh()\n\t\tself.resetInstancesEvent(context)\n\n\tdef resetInstancesEvent(self, context):\n\t\tself.resetInstances = True\n\t\tpropertyChanged()\n\n\tlinkedObjects: CollectionProperty(type = ObjectPropertyGroup)\n\tresetInstances: BoolProperty(default = False, update = propertyChanged)\n\n\tcopyFromSource: BoolProperty(name = \"Copy from Source\",\n\t\tdefault = True, update = copyFromSourceChanged)\n\n\tdeepCopy: BoolProperty(name = \"Deep Copy\", default = False, update = resetInstancesEvent,\n\t\tdescription = \"Make the instances independent of the source object (e.g. copy mesh)\")\n\n\tobjectType: EnumProperty(name = \"Object Type\", default = \"MESH\",\n\t\titems = objectTypeItems, update = resetInstancesEvent)\n\n\tcopyObjectProperties: BoolProperty(name = \"Copy Full Object\", default = False,\n\t\tdescription = \"Enable this to copy modifiers/constraints/... from the source object\",\n\t\tupdate = resetInstancesEvent)\n\n\tremoveAnimationData: BoolProperty(name = \"Remove Animation Data\", default = True,\n\t\tdescription = \"Remove the active action on the instance; This is useful when you want to animate the object yourself\",\n\t\tupdate = resetInstancesEvent)\n\n\taddToMainContainer: BoolProperty(name = \"Add To Main Container\",\n\t\tdefault = True, update = resetInstancesEvent)\n\n\temptyDisplayType: EnumProperty(name = \"Empty Draw Type\", default = \"PLAIN_AXES\",\n\t\titems = emptyDisplayTypeItems, update = resetInstancesEvent)\n\n\tdef create(self):\n\t\tself.newInput(\"Integer\", \"Instances\", \"instancesAmount\", minValue = 0)\n\t\tif self.copyFromSource:\n\t\t\tself.newInput(\"Object\", \"Source\", \"sourceObject\",\n\t\t\t\tdefaultDrawType = \"PROPERTY_ONLY\", showHideToggle = True)\n\t\tself.newInput(\"Scene List\", \"Scenes\", \"scenes\", hide = True)\n\n\t\tself.newOutput(\"an_ObjectListSocket\", \"Objects\", \"objects\")\n\n\tdef draw(self, layout):\n\t\tlayout.prop(self, \"copyFromSource\")\n\t\tif self.copyFromSource:\n\t\t\tlayout.prop(self, \"copyObjectProperties\", text = \"Copy Full Object\")\n\t\t\tlayout.prop(self, \"deepCopy\")\n\t\telse:\n\t\t\tlayout.prop(self, \"objectType\", text = \"\")\n\t\t\tif self.objectType == \"Empty\":\n\t\t\t\tlayout.prop(self, \"emptyDisplayType\", text = \"\")\n\n\tdef drawAdvanced(self, layout):\n\t\tlayout.prop(self, \"addToMainContainer\")\n\t\tlayout.prop(self, \"removeAnimationData\")\n\n\t\tself.invokeFunction(layout, \"resetObjectDataOnAllInstances\",\n\t\t\ttext = \"Reset Source Data\",\n\t\t\tdescription = \"Reset the source data on all instances\")\n\t\tself.invokeFunction(layout, \"unlinkInstancesFromNode\",\n\t\t\tconfirm = True,\n\t\t\ttext = \"Unlink Instances from Node\",\n\t\t\tdescription = \"This will make sure that the objects won't be removed if you remove the Instancer Node\")\n\n\t\tlayout.separator()\n\t\tself.invokeFunction(layout, \"toggleRelationshipLines\",\n\t\t\ttext = \"Toggle Relationship Lines\",\n\t\t\ticon = \"RESTRICT_VIEW_OFF\")\n\n\tdef getExecutionCode(self, required):\n\t\t# support for older nodes which didn't have a scene list input\n\t\tif \"Scenes\" in self.inputs: yield \"_scenes = set(scenes)\"\n\t\telse: yield \"_scenes = {scene}\"\n\n\t\tif self.copyFromSource:\n\t\t\tyield \"objects = self.getInstances_WithSource(instancesAmount, sourceObject, _scenes)\"\n\t\telse:\n\t\t\tyield \"objects = self.getInstances_WithoutSource(instancesAmount, _scenes)\"\n\n\tdef getInstances_WithSource(self, instancesAmount, sourceObject, scenes):\n\t\tif sourceObject is None:\n\t\t\tself.removeAllObjects()\n\t\t\treturn []\n\t\telse:\n\t\t\tsourceHash = hash(sourceObject)\n\t\t\tif self.identifier in lastSourceHashes:\n\t\t\t\tif lastSourceHashes[self.identifier] != sourceHash:\n\t\t\t\t\tself.removeAllObjects()\n\t\t\tlastSourceHashes[self.identifier] = sourceHash\n\n\t\treturn self.getInstances_Base(instancesAmount, sourceObject, scenes)\n\n\tdef getInstances_WithoutSource(self, instancesAmount, scenes):\n\t\treturn self.getInstances_Base(instancesAmount, None, scenes)\n\n\tdef getInstances_Base(self, instancesAmount, sourceObject, scenes):\n\t\tinstancesAmount = max(instancesAmount, 0)\n\n\t\tif not any(scenes):\n\t\t\tself.removeAllObjects()\n\t\t\treturn []\n\t\telse:\n\t\t\tsceneHash = set(hash(scene) for scene in scenes)\n\t\t\tif self.identifier in lastSceneHashes:\n\t\t\t\tif lastSceneHashes[self.identifier] != sceneHash:\n\t\t\t\t\tself.removeAllObjects()\n\t\t\tlastSceneHashes[self.identifier] = sceneHash\n\n\t\tif self.resetInstances:\n\t\t\tself.removeAllObjects()\n\t\t\tself.resetInstances = False\n\n\t\tself.removeObjectsInRange(instancesAmount, len(self.linkedObjects))\n\n\t\treturn self.getOutputObjects(instancesAmount, sourceObject, scenes)\n\n\n\tdef getOutputObjects(self, instancesAmount, sourceObject, scenes):\n\t\tobjects = []\n\n\t\tfor i, objectGroup in enumerate(self.linkedObjects):\n\t\t\tobject = objectGroup.object\n\t\t\tif object is None:\n\t\t\t\tself.linkedObjects.remove(i)\n\t\t\telse:\n\t\t\t\tobjects.append(object)\n\n\t\tmissingAmount = instancesAmount - len(objects)\n\t\tif missingAmount == 0:\n\t\t\treturn objects\n\n\t\tnewObjects = self.createNewObjects(missingAmount, sourceObject, scenes)\n\t\tobjects.extend(newObjects)\n\n\t\treturn objects\n\n\tdef removeAllObjects(self):\n\t\tfor objectGroup in self.linkedObjects:\n\t\t\tobject = objectGroup.object\n\t\t\tif object is not None:\n\t\t\t\tself.removeObject(object)\n\n\t\tself.linkedObjects.clear()\n\n\tdef removeObjectsInRange(self, start, end):\n\t\tfor i in reversed(range(start, end)):\n\t\t\tself.removeObjectAtIndex(i)\n\n\tdef removeObjectAtIndex(self, index):\n\t\tobject = self.linkedObjects[index].object\n\t\tif object is not None:\n\t\t\tself.removeObject(object)\n\t\tself.linkedObjects.remove(index)\n\n\tdef removeObject(self, object):\n\t\tdata = object.data\n\t\ttype = object.type\n\t\tself.removeShapeKeys(object)\n\t\tbpy.data.objects.remove(object)\n\t\tself.removeObjectData(data, type)\n\n\tdef removeObjectData(self, data, type):\n\t\tif data is None: return # the object was an empty\n\t\tif data.an_data.removeOnZeroUsers and data.users == 0:\n\t\t\tremoveNotUsedDataBlock(data, type)\n\n\tdef removeShapeKeys(self, object):\n\t\t# don't remove the shape key if it is used somewhere else\n\t\tif object.type not in (\"MESH\", \"CURVE\", \"LATTICE\"): return\n\t\tif object.data.shape_keys is None: return\n\t\tif object.data.shape_keys.user.users > 1: return\n\n\t\tobject.active_shape_key_index = 0\n\t\twhile object.active_shape_key is not None:\n\t\t\tobject.shape_key_remove(object.active_shape_key)\n\n\tdef createNewObjects(self, amount, sourceObject, scenes):\n\t\tobjects = []\n\t\tnameSuffix = \"instance_{}_\".format(getRandomString(5))\n\t\tfor i in range(amount):\n\t\t\tname = nameSuffix + str(i)\n\t\t\tnewObject = self.appendNewObject(name, sourceObject, scenes)\n\t\t\tobjects.append(newObject)\n\t\treturn objects\n\n\tdef appendNewObject(self, name, sourceObject, scenes):\n\t\tobject = self.newInstance(name, sourceObject, scenes)\n\t\tself.linkObject(object, scenes)\n\t\tself.linkedObjects.add().object = object\n\t\treturn object\n\n\tdef linkObject(self, object, scenes):\n\t\tif self.addToMainContainer:\n\t\t\tfor scene in scenes:\n\t\t\t\tif scene is not None:\n\t\t\t\t\tgetMainObjectContainer(scene).objects.link(object)\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tfor scene in scenes:\n\t\t\t\tif scene is not None:\n\t\t\t\t\tscene.collection.objects.link(object)\n\t\t\t\t\tbreak\n\n\tdef newInstance(self, name, sourceObject, scenes):\n\t\tinstanceData = self.getSourceObjectData(sourceObject)\n\t\tif self.copyObjectProperties and self.copyFromSource:\n\t\t\tnewObject = sourceObject.copy()\n\t\t\tnewObject.data = instanceData\n\t\telse:\n\t\t\tnewObject = bpy.data.objects.new(name, instanceData)\n\n\t\tif self.removeAnimationData and newObject.animation_data is not None:\n\t\t\tnewObject.animation_data.action = None\n\n\t\tif not self.copyFromSource and self.objectType == \"Empty\":\n\t\t\tnewObject.empty_display_type = self.emptyDisplayType\n\t\treturn newObject\n\n\tdef getSourceObjectData(self, sourceObject):\n\t\tdata = None\n\t\tif self.copyFromSource:\n\t\t\tif self.deepCopy and sourceObject.data is not None:\n\t\t\t\tdata = sourceObject.data.copy()\n\t\t\telse:\n\t\t\t\treturn sourceObject.data\n\t\telse:\n\t\t\tif self.objectType == \"MESH\":\n\t\t\t\tdata = bpy.data.meshes.new(getPossibleMeshName(\"instance mesh\"))\n\t\t\telif self.objectType == \"TEXT\":\n\t\t\t\tdata = bpy.data.curves.new(getPossibleCurveName(\"instance text\"), type = \"FONT\")\n\t\t\telif self.objectType == \"CAMERA\":\n\t\t\t\tdata = bpy.data.cameras.new(getPossibleCameraName(\"instance camera\"))\n\t\t\telif self.objectType == \"POINT_LAMP\":\n\t\t\t\tdata = bpy.data.lights.new(getPossibleLightName(\"instance lamp\"), type = \"POINT\")\n\t\t\telif self.objectType.startswith(\"CURVE\"):\n\t\t\t\tdata = bpy.data.curves.new(getPossibleCurveName(\"instance curve\"), type = \"CURVE\")\n\t\t\t\tdata.dimensions = self.objectType[-2:]\n\t\t\telif self.objectType == \"GREASE_PENCIL\":\n\t\t\t\tdata = bpy.data.grease_pencils.new(getPossibleGreasePencilName(\"instance grease pencil\"))\n\n\t\tif data is None:\n\t\t\treturn None\n\t\telse:\n\t\t\tdata.an_data.removeOnZeroUsers = True\n\t\t\treturn data\n\n\tdef resetObjectDataOnAllInstances(self):\n\t\tself.resetInstances = True\n\n\tdef unlinkInstancesFromNode(self):\n\t\tself.linkedObjects.clear()\n\t\tself.inputs.get(\"Instances\").number = 0\n\n\tdef delete(self):\n\t\tself.removeAllObjects()\n\n\tdef duplicate(self, sourceNode):\n\t\tself.linkedObjects.clear()\n\n\tdef toggleRelationshipLines(self):\n\t\tfor space in iterActiveSpacesByType(\"VIEW_3D\"):\n\t\t\tspace.overlay.show_relationship_lines = not space.overlay.show_relationship_lines\n", "description": null, "category": "remove", "imports": ["import bpy", "from bpy.props import *", "from ... events import propertyChanged", "from ... base_types import AnimationNode", "from ... utils.names import getRandomString", "from ... utils.blender_ui import iterActiveSpacesByType", "from ... utils.data_blocks import removeNotUsedDataBlock", "from ... nodes.container_provider import getMainObjectContainer", "from ... utils.names import (getPossibleMeshName,"]}], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [{"term": "def", "name": "cut", "data": "def cut(rect1, rect2):\n\tx11, y11, x12, y12 = rect1\n\tx21, y21, x22, y22 = rect2\n\tx1 = max(x11, x21)\n\ty1 = max(y11, y21)\n\tx2 = min(x12, x22)\n\ty2 = min(y12, y22)\n\treturn x1, y1, x2, y2\n\n", "description": null, "category": "remove", "imports": ["from zad1testy import runtests"]}, {"term": "def", "name": "rect_area", "data": "def rect_area(x1, y1, x2, y2):\n\treturn (x2 - x1) * (y2 - y1)\n\n", "description": null, "category": "remove", "imports": ["from zad1testy import runtests"]}, {"term": "def", "name": "rect", "data": "def rect(D):\n\tn = len(D)\n\n\t# If there is nothing to remove\n\tif n < 3: return None\n\n\t# Choose which rectangle from the first 3 ones\n\t# might be removed if we had only 3 rectangles\n\tcuts = [rect_area(*cut(D[i], D[j])) for i, j in ((1, 2), (0, 2), (0, 1))]\n\tto_remove = cuts.index(max(cuts))\n\tremaining = [0, 1, 2]\n\tremaining.remove(to_remove)\n\tcut_coords = cut(D[remaining[0]], D[remaining[1]])\n\n\tfor i in range(3, n):\n\t\t# If we remove the i-th rectangle\n\t\tcurr_remove_cut_coords = cut(cut_coords, D[to_remove])\n\t\tcurr_remove_area = rect_area(*curr_remove_cut_coords)\n\t\t# If we remove the previously chosen rectangle\n\t\tprev_remove_cut_coords = cut(cut_coords, D[i])\n\t\tprev_remove_area = rect_area(*prev_remove_cut_coords)\n\n\t\t# print('prev', to_remove, 'curr', i, 'prev area', prev_remove_area, 'curr area', curr_remove_area)\n\n\t\t# Choose better of both solutions above\n\t\tif curr_remove_area > prev_remove_area:\n\t\t\tcut_coords = curr_remove_cut_coords\n\t\t\tto_remove = i\n\t\telse:\n\t\t\tcut_coords = prev_remove_cut_coords\n\n\treturn to_remove\n\n", "description": null, "category": "remove", "imports": ["from zad1testy import runtests"]}], [], [], [], [], [], [], [], [{"term": "class", "name": "TestRemoveEpochs", "data": "class TestRemoveEpochs(unittest.TestCase):\n\n\tdef setUp(self):\n\t\tones = np.ones((10, 5))\n\t\tchannels = ['ca1', 'ca2', 'cb1', 'cb2', 'cc1']\n\t\ttime = np.linspace(0, 1000, 10, endpoint=False)\n\t\tclasses = [0, 1, 2, 1]\n\t\tclass_names = ['zeros', 'ones', 'twoes']\n\t\t# four cnts: 0s, 1s, -1s, and 0s\n\t\tdata = np.array([ones * 0, ones * 1, ones * 2, ones * 0])\n\t\tself.dat = Data(data, [classes, time, channels], ['class', 'time', 'channel'], ['#', 'ms', '#'])\n\t\tself.dat.class_names = class_names\n\n\tdef test_remove_epochs(self):\n\t\t\"\"\"Removing Epochs.\"\"\"\n\t\t# normal case\n\t\tdat = remove_epochs(self.dat, [0])\n\t\tself.assertEqual(dat.data.shape[0], 3)\n\t\tnp.testing.assert_array_equal(dat.data, self.dat.data[1:])\n\t\t# normal every second\n\t\tdat = remove_epochs(self.dat, [0, 2])\n\t\tself.assertEqual(dat.data.shape[0], 2)\n\t\tnp.testing.assert_array_equal(dat.data, self.dat.data[1::2])\n\t\t# the full epo\n\t\tdat = remove_epochs(self.dat, list(range(self.dat.data.shape[0])))\n\t\tnp.testing.assert_array_equal(dat.data.shape[0], 0)\n\n\tdef test_remove_epochs_with_cnt(self):\n\t\t\"\"\"Remove epochs must raise an exception if called with cnt argument.\"\"\"\n\t\tdel(self.dat.class_names)\n\t\twith self.assertRaises(AssertionError):\n\t\t\tremove_epochs(self.dat, [0, 1])\n\n\tdef test_remove_epochs_swapaxes(self):\n\t\t\"\"\"Remove epochs must work with nonstandard classaxis.\"\"\"\n\t\tdat = remove_epochs(swapaxes(self.dat, 0, 2), [0, 1], classaxis=2)\n\t\tdat = swapaxes(dat, 0, 2)\n\t\tdat2 = remove_epochs(self.dat, [0, 1])\n\t\tself.assertEqual(dat, dat2)\n\n\tdef test_remove_epochs_copy(self):\n\t\t\"\"\"Remove Epochs must not modify argument.\"\"\"\n\t\tcpy = self.dat.copy()\n\t\tremove_epochs(self.dat, [0, 1])\n\t\tself.assertEqual(self.dat, cpy)\n\n\n", "description": "Removing Epochs.", "category": "remove", "imports": ["from __future__ import division", "import unittest", "import numpy as np", "from wyrm.types import Data", "from wyrm.processing import remove_epochs", "from wyrm.processing import swapaxes"]}], [{"term": "class", "name": "TestRemoveClasses", "data": "class TestRemoveClasses(unittest.TestCase):\n\n\tdef setUp(self):\n\t\tones = np.ones((10, 5))\n\t\tchannels = ['ca1', 'ca2', 'cb1', 'cb2', 'cc1']\n\t\ttime = np.linspace(0, 1000, 10, endpoint=False)\n\t\tclasses = [0, 1, 2, 1]\n\t\tclass_names = ['zeros', 'ones', 'twoes']\n\t\t# four cnts: 0s, 1s, -1s, and 0s\n\t\tdata = np.array([ones * 0, ones * 1, ones * 2, ones * 0])\n\t\tself.dat = Data(data, [classes, time, channels], ['class', 'time', 'channel'], ['#', 'ms', '#'])\n\t\tself.dat.class_names = class_names\n\n\tdef test_remove_classes(self):\n\t\t\"\"\"Removing Classes.\"\"\"\n\t\t# normal case\n\t\tdat = remove_classes(self.dat, [0])\n\t\tself.assertEqual(dat.data.shape[0], 3)\n\t\tnp.testing.assert_array_equal(dat.data, self.dat.data[1:])\n\t\t# normal every second\n\t\tdat = remove_classes(self.dat, [0, 2])\n\t\tself.assertEqual(dat.data.shape[0], 2)\n\t\tnp.testing.assert_array_equal(dat.data, self.dat.data[[1, 3]])\n\t\t# the full epo\n\t\tdat = remove_classes(self.dat, list(range(self.dat.data.shape[0])))\n\t\tnp.testing.assert_array_equal(dat.data.shape[0], 0)\n\n\tdef test_remove_classes_with_cnt(self):\n\t\t\"\"\"Remove epochs must raise an exception if called with cnt argument.\"\"\"\n\t\tdel(self.dat.class_names)\n\t\twith self.assertRaises(AssertionError):\n\t\t\tremove_classes(self.dat, [0, 1])\n\n\tdef test_remove_classes_swapaxes(self):\n\t\t\"\"\"Remove epochs must work with nonstandard classaxis.\"\"\"\n\t\tdat = remove_classes(swapaxes(self.dat, 0, 2), [0, 2], classaxis=2)\n\t\tdat = swapaxes(dat, 0, 2)\n\t\tdat2 = remove_classes(self.dat, [0, 2])\n\t\tself.assertEqual(dat, dat2)\n\n\tdef test_remove_classes_copy(self):\n\t\t\"\"\"Remove Classes must not modify argument.\"\"\"\n\t\tcpy = self.dat.copy()\n\t\tremove_classes(self.dat, [0, 1])\n\t\tself.assertEqual(self.dat, cpy)\n\n\n", "description": "Removing Classes.", "category": "remove", "imports": ["from __future__ import division", "import unittest", "import numpy as np", "from wyrm.types import Data", "from wyrm.processing import remove_classes", "from wyrm.processing import swapaxes"]}], [{"term": "def", "name": "rule_start_activity", "data": "def rule_start_activity():\n\ta = request.form[\"act1\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile()   \n\tresult = []\n\tfor act in segments:\n\t\tif act[1] == a and act not in result:\n\t\t\tresult.append(act)\n\t\telse:\n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment) \n\treturn result, removeSegment  \n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_end_activity", "data": "def rule_end_activity():\n\ta = request.form[\"act1\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile()   \n\tresult = []\n\tfor act in segments:\n\t\tif act[-2] == a and act not in result:\n\t\t\tresult.append(act)\n\t\telse:\n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment) \n\treturn result, removeSegment\t\t\t\t\t \n\t\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_existence", "data": "def rule_existence():\n\ta = request.form[\"act1\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile()   \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and act not in result:\t\t\n\t\t\tresult.append(act)\n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t\t\t   \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment) \n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_absence", "data": "def rule_absence():\n\ta = request.form[\"act1\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile()   \n\tresult = []\n\tfor act in segments:\n\t\tif a not in act and act not in result:\n\t\t\tresult.append(act)\n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t\n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment) \n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_choice", "data": "def rule_choice():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act or b in act and act not in result:\n\t\t\tresult.append(act)\n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_exclusive_choice", "data": "def rule_exclusive_choice():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b not in act and act not in result:\n\t\t\tresult.append(act)\n\t\telif b in act and a not in act and act not in result:\n\t\t\tresult.append(act)\n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_responded_existence", "data": "def rule_responded_existence():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif b not in act and a not in act and act not in result:\n\t\t\tresult.append(act)\n\t\telif a in act and b in act and act not in result:\n\t\t\tresult.append(act) \n\t\telif a not in act and act not in result:\n\t\t\tresult.append(act)  \n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_response", "data": "def rule_response():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tposition_a = act.index(a)\n\t\t\tposition_b = act.index(b)\t \n\t\t\tif position_b > position_a and act not in result:\n\t\t\t\tresult.append(act)\n\t\t\telse : \n\t\t\t\tif act not in removeSegment:\n\t\t\t\t\tremoveSegment.append(act)\t  \n\t\telif a not in act and act not in result:\n\t\t\tresult.append(act)   \n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_alternate_response", "data": "def rule_alternate_response():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tcounter = collections.Counter(act)\n\t\t\tif counter[a] == 1:\n\t\t\t\tposition_a = act.index(a)\n\t\t\t\tposition_b = act.index(b)\t \n\t\t\t\tif position_b > position_a and act not in result:\n\t\t\t\t\tresult.append(act)\n\t\t\t\telse:\n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act)\t \n\t\t\telif counter[a] > 1:\n\t\t\t\tlist_a = []\n\t\t\t\tlist_b = []\n\t\t\t\tcount = -1\n\t\t\t\tfor elem in act:\n\t\t\t\t\tcount += 1\n\t\t\t\t\tif elem == a:\n\t\t\t\t\t\tlist_a.append(count)\n\t\t\t\t\telif elem == b:\n\t\t\t\t\t\tlist_b.append(count)\t\t\t\n\t\t\t\ti = 0\n\t\t\t\tif(len(list_a) == len(list_b)):\n\t\t\t\t\tfor i in range(len(list_a)-1):\n\t\t\t\t\t\tif list_a[i] < list_b[i] and list_b[i] < list_a[i+1]and list_b[i+1] > list_a[i+1] and act not in result:\n\t\t\t\t\t\t\tresult.append(act)\n\t\t\t\t\t\telse :\n\t\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\t\tremoveSegment.append(act) \n\t\t\t\telse:\n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act)\t\t\t\t\t   \n\t\telif a not in act:\n\t\t\tresult.append(act)  \n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_chain_response", "data": "def rule_chain_response():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tcounter = collections.Counter(act)\n\t\t\tif counter[a] == 1:\n\t\t\t\tposition_a = act.index(a)\n\t\t\t\tposition_b = act.index(b)\t\n\t\t\t\tif position_a + 1 == position_b and act not in result:\n\t\t\t\t\tresult.append(act)\n\t\t\t\telse : \n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act)\t \n\t\t\telif counter[a] > 1:\n\t\t\t\tlist_a = []\n\t\t\t\tlist_b = []\n\t\t\t\tcount = -1\n\t\t\t\tfor elem in act:\n\t\t\t\t\tcount += 1\n\t\t\t\t\tif elem == a:\n\t\t\t\t\t\tlist_a.append(count)\n\t\t\t\t\telif elem == b:\n\t\t\t\t\t\tlist_b.append(count)\t\t\t\n\t\t\t\ti = 0\n\t\t\t\tj = 0\n\t\t\t\tfor i in range(len(list_a)):\n\t\t\t\t\tfor j in range(len(list_b)):\n\t\t\t\t\t\tif list_a[i] + 1 == list_b[j]:\n\t\t\t\t\t\t\tif act not in result and act not in removeSegment:\n\t\t\t\t\t\t\t\tresult.append(act) \n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif act in result: \n\t\t\t\t\t\t\t\tresult.remove(act)\n\t\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\t\tremoveSegment.append(act)\t   \n\t\telif a not in act:\n\t\t\tresult.append(act)   \n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_precedence", "data": "def rule_precedence():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tposition_a = act.index(a)\n\t\t\tposition_b = act.index(b)\t \n\t\t\tif position_b > position_a and act not in result:\n\t\t\t\tresult.append(act) \n\t\t\telse : \n\t\t\t\tif act not in removeSegment:\n\t\t\t\t\tremoveSegment.append(act)\t \n\t\telif b not in act and act not in result:\n\t\t\tresult.append(act)  \n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_alternate_precedence", "data": "def rule_alternate_precedence():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tcounter = collections.Counter(act)\n\t\t\tif counter[b] == 1:\n\t\t\t\tposition_a = act.index(a)\n\t\t\t\tposition_b = act.index(b)\t \n\t\t\t\tif position_a < position_b and act not in result:\n\t\t\t\t\tresult.append(act)\n\t\t\t\telse : \n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act)\t \n\t\t\telif counter[b] > 1:\n\t\t\t\tlist = []\n\t\t\t\tfor elem in act:\n\t\t\t\t\tif elem == a:\n\t\t\t\t\t\tlist.append(a)\n\t\t\t\t\telif elem == b:\n\t\t\t\t\t\tlist.append(b)\t\t\t\n\t\t\t\ti = 0\n\t\t\t\tfor i in range(len(list)-1):\n\t\t\t\t\tif list[i] != list[i+1] and act not in result:\n\t\t\t\t\t\tresult.append(act)\n\t\t\t\t\telif list[0] == b and list[1] == a:\n\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\tremoveSegment.append(act)\t\n\t\t\t\t\telif list[i] == a and list[i+1] == a and act not in result:\n\t\t\t\t\t\tresult.append(act)\n\t\t\t\t\telif list[i] == b and list[i+1] == b and act in result:\n\t\t\t\t\t\tresult.remove(act)\n\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\tremoveSegment.append(act) \n\t\t\t\t\t\tbreak\t  \n\t\telif b not in act:\n\t\t\tresult.append(act)\n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_chain_precedence", "data": "def rule_chain_precedence():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tcounter = collections.Counter(act)\n\t\t\tif counter[b] == 1:\n\t\t\t\tposition_a = act.index(a)\n\t\t\t\tposition_b = act.index(b)\t \n\t\t\t\tif position_a + 1 == position_b and act not in result:\n\t\t\t\t\tresult.append(act)\n\t\t\t\telse : \n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act)\t  \n\t\t\telif counter[b] > 1:\n\t\t\t\tlist_a = []\n\t\t\t\tlist_b = []\n\t\t\t\tcount = -1\n\t\t\t\tfor elem in act:\n\t\t\t\t\tcount += 1\n\t\t\t\t\tif elem == a:\n\t\t\t\t\t\tlist_a.append(count)\n\t\t\t\t\telif elem == b:\n\t\t\t\t\t\tlist_b.append(count)\t\t\t\n\t\t\t\ti = 0\n\t\t\t\tj = 0\n\t\t\t\tfor i in range(len(list_b)):\n\t\t\t\t\tfor j in range(len(list_a)):\n\t\t\t\t\t\tif list_a[j] + 1 == list_b[i]:\n\t\t\t\t\t\t\tif act not in result:\n\t\t\t\t\t\t\t\tresult.append(act)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif act in result:\n\t\t\t\t\t\t\t\tresult.remove(act)\n\t\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\t\tremoveSegment.append(act)\t \n\t\telif b not in act:\n\t\t\tresult.append(act) \n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_co_existence", "data": "def rule_co_existence():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act and act not in result:\n\t\t\tresult.append(act)\n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\t   \n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_succession", "data": "def rule_succession():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tcounter = collections.Counter(act)\n\t\t\tif counter[a] == 1:\n\t\t\t\tposition_a = act.index(a)\n\t\t\t\tposition_b = act.index(b)\t \n\t\t\t\tif position_a < position_b and act not in result:\n\t\t\t\t\tresult.append(act)\n\t\t\t\telse : \n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act)\t\n\t\t\telif counter[a] > 1:\n\t\t\t\tlist_a = []\n\t\t\t\tlist_b = []\n\t\t\t\tcount = -1\n\t\t\t\tfor elem in act:\n\t\t\t\t\tcount += 1\n\t\t\t\t\tif elem == a:\n\t\t\t\t\t\tlist_a.append(count)\n\t\t\t\t\telif elem == b:\n\t\t\t\t\t\tlist_b.append(count)\t\t\t\n\t\t\t\ti = 0\n\t\t\t\tj = 0\n\t\t\t\tfor i in range(len(list_b)):\n\t\t\t\t\tfor j in range(len(list_a)):\n\t\t\t\t\t\tif list_a[j] < list_b[i]:\n\t\t\t\t\t\t\tif act not in result:\n\t\t\t\t\t\t\t\tresult.append(act)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif act in result:\n\t\t\t\t\t\t\t\tresult.remove(act)\n\t\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\t\tremoveSegment.append(act)\t\t\t\t \n\t\telif a not in act and b in act and act not in result:\n\t\t\tresult.append(act) \n\t\telif a not in act and b not in act and act not in result:\n\t\t\tresult.append(act)\t \n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_alternate_succession", "data": "def rule_alternate_succession():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tcounter = collections.Counter(act)\n\t\t\tif counter[a] == 1 and counter[b] == 1:\n\t\t\t\tposition_a = act.index(a)\n\t\t\t\tposition_b = act.index(b)\t \n\t\t\t\tif position_a < position_b and act not in result:\n\t\t\t\t\tresult.append(act) \n\t\t\t\telse : \n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act)\t   \n\t\t\telif counter[a] > 1 and counter[b] > 1:\n\t\t\t\tlist_a = []\n\t\t\t\tlist_b = []\n\t\t\t\tcount = -1\n\t\t\t\tfor elem in act:\n\t\t\t\t\tcount += 1\n\t\t\t\t\tif elem == a:\n\t\t\t\t\t\tlist_a.append(count)\n\t\t\t\t\telif elem == b:\n\t\t\t\t\t\tlist_b.append(count)\t\t\t\n\t\t\t\ti = 0\n\t\t\t\tif(len(list_a) == len(list_b)):\n\t\t\t\t\tfor i in range(len(list_b)-1):\n\t\t\t\t\t\tif list_a[i] < list_b[i] and list_a[i+1] > list_b[i]:\n\t\t\t\t\t\t\tif act not in result:\n\t\t\t\t\t\t\t\tresult.append(act)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif act in result:\n\t\t\t\t\t\t\t\tresult.remove(act)\n\t\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\t\tremoveSegment.append(act)\n\t\t\t\telse : \n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act) \n\t\t\telse : \n\t\t\t\tif act not in removeSegment:\n\t\t\t\t\tremoveSegment.append(act)\t\t\t\t\t\t\t\t \n\t\telif a not in act and b in act and act not in result:\n\t\t\tresult.append(act)\n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_chain_succession", "data": "def rule_chain_succession():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tlist_a = []\n\t\t\tlist_b = []\n\t\t\tcount = -1\n\t\t\tfor elem in act:\n\t\t\t\tcount += 1\n\t\t\t\tif elem == a:\n\t\t\t\t\tlist_a.append(count)\n\t\t\t\telif elem == b:\n\t\t\t\t\tlist_b.append(count)\t\t\t\n\t\t\ti = 0\n\t\t\tif(len(list_a) == len(list_b)):\n\t\t\t\tfor i in range(len(list_b)):\n\t\t\t\t\tif list_a[i] + 1 == list_b[i]:\n\t\t\t\t\t\tif act not in result:\n\t\t\t\t\t\t\tresult.append(act)\n\t\t\t\t\telse:\n\t\t\t\t\t\tif act in result:\n\t\t\t\t\t\t\tresult.remove(act)\n\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\tremoveSegment.append(act)\t\t\t\t\t\t\t\n\t\t\telse:\n\t\t\t\tif act not in removeSegment:\n\t\t\t\t\tremoveSegment.append(act)\t\t\t\t  \n\t\telif a not in act and b not in act and act not in result:\n\t\t\tresult.append(act)\t\n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_not_co_existence", "data": "def rule_not_co_existence():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b not in act and act not in result:\n\t\t\tresult.append(act)\n\t\telif a not in act and b in act and act not in result:\n\t\t\tresult.append(act)\n\t\telif a not in act and b not in act and act not in result:\n\t\t\tresult.append(act)\t\n\t\telse : \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_not_succession", "data": "def rule_not_succession():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tcounter = collections.Counter(act)\n\t\t\tif counter[a] == 1 and counter[b] == 1:\n\t\t\t\tposition_a = act.index(a)\n\t\t\t\tposition_b = act.index(b)\n\t\t\t\tif position_a > position_b and act not in result:\n\t\t\t\t\tresult.append(act)\n\t\t\t\telse:\n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act)\t\t   \n\t\t\telse:\n\t\t\t\tlist_a = []\n\t\t\t\tlist_b = []\n\t\t\t\tcount = -1\n\t\t\t\tfor elem in act:\n\t\t\t\t\tcount += 1\n\t\t\t\t\tif elem == a:\n\t\t\t\t\t\tlist_a.append(count)\n\t\t\t\t\telif elem == b:\n\t\t\t\t\t\tlist_b.append(count)\t\t\t\n\t\t\t\ti = 0\n\t\t\t\tj = 0\n\t\t\t\tfor i in range(len(list_b)):\n\t\t\t\t\tfor j in range(len(list_a)):\n\t\t\t\t\t\tif list_a[j] > list_b[i]:\n\t\t\t\t\t\t\tif act not in result:\n\t\t\t\t\t\t\t\tresult.append(act)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif act in result:\n\t\t\t\t\t\t\t\tresult.remove(act) \n\t\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\t\tremoveSegment.append(act) \n\t\telif a in act and b not in act:\n\t\t\tresult.append(act)\n\t\telif a not in act and b in act:\n\t\t\tresult.append(act)\n\t\telse:\n\t\t\tresult.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "rule_not_chain_succession", "data": "def rule_not_chain_succession():\n\ta = request.form[\"act1\"]\n\tb = request.form[\"act2\"]\n\tsegments = takeSegmentFromFile()\n\tremoveSegment = takeRemoveSegmentFromFile() \n\tresult = []\n\tfor act in segments:\n\t\tif a in act and b in act:\n\t\t\tcounter = collections.Counter(act)\n\t\t\tif counter[a] == 1 and counter[b] == 1:\n\t\t\t\tposition_a = act.index(a)\n\t\t\t\tposition_b = act.index(b)\t \n\t\t\t\tif position_b != position_a + 1 and act not in result:\n\t\t\t\t\tresult.append(act)\n\t\t\t\telse : \n\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\tremoveSegment.append(act)\t\t\n\t\t\telse:\n\t\t\t\tlist_a = []\n\t\t\t\tlist_b = []\n\t\t\t\tcount = -1\n\t\t\t\tfor elem in act:\n\t\t\t\t\tcount += 1\n\t\t\t\t\tif elem == a:\n\t\t\t\t\t\tlist_a.append(count)\n\t\t\t\t\telif elem == b:\n\t\t\t\t\t\tlist_b.append(count)\t\t\t\n\t\t\t\ti = 0\n\t\t\t\tj = 0\n\t\t\t\tfor i in range(len(list_a)):\n\t\t\t\t\tfor j in range(len(list_b)):\n\t\t\t\t\t\tif list_b[j] != list_a[i] + 1:\n\t\t\t\t\t\t\tif act not in result:\n\t\t\t\t\t\t\t\tresult.append(act)\n\t\t\t\t\t\telse: \n\t\t\t\t\t\t\tif act in result:\n\t\t\t\t\t\t\t\tresult.remove(act)\n\t\t\t\t\t\t\tif act not in removeSegment:\n\t\t\t\t\t\t\t\tremoveSegment.append(act)\t\t\t\t\t\n\t\telif a not in act and b not in act:\n\t\t\tresult.append(act)\n\t\telse: \n\t\t\tif act not in removeSegment:\n\t\t\t\tremoveSegment.append(act)\t \n\twriteOnSegmentFile(result)   \n\twriteOnRemoveSegmentFile(removeSegment)\n\treturn result, removeSegment\n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}, {"term": "def", "name": "del_rule", "data": "def del_rule():\n\tclear() \n\tsegments = takeSegmentFromTrace()\n\tremove = []\t\t\t\n\twriteOnSegmentFile(segments)   \n\twriteOnRemoveSegmentFile(remove) \n", "description": null, "category": "remove", "imports": ["from utilities import *", "from flask import request", "import collections"]}], [{"term": "class", "name": "RSZeroError", "data": "class RSZeroError(RuntimeError):\n\tpass\n\n", "description": null, "category": "remove", "imports": ["  from random import SystemRandom", "from six import int2byte, b", "from . import ellipticcurve", "from . import numbertheory", "from .util import bit_length", "from ._compat import remove_whitespace", "\tfrom hashlib import sha1"]}, {"term": "class", "name": "InvalidPointError", "data": "class InvalidPointError(RuntimeError):\n\tpass\n\n", "description": null, "category": "remove", "imports": ["  from random import SystemRandom", "from six import int2byte, b", "from . import ellipticcurve", "from . import numbertheory", "from .util import bit_length", "from ._compat import remove_whitespace", "\tfrom hashlib import sha1"]}, {"term": "class", "name": "Signature", "data": "class Signature(object):\n\t\"\"\"ECDSA signature.\"\"\"\n\n\tdef __init__(self, r, s):\n\t\tself.r = r\n\t\tself.s = s\n\n\tdef recover_public_keys(self, hash, generator):\n\t\t\"\"\"Returns two public keys for which the signature is valid\n\t\thash is signed hash\n\t\tgenerator is the used generator of the signature\n\t\t\"\"\"\n\t\tcurve = generator.curve()\n\t\tn = generator.order()\n\t\tr = self.r\n\t\ts = self.s\n\t\te = hash\n\t\tx = r\n\n\t\t# Compute the curve point with x as x-coordinate\n\t\talpha = (\n\t\t\tpow(x, 3, curve.p()) + (curve.a() * x) + curve.b()\n\t\t) % curve.p()\n\t\tbeta = numbertheory.square_root_mod_prime(alpha, curve.p())\n\t\ty = beta if beta % 2 == 0 else curve.p() - beta\n\n\t\t# Compute the public key\n\t\tR1 = ellipticcurve.PointJacobi(curve, x, y, 1, n)\n\t\tQ1 = numbertheory.inverse_mod(r, n) * (s * R1 + (-e % n) * generator)\n\t\tPk1 = Public_key(generator, Q1)\n\n\t\t# And the second solution\n\t\tR2 = ellipticcurve.PointJacobi(curve, x, -y, 1, n)\n\t\tQ2 = numbertheory.inverse_mod(r, n) * (s * R2 + (-e % n) * generator)\n\t\tPk2 = Public_key(generator, Q2)\n\n\t\treturn [Pk1, Pk2]\n\n", "description": "ECDSA signature.", "category": "remove", "imports": ["  from random import SystemRandom", "from six import int2byte, b", "from . import ellipticcurve", "from . import numbertheory", "from .util import bit_length", "from ._compat import remove_whitespace", "\tfrom hashlib import sha1"]}, {"term": "class", "name": "Public_key", "data": "class Public_key(object):\n\t\"\"\"Public key for ECDSA.\"\"\"\n\n\tdef __init__(self, generator, point, verify=True):\n\t\t\"\"\"Low level ECDSA public key object.\n\n\t\t:param generator: the Point that generates the group (the base point)\n\t\t:param point: the Point that defines the public key\n\t\t:param bool verify: if True check if point is valid point on curve\n\n\t\t:raises InvalidPointError: if the point parameters are invalid or\n\t\t\tpoint does not lie on the curve\n\t\t\"\"\"\n\n\t\tself.curve = generator.curve()\n\t\tself.generator = generator\n\t\tself.point = point\n\t\tn = generator.order()\n\t\tp = self.curve.p()\n\t\tif not (0 <= point.x() < p) or not (0 <= point.y() < p):\n\t\t\traise InvalidPointError(\n\t\t\t\t\"The public point has x or y out of range.\"\n\t\t\t)\n\t\tif verify and not self.curve.contains_point(point.x(), point.y()):\n\t\t\traise InvalidPointError(\"Point does not lie on the curve\")\n\t\tif not n:\n\t\t\traise InvalidPointError(\"Generator point must have order.\")\n\t\t# for curve parameters with base point with cofactor 1, all points\n\t\t# that are on the curve are scalar multiples of the base point, so\n\t\t# verifying that is not necessary. See Section 3.2.2.1 of SEC 1 v2\n\t\tif (\n\t\t\tverify\n\t\t\tand self.curve.cofactor() != 1\n\t\t\tand not n * point == ellipticcurve.INFINITY\n\t\t):\n\t\t\traise InvalidPointError(\"Generator point order is bad.\")\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, Public_key):\n\t\t\t\"\"\"Return True if the points are identical, False otherwise.\"\"\"\n\t\t\treturn self.curve == other.curve and self.point == other.point\n\t\treturn NotImplemented\n\n\tdef verifies(self, hash, signature):\n\t\t\"\"\"Verify that signature is a valid signature of hash.\n\t\tReturn True if the signature is valid.\n\t\t\"\"\"\n\n\t\t# From X9.62 J.3.1.\n\n\t\tG = self.generator\n\t\tn = G.order()\n\t\tr = signature.r\n\t\ts = signature.s\n\t\tif r < 1 or r > n - 1:\n\t\t\treturn False\n\t\tif s < 1 or s > n - 1:\n\t\t\treturn False\n\t\tc = numbertheory.inverse_mod(s, n)\n\t\tu1 = (hash * c) % n\n\t\tu2 = (r * c) % n\n\t\tif hasattr(G, \"mul_add\"):\n\t\t\txy = G.mul_add(u1, self.point, u2)\n\t\telse:\n\t\t\txy = u1 * G + u2 * self.point\n\t\tv = xy.x() % n\n\t\treturn v == r\n\n", "description": "Public key for ECDSA.", "category": "remove", "imports": ["  from random import SystemRandom", "from six import int2byte, b", "from . import ellipticcurve", "from . import numbertheory", "from .util import bit_length", "from ._compat import remove_whitespace", "\tfrom hashlib import sha1"]}, {"term": "class", "name": "Private_key", "data": "class Private_key(object):\n\t\"\"\"Private key for ECDSA.\"\"\"\n\n\tdef __init__(self, public_key, secret_multiplier):\n\t\t\"\"\"public_key is of class Public_key;\n\t\tsecret_multiplier is a large integer.\n\t\t\"\"\"\n\n\t\tself.public_key = public_key\n\t\tself.secret_multiplier = secret_multiplier\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, Private_key):\n\t\t\t\"\"\"Return True if the points are identical, False otherwise.\"\"\"\n\t\t\treturn (\n\t\t\t\tself.public_key == other.public_key\n\t\t\t\tand self.secret_multiplier == other.secret_multiplier\n\t\t\t)\n\t\treturn NotImplemented\n\n\tdef sign(self, hash, random_k):\n\t\t\"\"\"Return a signature for the provided hash, using the provided\n\t\trandom nonce.  It is absolutely vital that random_k be an unpredictable\n\t\tnumber in the range [1, self.public_key.point.order()-1].  If\n\t\tan attacker can guess random_k, he can compute our private key from a\n\t\tsingle signature.  Also, if an attacker knows a few high-order\n\t\tbits (or a few low-order bits) of random_k, he can compute our private\n\t\tkey from many signatures.  The generation of nonces with adequate\n\t\tcryptographic strength is very difficult and far beyond the scope\n\t\tof this comment.\n\n\t\tMay raise RuntimeError, in which case retrying with a new\n\t\trandom value k is in order.\n\t\t\"\"\"\n\n\t\tG = self.public_key.generator\n\t\tn = G.order()\n\t\tk = random_k % n\n\t\t# Fix the bit-length of the random nonce,\n\t\t# so that it doesn't leak via timing.\n\t\t# This does not change that ks = k mod n\n\t\tks = k + n\n\t\tkt = ks + n\n\t\tif bit_length(ks) == bit_length(n):\n\t\t\tp1 = kt * G\n\t\telse:\n\t\t\tp1 = ks * G\n\t\tr = p1.x() % n\n\t\tif r == 0:\n\t\t\traise RSZeroError(\"amazingly unlucky random number r\")\n\t\ts = (\n\t\t\tnumbertheory.inverse_mod(k, n)\n\t\t\t* (hash + (self.secret_multiplier * r) % n)\n\t\t) % n\n\t\tif s == 0:\n\t\t\traise RSZeroError(\"amazingly unlucky random number s\")\n\t\treturn Signature(r, s)\n\n", "description": "Private key for ECDSA.", "category": "remove", "imports": ["  from random import SystemRandom", "from six import int2byte, b", "from . import ellipticcurve", "from . import numbertheory", "from .util import bit_length", "from ._compat import remove_whitespace", "\tfrom hashlib import sha1"]}, {"term": "def", "name": "int_to_string", "data": "def int_to_string(x):\n\t\"\"\"Convert integer x into a string of bytes, as per X9.62.\"\"\"\n\tassert x >= 0\n\tif x == 0:\n\t\treturn b(\"\\0\")\n\tresult = []\n\twhile x:\n\t\tordinal = x & 0xFF\n\t\tresult.append(int2byte(ordinal))\n\t\tx >>= 8\n\n\tresult.reverse()\n\treturn b(\"\").join(result)\n\n", "description": "Convert integer x into a string of bytes, as per X9.62.", "category": "remove", "imports": ["  from random import SystemRandom", "from six import int2byte, b", "from . import ellipticcurve", "from . import numbertheory", "from .util import bit_length", "from ._compat import remove_whitespace", "\tfrom hashlib import sha1"]}, {"term": "def", "name": "string_to_int", "data": "def string_to_int(s):\n\t\"\"\"Convert a string of bytes into an integer, as per X9.62.\"\"\"\n\tresult = 0\n\tfor c in s:\n\t\tif not isinstance(c, int):\n\t\t\tc = ord(c)\n\t\tresult = 256 * result + c\n\treturn result\n\n", "description": "Convert a string of bytes into an integer, as per X9.62.", "category": "remove", "imports": ["  from random import SystemRandom", "from six import int2byte, b", "from . import ellipticcurve", "from . import numbertheory", "from .util import bit_length", "from ._compat import remove_whitespace", "\tfrom hashlib import sha1"]}, {"term": "def", "name": "digest_integer", "data": "def digest_integer(m):\n\t\"\"\"Convert an integer into a string of bytes, compute\n\t its SHA-1 hash, and convert the result to an integer.\"\"\"\n\t#\n\t# I don't expect this function to be used much. I wrote\n\t# it in order to be able to duplicate the examples\n\t# in ECDSAVS.\n\t#\n\tfrom hashlib import sha1\n\n\treturn string_to_int(sha1(int_to_string(m)).digest())\n\n", "description": "Convert an integer into a string of bytes, compute\n\t its SHA-1 hash, and convert the result to an integer.", "category": "remove", "imports": ["  from random import SystemRandom", "from six import int2byte, b", "from . import ellipticcurve", "from . import numbertheory", "from .util import bit_length", "from ._compat import remove_whitespace", "\tfrom hashlib import sha1"]}, {"term": "def", "name": "point_is_valid", "data": "def point_is_valid(generator, x, y):\n\t\"\"\"Is (x,y) a valid public key based on the specified generator?\"\"\"\n\n\t# These are the tests specified in X9.62.\n\n\tn = generator.order()\n\tcurve = generator.curve()\n\tp = curve.p()\n\tif not (0 <= x < p) or not (0 <= y < p):\n\t\treturn False\n\tif not curve.contains_point(x, y):\n\t\treturn False\n\tif (\n\t\tcurve.cofactor() != 1\n\t\tand not n * ellipticcurve.PointJacobi(curve, x, y, 1)\n\t\t== ellipticcurve.INFINITY\n\t):\n\t\treturn False\n\treturn True\n\n", "description": "Is (x,y) a valid public key based on the specified generator?", "category": "remove", "imports": ["  from random import SystemRandom", "from six import int2byte, b", "from . import ellipticcurve", "from . import numbertheory", "from .util import bit_length", "from ._compat import remove_whitespace", "\tfrom hashlib import sha1"]}], [{"term": "def", "name": "test_ap_change_ssid", "data": "def test_ap_change_ssid(dev, apdev):\n\t\"\"\"Dynamic SSID change with hostapd and WPA2-PSK\"\"\"\n\tparams = hostapd.wpa2_params(ssid=\"test-wpa2-psk-start\",\n\t\t\t\t\t\t\t\t passphrase=\"12345678\")\n\thapd = hostapd.add_ap(apdev[0], params)\n\tid = dev[0].connect(\"test-wpa2-psk-start\", psk=\"12345678\",\n\t\t\t\t\t\tscan_freq=\"2412\")\n\tdev[0].request(\"DISCONNECT\")\n\n\tlogger.info(\"Change SSID dynamically\")\n\tres = hapd.request(\"SET ssid test-wpa2-psk-new\")\n\tif \"OK\" not in res:\n\t\traise Exception(\"SET command failed\")\n\tres = hapd.request(\"RELOAD\")\n\tif \"OK\" not in res:\n\t\traise Exception(\"RELOAD command failed\")\n\n\tdev[0].set_network_quoted(id, \"ssid\", \"test-wpa2-psk-new\")\n\tdev[0].connect_network(id)\n", "description": "Dynamic SSID change with hostapd and WPA2-PSK", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_change_ssid_wps", "data": "def test_ap_change_ssid_wps(dev, apdev):\n\t\"\"\"Dynamic SSID change with hostapd and WPA2-PSK using WPS\"\"\"\n\tparams = hostapd.wpa2_params(ssid=\"test-wpa2-psk-start\",\n\t\t\t\t\t\t\t\t passphrase=\"12345678\")\n\t# Use a PSK and not the passphrase, because the PSK will have to be computed\n\t# again if we use a passphrase.\n\tdel params[\"wpa_passphrase\"]\n\tparams[\"wpa_psk\"] = \"0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef\"\n\n\tparams.update({\"wps_state\": \"2\", \"eap_server\": \"1\"})\n\tbssid = apdev[0]['bssid']\n\thapd = hostapd.add_ap(apdev[0], params)\n\n\tnew_ssid = \"test-wpa2-psk-new\"\n\tlogger.info(\"Change SSID dynamically (WPS)\")\n\tres = hapd.request(\"SET ssid \" + new_ssid)\n\tif \"OK\" not in res:\n\t\traise Exception(\"SET command failed\")\n\tres = hapd.request(\"RELOAD\")\n\tif \"OK\" not in res:\n\t\traise Exception(\"RELOAD command failed\")\n\n\t# Connect to the new ssid using wps:\n\thapd.request(\"WPS_PBC\")\n\tif \"PBC Status: Active\" not in hapd.request(\"WPS_GET_STATUS\"):\n\t\traise Exception(\"PBC status not shown correctly\")\n\n\tdev[0].scan_for_bss(apdev[0]['bssid'], freq=\"2412\", force_scan=True)\n\tdev[0].request(\"WPS_PBC\")\n\tdev[0].wait_connected(timeout=20)\n\tstatus = dev[0].get_status()\n\tif status['wpa_state'] != 'COMPLETED' or status['bssid'] != bssid:\n\t\traise Exception(\"Not fully connected\")\n\tif status['ssid'] != new_ssid:\n\t\traise Exception(\"Unexpected SSID %s != %s\" % (status['ssid'], new_ssid))\n\tdev[0].request(\"DISCONNECT\")\n\tdev[0].wait_disconnected()\n", "description": "Dynamic SSID change with hostapd and WPA2-PSK using WPS", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_reload_invalid", "data": "def test_ap_reload_invalid(dev, apdev):\n\t\"\"\"hostapd RELOAD with invalid configuration\"\"\"\n\tparams = hostapd.wpa2_params(ssid=\"test-wpa2-psk-start\",\n\t\t\t\t\t\t\t\t passphrase=\"12345678\")\n\thapd = hostapd.add_ap(apdev[0], params)\n\t# Enable IEEE 802.11d without specifying country code\n\thapd.set(\"ieee80211d\", \"1\")\n\tif \"FAIL\" not in hapd.request(\"RELOAD\"):\n\t\traise Exception(\"RELOAD command succeeded\")\n\tdev[0].connect(\"test-wpa2-psk-start\", psk=\"12345678\", scan_freq=\"2412\")\n", "description": "hostapd RELOAD with invalid configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "multi_check", "data": "def multi_check(apdev, dev, check, scan_opt=True):\n\tid = []\n\tnum_bss = len(check)\n\tfor i in range(0, num_bss):\n\t\tdev[i].request(\"BSS_FLUSH 0\")\n\t\tdev[i].dump_monitor()\n\tfor i in range(0, num_bss):\n\t\tif check[i]:\n\t\t\tcontinue\n\t\tid.append(dev[i].connect(\"bss-\" + str(i + 1), key_mgmt=\"NONE\",\n\t\t\t\t\t\t\t\t scan_freq=\"2412\", wait_connect=False))\n\tfor i in range(num_bss):\n\t\tif not check[i]:\n\t\t\tcontinue\n\t\tbssid = hostapd.bssid_inc(apdev, i)\n\t\tif scan_opt:\n\t\t\tdev[i].scan_for_bss(bssid, freq=2412)\n\t\tid.append(dev[i].connect(\"bss-\" + str(i + 1), key_mgmt=\"NONE\",\n\t\t\t\t\t\t\t\t scan_freq=\"2412\", wait_connect=True))\n\tfirst = True\n\tfor i in range(num_bss):\n\t\tif not check[i]:\n\t\t\ttimeout = 0.2 if first else 0.01\n\t\t\tfirst = False\n\t\t\tev = dev[i].wait_event([\"CTRL-EVENT-CONNECTED\"], timeout=timeout)\n\t\t\tif ev:\n\t\t\t\traise Exception(\"Unexpected connection\")\n\n\tfor i in range(0, num_bss):\n\t\tdev[i].remove_network(id[i])\n\tfor i in range(num_bss):\n\t\tif check[i]:\n\t\t\tdev[i].wait_disconnected(timeout=5)\n\n\tres = ''\n\tfor i in range(0, num_bss):\n\t\tres = res + dev[i].request(\"BSS RANGE=ALL MASK=0x2\")\n\n\tfor i in range(0, num_bss):\n\t\tif not check[i]:\n\t\t\tbssid = '02:00:00:00:03:0' + str(i)\n\t\t\tif bssid in res:\n\t\t\t\traise Exception(\"Unexpected BSS\" + str(i) + \" in scan results\")\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_remove", "data": "def test_ap_bss_add_remove(dev, apdev):\n\t\"\"\"Dynamic BSS add/remove operations with hostapd\"\"\"\n\ttry:\n\t\t_test_ap_bss_add_remove(dev, apdev)\n\tfinally:\n\t\tfor i in range(3):\n\t\t\tdev[i].request(\"SCAN_INTERVAL 5\")\n", "description": "Dynamic BSS add/remove operations with hostapd", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "_test_ap_bss_add_remove", "data": "def _test_ap_bss_add_remove(dev, apdev):\n\tfor i in range(3):\n\t\tdev[i].flush_scan_cache()\n\t\tdev[i].request(\"SCAN_INTERVAL 1\")\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\tifname3 = apdev[0]['ifname'] + '-3'\n\tlogger.info(\"Set up three BSSes one by one\")\n\thostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\tmulti_check(apdev[0], dev, [True, False, False])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tmulti_check(apdev[0], dev, [True, True, False])\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(apdev[0], dev, [True, True, True])\n\n\tlogger.info(\"Remove the last BSS and re-add it\")\n\thostapd.remove_bss(apdev[0], ifname3)\n\tmulti_check(apdev[0], dev, [True, True, False])\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(apdev[0], dev, [True, True, True])\n\n\tlogger.info(\"Remove the middle BSS and re-add it\")\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(apdev[0], dev, [True, False, True])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tmulti_check(apdev[0], dev, [True, True, True])\n\n\tlogger.info(\"Remove the first BSS and re-add it and other BSSs\")\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(apdev[0], dev, [False, False, False])\n\thostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(apdev[0], dev, [True, True, True])\n\n\tlogger.info(\"Remove two BSSes and re-add them\")\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(apdev[0], dev, [True, False, True])\n\thostapd.remove_bss(apdev[0], ifname3)\n\tmulti_check(apdev[0], dev, [True, False, False])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tmulti_check(apdev[0], dev, [True, True, False])\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(apdev[0], dev, [True, True, True])\n\n\tlogger.info(\"Remove three BSSes in and re-add them\")\n\thostapd.remove_bss(apdev[0], ifname3)\n\tmulti_check(apdev[0], dev, [True, True, False])\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(apdev[0], dev, [True, False, False])\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(apdev[0], dev, [False, False, False])\n\thostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\tmulti_check(apdev[0], dev, [True, False, False])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tmulti_check(apdev[0], dev, [True, True, False])\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\tmulti_check(apdev[0], dev, [True, True, True])\n\n\tlogger.info(\"Test error handling if a duplicate ifname is tried\")\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf', ignore_error=True)\n\tmulti_check(apdev[0], dev, [True, True, True])\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_remove_during_ht_scan", "data": "def test_ap_bss_add_remove_during_ht_scan(dev, apdev):\n\t\"\"\"Dynamic BSS add during HT40 co-ex scan\"\"\"\n\tfor i in range(3):\n\t\tdev[i].flush_scan_cache()\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\tconfname1 = hostapd.cfg_file(apdev[0], \"bss-ht40-1.conf\")\n\tconfname2 = hostapd.cfg_file(apdev[0], \"bss-ht40-2.conf\")\n\thapd_global = hostapd.HostapdGlobal(apdev)\n\thapd_global.send_file(confname1, confname1)\n\thapd_global.send_file(confname2, confname2)\n\thostapd.add_bss(apdev[0], ifname1, confname1)\n\thostapd.add_bss(apdev[0], ifname2, confname2)\n\tmulti_check(apdev[0], dev, [True, True], scan_opt=False)\n\thostapd.remove_bss(apdev[0], ifname2)\n\thostapd.remove_bss(apdev[0], ifname1)\n\n\thostapd.add_bss(apdev[0], ifname1, confname1)\n\thostapd.add_bss(apdev[0], ifname2, confname2)\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(apdev[0], dev, [True, False], scan_opt=False)\n\thostapd.remove_bss(apdev[0], ifname1)\n\n\thostapd.add_bss(apdev[0], ifname1, confname1)\n\thostapd.add_bss(apdev[0], ifname2, confname2)\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(apdev[0], dev, [False, False])\n", "description": "Dynamic BSS add during HT40 co-ex scan", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_multi_bss_config", "data": "def test_ap_multi_bss_config(dev, apdev):\n\t\"\"\"hostapd start with a multi-BSS configuration file\"\"\"\n\tfor i in range(3):\n\t\tdev[i].flush_scan_cache()\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\tifname3 = apdev[0]['ifname'] + '-3'\n\tlogger.info(\"Set up three BSSes with one configuration file\")\n\thapd = hostapd.add_iface(apdev[0], 'multi-bss.conf')\n\thapd.enable()\n\tmulti_check(apdev[0], dev, [True, True, True])\n\thostapd.remove_bss(apdev[0], ifname2)\n\tmulti_check(apdev[0], dev, [True, False, True])\n\thostapd.remove_bss(apdev[0], ifname3)\n\tmulti_check(apdev[0], dev, [True, False, False])\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(apdev[0], dev, [False, False, False])\n\n\thapd = hostapd.add_iface(apdev[0], 'multi-bss.conf')\n\thapd.enable()\n\thostapd.remove_bss(apdev[0], ifname1)\n\tmulti_check(apdev[0], dev, [False, False, False])\n", "description": "hostapd start with a multi-BSS configuration file", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "invalid_ap", "data": "def invalid_ap(ap):\n\tlogger.info(\"Trying to start AP \" + ap['ifname'] + \" with invalid configuration\")\n\thapd = hostapd.add_ap(ap, {}, no_enable=True)\n\thapd.set(\"ssid\", \"invalid-config\")\n\thapd.set(\"channel\", \"12345\")\n\ttry:\n\t\thapd.enable()\n\t\tstarted = True\n\texcept Exception as e:\n\t\tstarted = False\n\tif started:\n\t\traise Exception(\"ENABLE command succeeded unexpectedly\")\n\treturn hapd\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_invalid_config", "data": "def test_ap_invalid_config(dev, apdev):\n\t\"\"\"Try to start AP with invalid configuration and fix configuration\"\"\"\n\thapd = invalid_ap(apdev[0])\n\n\tlogger.info(\"Fix configuration and start AP again\")\n\thapd.set(\"channel\", \"1\")\n\thapd.enable()\n\tdev[0].connect(\"invalid-config\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n", "description": "Try to start AP with invalid configuration and fix configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_invalid_config2", "data": "def test_ap_invalid_config2(dev, apdev):\n\t\"\"\"Try to start AP with invalid configuration and remove interface\"\"\"\n\thapd = invalid_ap(apdev[0])\n\tlogger.info(\"Remove interface with failed configuration\")\n\thostapd.remove_bss(apdev[0])\n", "description": "Try to start AP with invalid configuration and remove interface", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_acs", "data": "def test_ap_remove_during_acs(dev, apdev):\n\t\"\"\"Remove interface during ACS\"\"\"\n\tforce_prev_ap_on_24g(apdev[0])\n\tparams = hostapd.wpa2_params(ssid=\"test-acs-remove\", passphrase=\"12345678\")\n\tparams['channel'] = '0'\n\thostapd.add_ap(apdev[0], params)\n\thostapd.remove_bss(apdev[0])\n", "description": "Remove interface during ACS", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_acs2", "data": "def test_ap_remove_during_acs2(dev, apdev):\n\t\"\"\"Remove BSS during ACS in multi-BSS configuration\"\"\"\n\tforce_prev_ap_on_24g(apdev[0])\n\tifname = apdev[0]['ifname']\n\tifname2 = ifname + \"-2\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"test-acs-remove\")\n\thapd.set(\"channel\", \"0\")\n\thapd.set(\"bss\", ifname2)\n\thapd.set(\"ssid\", \"test-acs-remove2\")\n\thapd.enable()\n\thostapd.remove_bss(apdev[0])\n", "description": "Remove BSS during ACS in multi-BSS configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_acs3", "data": "def test_ap_remove_during_acs3(dev, apdev):\n\t\"\"\"Remove second BSS during ACS in multi-BSS configuration\"\"\"\n\tforce_prev_ap_on_24g(apdev[0])\n\tifname = apdev[0]['ifname']\n\tifname2 = ifname + \"-2\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"test-acs-remove\")\n\thapd.set(\"channel\", \"0\")\n\thapd.set(\"bss\", ifname2)\n\thapd.set(\"ssid\", \"test-acs-remove2\")\n\thapd.enable()\n\thostapd.remove_bss(apdev[0], ifname2)\n", "description": "Remove second BSS during ACS in multi-BSS configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_ht_coex_scan", "data": "def test_ap_remove_during_ht_coex_scan(dev, apdev):\n\t\"\"\"Remove interface during HT co-ex scan\"\"\"\n\tparams = hostapd.wpa2_params(ssid=\"test-ht-remove\", passphrase=\"12345678\")\n\tparams['channel'] = '1'\n\tparams['ht_capab'] = \"[HT40+]\"\n\tifname = apdev[0]['ifname']\n\thostapd.add_ap(apdev[0], params)\n\thostapd.remove_bss(apdev[0])\n", "description": "Remove interface during HT co-ex scan", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_ht_coex_scan2", "data": "def test_ap_remove_during_ht_coex_scan2(dev, apdev):\n\t\"\"\"Remove BSS during HT co-ex scan in multi-BSS configuration\"\"\"\n\tifname = apdev[0]['ifname']\n\tifname2 = ifname + \"-2\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"test-ht-remove\")\n\thapd.set(\"channel\", \"1\")\n\thapd.set(\"ht_capab\", \"[HT40+]\")\n\thapd.set(\"bss\", ifname2)\n\thapd.set(\"ssid\", \"test-ht-remove2\")\n\thapd.enable()\n\thostapd.remove_bss(apdev[0])\n", "description": "Remove BSS during HT co-ex scan in multi-BSS configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_remove_during_ht_coex_scan3", "data": "def test_ap_remove_during_ht_coex_scan3(dev, apdev):\n\t\"\"\"Remove second BSS during HT co-ex scan in multi-BSS configuration\"\"\"\n\tifname = apdev[0]['ifname']\n\tifname2 = ifname + \"-2\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"test-ht-remove\")\n\thapd.set(\"channel\", \"1\")\n\thapd.set(\"ht_capab\", \"[HT40+]\")\n\thapd.set(\"bss\", ifname2)\n\thapd.set(\"ssid\", \"test-ht-remove2\")\n\thapd.enable()\n\thostapd.remove_bss(apdev[0], ifname2)\n", "description": "Remove second BSS during HT co-ex scan in multi-BSS configuration", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_enable_disable_reenable", "data": "def test_ap_enable_disable_reenable(dev, apdev):\n\t\"\"\"Enable, disable, re-enable AP\"\"\"\n\thapd = hostapd.add_ap(apdev[0], {}, no_enable=True)\n\thapd.set(\"ssid\", \"dynamic\")\n\thapd.enable()\n\tev = hapd.wait_event([\"AP-ENABLED\"], timeout=30)\n\tif ev is None:\n\t\traise Exception(\"AP startup timed out\")\n\tdev[0].connect(\"dynamic\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\thapd.disable()\n\tev = hapd.wait_event([\"AP-DISABLED\"], timeout=30)\n\tif ev is None:\n\t\traise Exception(\"AP disabling timed out\")\n\tdev[0].wait_disconnected(timeout=10)\n\thapd.enable()\n\tev = hapd.wait_event([\"AP-ENABLED\"], timeout=30)\n\tif ev is None:\n\t\traise Exception(\"AP startup timed out\")\n\tdev[1].connect(\"dynamic\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[0].wait_connected(timeout=10)\n", "description": "Enable, disable, re-enable AP", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_double_disable", "data": "def test_ap_double_disable(dev, apdev):\n\t\"\"\"Double DISABLE regression test\"\"\"\n\thapd = hostapd.add_bss(apdev[0], apdev[0]['ifname'], 'bss-1.conf')\n\thostapd.add_bss(apdev[0], apdev[0]['ifname'] + '-2', 'bss-2.conf')\n\thapd.disable()\n\tif \"FAIL\" not in hapd.request(\"DISABLE\"):\n\t\traise Exception(\"Second DISABLE accepted unexpectedly\")\n\thapd.enable()\n\thapd.disable()\n\tif \"FAIL\" not in hapd.request(\"DISABLE\"):\n\t\traise Exception(\"Second DISABLE accepted unexpectedly\")\n", "description": "Double DISABLE regression test", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_many", "data": "def test_ap_bss_add_many(dev, apdev):\n\t\"\"\"Large number of BSS add operations with hostapd\"\"\"\n\ttry:\n\t\t_test_ap_bss_add_many(dev, apdev)\n\tfinally:\n\t\tdev[0].request(\"SCAN_INTERVAL 5\")\n\t\tifname = apdev[0]['ifname']\n\t\thapd = hostapd.HostapdGlobal(apdev[0])\n\t\thapd.flush()\n\t\tfor i in range(16):\n\t\t\tifname2 = ifname + '-' + str(i)\n\t\t\thapd.remove(ifname2)\n\t\ttry:\n\t\t\tos.remove('/tmp/hwsim-bss.conf')\n\t\texcept:\n\t\t\tpass\n", "description": "Large number of BSS add operations with hostapd", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "_test_ap_bss_add_many", "data": "def _test_ap_bss_add_many(dev, apdev):\n\tifname = apdev[0]['ifname']\n\thostapd.add_bss(apdev[0], ifname, 'bss-1.conf')\n\tfname = '/tmp/hwsim-bss.conf'\n\tfor i in range(16):\n\t\tifname2 = ifname + '-' + str(i)\n\t\twith open(fname, 'w') as f:\n\t\t\tf.write(\"driver=nl80211\\n\")\n\t\t\tf.write(\"hw_mode=g\\n\")\n\t\t\tf.write(\"channel=1\\n\")\n\t\t\tf.write(\"ieee80211n=1\\n\")\n\t\t\tf.write(\"interface=%s\\n\" % ifname2)\n\t\t\tf.write(\"bssid=02:00:00:00:03:%02x\\n\" % (i + 1))\n\t\t\tf.write(\"ctrl_interface=/var/run/hostapd\\n\")\n\t\t\tf.write(\"ssid=test-%d\\n\" % i)\n\t\thostapd.add_bss(apdev[0], ifname2, fname)\n\t\tos.remove(fname)\n\n\tdev[0].request(\"SCAN_INTERVAL 1\")\n\tdev[0].connect(\"bss-1\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[0].request(\"DISCONNECT\")\n\tdev[0].wait_disconnected(timeout=5)\n\tfor i in range(16):\n\t\tdev[0].connect(\"test-%d\" % i, key_mgmt=\"NONE\", scan_freq=\"2412\")\n\t\tdev[0].request(\"DISCONNECT\")\n\t\tdev[0].wait_disconnected(timeout=5)\n\t\tifname2 = ifname + '-' + str(i)\n\t\thostapd.remove_bss(apdev[0], ifname2)\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_reuse_existing", "data": "def test_ap_bss_add_reuse_existing(dev, apdev):\n\t\"\"\"Dynamic BSS add operation reusing existing interface\"\"\"\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\thostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\tsubprocess.check_call([\"iw\", \"dev\", ifname1, \"interface\", \"add\", ifname2,\n\t\t\t\t\t\t   \"type\", \"__ap\"])\n\thostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\thostapd.remove_bss(apdev[0], ifname2)\n\tsubprocess.check_call([\"iw\", \"dev\", ifname2, \"del\"])\n", "description": "Dynamic BSS add operation reusing existing interface", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "hapd_bss_out_of_mem", "data": "def hapd_bss_out_of_mem(hapd, phy, confname, count, func):\n\twith alloc_fail(hapd, count, func):\n\t\thapd_global = hostapd.HostapdGlobal()\n\t\tres = hapd_global.ctrl.request(\"ADD bss_config=\" + phy + \":\" + confname)\n\t\tif \"OK\" in res:\n\t\t\traise Exception(\"add_bss succeeded\")\n", "description": null, "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_add_out_of_memory", "data": "def test_ap_bss_add_out_of_memory(dev, apdev):\n\t\"\"\"Running out of memory while adding a BSS\"\"\"\n\thapd2 = hostapd.add_ap(apdev[1], {\"ssid\": \"open\"})\n\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\n\tconfname1 = hostapd.cfg_file(apdev[0], \"bss-1.conf\")\n\tconfname2 = hostapd.cfg_file(apdev[0], \"bss-2.conf\")\n\thapd_bss_out_of_mem(hapd2, 'phy3', confname1, 1, 'hostapd_add_iface')\n\tfor i in range(1, 3):\n\t\thapd_bss_out_of_mem(hapd2, 'phy3', confname1,\n\t\t\t\t\t\t\ti, 'hostapd_interface_init_bss')\n\thapd_bss_out_of_mem(hapd2, 'phy3', confname1,\n\t\t\t\t\t\t1, 'ieee802_11_build_ap_params')\n\n\thostapd.add_bss(apdev[0], ifname1, confname1)\n\n\thapd_bss_out_of_mem(hapd2, 'phy3', confname2,\n\t\t\t\t\t\t1, 'hostapd_interface_init_bss')\n\thapd_bss_out_of_mem(hapd2, 'phy3', confname2,\n\t\t\t\t\t\t1, 'ieee802_11_build_ap_params')\n\n\thostapd.add_bss(apdev[0], ifname2, confname2)\n\thostapd.remove_bss(apdev[0], ifname2)\n\thostapd.remove_bss(apdev[0], ifname1)\n", "description": "Running out of memory while adding a BSS", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_multi_bss", "data": "def test_ap_multi_bss(dev, apdev):\n\t\"\"\"Multiple BSSes with hostapd\"\"\"\n\tifname1 = apdev[0]['ifname']\n\tifname2 = apdev[0]['ifname'] + '-2'\n\thapd1 = hostapd.add_bss(apdev[0], ifname1, 'bss-1.conf')\n\thapd2 = hostapd.add_bss(apdev[0], ifname2, 'bss-2.conf')\n\tdev[0].connect(\"bss-1\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[1].connect(\"bss-2\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\n\thwsim_utils.test_connectivity(dev[0], hapd1)\n\thwsim_utils.test_connectivity(dev[1], hapd2)\n\n\tsta0 = hapd1.get_sta(dev[0].own_addr())\n\tsta1 = hapd2.get_sta(dev[1].own_addr())\n\tif 'rx_packets' not in sta0 or int(sta0['rx_packets']) < 1:\n\t\traise Exception(\"sta0 did not report receiving packets\")\n\tif 'rx_packets' not in sta1 or int(sta1['rx_packets']) < 1:\n\t\traise Exception(\"sta1 did not report receiving packets\")\n", "description": "Multiple BSSes with hostapd", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_add_with_driver", "data": "def test_ap_add_with_driver(dev, apdev):\n\t\"\"\"Add hostapd interface with driver specified\"\"\"\n\tifname = apdev[0]['ifname']\n\ttry:\n\t   hostname = apdev[0]['hostname']\n\texcept:\n\t   hostname = None\n\thapd_global = hostapd.HostapdGlobal(apdev[0])\n\thapd_global.add(ifname, driver=\"nl80211\")\n\tport = hapd_global.get_ctrl_iface_port(ifname)\n\thapd = hostapd.Hostapd(ifname, hostname, port)\n\thapd.set_defaults()\n\thapd.set(\"ssid\", \"dynamic\")\n\thapd.enable()\n\tev = hapd.wait_event([\"AP-ENABLED\"], timeout=30)\n\tif ev is None:\n\t\traise Exception(\"AP startup timed out\")\n\tdev[0].connect(\"dynamic\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[0].request(\"DISCONNECT\")\n\tdev[0].wait_disconnected()\n\thapd.disable()\n", "description": "Add hostapd interface with driver specified", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_duplicate_bssid", "data": "def test_ap_duplicate_bssid(dev, apdev):\n\t\"\"\"Duplicate BSSID\"\"\"\n\tparams = {\"ssid\": \"test\"}\n\thapd = hostapd.add_ap(apdev[0], params, no_enable=True)\n\thapd.enable()\n\tifname2 = apdev[0]['ifname'] + '-2'\n\tifname3 = apdev[0]['ifname'] + '-3'\n\t# \"BSS 'wlan3-2' may not have BSSID set to the MAC address of the radio\"\n\ttry:\n\t\thostapd.add_bss(apdev[0], ifname2, 'bss-2-dup.conf')\n\t\traise Exception(\"BSS add succeeded unexpectedly\")\n\texcept Exception as e:\n\t\tif \"Could not add hostapd BSS\" in str(e):\n\t\t\tpass\n\t\telse:\n\t\t\traise\n\n\thostapd.add_bss(apdev[0], ifname3, 'bss-3.conf')\n\n\tdev[0].connect(\"test\", key_mgmt=\"NONE\", scan_freq=\"2412\")\n\tdev[0].request(\"DISCONNECT\")\n\tdev[0].wait_disconnected()\n\n\thapd.set(\"bssid\", \"02:00:00:00:03:02\")\n\thapd.disable()\n\t# \"Duplicate BSSID 02:00:00:00:03:02 on interface 'wlan3-3' and 'wlan3'.\"\n\tif \"FAIL\" not in hapd.request(\"ENABLE\"):\n\t\traise Exception(\"ENABLE with duplicate BSSID succeeded unexpectedly\")\n", "description": "Duplicate BSSID", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}, {"term": "def", "name": "test_ap_bss_config_file", "data": "def test_ap_bss_config_file(dev, apdev, params):\n\t\"\"\"hostapd BSS config file\"\"\"\n\tpidfile = params['prefix'] + \".hostapd.pid\"\n\tlogfile = params['prefix'] + \".hostapd-log\"\n\tprg = os.path.join(params['logdir'], 'alt-hostapd/hostapd/hostapd')\n\tif not os.path.exists(prg):\n\t\tprg = '../../hostapd/hostapd'\n\tphy = get_phy(apdev[0])\n\tconfname1 = hostapd.cfg_file(apdev[0], \"bss-1.conf\")\n\tconfname2 = hostapd.cfg_file(apdev[0], \"bss-2.conf\")\n\tconfname3 = hostapd.cfg_file(apdev[0], \"bss-3.conf\")\n\n\tcmd = [prg, '-B', '-dddt', '-P', pidfile, '-f', logfile, '-S', '-T',\n\t\t   '-b', phy + ':' + confname1, '-b', phy + ':' + confname2,\n\t\t   '-b', phy + ':' + confname3]\n\tres = subprocess.check_call(cmd)\n\tif res != 0:\n\t\traise Exception(\"Could not start hostapd: %s\" % str(res))\n\tmulti_check(apdev[0], dev, [True, True, True])\n\tfor i in range(0, 3):\n\t\tdev[i].request(\"DISCONNECT\")\n\n\thapd = hostapd.Hostapd(apdev[0]['ifname'])\n\thapd.ping()\n\tif \"OK\" not in hapd.request(\"TERMINATE\"):\n\t\traise Exception(\"Failed to terminate hostapd process\")\n\tev = hapd.wait_event([\"CTRL-EVENT-TERMINATING\"], timeout=15)\n\tif ev is None:\n\t\traise Exception(\"CTRL-EVENT-TERMINATING not seen\")\n\tfor i in range(30):\n\t\ttime.sleep(0.1)\n\t\tif not os.path.exists(pidfile):\n\t\t\tbreak\n\tif os.path.exists(pidfile):\n\t\traise Exception(\"PID file exits after process termination\")\n", "description": "hostapd BSS config file", "category": "remove", "imports": ["from remotehost import remote_compatible", "import time", "import subprocess", "import logging", "import os", "import hwsim_utils", "import hostapd", "from utils import *", "from test_ap_acs import force_prev_ap_on_24g"]}], [], [], [{"term": "def", "name": "get_suffix_to_folder", "data": "def get_suffix_to_folder(type_graph, remove_cerebellum_and_vermis, remove_ofc):\n\tsuffix_folder = ''\n\tif 'realistic_connectome' in type_graph:\n\t\tif remove_cerebellum_and_vermis:\n\t\t\tsuffix_folder = suffix_folder + '_without_cerebellum'\n\t\tif remove_ofc:\n\t\t\tsuffix_folder = suffix_folder + '_without_ofc'\n\treturn suffix_folder\n\t\n", "description": null, "category": "remove", "imports": ["import numpy as np", "from utils_CI_BP import *", "from utils_graph_rendering import *", "from graph_generator import generate_graph", "from generate_Mext import *", "from pprint import pprint", "import networkx as nx", "import itertools", "import dill", "import os, sys", "import multiprocessing", "from datetime import datetime", "import time", "from simulate import *", "from utils_define_paths import *"]}, {"term": "def", "name": "spawn", "data": "def spawn(filename_save, type_M_ext, type_graph, keep_history, begin, list_alphac_alphad,\n\t\t  remove_cerebellum_and_vermis, remove_ofc, binarize_realistic_connectome\n\t\t ):\n\tnp.random.seed() #to make sure that simulations are different (otherwise they are given the same random seed)\n\t\n\tres = Simulate(type_graph=type_graph, \n\t\t\t\t   remove_cerebellum_and_vermis=remove_cerebellum_and_vermis, remove_ofc=remove_ofc,\n\t\t\t\t   method_weighting=\"bimodal_w\", w_uniform=None,\n\t\t\t\t   binarize_realistic_connectome=binarize_realistic_connectome,\n\t\t\t\t   \n\t\t\t\t   type_M_ext=type_M_ext,\n\t\t\t\t   n_periods=1, stimulated_nodes=\"all\",\n\t\t\t\t   variance_Mext=30, T_period=1000, #mean_Mext='random_2',\n\t\t\t\t   \n\t\t\t\t   list_alphac_alphad=list_alphac_alphad,\n\t\t\t\t   run_also_BP=False,\n\t\t\t\t   keep_history=keep_history,\n\t\t\t\t   \n\t\t\t\t   begin=begin, \n", "description": null, "category": "remove", "imports": ["import numpy as np", "from utils_CI_BP import *", "from utils_graph_rendering import *", "from graph_generator import generate_graph", "from generate_Mext import *", "from pprint import pprint", "import networkx as nx", "import itertools", "import dill", "import os, sys", "import multiprocessing", "from datetime import datetime", "import time", "from simulate import *", "from utils_define_paths import *"]}], [], [], [{"term": "def", "name": "remove_smallest", "data": "def remove_smallest(numbers):\n\ta = numbers[:] #copy of list numbers in a\n\tif a:\n\t\ta.remove(min(a))\n\treturn a\n", "description": null, "category": "remove", "imports": ["However, just as she finished rating all exhibitions, she's off to an important fair,"]}], [], [], [], [], [{"term": "class", "name": "classEnemyScriptDialogs:", "data": "class EnemyScriptDialogs:\n\tdef __init__(self, args, enemy_scripts):\n\t\tself.args = args\n\t\tself.enemy_scripts = enemy_scripts\n\n\tdef cleanup_mod(self):\n\t\tself.vargas_dialog_mod()\n\t\tself.kefka_narshe_dialog_mod()\n\t\tself.ifrit_shiva_dialog_mod()\n\t\tself.atmaweapon_dialog_mod()\n\t\tself.nerapa_dialog_mod()\n\t\tself.ultros_lete_river_dialog_mod()\n\t\tself.ultros_opera_dialog_mod()\n\t\tself.ultros_esper_mountain_dialog_mod()\n\t\tself.ultros_chupon_dialog_mod()\n\t\tself.srbehemoth_dialog_mod()\n\t\tself.chadarnook_dialog_mod()\n\t\tself.atma_dialog_mod()\n\n\tdef _remove_dialog(self, script, dialog_id, count = 1):\n\t\tdialog_instr = ai_instr.Message(dialog_id)\n\t\tscript.remove(dialog_instr, count)\n\n\tdef _remove_dialogs(self, script, dialog_ids):\n\t\tfor dialog_id in dialog_ids:\n\t\t\tself._remove_dialog(script, dialog_id)\n\n\tdef vargas_dialog_mod(self):\n\t\tvargas_script = self.enemy_scripts.get_script(\"Vargas\")\n\n\t\tdialog_ids = [\n\t\t\t0x0012, # phew... i tire of this\n\t\t\t0x0043, # come on! there's no going back\n\t\t\t0x000a, # come on. what's the matter\n\t\t\t0x0042, # enough!! off with ya now\n\t\t]\n\n\t\tself._remove_dialogs(vargas_script, dialog_ids)\n\n\tdef kefka_narshe_dialog_mod(self):\n\t\tkefka_narshe_id = 330\n\t\tkefka_narshe_script = self.enemy_scripts.scripts[kefka_narshe_id]\n\n\t\tself._remove_dialog(kefka_narshe_script, 0x0015) # don't think you've won\n\n\tdef ifrit_shiva_dialog_mod(self):\n\t\tifrit_script = self.enemy_scripts.get_script(\"Ifrit\")\n\t\tshiva_script = self.enemy_scripts.get_script(\"Shiva\")\n\n\t\t# dialogs are the same in ifrit/shiva scripts but the order of them is different\n\t\tdialog_ids = [\n\t\t\t0x001b, # who're you\n\t\t\t0x001c, # i sensed a kindred spirit\n\t\t\t0x001d, # wait we're espers\n\t\t]\n\n\t\tself._remove_dialogs(ifrit_script, dialog_ids)\n\t\tself._remove_dialogs(shiva_script, dialog_ids)\n\n\tdef atmaweapon_dialog_mod(self):\n\t\tatmaweapon_id = 279\n\t\tatmaweapon_script = self.enemy_scripts.scripts[atmaweapon_id]\n\n\t\tself._remove_dialog(atmaweapon_script, 0x0085) # my name is atma...\n\n\tdef nerapa_dialog_mod(self):\n\t\tnerapa_script = self.enemy_scripts.get_script(\"Nerapa\")\n\n\t\tself._remove_dialog(nerapa_script, 0x0066) # mwa ha ha... ... you can't run\n\n\tdef ultros_lete_river_dialog_mod(self):\n\t\tultros_id = 300\n\t\tultros_script = self.enemy_scripts.scripts[ultros_id]\n\n\t\tdialog_ids = [\n\t\t\t0x000c, # game over! don't tease the octopus kids\n\t\t\t0x000d, # delicious morsel! let me get my bib\n\t\t\t0x000e, # muscle-heads? hate em\n\t\t\t0x004d, # y... you frighten me\n\t\t\t0x0009, # th... that's all friends\n\t\t\t0x000b, # seafood soup\n\t\t]\n\n\t\tself._remove_dialogs(ultros_script, dialog_ids)\n\n\tdef ultros_opera_dialog_mod(self):\n\t\tultros_id = 301\n\t\tultros_script = self.enemy_scripts.scripts[ultros_id]\n\n\t\tdialog_ids = [\n\t\t\t0x0013, # long time no see\n\t\t\t0x0062, # imp! pal! buddy!\n\t\t\t0x0004, # what an unlucky day\n\t\t\t0x001e, # i ain't ready to go yet\n\t\t\t0x001a, # how sweet it is!\n\t\t\t0x0018, # have ya read it?\n\t\t\t0x0017, # havin fun?\n\t\t]\n\n\t\tself._remove_dialogs(ultros_script, dialog_ids)\n\n\t\t# the last two dialogs occur twice in the script, delete both\n\t\tself._remove_dialog(ultros_script, 0x0019, count = 2) # i ain't no garden-variety octopus\n\t\tself._remove_dialog(ultros_script, 0x0016, count = 2) # here! over here!\n\n\tdef ultros_esper_mountain_dialog_mod(self):\n\t\tultros_id = 302\n\t\tultros_script = self.enemy_scripts.scripts[ultros_id]\n\n\t\tdialog_ids = [\n\t\t\t0x004f, # i was just thinking about you\n\t\t\t0x004e, # hope i'm not making a nuiscance of myself\n\t\t\t0x0050, # how can this be? i-i'm nothing more than a stupid octopus\n\t\t]\n\n\t\tself._remove_dialogs(ultros_script, dialog_ids)\n\n\tdef ultros_chupon_dialog_mod(self):\n\t\tultros_id = 360 # chupon is 303\n\t\tultros_script = self.enemy_scripts.scripts[ultros_id]\n\n\t\tdialog_ids = [\n\t\t\t0x0051, # no, really... this is our last battle\n\t\t\t0x0056, # i was drowsing the other day\n\t\t\t0x0052, # better not irritate him\n\t\t\t0x0054, # mr. chupon's taciturn\n\t\t\t0x0053, # i lose again\n\t\t\t0x0057, # fungahhh!\n\t\t]\n\n\t\tself._remove_dialogs(ultros_script, dialog_ids)\n\n\tdef srbehemoth_dialog_mod(self):\n\t\tsrbehemoth_id = 281 # first battle\n\t\tsrbehemoth_script = self.enemy_scripts.scripts[srbehemoth_id]\n\n\t\tdialog_ids = [\n\t\t\t0x007c, # enemy's coming from behind\n\t\t\t0x007d, # another monster appeared\n\t\t]\n\n\t\tself._remove_dialogs(srbehemoth_script, dialog_ids)\n\n\tdef chadarnook_dialog_mod(self):\n\t\tchadarnook_painting_script = self.enemy_scripts.get_script(\"Chadarnook\")\n\n\t\tself._remove_dialog(chadarnook_painting_script, 0x005f) # the girl in the picture is mine\n\n\t\tchadarnook_demon_id = 328\n\t\tchadarnook_demon_script = self.enemy_scripts.scripts[chadarnook_demon_id]\n\n\t\tself._remove_dialog(chadarnook_demon_script, 0x007a) # i... i'm... this can't be\n\n\tdef atma_dialog_mod(self):\n\t\tatma_id = 381\n\t\tatma_script = self.enemy_scripts.scripts[atma_id]\n\n\t\tself._remove_dialog(atma_script, 0x008b) # i'm atma...\n", "description": null, "category": "remove", "imports": ["import data.enemy_script_commands as ai_instr"]}], [], [{"term": "def", "name": "setUp", "data": "def setUp():\n\t\"\"\" Setup feature requests test \"\"\"\n\tutils.setup_simple_feature_test(NS)\n\tutils.setup_image_upload(NS)\n", "description": " Setup feature requests test ", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "tearDown", "data": "def tearDown():\n\t\"\"\" Teardown feature requests test \"\"\"\n\tutils.tear_down_simple_feature_test(NS)\n\tutils.teardown_image_remove(NS)\n", "description": " Teardown feature requests test ", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "setup_image_upload", "data": "def setup_image_upload():\n\tpass\n\t#utils.setup_image_upload(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "teardown_image_remove", "data": "def teardown_image_remove():\n\tpass\n\t#utils.teardown_image_remove(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "setup_mask_upload", "data": "def setup_mask_upload():\n\tutils.setup_mask_upload(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "teardown_mask_remove", "data": "def teardown_mask_remove():\n\tutils.teardown_mask_remove(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "setup_polygon_upload", "data": "def setup_polygon_upload():\n\tutils.setup_polygon_upload(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "teardown_polygon_remove", "data": "def teardown_polygon_remove():\n\tutils.teardown_gobject_remove(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "setup_rectangle_upload", "data": "def setup_rectangle_upload():\n\tutils.setup_rectangle_upload(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "teardown_rectangle_remove", "data": "def teardown_rectangle_remove():\n\tutils.teardown_gobject_remove(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "setup_circle_upload", "data": "def setup_circle_upload():\n\tutils.setup_circle_upload(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "teardown_circle_remove", "data": "def teardown_circle_remove():\n\tutils.teardown_gobject_remove(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "setup_point_upload", "data": "def setup_point_upload():\n\tutils.setup_point_upload(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "teardown_point_remove", "data": "def teardown_point_remove():\n\tutils.teardown_gobject_remove(NS)\n", "description": null, "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_HTD", "data": "def test_HTD():\n\t\"\"\"\n\t\tTest HTD request\n\t\"\"\"\n\tname = 'HTD'\n\ttest_name = 'test_HTD'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n\n", "description": "\n\t\tTest HTD request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_HTD_with_mask", "data": "def test_HTD_with_mask():\n\t\"\"\"\n\t\tTest HTD with mask request\n\t\"\"\"\n\tname = 'HTD'\n\ttest_name = 'test_HTD_with_mask'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, mask=NS.mask_uri)\n\n", "description": "\n\t\tTest HTD with mask request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_HTD_with_polygon", "data": "def test_HTD_with_polygon():\n\t\"\"\"\n\t\tTest HTD with polygon request\n\t\"\"\"\n\tname = 'HTD'\n\ttest_name = 'test_HTD_with_polygon'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n\n", "description": "\n\t\tTest HTD with polygon request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_HTD_with_rectangle", "data": "def test_HTD_with_rectangle():\n\t\"\"\"\n\t\tTest HTD with rectangle request\n\t\"\"\"\n\tname = 'HTD'\n\ttest_name = 'test_HTD_with_rectangle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n\n", "description": "\n\t\tTest HTD with rectangle request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_HTD_with_circle", "data": "def test_HTD_with_circle():\n\t\"\"\"\n\t\tTest HTD with circle request\n\t\"\"\"\n\tname = 'HTD'\n\ttest_name = 'test_HTD_with_circle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest HTD with circle request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_EHD", "data": "def test_EHD():\n\t\"\"\"\n\t\tTest EHD request\n\t\"\"\"\n\tname = 'EHD'\n\ttest_name = 'test_EHD'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)   \n\t\n", "description": "\n\t\tTest EHD request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CLD", "data": "def test_CLD():\n\t\"\"\"\n\t\tTest CLD request\n\t\"\"\"\n\tname = 'CLD'\n\ttest_name = 'test_CLD'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n\t\n", "description": "\n\t\tTest CLD request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CLD_with_mask", "data": "def test_CLD_with_mask():\n\t\"\"\"\n\t\tTest CLD with mask request\n\t\"\"\"\n\tname = 'CLD'\n\ttest_name = 'test_CLD_with_mask'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, mask=NS.mask_uri)\n\t\n", "description": "\n\t\tTest CLD with mask request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CLD_with_polygon", "data": "def test_CLD_with_polygon():\n\t\"\"\"\n\t\tTest CLD with polygon request\n\t\"\"\"\n\tname = 'CLD'\n\ttest_name = 'test_CLD_with_polygon'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n\t\n", "description": "\n\t\tTest CLD with polygon request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CLD_with_rectangle", "data": "def test_CLD_with_rectangle():\n\t\"\"\"\n\t\tTest CLD with rectangle request\n\t\"\"\"\n\tname = 'CLD'\n\ttest_name = 'test_CLD_with_rectangle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n\t\n", "description": "\n\t\tTest CLD with rectangle request\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CLD_with_circle", "data": "def test_CLD_with_circle():\n\t\"\"\"\n\t\tTest CLD with circle\n\t\"\"\"\n\tname = 'CLD'\n\ttest_name = 'test_CLD_with_circle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n\t\n", "description": "\n\t\tTest CLD with circle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CSD", "data": "def test_CSD():\n\t\"\"\"\n\t\tTest CSD\n\t\"\"\"\n\tname = 'CSD'\n\ttest_name = 'test_CSD'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n\t\n", "description": "\n\t\tTest CSD\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CSD_with_mask", "data": "def test_CSD_with_mask():\n\t\"\"\"\n\t\tTest CSD with mask\n\t\"\"\"\n\tname = 'CSD'\n\ttest_name = 'test_CSD_with_mask'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, mask=NS.mask_uri)\n", "description": "\n\t\tTest CSD with mask\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CSD_with_polygon", "data": "def test_CSD_with_polygon():\n\t\"\"\"\n\t\tTest CSD with polygon\n\t\"\"\"\n\tname = 'CSD'\n\ttest_name = 'test_CSD_with_polygon'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest CSD with polygon\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CSD_with_rectangle", "data": "def test_CSD_with_rectangle():\n\t\"\"\"\n\t\tTest CSD with rectangle\n\t\"\"\"\n\tname = 'CSD'\n\ttest_name = 'test_CSD_with_rectangle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest CSD with rectangle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_CSD_with_cirle", "data": "def test_CSD_with_cirle():\n\t\"\"\"\n\t\tTest CSD with circle\n\t\"\"\"\n\tname = 'CSD'\n\ttest_name = 'test_CSD_with_cirle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest CSD with circle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_SCD", "data": "def test_SCD():\n\t\"\"\"\n\t\tTest SCD\n\t\"\"\"\n\tname = 'SCD'\n\ttest_name = 'test_SCD'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n\n", "description": "\n\t\tTest SCD\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_DCD", "data": "def test_DCD():\n\t\"\"\"\n\t\tTest DCD\n\t\"\"\"\n\tname = 'DCD'\n\ttest_name = 'test_DCD'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest DCD\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_DCD_with_mask", "data": "def test_DCD_with_mask():\n\t\"\"\"\n\t\tTest DCD with mask\n\t\"\"\"\n\tname = 'DCD'\n\ttest_name = 'test_DCD_with_mask'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, mask=NS.mask_uri)\n", "description": "\n\t\tTest DCD with mask\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_DCD_with_polygon", "data": "def test_DCD_with_polygon():\n\t\"\"\"\n\t\tTest DCD with polygon\n\t\"\"\"\n\tname = 'DCD'\n\ttest_name = 'test_DCD_with_polygon'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest DCD with polygon\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_DCD_with_rectangle", "data": "def test_DCD_with_rectangle():\n\t\"\"\"\n\t\tTest DCD with rectangle\n\t\"\"\"\n\tname = 'DCD'\n\ttest_name = 'test_DCD_with_rectangle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest DCD with rectangle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_DCD_with_circle", "data": "def test_DCD_with_circle():\n\t\"\"\"\n\t\tTest DCD with circle\n\t\"\"\"\n\tname = 'DCD'\n\ttest_name = 'test_DCD_with_circle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest DCD with circle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_HTD2", "data": "def test_HTD2():\n\t\"\"\"\n\t\tTest HTD2\n\t\"\"\"\n\tname = 'HTD2'\n\ttest_name = 'test_HTD2'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest HTD2\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_EHD2", "data": "def test_EHD2():\n\t\"\"\"\n\t\tTest EHD2\n\t\"\"\"\n\tname = 'EHD2'\n\ttest_name = 'test_EHD2'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest EHD2\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_RSD", "data": "def test_RSD():\n\t\"\"\"\n\t\tTest RSD\n\t\"\"\"\n\tname = 'RSD'\n\ttest_name = 'test_RSD'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest RSD\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_RSD_with_mask", "data": "def test_RSD_with_mask():\n\t\"\"\"\n\t\tTest RSD with mask\n\t\"\"\"\n\tname = 'RSD'\n\ttest_name = 'test_RSD_with_mask'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, mask=NS.mask_uri)\n", "description": "\n\t\tTest RSD with mask\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_RSD_with_polygon", "data": "def test_RSD_with_polygon():\n\t\"\"\"\n\t\tTest RSD with polygon\n\t\"\"\"\n\tname = 'RSD'\n\ttest_name = 'test_RSD_with_polygon'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest RSD with polygon\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_RSD_with_rectangle", "data": "def test_RSD_with_rectangle():\n\t\"\"\"\n\t\tTest RSD with rectangle\n\t\"\"\"\n\tname = 'RSD'\n\ttest_name = 'test_RSD_with_rectangle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest RSD with rectangle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_RSD_with_circle", "data": "def test_RSD_with_circle():\n\t\"\"\"\n\t\tTest RSD with circle\n\t\"\"\"\n\tname = 'RSD'\n\ttest_name = 'test_RSD_with_circle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest RSD with circle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Chebishev_Statistics", "data": "def test_Chebishev_Statistics():\n\t\"\"\"\n\t\tTest Chebishev Statistics\n\t\"\"\"\n\tname = 'Chebishev_Statistics'\n\ttest_name = 'test_Chebishev_Statistics'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Chebishev Statistics\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Chebyshev_Fourier_Transform", "data": "def test_Chebyshev_Fourier_Transform():\n\t\"\"\"\n\t\tTest Chebyshev Fourier Transform\n\t\"\"\"\n\tname = 'Chebyshev_Fourier_Transform'\n\ttest_name = 'test_Chebyshev_Fourier_Transform'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Chebyshev Fourier Transform\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Color_Histogram", "data": "def test_Color_Histogram():\n\t\"\"\"\n\t\tTest Color Histogram\n\t\"\"\"\n\tname = 'Color_Histogram'\n\ttest_name = 'test_Color_Histogram'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Color Histogram\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_TestComb_Moments", "data": "def test_TestComb_Moments():\n\t\"\"\"\n\t\tTest TestComb Moments\n\t\"\"\"\n\tname = 'Comb_Moments'\n\ttest_name = 'test_TestComb_Moments'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest TestComb Moments\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Edge_Features", "data": "def test_Edge_Features():\n\t\"\"\"\n\t\tTest Edge Features\n\t\"\"\"\n\tname = 'Edge_Features'\n\ttest_name = 'test_Edge_Features'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Edge Features\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Fractal_Features", "data": "def test_Fractal_Features():\n\t\"\"\"\n\t\tTest Fractal Features\n\t\"\"\"\n\tname = 'Fractal_Features'\n\ttest_name = 'test_Fractal_Features'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Fractal Features\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Gini_Coefficient", "data": "def test_Gini_Coefficient():\n\t\"\"\"\n\t\tTest Gini Coefficient\n\t\"\"\"\n\tname = 'Gini_Coefficient'\n\ttest_name = 'test_Gini_Coefficient'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Gini Coefficient\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Gabor_Textures", "data": "def test_Gabor_Textures():\n\t\"\"\"\n\t\tTest Gabor Textures\n\t\"\"\"\n\tname = 'Gabor_Textures'\n\ttest_name = 'test_Gabor_Textures'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Gabor Textures\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Multiscale_Historgram", "data": "def test_Multiscale_Historgram():\n\t\"\"\"\n\t\tTest Multiscale Historgram\n\t\"\"\"\n\tname = 'Multiscale_Historgram'\n\ttest_name = 'test_Multiscale_Historgram'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Multiscale Historgram\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Pixel_Intensity_Statistics", "data": "def test_Pixel_Intensity_Statistics():\n\t\"\"\"\n\t\tTest Pixel Intensity Statistics\n\t\"\"\"\n\tname = 'Pixel_Intensity_Statistics'\n\ttest_name = 'test_Pixel_Intensity_Statistics'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Pixel Intensity Statistics\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Radon_Coefficients", "data": "def test_Radon_Coefficients():\n\t\"\"\"\n\t\tTest Radon Coefficients\n\t\"\"\"\n\tname = 'Radon_Coefficients'\n\ttest_name = 'test_Radon_Coefficients'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Radon Coefficients\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Tamura_Textures", "data": "def test_Tamura_Textures():\n\t\"\"\"\n\t\tTest Tamura Textures\n\t\"\"\"\n\tname = 'Tamura_Textures'\n\ttest_name = 'test_Tamura_Textures'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest Tamura Textures\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_Zernike_Coefficients", "data": "def test_Zernike_Coefficients():\n\t\"\"\"\n\t\tTest Zernike Coefficients\n\t\"\"\"\n\tname = 'Zernike_Coefficients'\n\ttest_name = 'test_Zernike_Coefficients'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n\n\n", "description": "\n\t\tTest Zernike Coefficients\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_BRISK", "data": "def test_BRISK():\n\t\"\"\"\n\t\tTest BRISK\n\t\"\"\"\n\tname = 'BRISK'\n\ttest_name = 'test_BRISK'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest BRISK\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_BRISK_with_circle", "data": "def test_BRISK_with_circle():\n\t\"\"\"\n\t\tTest BRISK with circle\n\t\"\"\"\n\tname = 'BRISK'\n\ttest_name = 'test_BRISK_with_circle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest BRISK with circle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_BRISK_with_point", "data": "def test_BRISK_with_point():\n\t\"\"\"\n\t\tTest BRISK with point\t\t\n\t\"\"\"\n\tname = 'BRISK'\n\ttest_name = 'test_BRISK_with_point'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest BRISK with point\t\t\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_ORB", "data": "def test_ORB():\n\t\"\"\"\n\t\ttest ORB\n\t\"\"\"\n\tname = 'ORB'\n\ttest_name = 'test_ORB'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\ttest ORB\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_ORB_with_circle", "data": "def test_ORB_with_circle():\n\t\"\"\"\n\t\tTest ORB with circle\n\t\"\"\"\n\tname = 'ORB'\n\ttest_name = 'test_ORB_with_circle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest ORB with circle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_ORB_with_point", "data": "def test_ORB_with_point():\n\t\"\"\"\n\t\tTest ORB with point\n\t\"\"\"\n\tname = 'ORB'\n\ttest_name = 'test_ORB_with_point'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest ORB with point\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_SIFT", "data": "def test_SIFT():\n\t\"\"\"\n\t\tTest SIFT\n\t\"\"\"\n\tname = 'SIFT'\n\ttest_name = 'test_SIFT'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest SIFT\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_SIFT_with_circle", "data": "def test_SIFT_with_circle():\n\t\"\"\"\n\t\tTest SIFT with circle\n\t\"\"\"\n\tname = 'SIFT'\n\ttest_name = 'test_SIFT_with_circle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest SIFT with circle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_SIFT_with_point", "data": "def test_SIFT_with_point():\n\t\"\"\"\n\t\tTest SIFT with point\n\t\"\"\"\n\tname = 'SIFT'\n\ttest_name = 'test_SIFT_with_point'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest SIFT with point\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_SURF", "data": "def test_SURF():\n\t\"\"\"\n\t\tTest SURF\n\t\"\"\"\n\tname = 'SURF'\n\ttest_name = 'test_SURF'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest SURF\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_SURF_with_circle", "data": "def test_SURF_with_circle():\n\t\"\"\"\n\t\tTest SURF with circle\n\t\"\"\"\n\tname = 'SURF'\n\ttest_name = 'test_SURF_with_circle'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest SURF with circle\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_SURF_with_point", "data": "def test_SURF_with_point():\n\t\"\"\"\n\t\tTest SURF with point\n\t\"\"\"\n\tname = 'SURF'\n\ttest_name = 'test_SURF_with_point'\n\tcheck_feature(NS, test_name,  name, image=NS.image_uri, gobject=NS.gobject_uri)\n", "description": "\n\t\tTest SURF with point\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_LBP", "data": "def test_LBP():\n\t\"\"\"\n\t\tTest LBP\n\t\"\"\"\n\tname = 'LBP'\n\ttest_name = 'test_LBP'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest LBP\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_PFTAS", "data": "def test_PFTAS():\n\t\"\"\"\n\t\tTest PFTAS\n\t\"\"\"\n\tname = 'PFTAS'\n\ttest_name = 'test_PFTAS'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest PFTAS\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_PFTASColored", "data": "def test_PFTASColored():\n\t\"\"\"\n\t\tTest PFTASColored\n\t\"\"\"\n\tname = 'PFTASColored'\n\ttest_name = 'test_PFTASColored'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest PFTASColored\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_TAS", "data": "def test_TAS():\n\t\"\"\"\n\t\tTest TAS \n\t\"\"\"\n\tname = 'TAS'\n\ttest_name = 'test_TAS'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest TAS \n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_TASColored", "data": "def test_TASColored():\n\t\"\"\"\n\t\tTest TASColored\n\t\"\"\"\n\tname = 'TASColored'\n\ttest_name = 'test_TASColored'\t\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest TASColored\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_ZM", "data": "def test_ZM():\n\t\"\"\"\n\t\tTest ZM\n\t\"\"\"\n\tname = 'ZM'\n\ttest_name = 'test_ZM'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest ZM\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_HARColored", "data": "def test_HARColored():\n\t\"\"\"\n\t\tTest HARColored\n\t\"\"\"\n\tname = 'HARColored'\n\ttest_name = 'test_HARColored'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest HARColored\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}, {"term": "def", "name": "test_HAR", "data": "def test_HAR():\n\t\"\"\"\n\t\tTest HAR\n\t\"\"\"\n\tname = 'HAR'\n\ttest_name = 'test_HAR'\n\tcheck_feature(NS, test_name, name, image=NS.image_uri)\n", "description": "\n\t\tTest HAR\n\t", "category": "remove", "imports": ["from nose.plugins.attrib import attr", "import utils", "from utils import TestNameSpace", "from nose import with_setup", "from assert_util import check_feature"]}], [], [], [], [], [], [], [], [], [], [], [], [], [{"term": "def", "name": "random_removal", "data": "def random_removal(request_handler, number_to_remove):\n\t\"\"\"Remove n scheduled tasks at random using a uniform distribution.\n\n\tParameters\n\t----------\n\trequest_handler : RequestHandler\n\tnumber_to_remove : int\n\n\tReturns\n\t-------\n\tRequestHandler\n\t\t`request_handler` with `number_to_remove` requests unscheduled.\n\t\"\"\"\n\n\tif number_to_remove >= request_handler.number_of_scheduled_requests:\n\t\trequest_handler.unschedule_all_requests()\n\t\treturn request_handler\n\n\tunchecked_indices = [i for i in range(0, len(request_handler) - 1)]\n\twhile number_to_remove > 0:\n\t\trequest_index = random.choice(unchecked_indices)\n\t\tif request_handler.is_request_scheduled(request_index):\n\t\t\trequest_handler.unschedule_request(request_index)\n\t\t\tnumber_to_remove -= 1\n\t\tunchecked_indices.remove(request_index)\n\n\treturn request_handler\n\n", "description": "Remove n scheduled tasks at random using a uniform distribution.\n\n\tParameters\n\t----------\n\trequest_handler : RequestHandler\n\tnumber_to_remove : int\n\n\tReturns\n\t-------\n\tRequestHandler\n\t\t`request_handler` with `number_to_remove` requests unscheduled.\n\t", "category": "remove", "imports": ["import random"]}, {"term": "def", "name": "priority_removal", "data": "def priority_removal(request_handler, number_to_remove):\n\t\"\"\"Remove n scheduled tasks in order of decreasing priority.\n\n\tParameters\n\t----------\n\trequest_handler : RequestHandler\n\tnumber_to_remove : int\n\n\tReturns\n\t-------\n\tRequestHandler\n\t\t`request_handler` with `number_to_remove` requests unscheduled.\n\t\"\"\"\n\n\trequest_handler.sort_by_decreasing_priority()\n\treturn _remove_first_n_scheduled_requests(request_handler, number_to_remove)\n\n", "description": "Remove n scheduled tasks in order of decreasing priority.\n\n\tParameters\n\t----------\n\trequest_handler : RequestHandler\n\tnumber_to_remove : int\n\n\tReturns\n\t-------\n\tRequestHandler\n\t\t`request_handler` with `number_to_remove` requests unscheduled.\n\t", "category": "remove", "imports": ["import random"]}, {"term": "def", "name": "_remove_first_n_scheduled_requests", "data": "def _remove_first_n_scheduled_requests(request_handler, number_to_remove):\n\tif number_to_remove >= request_handler.number_of_scheduled_requests:\n\t\trequest_handler.unschedule_all_requests()\n\t\treturn request_handler\n\n\tindex = 0\n\twhile number_to_remove > 0:\n\t\tif request_handler.is_request_scheduled(index):\n\t\t\trequest_handler.unschedule_request(index)\n\t\t\tnumber_to_remove -= 1\n\t\tindex += 1\n\n\treturn request_handler\n\n", "description": null, "category": "remove", "imports": ["import random"]}, {"term": "def", "name": "opportunity_removal", "data": "def opportunity_removal(request_handler, number_to_remove):\n\t\"\"\"Remove n scheduled tasks in order of decreasing opportunity.\n\n\tParameters\n\t----------\n\trequest_handler : RequestHandler\n\tnumber_to_remove : int\n\n\tReturns\n\t-------\n\tRequestHandler\n\t\t`request_handler` with `number_to_remove` requests unscheduled.\n\n\tNotes\n\t-----\n\tThe opportunity of a request is measured as the number of opportunities for\n\tthe request to be fulfilled. Opportunity removal removes requests with the\n\thighest number of opportunities since they can more easily be inserted into\n\ta solution without conflict.\n\t\"\"\"\n\n\trequest_handler.sort_by_decreasing_opportunity()\n\treturn _remove_first_n_scheduled_requests(request_handler, number_to_remove)\n\n", "description": "Remove n scheduled tasks in order of decreasing opportunity.\n\n\tParameters\n\t----------\n\trequest_handler : RequestHandler\n\tnumber_to_remove : int\n\n\tReturns\n\t-------\n\tRequestHandler\n\t\t`request_handler` with `number_to_remove` requests unscheduled.\n\n\tNotes\n\t-----\n\tThe opportunity of a request is measured as the number of opportunities for\n\tthe request to be fulfilled. Opportunity removal removes requests with the\n\thighest number of opportunities since they can more easily be inserted into\n\ta solution without conflict.\n\t", "category": "remove", "imports": ["import random"]}, {"term": "def", "name": "conflict_removal", "data": "def conflict_removal(request_handler, number_to_remove):\n\t\"\"\"Remove n scheduled tasks in order of decreasing conflict degree.\n\n\tParameters\n\t----------\n\trequest_handler : RequestHandler\n\tnumber_to_remove : int\n\n\tReturns\n\t-------\n\tRequestHandler\n\t\t`request_handler` with `number_to_remove` requests unscheduled.\n\n\tNotes\n\t-----\n\tThe conflict degree of a request is a measure of how much the opportunity\n\twindows of a request overlap with those of other requests. Conflict removal\n\tremoves requests with the highest conflict degree as these are most likely\n\tto limit the number of scheduled requests.\n\t\"\"\"\n\n\trequest_handler.sort_by_decreasing_conflict_degree()\n\treturn _remove_first_n_scheduled_requests(request_handler, number_to_remove)\n\n", "description": "Remove n scheduled tasks in order of decreasing conflict degree.\n\n\tParameters\n\t----------\n\trequest_handler : RequestHandler\n\tnumber_to_remove : int\n\n\tReturns\n\t-------\n\tRequestHandler\n\t\t`request_handler` with `number_to_remove` requests unscheduled.\n\n\tNotes\n\t-----\n\tThe conflict degree of a request is a measure of how much the opportunity\n\twindows of a request overlap with those of other requests. Conflict removal\n\tremoves requests with the highest conflict degree as these are most likely\n\tto limit the number of scheduled requests.\n\t", "category": "remove", "imports": ["import random"]}], [], [], [], [{"term": "class", "name": "Projectile", "data": "class Projectile(pg.sprite.Sprite):\n\tdef __init__(self, class_Player):\n\t\tsuper().__init__()\t\t\n\t\tself.class_Player = class_Player\n\t\tself.velocity = 5\n\t\tself.image = pg.image.load('data/laser.png')\n\t\tself.image = pg.transform.scale(self.image, (50, 50))\n\t\tself.rect = self.image.get_rect()\n\t\tself.rect.x = class_Player.rect.x + 100\n\t\tself.rect.y = class_Player.rect.y\n\t\n\tdef remove (self):\n\t\tself.class_Player.all_projectiles.remove(self)\n\t\n\tdef move1(self):\n\t\t# d\u00e9place le projectile\n\t\tself.rect.x += self.velocity\n\t\t# verifie si le projectile entre en collision avec un joueur\n\t\tfor player in self.class_Player.game.check_collision(self, self.class_Player.game.all_players):\n\t\t\t# supprime le projectile\n\t\t\tself.remove()\n\t\t\t# inflige des d\u00e9gats aux joueurs \n\t\t\tplayer.damage(self.class_Player.attack)\n\t\tfor asteroid in self.class_Player.game.check_collision(self, self.class_Player.game.all_asteroids):\n\t\t\t# supprime le projectile\n\t\t\tself.remove()\t\t\t\n\t\t\t# inflige des d\u00e9gats aux asteroids\n\t\t\tasteroid.damage(self.class_Player.attack)\n\t\t# si le projectile sort de l'\u00e9cran, on le supprime\n\t\tif self.rect.x > 1280 :\n\t\t\tself.remove()\n\t\t\t\t   \n\tdef move2(self):\n\t\t# d\u00e9place le projectile\n\t\tself.rect.x -= self.velocity\n\t\t# si le projectile sort de l'\u00e9cran, on le supprime\n\t\tif self.rect.x < -50:\n\t\t\tself.remove()\n\t\t# verifie si le projectile entre en collision avec un joueur\n\t\tfor player in self.class_Player.game.check_collision(self, self.class_Player.game.all_players):\n\t\t\t# supprime le projectile\n\t\t\tself.remove()\t\t\t\n\t\t\t# inflige des d\u00e9gats aux joueurs \n\t\t\tplayer.damage(self.class_Player.attack)\n\t\tfor asteroid in self.class_Player.game.check_collision(self, self.class_Player.game.all_asteroids):\n\t\t\t# supprime le projectile\n\t\t\tself.remove()\t\t\t\n\t\t\t# inflige des d\u00e9gats aux asteroids\n\t\t\tasteroid.damage(self.class_Player.attack)\n\t\t# si le projectile sort de l'\u00e9cran, on le supprime\n\t\tif self.rect.x > 1280 :\n\t\t\tself.remove()\n", "description": null, "category": "remove", "imports": ["import pygame as pg"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('app', '0010_auto_20210616_0921'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='category',\n\t\t\tname='title_ru',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='audio_file_ru',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_alt',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_ash',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_az',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_ba',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_crh',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_cv',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_kaa',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_kk',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_krc',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_ksk',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_ky',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_ru',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_sah',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_tk',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_tr',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_tt',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_ug',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='images_uz',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='description',\n\t\t\tname='title_ru',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='favorite',\n\t\t\tname='favorite',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='word',\n\t\t\tname='title_ru',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from django.db import migrations"]}], [{"term": "def", "name": "remove_char", "data": "def remove_char(s):\n", "description": null, "category": "remove", "imports": []}], [], [], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('orm', '0009_target_package_manifest_path'),\n\t]\n\n\toperations = [\n\t\tmigrations.AlterUniqueTogether(\n\t\t\tname='releaselayersourcepriority',\n\t\t\tunique_together=set([]),\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='releaselayersourcepriority',\n\t\t\tname='layer_source',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='releaselayersourcepriority',\n\t\t\tname='release',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ImportedLayerSource',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='LayerIndexLayerSource',\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='LocalLayerSource',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='recipe',\n\t\t\tname='layer_source',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='recipe',\n\t\t\tname='up_id',\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='layer',\n\t\t\tname='up_date',\n\t\t\tfield=models.DateTimeField(default=django.utils.timezone.now, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='layer_version',\n\t\t\tname='layer_source',\n\t\t\tfield=models.IntegerField(default=0, choices=[(0, 'local'), (1, 'layerindex'), (2, 'imported'), (3, 'build')]),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='layer_version',\n\t\t\tname='up_date',\n\t\t\tfield=models.DateTimeField(default=django.utils.timezone.now, null=True),\n\t\t),\n\t\tmigrations.AlterUniqueTogether(\n\t\t\tname='branch',\n\t\t\tunique_together=set([]),\n\t\t),\n\t\tmigrations.AlterUniqueTogether(\n\t\t\tname='layer',\n\t\t\tunique_together=set([]),\n\t\t),\n\t\tmigrations.AlterUniqueTogether(\n\t\t\tname='layer_version',\n\t\t\tunique_together=set([]),\n\t\t),\n\t\tmigrations.AlterUniqueTogether(\n\t\t\tname='layerversiondependency',\n\t\t\tunique_together=set([]),\n\t\t),\n\t\tmigrations.AlterUniqueTogether(\n\t\t\tname='machine',\n\t\t\tunique_together=set([]),\n\t\t),\n\t\tmigrations.DeleteModel(\n\t\t\tname='ReleaseLayerSourcePriority',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branch',\n\t\t\tname='layer_source',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='branch',\n\t\t\tname='up_id',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='layer',\n\t\t\tname='layer_source',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='layer',\n\t\t\tname='up_id',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='layer_version',\n\t\t\tname='up_id',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='layerversiondependency',\n\t\t\tname='layer_source',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='layerversiondependency',\n\t\t\tname='up_id',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='machine',\n\t\t\tname='layer_source',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='machine',\n\t\t\tname='up_id',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations, models", "import django.utils.timezone", "\t\t\tfield=models.IntegerField(default=0, choices=[(0, 'local'), (1, 'layerindex'), (2, 'imported'), (3, 'build')]),"]}], [], [], [{"term": "def", "name": "remove_repeats", "data": "def remove_repeats(s):\n\ts=list(s)\n\tf=\"\".join([s[i] for i in range(0,len(s)) if s[i]!=s[i-1]])\n\treturn s[0]+f if f[0]!=s[0] else f\n", "description": null, "category": "remove", "imports": []}], [], [], [], [{"term": "class", "name": "classNode:\r", "data": "class Node:\r\n\tdef __init__(self, value):\r\n\t\tself.value = value\r\n\t\tself.prev = None\r\n\t\tself.next = None\r\n\r\n", "description": null, "category": "remove", "imports": []}, {"term": "class", "name": "classDoublyLinkedList:\r", "data": "class DoublyLinkedList:\r\n\tdef __init__(self):\r\n\t\tself.head = None\r\n\t\tself.tail = None\r\n\r\n\tdef containsNodeWithValue(self, value):\r\n\t\tnode = self.head\r\n\t\twhile node is not None and node.value != value:\r\n\t\t\tnode = node.next\r\n\t\treturn node is not None\r\n\r\n\tdef remove(self, nodeToRemove):\r\n\t\tnode = self.head\r\n\t\tif nodeToRemove is self.head:\r\n\t\t\tself.head = self.head.next\r\n\t\telif nodeToRemove is self.tail:\r\n\t\t\tself.tail = self.tail.prev\r\n\t\tself.removeNodeBindings(nodeToRemove)\r\n\r\n\tdef removeNodeBindings(self, node):\r\n\t\tif node.prev is not None:\r\n\t\t\tnode.prev.next = node.next\r\n\t\tif node.next is not None:\r\n\t\t\tnode.next.prev = node.prev\r\n\t\tnode.prev = None\r\n\t\tnode.next = None\r\n\r\n\tdef removeNodeWithValue(self, value):\r\n\t\tnode = self.head\r\n\t\twhile node is not None:\r\n\t\t\tnodeToRemove = node\r\n\t\t\tnode = node.next\r\n\t\t\tif nodeToRemove.value == value:\r\n\t\t\t\tself.remove(nodeToRemove)\r\n\r\n\tdef insertBefore(self, node, nodeToInsert):\r\n\t\tif nodeToInsert is self.head:\r\n\t\t\treturn\r\n\t\tself.remove(nodeToInsert)\r\n\t\tnodeToInsert.next = node\r\n\t\tnodeToInsert.prev = node.prev\r\n\t\tif node.prev is None:\r\n\t\t\tself.head = nodeToInsert\r\n\t\telse:\r\n\t\t\tnode.prev.next = nodeToInsert\r\n\t\tnode.prev = nodeToInsert\r\n\r\n\tdef insertAfter(self, node, nodeToInsert):\r\n\t\tif self.tail is nodeToInsert:\r\n\t\t\treturn\r\n\t\tnodeToInsert.prev = node\r\n\t\tnodeToInsert.next = node.next\r\n\t\tif node.next is None:\r\n\t\t\tself.tail = nodeToInsert\r\n\t\telse:\r\n\t\t\tnode.next.prev = nodeToInsert\r\n\t\tnode.next = nodeToInsert\r\n\r\n\tdef insertAtPosition(self, nodeToInsert, position):\r\n\t\tcurrPosition = 1\r\n\t\tif position == 1:\r\n\t\t\tself.setHead(nodeToInsert)\r\n\t\t\treturn\r\n\t\tnode = self.head\r\n\t\twhile node is not None and currPosition < position:\r\n\t\t\tcurrPosition += 1\r\n\t\t\tnode = node.next\r\n\t\tif node is not None:\r\n\t\t\tself.insertBefore(node, nodeToInsert)\r\n\t\telse:\r\n\t\t\tself.setTail(nodeToInsert)\r\n\r\n\tdef setHead(self, node):\r\n\t\tif self.head is None and self.tail is None:\r\n\t\t\tself.head = node\r\n\t\t\tself.tail = node\r\n\t\t\treturn\r\n\t\tself.insertBefore(self.head, node)\r\n\r\n\tdef setTail(self, node):\r\n\t\tif self.head is None and self.tail is None:\r\n\t\t\tself.setHead(node)\r\n\t\telse:\r\n\t\t\tself.insertAfter(self.tail, node)\r\n\r\n\tdef printList(self):\r\n\t\tnode = self.head\r\n\t\twhile node is not None:\r\n\t\t\tprint(\"--->\", node.value, end=\" <---\")\r\n\t\t\tnode = node.next\r\n\r\n", "description": null, "category": "remove", "imports": []}], [], [], [], [], [], [], [{"term": "def", "name": "compute_api_add_fixed_ip", "data": "def compute_api_add_fixed_ip(self, context, instance, network_id):\n\tglobal last_add_fixed_ip\n\n\tlast_add_fixed_ip = (instance['uuid'], network_id)\n\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova import compute", "from nova import exception", "from nova import objects", "from nova.openstack.common import jsonutils", "from nova import test", "from nova.tests.api.openstack import fakes"]}, {"term": "def", "name": "compute_api_remove_fixed_ip", "data": "def compute_api_remove_fixed_ip(self, context, instance, address):\n\tglobal last_remove_fixed_ip\n\n\tlast_remove_fixed_ip = (instance['uuid'], address)\n\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova import compute", "from nova import exception", "from nova import objects", "from nova.openstack.common import jsonutils", "from nova import test", "from nova.tests.api.openstack import fakes"]}, {"term": "def", "name": "compute_api_get", "data": "def compute_api_get(self, context, instance_id, want_objects=False,\n\t\t\t\t\texpected_attrs=None):\n\tinstance = objects.Instance()\n\tinstance.uuid = instance_id\n\tinstance.id = 1\n\tinstance.vm_state = 'fake'\n\tinstance.task_state = 'fake'\n\tinstance.obj_reset_changes()\n\treturn instance\n\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova import compute", "from nova import exception", "from nova import objects", "from nova.openstack.common import jsonutils", "from nova import test", "from nova.tests.api.openstack import fakes"]}, {"term": "class", "name": "FixedIpTestV21", "data": "class FixedIpTestV21(test.NoDBTestCase):\n\tdef setUp(self):\n\t\tsuper(FixedIpTestV21, self).setUp()\n\t\tfakes.stub_out_networking(self.stubs)\n\t\tfakes.stub_out_rate_limiting(self.stubs)\n\t\tself.stubs.Set(compute.api.API, \"add_fixed_ip\",\n\t\t\t\t\t   compute_api_add_fixed_ip)\n\t\tself.stubs.Set(compute.api.API, \"remove_fixed_ip\",\n\t\t\t\t\t   compute_api_remove_fixed_ip)\n\t\tself.stubs.Set(compute.api.API, 'get', compute_api_get)\n\t\tself.app = self._get_app()\n\n\tdef _get_app(self):\n\t\treturn fakes.wsgi_app_v3(init_only=('servers', 'os-multinic'))\n\n\tdef _get_url(self):\n\t\treturn '/v3'\n\n\tdef test_add_fixed_ip(self):\n\t\tglobal last_add_fixed_ip\n\t\tlast_add_fixed_ip = (None, None)\n\n\t\tbody = dict(addFixedIp=dict(networkId='test_net'))\n\t\treq = webob.Request.blank(\n\t\t\tself._get_url() + '/servers/%s/action' % UUID)\n\t\treq.method = 'POST'\n\t\treq.body = jsonutils.dumps(body)\n\t\treq.headers['content-type'] = 'application/json'\n\n\t\tresp = req.get_response(self.app)\n\t\tself.assertEqual(resp.status_int, 202)\n\t\tself.assertEqual(last_add_fixed_ip, (UUID, 'test_net'))\n\n\tdef _test_add_fixed_ip_bad_request(self, body):\n\t\treq = webob.Request.blank(\n\t\t\tself._get_url() + '/servers/%s/action' % UUID)\n\t\treq.method = 'POST'\n\t\treq.body = jsonutils.dumps(body)\n\t\treq.headers['content-type'] = 'application/json'\n\t\tresp = req.get_response(self.app)\n\t\tself.assertEqual(400, resp.status_int)\n\n\tdef test_add_fixed_ip_empty_network_id(self):\n\t\tbody = {'addFixedIp': {'network_id': ''}}\n\t\tself._test_add_fixed_ip_bad_request(body)\n\n\tdef test_add_fixed_ip_network_id_bigger_than_36(self):\n\t\tbody = {'addFixedIp': {'network_id': 'a' * 37}}\n\t\tself._test_add_fixed_ip_bad_request(body)\n\n\tdef test_add_fixed_ip_no_network(self):\n\t\tglobal last_add_fixed_ip\n\t\tlast_add_fixed_ip = (None, None)\n\n\t\tbody = dict(addFixedIp=dict())\n\t\treq = webob.Request.blank(\n\t\t\tself._get_url() + '/servers/%s/action' % UUID)\n\t\treq.method = 'POST'\n\t\treq.body = jsonutils.dumps(body)\n\t\treq.headers['content-type'] = 'application/json'\n\n\t\tresp = req.get_response(self.app)\n\t\tself.assertEqual(resp.status_int, 400)\n\t\tself.assertEqual(last_add_fixed_ip, (None, None))\n\n\t@mock.patch.object(compute.api.API, 'add_fixed_ip')\n\tdef test_add_fixed_ip_no_more_ips_available(self, mock_add_fixed_ip):\n\t\tmock_add_fixed_ip.side_effect = exception.NoMoreFixedIps\n\n\t\tbody = dict(addFixedIp=dict(networkId='test_net'))\n\t\treq = webob.Request.blank(\n\t\t\tself._get_url() + '/servers/%s/action' % UUID)\n\t\treq.method = 'POST'\n\t\treq.body = jsonutils.dumps(body)\n\t\treq.headers['content-type'] = 'application/json'\n\n\t\tresp = req.get_response(self.app)\n\t\tself.assertEqual(resp.status_int, 400)\n\n\tdef test_remove_fixed_ip(self):\n\t\tglobal last_remove_fixed_ip\n\t\tlast_remove_fixed_ip = (None, None)\n\n\t\tbody = dict(removeFixedIp=dict(address='10.10.10.1'))\n\t\treq = webob.Request.blank(\n\t\t\tself._get_url() + '/servers/%s/action' % UUID)\n\t\treq.method = 'POST'\n\t\treq.body = jsonutils.dumps(body)\n\t\treq.headers['content-type'] = 'application/json'\n\n\t\tresp = req.get_response(self.app)\n\t\tself.assertEqual(resp.status_int, 202)\n\t\tself.assertEqual(last_remove_fixed_ip, (UUID, '10.10.10.1'))\n\n\tdef test_remove_fixed_ip_no_address(self):\n\t\tglobal last_remove_fixed_ip\n\t\tlast_remove_fixed_ip = (None, None)\n\n\t\tbody = dict(removeFixedIp=dict())\n\t\treq = webob.Request.blank(\n\t\t\tself._get_url() + '/servers/%s/action' % UUID)\n\t\treq.method = 'POST'\n\t\treq.body = jsonutils.dumps(body)\n\t\treq.headers['content-type'] = 'application/json'\n\n\t\tresp = req.get_response(self.app)\n\t\tself.assertEqual(resp.status_int, 400)\n\t\tself.assertEqual(last_remove_fixed_ip, (None, None))\n\n\tdef test_remove_fixed_ip_invalid_address(self):\n\t\tbody = {'remove_fixed_ip': {'address': ''}}\n\t\treq = webob.Request.blank(\n\t\t\tself._get_url() + '/servers/%s/action' % UUID)\n\t\treq.method = 'POST'\n\t\treq.body = jsonutils.dumps(body)\n\t\treq.headers['content-type'] = 'application/json'\n\t\tresp = req.get_response(self.app)\n\t\tself.assertEqual(400, resp.status_int)\n\n\t@mock.patch.object(compute.api.API, 'remove_fixed_ip',\n\t\tside_effect=exception.FixedIpNotFoundForSpecificInstance(\n\t\t\tinstance_uuid=UUID, ip='10.10.10.1'))\n\tdef test_remove_fixed_ip_not_found(self, _remove_fixed_ip):\n\n\t\tbody = {'remove_fixed_ip': {'address': '10.10.10.1'}}\n\t\treq = webob.Request.blank(\n\t\t\tself._get_url() + '/servers/%s/action' % UUID)\n\t\treq.method = 'POST'\n\t\treq.body = jsonutils.dumps(body)\n\t\treq.headers['content-type'] = 'application/json'\n\n\t\tresp = req.get_response(self.app)\n\t\tself.assertEqual(400, resp.status_int)\n\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova import compute", "from nova import exception", "from nova import objects", "from nova.openstack.common import jsonutils", "from nova import test", "from nova.tests.api.openstack import fakes"]}, {"term": "class", "name": "FixedIpTestV2", "data": "class FixedIpTestV2(FixedIpTestV21):\n\tdef setUp(self):\n\t\tsuper(FixedIpTestV2, self).setUp()\n\t\tself.flags(\n\t\t\tosapi_compute_extension=[\n\t\t\t\t'nova.api.openstack.compute.contrib.select_extensions'],\n\t\t\tosapi_compute_ext_list=['Multinic'])\n\n\tdef _get_app(self):\n\t\treturn fakes.wsgi_app(init_only=('servers',))\n\n\tdef _get_url(self):\n\t\treturn '/v2/fake'\n\n\tdef test_remove_fixed_ip_invalid_address(self):\n\t\t# NOTE(cyeoh): This test is disabled for the V2 API because it is\n\t\t# has poorer input validation.\n\t\tpass\n", "description": null, "category": "remove", "imports": ["import mock", "import webob", "from nova import compute", "from nova import exception", "from nova import objects", "from nova.openstack.common import jsonutils", "from nova import test", "from nova.tests.api.openstack import fakes"]}], [{"term": "def", "name": "stem_remove", "data": "def stem_remove(indiv, Optimizer):\n\tatms = indiv[0].copy()\n\tpixelshift = [0,0,0]\n\tif Optimizer.stemcalc.parameters['pixelshift'] == True:\n\t   points = [-0.5993457/2,0,0.5993457/2]\n\t   chisq = numpy.zeros([3,3],dtype=float)\n\t   for x in range(3):\n\t\t  for y in range(3):\n\t\t\t pixelshift = [points[x],points[y],0]\n\t\t\t simfun=Optimizer.stemcalc.get_image(Optimizer.stemcalc.psf,atms,Optimizer.stemcalc.parameters['slice_size'],Optimizer.stemcalc.parameters['pixels'],pixelshift)\n\t\t\t chisq[x][y]=Optimizer.stemcalc.compare_functions(Optimizer.stemcalc.expfun,simfun)\n\t   i,j = numpy.unravel_index(chisq.argmin(),chisq.shape)\n\t   pixelshift = [points[i],points[j],0]\n\tsimfun = Optimizer.stemcalc.get_image(Optimizer.stemcalc.psf,atms,Optimizer.stemcalc.parameters['slice_size'],Optimizer.stemcalc.parameters['pixels'], pixelshift) \n\texpfun = Optimizer.stemcalc.expfun\n\n\txmax = Optimizer.stemcalc.parameters['slice_size']\n\tcom = atms.get_center_of_mass()\n\t#com = [ 44.40963074 , 44.65497562 , 44.90406073]\n\t#com = numpy.array(com)\n\t#com += [-0.149836425, 0.29967285, 0]  #pixelshift\n\tcop = xmax/2.0 \n\ttrans = [cop-i for i in com]\n\tatms.translate(trans)\n\tR = atms.arrays['positions']\n\tax=[]\n\tay=[]\n\taz=[] \n\tfor o,t,h in R:\n\t\tax.append(o)\n\t\tay.append(t)\n\t\taz.append(h)\n\tax = numpy.array(ax)\n\tay = numpy.array(ay)\n\taz = numpy.array(az)\n\n\tnx = Optimizer.stemcalc.parameters['pixels']\n\tny = nx\n\tdx = xmax/nx\n\tdy = dx\n\tix = numpy.array([math.floor(axi/dx) for axi in ax])\n\tiax = numpy.array([int(math.fmod(iaxi,nx)) for iaxi in ix])\n\tibx = numpy.array([int(math.fmod(iaxi+1,nx)) for iaxi in ix])\n\tiy = numpy.array([math.floor(ayi/dy) for ayi in ay])\n\tiay = numpy.array([int(math.fmod(iayi,ny)) for iayi in iy])\n\tiby = numpy.array([int(math.fmod(iayi+1,ny)) for iayi in iy])\n\n", "description": null, "category": "remove", "imports": ["import random", "from StructOpt.tools.StemCalc import ConvStem", "from ase import Atoms,Atom", "import math", "from math import fabs, sqrt, acos, pi, atan", "import numpy "]}], [], [], [], [{"term": "def", "name": "simple_extract", "data": "def simple_extract(content):\n\t\"\"\"\u4f7f\u7528\u7b80\u5355\u7b97\u6cd5\u63d0\u53d6\u6b63\u6587\u6587\u672c\uff0ccontent\u4e3aunicode\u6587\u672c\u3002\"\"\"\n\tcontent = remove_empty_line(remove_js_css(content))\n\tleft,right = rc_extract(content)\n\tcontent = '\\n'.join(content.split('\\n')[left:right])\n\treturn content\n", "description": "\u4f7f\u7528\u7b80\u5355\u7b97\u6cd5\u63d0\u53d6\u6b63\u6587\u6587\u672c\uff0ccontent\u4e3aunicode\u6587\u672c\u3002", "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "remove_js_css", "data": "def remove_js_css(content):\n\t\"\"\" remove the the javascript and the stylesheet and the comment content( and  ) \"\"\"\n\tr = re.compile(r'''''',re.I|re.M|re.S)\n\ts = r.sub('',content)\n\tr = re.compile(r'''''',re.I|re.M|re.S)\n\ts = r.sub('', s)\n\tr = re.compile(r'''''', re.I|re.M|re.S)\n\ts = r.sub('',s)\n\tr = re.compile(r'''''', re.I|re.M|re.S)\n\ts = r.sub('',s)\n\tr = re.compile(r'''''', re.I|re.M|re.S)\n\ts = r.sub('',s)\n\treturn s\n", "description": " remove the the javascript and the stylesheet and the comment content( and  ) ", "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "remove_empty_line", "data": "def remove_empty_line(content):\n\t\"\"\"remove multi space \"\"\"\n\tr = re.compile(r'''^\\s+$''', re.M|re.S)\n\ts = r.sub('', content)\n\tr = re.compile(r'''\\n+''',re.M|re.S)\n\ts = r.sub('\\n',s)\n\treturn s\n", "description": "remove multi space ", "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "remove_any_tag", "data": "def remove_any_tag(s):\n\ts = re.sub(r'''<[^>]+>''','',s)\n\treturn s.strip()\n", "description": null, "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "remove_any_tag_but_a", "data": "def remove_any_tag_but_a(s):\n\ttext = re.findall(r''']*>(.*?)''',s,re.I|re.S)\n\ttext_b = remove_any_tag(s)\n\treturn len(''.join(text)),len(text_b)\n", "description": null, "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "remove_image", "data": "def remove_image(s,n=50):\n\timage = 'a' * n\n\tr = re.compile(r'''''',re.I|re.M|re.S)\n\ts = r.sub(image,s)\n\treturn s\n", "description": null, "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "remove_video", "data": "def remove_video(s,n=1000):\n\tvideo = 'a' * n\n\tr = re.compile(r'''''',re.I|re.M|re.S)\n\ts = r.sub(video,s)\n\treturn s\n", "description": null, "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "sum_max", "data": "def sum_max(values):\n\tcur_max = values[0]\n\tglo_max = -999999\n\tleft,right = 0,0\n\tfor index,value in enumerate(values):\n\t\tcur_max += value\n\t\tif(cur_max > glo_max) :\n\t\t\tglo_max = cur_max\n\t\t\tright = index\n\t\telif(cur_max < 0):\n\t\t\tcur_max = 0\n\n\tfor i in range(right, -1, -1):\n\t\tglo_max -= values[i]\n\t\tif abs(glo_max < 0.00001):\n\t\t\tleft = i\n\t\t\tbreak\n\treturn left,right+1\n", "description": null, "category": "remove", "imports": ["import re"]}, {"term": "def", "name": "rc_extract", "data": "def rc_extract(content, k=1):\n\tif not content:\n\t\treturn None,None #,None,None\n\tlines = content.split('\\n')\n\tgroup_value = []\n\tfor i in range(0,len(lines),k):\n\t\tgroup = '\\n'.join(lines[i:i+k])\n\t\tgroup = remove_image(group)\n\t\tgroup = remove_video(group)\n\t\ttext_a,text_b= remove_any_tag_but_a(group)\n\t\ttemp =(text_b - text_a) - 8\n\t\tgroup_value.append(temp)\n\tleft,right = sum_max(group_value)\n\treturn left,right #, len('\\n'.join(tmp[:left])), len('\\n'.join(tmp[:right]))\n", "description": null, "category": "remove", "imports": ["import re"]}], [], [], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('mysite', '0009_player_runs'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player1',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player10',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player11',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player2',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player3',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player4',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player5',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player6',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player7',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player8',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='team',\n\t\t\tname='player9',\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='player',\n\t\t\tname='team',\n\t\t\tfield=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, to='mysite.Team'),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player1',\n\t\t\tfield=models.CharField(max_length=120),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player10',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player11',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player2',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player3',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player4',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player5',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player6',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player7',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player8',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='batting_log',\n\t\t\tname='player9',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='bowling_log',\n\t\t\tname='player1',\n\t\t\tfield=models.CharField(max_length=120),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='bowling_log',\n\t\t\tname='player2',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='bowling_log',\n\t\t\tname='player3',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='bowling_log',\n\t\t\tname='player4',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='bowling_log',\n\t\t\tname='player5',\n\t\t\tfield=models.CharField(blank=True, max_length=120, null=True),\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from django.db import migrations, models", "import django.db.models.deletion"]}], [{"term": "class", "name": "WordListDictModel", "data": "class WordListDictModel(SqlQueryModel):\n\t@need_refresh\n\tdef __init__(self, dictListModel, srcLang, dstLang, *args, **kwargs):\n\t\tsuper(WordListDictModel, self).__init__(*args, **kwargs)\n\t\tself.wordUtils = WordUtils(srcLang=srcLang, dstLang=dstLang)\n\t\tself.initLang(srcLang, dstLang)\n\n\t\tself.dictListModel = dictListModel\n\t\tif srcLang == Lang.Eng:\n\t\t\tself.headerFields = ['',\t\t  '',\t  'eng',\t  u'\u0440\u0443\u0441',\t '',\t '',\t   '']\n\t\t\tself.fields =\t   ['d_id', 'we_id', 'we_value',  'wr_value', 'play', 'edit', 'remove']\n\t\telse:\n\t\t\tself.headerFields = ['',\t\t  '',\t  u'\u0440\u0443\u0441',\t  'eng',\t '',\t '',\t   '']\n\t\t\tself.fields =\t   ['d_id', 'wr_id', 'wr_value',  'we_value', 'play', 'edit', 'remove']\n\n\t\tself.playFieldNum = self.fields.index('play')\n\t\tself.editFieldNum = self.fields.index('edit')\n\t\tself.removeFieldNum = self.fields.index('remove')\n\n\tdef wordValue(self, recordIndex):\n\t\treturn self.record(recordIndex).value(SqlQuery(self, 'w[e]_value').str())\n\n\tdef translateValue(self, recordIndex):\n\t\treturn self.record(recordIndex).value(SqlQuery(self, 'w[r]_value').str())\n\n\tdef dictId(self, recordIndex):\n\t\treturn self.record(recordIndex).value('d_id')\n\n\tdef wordId(self, recordIndex):\n\t\treturn self.record(recordIndex).value(SqlQuery(self, 'w[e]_id').str())\n\n\tdef refresh(self):\n\t\tquery = SqlQuery(\n\t\t\tself,\n\t\t\tu'''\n\t\t\tSELECT d_id, w[e]_id, w[e]_value, w[r]_value  FROM (\n\t\t\t\tSELECT\n\t\t\t\t\tDISTINCT word_[eng].id as w[e]_id, dict_[eng].id as d_id, word_[eng].value as w[e]_value, word_[rus].value as w[r]_value\n\t\t\t\tfrom\n\t\t\t\t\tdict_[eng]\n\t\t\t\tJOIN word_[eng]_dict ON word_[eng]_dict.dict_[eng]_id = dict_[eng].id\n\t\t\t\tJOIN word_[eng] ON word_[eng].id = word_[eng]_dict.word_[eng]_id\n\t\t\t\tLEFT JOIN rus_eng ON rus_eng.word_[eng]_id = word_[eng].id\n\t\t\t\tLEFT JOIN word_[rus] ON word_[rus].id = rus_eng.word_[rus]_id\n\t\t\t\tWHERE dict_[eng].id = {dict_id}\n\t\t\t\tORDER BY rus_eng.[rus]_order\n\t\t\t) as x\n\t\t\tGROUP BY d_id, w[e]_id\n\t\t\t'''.format(dict_id=self.dictListModel.dictId),\n\t\t).str()\n\t\tself.setQuery(query)\n\n\t\tfor idx, field in enumerate(self.headerFields):\n\t\t\tself.setHeaderData(idx, QtCore.Qt.Horizontal, field)\n\t\tself.onRefresh()\n\n\t@need_refresh\n\tdef addWordLink(self, wordId):\n\t\treturn SqlQuery(\n\t\t\tself,\n\t\t\t'''\n\t\t\tINSERT INTO\n\t\t\t  word_[eng]_dict\n\t\t\t(dict_[eng]_id, word_[eng]_id)\n\t\t\tVALUES\n\t\t\t  (:dict_id, :w[e]_id)\n\t\t\t''',\n\t\t\t{\n\t\t\t\t':dict_id': self.dictListModel.dictId,\n\t\t\t\t':w[e]_id': wordId,\n\t\t\t}\n\t\t).execute()\n\n\t#TODO: \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0441\u0441\u044b\u043b\u043a\u0438 + \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0441 \u043f\u0443\u0441\u0442\u044b\u043c \u0441\u043b\u043e\u0432\u043e\u043c. \u0410\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u0438 \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0430\n\t@need_refresh\n\tdef removeLinkWord(self, wordId, silent=False, removeWord=False):\n\t\tdef removeLink():\n\t\t\treturn SqlQuery(\n\t\t\tself,\n\t\t\t'''\n\t\t\tDELETE FROM\n\t\t\t\tword_[eng]_dict\n\t\t\tWHERE\n\t\t\t\tdict_[eng]_id = :dict_id AND word_[eng]_id = :word_id\n\t\t\t''',\n\t\t\t{\n\t\t\t\t':dict_id': self.dictListModel.dictId,\n\t\t\t\t':word_id': wordId\n\t\t\t}\n\t\t).execute()\n\n\t\tif silent == False:\n\t\t\tmsgBox = QtGui.QMessageBox()\n\t\t\tmsgBox.setIcon(QtGui.QMessageBox.Question)\n\t\t\tmsgBox.setWindowIcon(QtGui.QIcon(\":/res/images/dictionary.png\"))\n\t\t\tmsgBox.setText(u\"\u0412\u044b \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0445\u043e\u0442\u0438\u0442\u0435 \u0443\u0434\u0430\u043b\u0438\u0442\u044c \u0441\u043b\u043e\u0432\u043e: id = {id}\".format(id=wordId))\n\t\t\tmsgBox.setWindowTitle(u\"\u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432\u0430\")\n\t\t\tmsgBox.setStandardButtons(QtGui.QMessageBox.Ok | QtGui.QMessageBox.Cancel)\n\t\t\tif msgBox.exec_() != QtGui.QMessageBox.Ok:\n\t\t\t\treturn False\n\n\t\tif removeLink():\n\t\t\tif removeWord:\n\t\t\t\t# TODO: \u0415\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u043e \u0432\u0445\u043e\u0434\u0438\u0442 \u0445\u043e\u0442\u044f \u0431\u044b \u0432 \u043e\u0434\u0438\u043d \u0441\u043b\u043e\u0432\u0430\u0440\u044c, \u0442\u043e \u0443\u0434\u0430\u043b\u044f\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e link\n\t\t\t\t# TODO: \u0435\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u043e \u043d\u0435 \u0432\u0445\u043e\u0434\u0438\u0442 \u043d\u0438 \u0432 \u043e\u0434\u0438\u043d \u0441\u043b\u043e\u0432\u0430\u0440\u044c: \u0441\u043f\u0440\u043e\u0441\u0438\u0442\u044c - \u0443\u0434\u0430\u043b\u0438\u0442\u044c \u043b\u0438 \u0441\u043b\u043e\u0432\u043e \u0441\u043e\u0432\u0441\u0435\u043c?\n\t\t\t\t# TODO: \u0447\u0442\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u0441 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0430\u043c\u0438 \u043d\u0430 \u044d\u0442\u043e \u0441\u043b\u043e\u0432\u043e?\n\t\t\t\tself.wordUtils.remove(wordId)\n\n\tdef data(self, index, role):\n\t\tvalue = super(WordListDictModel, self).data(index, role)\n\t\tif role == QtCore.Qt.TextColorRole and index.column() == 2:\n\t\t\treturn QtGui.QColor(QtCore.Qt.blue)\n\n\t\tif role == QtCore.Qt.DisplayRole:\n\t\t\tif index.column() in [self.playFieldNum, self.editFieldNum, self.removeFieldNum]:\n\t\t\t\treturn ''\n\n\t\treturn value\n\n\tdef columnCount(self, *args, **kwargs):\n\t\treturn len(self.fields)\n\n", "description": null, "category": "remove", "imports": ["from PySide import QtCore, QtGui", "from PySide.QtCore import Slot as pyqtSlot", "from forms.forms_utils import WordEditMode", "from models.base.base_sql_query_model import SqlQueryModel, SqlQuery, need_refresh", "from models.delegates import EditButtonDelegate, PlayButtonDelegate, RemoveButtonDelegate", "from models import models_utils", "from models.word_model import WordModel, WordUtils, WordInfo", "from utils import Lang", "\t\tfrom forms.word_edit_window import WordEditWindow, WordEditContext"]}, {"term": "class", "name": "PlayButtonWordListDictDelegate", "data": "class PlayButtonWordListDictDelegate(PlayButtonDelegate):\n\tdef __init__(self, parentWindow, parent, wordListDictModel):\n\t\tPlayButtonDelegate.__init__(self, parentWindow, parent)\n\t\tself.wordListDictModel = wordListDictModel\n\n\n\t@pyqtSlot()\n\tdef onBtnClicked(self, recordIndex):\n\t\tprint u\"play '{}'\".format(self.wordListDictModel.wordValue(recordIndex))\n\t\tself.commitData.emit(self.sender())\n\n", "description": null, "category": "remove", "imports": ["from PySide import QtCore, QtGui", "from PySide.QtCore import Slot as pyqtSlot", "from forms.forms_utils import WordEditMode", "from models.base.base_sql_query_model import SqlQueryModel, SqlQuery, need_refresh", "from models.delegates import EditButtonDelegate, PlayButtonDelegate, RemoveButtonDelegate", "from models import models_utils", "from models.word_model import WordModel, WordUtils, WordInfo", "from utils import Lang", "\t\tfrom forms.word_edit_window import WordEditWindow, WordEditContext"]}, {"term": "class", "name": "EditButtonWordListDictDelegate", "data": "class EditButtonWordListDictDelegate(EditButtonDelegate):\n\tdef __init__(self, parentWindow, parent, wordListDictModel):\n\t\tEditButtonDelegate.__init__(self, parentWindow, parent)\n\t\tself.wordListDictModel = wordListDictModel\n\n\t@pyqtSlot()\n\tdef onBtnClicked(self, recordIndex):\n\t\tfrom forms.word_edit_window import WordEditWindow, WordEditContext\n\t\tprint u\"edit '{}'\".format(self.wordListDictModel.wordValue(recordIndex))\n\t\tself.commitData.emit(self.sender())\n\n\t\twordInfo = WordInfo(\n\t\t\tself.wordListDictModel.wordId(recordIndex),\n\t\t\tsrcLang=self.wordListDictModel.srcLang,\n\t\t\tdstLang=self.wordListDictModel.dstLang\n\t\t)\n\n\t\twordEditDialog = WordEditWindow(\n\t\t\twordInfo=wordInfo,\n\t\t\tmode=WordEditMode.EditWord,\n\t\t\twordEditContext=WordEditContext()\n\t\t)\n\t\tmodels_utils.setStartGeometry(self.parentWindow, wordEditDialog)\n\n\t\twordEditDialog.exec_()\n\t\tself.wordListDictModel.refresh()\n\n", "description": null, "category": "remove", "imports": ["from PySide import QtCore, QtGui", "from PySide.QtCore import Slot as pyqtSlot", "from forms.forms_utils import WordEditMode", "from models.base.base_sql_query_model import SqlQueryModel, SqlQuery, need_refresh", "from models.delegates import EditButtonDelegate, PlayButtonDelegate, RemoveButtonDelegate", "from models import models_utils", "from models.word_model import WordModel, WordUtils, WordInfo", "from utils import Lang", "\t\tfrom forms.word_edit_window import WordEditWindow, WordEditContext"]}, {"term": "class", "name": "RemoveButtonWordListDictDelegate", "data": "class RemoveButtonWordListDictDelegate(RemoveButtonDelegate):\n\tdef __init__(self, parentWindow, parent, wordListDictModel):\n\t\tRemoveButtonDelegate.__init__(self,parentWindow, parent)\n\t\tself.wordListDictModel = wordListDictModel\n\n\tdef onBtnClicked(self, recordIndex):\n\t\tprint u\"remove '{}'\".format(self.wordListDictModel.wordValue(recordIndex))\n\t\tself.wordListDictModel.removeLinkWord(\n\t\t\twordId=self.wordListDictModel.wordId(recordIndex),\n\t\t\tremoveWord=True\n\t\t)\n\t\tself.wordListDictModel.refresh()\n\t\tself.commitData.emit(self.sender())\n", "description": null, "category": "remove", "imports": ["from PySide import QtCore, QtGui", "from PySide.QtCore import Slot as pyqtSlot", "from forms.forms_utils import WordEditMode", "from models.base.base_sql_query_model import SqlQueryModel, SqlQuery, need_refresh", "from models.delegates import EditButtonDelegate, PlayButtonDelegate, RemoveButtonDelegate", "from models import models_utils", "from models.word_model import WordModel, WordUtils, WordInfo", "from utils import Lang", "\t\tfrom forms.word_edit_window import WordEditWindow, WordEditContext"]}], [{"term": "class", "name": "MissingInfoDialog", "data": "class MissingInfoDialog(dialogbase.DialogBaseEdit):\n\t\n\tartworkAndList = []\n\tartworkOrList = []\n\tinfoAndList = []\n\tinfoOrList = []\n\t\n\tsaveConfig = False\n\t\n\tdef __init__(self, *args, **kwargs):\n\t\tLogutil.log('init dialog missing info', util.LOG_LEVEL_INFO)\n\t\t\n\t\tself.gui = kwargs[ \"gui\" ]\n\t\t\n\t\tself.doModal()\n\t\n\t\n\tdef onInit(self):\n\t\tLogutil.log('onInit dialog missing info', util.LOG_LEVEL_INFO)\n\t\t\n\t\tself.artworkAndList = self.gui.config.missingFilterArtwork.andGroup\n\t\tlabel = self.getControlById(CONTROL_LABEL_ARTWORK_ANDGROUP)\n\t\tlabel.setLabel(', '.join(self.artworkAndList))\n\t\t\n\t\tself.artworkOrList = self.gui.config.missingFilterArtwork.orGroup\n\t\tlabel = self.getControlById(CONTROL_LABEL_ARTWORK_ORGROUP)\n\t\tlabel.setLabel(', '.join(self.artworkOrList))\n\t\t\n\t\tself.infoAndList = self.gui.config.missingFilterInfo.andGroup\n\t\tlabel = self.getControlById(CONTROL_LABEL_INFO_ANDGROUP)\n\t\tlabel.setLabel(', '.join(self.infoAndList))\n\t\t\n\t\tself.infoOrList = self.gui.config.missingFilterInfo.orGroup\n\t\tlabel = self.getControlById(CONTROL_LABEL_INFO_ORGROUP)\n\t\tlabel.setLabel(', '.join(self.infoOrList))\n\t\t\n\t\tLogutil.log('add show/hide missing info options', util.LOG_LEVEL_INFO)\n\t\t#showHideOptions = ['Ignore filter', 'Show only games with missing items', 'Hide games with missing items']\n\t\tself.addItemsToList(CONTROL_LIST_SHOWHIDEMISSING, config.missingFilterOptions.values())\n\t\t\n\t\tfor i in range(0, len(config.missingFilterOptions.keys())):\n\t\t\tkey = config.missingFilterOptions.keys()[i]\n\t\t\tif(key == self.gui.config.showHideOption):\n\t\t\t\tlistShowHide = self.getControlById(CONTROL_LIST_SHOWHIDEMISSING)\n\t\t\t\tlistShowHide.selectItem(i)\n\t\t\n\t\t\n\tdef onAction(self, action):\t\t\n\t\tif (action.getId() in ACTION_CANCEL_DIALOG):\n\t\t\tself.close()\n\t\t\n\t\n\tdef onClick(self, controlID):\n\t\t\n\t\tLogutil.log('onClick', util.LOG_LEVEL_INFO)\n\t\t\n\t\tif (controlID == CONTROL_BUTTON_EXIT): # Close window button\n\t\t\tLogutil.log('close', util.LOG_LEVEL_INFO)\n\t\t\tself.close()\n\t\telif (controlID == CONTROL_BUTTON_ADD_ARTWORK_ORGROUP):\n\t\t\tLogutil.log('Add artwork or', util.LOG_LEVEL_INFO)\n\t\t\tself.artworkOrList = self.addItemToMissingArtworkList(self.artworkOrList, CONTROL_LABEL_ARTWORK_ORGROUP)\t\t\t\n\t\t\t\n\t\telif (controlID == CONTROL_BUTTON_REMOVE_ARTWORK_ORGROUP):\n\t\t\tLogutil.log('Remove artwork or', util.LOG_LEVEL_INFO)\n\t\t\tself.artworkOrList = self.removeFromMissingList(self.artworkOrList, CONTROL_LABEL_ARTWORK_ORGROUP)\n\t\t\t\n\t\telif (controlID == CONTROL_BUTTON_ADD_ARTWORK_ANDGROUP):\n\t\t\tLogutil.log('Add artwork and', util.LOG_LEVEL_INFO)\n\t\t\tself.artworkAndList = self.addItemToMissingArtworkList(self.artworkAndList, CONTROL_LABEL_ARTWORK_ANDGROUP)\n\n\t\telif (controlID == CONTROL_BUTTON_REMOVE_ARTWORK_ANDGROUP):\n\t\t\tLogutil.log('Remove artwork and', util.LOG_LEVEL_INFO)\n\t\t\tself.artworkAndList = self.removeFromMissingList(self.artworkAndList, CONTROL_LABEL_ARTWORK_ANDGROUP)\n\t\t\t\n\t\telif (controlID == CONTROL_BUTTON_ADD_INFO_ORGROUP):\n\t\t\tLogutil.log('Add info or', util.LOG_LEVEL_INFO)\n\t\t\tself.infoOrList = self.addItemToMissingInfoList(self.infoOrList, CONTROL_LABEL_INFO_ORGROUP)\n\t\t\t\n\t\telif (controlID == CONTROL_BUTTON_REMOVE_INFO_ORGROUP):\n\t\t\tLogutil.log('Remove info and', util.LOG_LEVEL_INFO)\n\t\t\tself.infoOrList = self.removeFromMissingList(self.infoOrList, CONTROL_LABEL_INFO_ORGROUP)\n\t\t\n\t\telif (controlID == CONTROL_BUTTON_ADD_INFO_ANDGROUP):\n\t\t\tLogutil.log('Add info and', util.LOG_LEVEL_INFO)\n\t\t\tself.infoAndList = self.addItemToMissingInfoList(self.infoAndList, CONTROL_LABEL_INFO_ANDGROUP)\n\t\t\t\n\t\telif (controlID == CONTROL_BUTTON_REMOVE_INFO_ANDGROUP):\n\t\t\tLogutil.log('Remove info and', util.LOG_LEVEL_INFO)\n\t\t\tself.infoAndList = self.removeFromMissingList(self.infoAndList, CONTROL_LABEL_INFO_ANDGROUP)\n\t\t\t\n\t\t\t\n\t\t#Save\n\t\telif (controlID == CONTROL_BUTTON_SAVE):\t\t\t\n\t\t\tLogutil.log('save', util.LOG_LEVEL_INFO)\n\t\t\t\t\t\t\n\t\t\tshowHideList = self.getControlById(CONTROL_LIST_SHOWHIDEMISSING)\n\t\t\tindex = showHideList.getSelectedPosition()\n\t\t\tshowHideOptions = config.missingFilterOptions.keys()\n\t\t\tshowHideOption = showHideOptions[index]\t\t\t \n\t\t\t\n\t\t\tconfigWriter = ConfigXmlWriter(False)\n\t\t\tsuccess, message = configWriter.writeMissingFilter(showHideOption, self.artworkOrList, self.artworkAndList, self.infoOrList, self.infoAndList)\n\t\t\t\n\t\t\tif(success):\n\t\t\t\tself.saveConfig = True\n\t\t\tself.close()\n\t\t\n\t\t#Cancel\n\t\telif (controlID == CONTROL_BUTTON_CANCEL):\n\t\t\tLogutil.log('cancel', util.LOG_LEVEL_INFO)\n\t\t\tself.close()\n\t\t\t\t\t\t\n\t\n\tdef onFocus(self, controlId):\n\t\tpass\n\t\n\t\n\tdef addItemToMissingArtworkList(self, inList, labelId):\n\t\ttempList = []\n\t\tfor romCollection in self.gui.config.romCollections.values():\n\t\t\tfor mediaPath in romCollection.mediaPaths:\n\t\t\t\tif(not mediaPath.fileType.name in tempList and not mediaPath.fileType.name in inList):\n\t\t\t\t\ttempList.append(mediaPath.fileType.name)\n\t\t\n\t\tdialog = xbmcgui.Dialog()\n\t\tindex = dialog.select(util.localize(32155), tempList)\n\t\tdel dialog\n\t\tif(index == -1):\n\t\t\treturn inList\n\t\t\n\t\tinList.append(tempList[index])\n\t\tlabel = self.getControlById(labelId)\n\t\tlabel.setLabel(', '.join(inList))\n\t\t\n\t\treturn inList\n\t\n\t\t\n\tdef addItemToMissingInfoList(self, inList, labelId):\n\t\t\n\t\ttempList = []\n\t\tkeys = config.gameproperties.keys()\n\t\tkeys.sort()\n\t\tfor item in keys:\n\t\t\tif(not item in tempList and not item in inList):\n\t\t\t\ttempList.append(item)\n\t\t\t\n\t\tdialog = xbmcgui.Dialog()\n\t\tindex = dialog.select(util.localize(32156), tempList)\n\t\tdel dialog\n\t\tif(index == -1):\n\t\t\treturn inList\n\t\t\n\t\tinList.append(tempList[index])\n\t\tlabel = self.getControlById(labelId)\n\t\tlabel.setLabel(', '.join(inList))\n\t\t\n\t\treturn inList\n\t\n\t\n\tdef removeFromMissingList(self, inList, labelId):\n\t\tdialog = xbmcgui.Dialog()\n\t\tindex = dialog.select(util.localize(32856), inList)\n\t\tdel dialog\n\t\tif(index == -1):\n\t\t\treturn inList\n\t\tinList.remove(inList[index])\n\t\tlabel = self.getControlById(labelId)\n\t\tlabel.setLabel(', '.join(inList))\n\t\t\n\t\treturn inList\n", "description": null, "category": "remove", "imports": ["import xbmc, xbmcgui", "import os", "import util, config, dialogbase", "from util import *", "from configxmlwriter import *"]}], [], [], [{"term": "def", "name": "flatten_manually", "data": "def flatten_manually( module ):\n\t \"\"\"\n\t Run flattening twice to account for the recursive ones. \n\t We may need to do it more times (if there are still some modules that moved, do it again)\n\t \"\"\"\n\t def flatten_one_module( subModulePath, nodeModulePath ):\n\t\t somethingMoved = False;\n\t\t if (os.path.exists(os.path.join(subModulePath, \"node_modules\"))):\n\t\t\t for subSubModule in next(os.walk(os.path.join(subModulePath, \"node_modules\")))[1]:\n\t\t\t\t subSubModulePath = os.path.join(subModulePath, \"node_modules\", subSubModule );\n\t\t\t\t print( \"\\t MOVING: \" + subSubModulePath + \" to \" +  nodeModulePath);\n\t\t\t\t if (not os.path.exists(os.path.join( nodeModulePath, subSubModule ))):\n\t\t\t\t\t shutil.move( subSubModulePath, nodeModulePath );\n\t\t\t\t\t somethingMoved = True;\n\t\t\t try_remove_dirs(os.path.join(subModulePath, \"node_modules\"))\n\t\t return somethingMoved;\n\t\t \n\t nodeModulePath = os.path.join(os.getcwd(), module, \"node_modules\");\n\t print( \"FLATTENING Manually: \" + nodeModulePath);\n\t moved = True;\n\t while moved:\n\t\t for subModule in next(os.walk(nodeModulePath))[1]:\n\t\t\t subModulePath = os.path.join(nodeModulePath, subModule)\n\t\t\t moved = flatten_one_module( subModulePath, nodeModulePath );\n", "description": "\n\t Run flattening twice to account for the recursive ones. \n\t We may need to do it more times (if there are still some modules that moved, do it again)\n\t ", "category": "remove", "imports": ["from __future__ import print_function", "import os, shutil, subprocess, sys"]}, {"term": "def", "name": "delete_extra_node_module_dirs", "data": "def delete_extra_node_module_dirs():\n\t # Remove all the extra directories in node_modules that are not needed (test, example, src,...)\n\t serialPortDepsDir = os.path.join(\"serialport\", \"build\", \"Release\", \".deps\");\n\t for dirname, dirnames, filenames in os.walk('.'):\n\t\t # Also manually, delete serialport/build/Release/.deps - it is a leftover from the build\n\t\t # and it causes problems with the installers on windows.\n\t\t for subdirname in dirnames:\n\t\t\t fullPath = os.path.join(dirname, subdirname);\n\t\t\t # Skip the directories not in node_modules. \n\t\t\t if (fullPath.find(\"node_modules\") == -1):\n\t\t\t\t continue;\n\t\t\t if (fullPath.find(\"test\") != -1):\n\t\t\t\t print(\"\\t DELETING (test): \" + fullPath);\n\t\t\t\t try_remove_dirs(fullPath)\n\t\t\t if (fullPath.find(\"example\") != -1):\n\t\t\t\t print(\"\\t DELETING (example): \" + fullPath);\n\t\t\t\t try_remove_dirs(fullPath)\n\t\t\t if (fullPath.find(\"src\") != -1):\n\t\t\t\t print(\"\\t DELETING (src): \" + fullPath);\n\t\t\t\t try_remove_dirs(fullPath)\n\t\t\t if (fullPath.find(\"tools\") != -1):\n\t\t\t\t print(\"\\t DELETING (tools): \" + fullPath);\n\t\t\t\t try_remove_dirs(fullPath)\n\t\t\t if (fullPath.find(os.path.join(\"Release\", \"obj\")) != -1):\n\t\t\t\t path, subdir = os.path.split(fullPath);\n\t\t\t\t if (subdir == \"obj\"):\n\t\t\t\t\t print(\"\\t DELETING (obj): \" + os.path.join(\"Release\", \"obj\") + fullPath );\n\t\t\t\t\t try_remove_dirs(fullPath)\n\t\t\t #if (fullPath.find(serialPortDepsDir) != -1):\n\t\t\t\t #print(\"\\t DELETING (serialPortDeps): \" + fullPath);\n\t\t\t\t #try_remove_dirs(fullPath);\n", "description": null, "category": "remove", "imports": ["from __future__ import print_function", "import os, shutil, subprocess, sys"]}, {"term": "def", "name": "npmbuild", "data": "def npmbuild():\n\t clean()\n\t clean_node()\n\n\t nodeCmd   = \"npm\"\n\t if (sys.platform == \"win32\"):\n\t\t nodeCmd = find_program_location(\"npm.cmd\") \n\n\t print(\"Install Dependencies ...\")\n\n\t rc = subprocess.call([nodeCmd, \"install\", \"--production\"])\n\t if rc:\n\t\t print(\"node install failed, rc %d\" % rc)\n\t\t sys.exit(rc)\n\t \n\t if (sys.platform == \"win32\"):\n\t\t # Flatten node_module directory (since release engineering needs paths < 248 char)\n\t\t flattenCmd = find_program_location(\"flatten-packages.cmd\") \n\t\t if (flattenCmd):\n\t\t\t rc = subprocess.call([flattenCmd])\n\t\t\t \n\t\t\t # There seem to be a problem with usb modules when they get flatten so, delete\n\t\t\t # them and reinstall usb without flattening.\n\t\t\t shutil.rmtree('node_modules/usb')\n\t\t\t shutil.rmtree('node_modules/usb-shyp-win32-ia32')\n\t\t\t shutil.rmtree('node_modules/usb-shyp-win32-x64')\n\t\t\t \n\t\t\t rc = subprocess.call([nodeCmd, \"install\",  \"--production\", \"usb@1.0.5\" ])\n\t\t\t \n\t\t\t # Flatten the nedb/browser-version module manually. \n\t\t\t # flatten_manually( os.path.join(\"node_modules\", \"nedb\", \"browser-version\") );\n\t\t\t \n\t\t\t # Removed the browser-version module from nedb since we are not using it and\n\t\t\t # it is causing probems with windows installers (too long paths)\n\t\t\t #print (\"REMOVING: \", os.path.join( \".\", \"node_modules\", \"nedb\", \"browser-version\"));\n\t\t\t #try_remove_dirs( os.path.join( \".\", \"node_modules\", \"nedb\", \"browser-version\"), True );\n\n\t\t else:\n\t\t\t print(\"FAILED to flatten node_modules directory!\")\n\t \n\t # Delete the unnecessary dirs. \n\t delete_extra_node_module_dirs();\n\t\t\t\t \n\t print (\"Build Succesful....\")\n\n", "description": null, "category": "remove", "imports": ["from __future__ import print_function", "import os, shutil, subprocess, sys"]}, {"term": "def", "name": "clean", "data": "def clean():\n\t print(\"Removing files...\")\n\t try_remove(\"npm-debug.log\")\n\t try_remove(\"print_manager.jx\")\n\t try_remove(\"print_manager.jxp\")\n\n\t if (sys.platform == \"win32\"):\n\t\t try_remove(\"node.jxp\")\n\t\t try_remove(\"node.exe\")\n\t else:\n\t\t try_remove(\"print_manager\")\n\t \n", "description": null, "category": "remove", "imports": ["from __future__ import print_function", "import os, shutil, subprocess, sys"]}, {"term": "def", "name": "clean_node", "data": "def clean_node():\n\t print(\"Removing node files...\")\n\t try_remove_dirs(\"./node_modules\", True)\n\t try_remove_dirs(\"./files\")\n\t try_remove_dirs(\"./db\")\n\n", "description": null, "category": "remove", "imports": ["from __future__ import print_function", "import os, shutil, subprocess, sys"]}, {"term": "def", "name": "try_remove", "data": "def try_remove(path):\n\t try:\n\t\t  os.remove(path)\n\t except OSError:\n\t\t  print(\"FAILED TO REMOVE FILE: \", path)\n", "description": null, "category": "remove", "imports": ["from __future__ import print_function", "import os, shutil, subprocess, sys"]}, {"term": "def", "name": "try_remove_dirs", "data": "def try_remove_dirs(path, forceExit=False):\n\t if os.path.exists(path):\n\t\t try:\n\t\t\t shutil.rmtree(path)\n\t\t\t while os.path.exists(path): # check if it exists\n\t\t\t\t pass\n\t\t except OSError as err:\n\t\t\t print(\"FAILED TO REMOVE DIR: \", path, err.args)\n\t\t\t if forceExit:\n\t\t\t\t print(\"\\n\\t Build Failed ! because it cannot remove:\", path, err)\n\t\t\t\t sys.exit(0);\n\t else:\n\t\t print(\"DIRECTORY DOES NOT EXISTS:\", path);\n", "description": null, "category": "remove", "imports": ["from __future__ import print_function", "import os, shutil, subprocess, sys"]}, {"term": "def", "name": "find_program_location", "data": "def find_program_location(program):\n\tfor path in os.environ.get('PATH', '').split(';'):\n\t\tif os.path.exists(os.path.join(path, program)) and \\\n\t\t   not os.path.isdir(os.path.join(path, program)):\n\t\t\treturn os.path.join(path, program)\n\treturn None\n\n", "description": null, "category": "remove", "imports": ["from __future__ import print_function", "import os, shutil, subprocess, sys"]}, {"term": "def", "name": "main", "data": "def main(rules=None):\n\t if not rules:\n\t\t  rules = default_rules\n\n\t for rule in rules:\n\t\t  if rule == 'build':\n\t\t\t   npmbuild()\n\t\t  elif rule == 'clean':\n\t\t\t   clean_node()\n\t\t  elif rule == 'npmClean':\n\t\t\t   delete_extra_node_module_dirs();\n\t\t  else:\n\t\t\t   print(\"Unrecognized rule '%s'\" % rule)\n\t\t\t   sys.exit(1)\n\n\t print(\"Done.\")\n", "description": null, "category": "remove", "imports": ["from __future__ import print_function", "import os, shutil, subprocess, sys"]}], [], [], [], [{"term": "def", "name": "get_sentence_words", "data": "def get_sentence_words(sentence: str,\n\t\t\t\t\t   unique: Optional[bool] = False,\n\t\t\t\t\t   keep_case: Optional[bool] = False,\n\t\t\t\t\t   remove_punctuation: Optional[bool] = True,\n\t\t\t\t\t   remove_specials: Optional[bool] = True) -> List[str]:\n\t\"\"\"Splits input sentence into words.\n\t\n\t:param sentence: Sentence to split\n\t:type sentence: str\n\t:param unique: Returns a list of unique words.\n\t:type unique: bool\n\t:param keep_case: Keep capitals and small (True) or turn\\\n\t\t\t\t\t  everything to small case (False)\n\t:type keep_case: bool\n\t:param remove_punctuation: Remove punctuation from sentence?\n\t:type remove_punctuation: bool\n\t:param remove_specials: Remove special tokens?\n\t:type remove_specials: bool\n\t:return: Sentence words\n\t:rtype: list[str]\n\t\"\"\"\n\twords = clean_sentence(\n\t\tsentence, keep_case=keep_case,\n\t\tremove_punctuation=remove_punctuation,\n\t\tremove_specials=remove_specials).strip().split()\n\n\tif unique:\n\t\twords = list(set(words))\n\n\treturn words\n\n", "description": "Splits input sentence into words.\n\t\n\t:param sentence: Sentence to split\n\t:type sentence: str\n\t:param unique: Returns a list of unique words.\n\t:type unique: bool\n\t:param keep_case: Keep capitals and small (True) or turn\\\n\t\t\t\t\t  everything to small case (False)\n\t:type keep_case: bool\n\t:param remove_punctuation: Remove punctuation from sentence?\n\t:type remove_punctuation: bool\n\t:param remove_specials: Remove special tokens?\n\t:type remove_specials: bool\n\t:return: Sentence words\n\t:rtype: list[str]\n\t", "category": "remove", "imports": ["from typing import Optional, List, MutableSequence", "from re import sub as re_sub", "from collections import Counter", "from itertools import chain", "from functools import partial"]}, {"term": "def", "name": "clean_sentence", "data": "def clean_sentence(sentence: str,\n\t\t\t\t   keep_case: Optional[bool] = False,\n\t\t\t\t   remove_punctuation: Optional[bool] = True,\n\t\t\t\t   remove_specials: Optional[bool] = True) -> str:\n\t\"\"\"Cleans a sentence.\n\n\t:param sentence: Sentence to be clean.\n\t:type sentence: str\n\t:param keep_case: Keep capitals and small (True) or turn\\\n\t\t\t\t\t  everything to small case (False)\n\t:type keep_case: bool\n\t:param remove_punctuation: Remove punctuation from sentence?\n\t:type remove_punctuation: bool\n\t:param remove_specials: Remove special tokens?\n\t:type remove_specials: bool\n\t:return: Cleaned sentence.\n\t:rtype: str\n\t\"\"\"\n\tthe_sentence = sentence if keep_case else sentence.lower()\n\n\t# Remove any forgotten space before punctuation and double space.\n\tthe_sentence = re_sub(r'\\s([,.!?;:\"](?:\\s|$))', r'\\1', the_sentence).replace('  ', ' ')\n\n\tif remove_specials:\n\t\tthe_sentence = the_sentence.replace(' ', '').replace(' ', '')\n\t\tthe_sentence = the_sentence.replace(' ', '').replace(' ', '')\n\n\tif remove_punctuation:\n\t\tthe_sentence = re_sub('[,.!?;:\\\"]', '', the_sentence)\n\n\treturn the_sentence\n\n", "description": "Cleans a sentence.\n\n\t:param sentence: Sentence to be clean.\n\t:type sentence: str\n\t:param keep_case: Keep capitals and small (True) or turn\\\n\t\t\t\t\t  everything to small case (False)\n\t:type keep_case: bool\n\t:param remove_punctuation: Remove punctuation from sentence?\n\t:type remove_punctuation: bool\n\t:param remove_specials: Remove special tokens?\n\t:type remove_specials: bool\n\t:return: Cleaned sentence.\n\t:rtype: str\n\t", "category": "remove", "imports": ["from typing import Optional, List, MutableSequence", "from re import sub as re_sub", "from collections import Counter", "from itertools import chain", "from functools import partial"]}, {"term": "def", "name": "get_words_counter", "data": "def get_words_counter(captions: MutableSequence[str],\n\t\t\t\t\t  use_unique: Optional[bool] = False,\n\t\t\t\t\t  keep_case: Optional[bool] = False,\n\t\t\t\t\t  remove_punctuation: Optional[bool] = True,\n\t\t\t\t\t  remove_specials: Optional[bool] = True) -> Counter:\n\t\"\"\"Creates a Counter object from the\\\n\twords in the captions.\n\n\t:param captions: The captions.\n\t:type captions: list[str]|iterable\n\t:param use_unique: Use unique only words from the captions?\n\t:type use_unique: bool\n\t:param keep_case: Keep capitals and small (True) or turn\\\n\t\t\t\t\t  everything to small case (False)\n\t:type keep_case: bool\n\t:param remove_punctuation: Remove punctuation from captions?\n\t:type remove_punctuation: bool\n\t:param remove_specials: Remove special tokens?\n\t:type remove_specials: bool\n\t:return: Counter object from\\\n\t\t\t the words in the captions.\n\t:rtype: collections.Counter\n\t\"\"\"\n\tpartial_func = partial(\n\t\tget_sentence_words,\n\t\tunique=use_unique, keep_case=keep_case,\n\t\tremove_punctuation=remove_punctuation,\n\t\tremove_specials=remove_specials\n\t)\n\treturn Counter(chain.from_iterable(map(partial_func, captions)))\n", "description": "Creates a Counter object from the\\\n\twords in the captions.\n\n\t:param captions: The captions.\n\t:type captions: list[str]|iterable\n\t:param use_unique: Use unique only words from the captions?\n\t:type use_unique: bool\n\t:param keep_case: Keep capitals and small (True) or turn\\\n\t\t\t\t\t  everything to small case (False)\n\t:type keep_case: bool\n\t:param remove_punctuation: Remove punctuation from captions?\n\t:type remove_punctuation: bool\n\t:param remove_specials: Remove special tokens?\n\t:type remove_specials: bool\n\t:return: Counter object from\\\n\t\t\t the words in the captions.\n\t:rtype: collections.Counter\n\t", "category": "remove", "imports": ["from typing import Optional, List, MutableSequence", "from re import sub as re_sub", "from collections import Counter", "from itertools import chain", "from functools import partial"]}], [], [], [], [], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\n\tdef __init__(self, data=None, name=\"\", file=None, **attr):\n\t\tGraph.__init__(self, data=data, name=name, **attr)\n\t\tif file is None:\n\t\t\timport sys\n\n\t\t\tself.fh = sys.stdout\n\t\telse:\n\t\t\tself.fh = open(file, \"w\")\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tGraph.add_node(self, n, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add node: {n}\\n\")\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self, n):\n\t\tGraph.remove_node(self, n)\n\t\tself.fh.write(f\"Remove node: {n}\\n\")\n\n\tdef remove_nodes_from(self, nodes):\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tGraph.add_edge(self, u, v, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add edge: {u}-{v}\\n\")\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.add_edge(u, v, attr_dict=attr_dict, **attr)\n\n\tdef remove_edge(self, u, v):\n\t\tGraph.remove_edge(self, u, v)\n\t\tself.fh.write(f\"Remove edge: {u}-{v}\\n\")\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.remove_edge(u, v)\n\n\tdef clear(self):\n\t\tGraph.clear(self)\n\t\tself.fh.write(\"Clear graph\\n\")\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["import matplotlib.pyplot as plt", "import networkx as nx", "from networkx import Graph", "\t\t\timport sys"]}], [{"term": "class", "name": "PrintGraph", "data": "class PrintGraph(Graph):\n\t\"\"\"\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t\"\"\"\n\n\tdef __init__(self, data=None, name=\"\", file=None, **attr):\n\t\tGraph.__init__(self, data=data, name=name, **attr)\n\t\tif file is None:\n\t\t\timport sys\n\n\t\t\tself.fh = sys.stdout\n\t\telse:\n\t\t\tself.fh = open(file, \"w\")\n\n\tdef add_node(self, n, attr_dict=None, **attr):\n\t\tGraph.add_node(self, n, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add node: {n}\\n\")\n\n\tdef add_nodes_from(self, nodes, **attr):\n\t\tfor n in nodes:\n\t\t\tself.add_node(n, **attr)\n\n\tdef remove_node(self, n):\n\t\tGraph.remove_node(self, n)\n\t\tself.fh.write(f\"Remove node: {n}\\n\")\n\n\tdef remove_nodes_from(self, nodes):\n\t\tfor n in nodes:\n\t\t\tself.remove_node(n)\n\n\tdef add_edge(self, u, v, attr_dict=None, **attr):\n\t\tGraph.add_edge(self, u, v, attr_dict=attr_dict, **attr)\n\t\tself.fh.write(f\"Add edge: {u}-{v}\\n\")\n\n\tdef add_edges_from(self, ebunch, attr_dict=None, **attr):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.add_edge(u, v, attr_dict=attr_dict, **attr)\n\n\tdef remove_edge(self, u, v):\n\t\tGraph.remove_edge(self, u, v)\n\t\tself.fh.write(f\"Remove edge: {u}-{v}\\n\")\n\n\tdef remove_edges_from(self, ebunch):\n\t\tfor e in ebunch:\n\t\t\tu, v = e[0:2]\n\t\t\tself.remove_edge(u, v)\n\n\tdef clear(self):\n\t\tGraph.clear(self)\n\t\tself.fh.write(\"Clear graph\\n\")\n\n", "description": "\n\tExample subclass of the Graph class.\n\n\tPrints activity log to file or standard output.\n\t", "category": "remove", "imports": ["import matplotlib.pyplot as plt", "import networkx as nx", "from networkx import Graph", "\t\t\timport sys"]}], [], [], [], [], [], [], [], [], [{"term": "def", "name": "get_player_info", "data": "def get_player_info(player_name, player_id):\n\t\"\"\" Get the information about a player. \"\"\"\n\n\tlink = analyzer.player_link_assemble(player_name, player_id)\n\tplayer_page = crawler.get_page(link)\n\t#player_img = crawler.get_img(link)\n\n\tif not player_page:\n\t\tplayer_page = crawler.get_page(link, error=True)\n\t# if not player_img:\n\t#\t player_img = crawler.get_img(link, error=True)\n\n\t#print(\"\ud0c0\uc785\uc774 \ubb50\uc784? \",type(player_page))\n\tplayer_info = {}\n\n\tplayer_info['Transfers'] = get_player_transfer(player_page, player_id)\n\n\tplayer_info['Name'] = player_name.replace('-', ' ').capitalize()\n\n\tplayer_info['Id'] = player_id\n\n\t# player_info['Full Name'] = analyzer.retrieve_in_tags(\"Full Name:\",\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\"\", player_page)\n\t# player_info['Full Name'] = analyzer.remove_tokens(player_info['Full Name'],['\\t', '\\n'])\n\n\tplayer_info['Birth Date'] = analyzer.retrieve_in_tags(\"Date of Birth:\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"\", player_page)\n\tplayer_info['Birth Date'] = analyzer.remove_tokens(player_info['Birth Date'],['\\t', '\\n'])\n\tspan = ''\n\tplayer_info['Birth Place'] = analyzer.retrieve_in_tags('\"birthPlace\">',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t span, player_page)\n\tplayer_info['Birth Place'] = analyzer.remove_tokens(player_info['Birth Place'],['\\t', '\\n'])\n\t\n\ttoken = 'itemprop=\"nationality\">'\n\tplayer_info['Nationality'] = analyzer.retrieve_in_tags(token,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t span, player_page)\n\tplayer_info['Nationality'] = analyzer.remove_tokens(player_info['Nationality'],['\\t', '\\n'])\n\n\tplayer_info['Age'] = analyzer.retrieve_in_tags(\"Age:\", \"\",\n\t\t\t\t\t\t\t\t\t\t\t\t player_page)\n\tplayer_info['Age'] = analyzer.remove_tokens(player_info['Age'],['\\t', '\\n'])\n\n\tplayer_info['Height'] = analyzer.retrieve_in_tags('itemprop=\"height\"',\n\t\t\t\t\t\t\t\t\t\t\t\t\tspan, player_page)\n\tplayer_info['Height'] = analyzer.remove_tokens(player_info['Height'],['\\t', '\\n'])\n\n\t#player_info['Position'] = analyzer.retrieve_in_tags(\"Position:\",\n\t#\t\t\t\t\t\t\t\t\t\t\t\t  \"\", player_page)\n\t\n\tplayer_info['Position'] = analyzer.retrieve_in_tags(\"Main position\t\t\t\t\t\t\t\t\t\t\t\t\t\t:\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t  \"\", player_page)\n\t#player_info['Position'] = analyzer.remove_tokens(player_info['Position'],['\\t', '\\n'])\n\t\n\tprint(\"\uc120\uc218 \ud3ec\uc9c0\uc158 = \",player_info['Position'])\n\n\tplayer_info['Foot'] = analyzer.retrieve_in_tags(\"Foot:\\n\",\n\t\t\t\t\t\t\t\t\t\t\t\t  \"\", player_page)\n\tplayer_info['Foot'] = analyzer.remove_tokens(player_info['Foot'],['\\t', '\\n'])\n\n\t# player_info['Agent'] = analyzer.retrieve_in_tags(\"Player Agents:\",\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\"\", player_page)\n\t# player_info['Agent'] = analyzer.remove_tokens(player_info['Agent'],['\\t', '\\n'])\n\n\tplayer_info['Joined'] = analyzer.retrieve_in_tags(\"Joined:\",\n\t\t\t\t\t\t\t\t\t\t\t\t\tspan, player_page)\n\n\tplayer_info['Joined'] = analyzer.remove_tokens(player_info['Joined'],['\\t', '\\n'])\n\n\ttoken = \"Contract until:\"\n\tplayer_info['Contract Length'] = analyzer.retrieve_in_tags(token,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t span, player_page)\n\tplayer_info['Contract Length'] = analyzer.remove_tokens(player_info['Contract Length'],['\\t', '\\n'])\n\n\t# player_info['Outfiter'] = analyzer.retrieve_in_tags(\"Outfitter:\",\n\t#\t\t\t\t\t\t\t\t\t\t\t\t   \"\", player_page)\n\t# player_info['Outfiter'] = analyzer.remove_tokens(player_info['Outfiter'],['\\t', '\\n'])\n\t#img src \uac00\uc838\uc624\uae30\n\tplayers_img = crawler.get_img(link)\n\tplayer_info['img'] = players_img\t\t\t\t\t\t\t\t\t\t\t   \n\t# token = '\"dataBild\">'\n\t# player_info['img'] = analyzer.get_img(token,player_page)\n\t# print(player_info['img'])\n\n\t#token = ''\t\t\t\t\t\t\t\t\t\t\t\t   \n\t# player_info['playerImg_url'] = analyzer.retrieve_in_tags(token,\n\t#\t\t\t\t\t\t\t\t\t\t\t\t   \"/>\", player_page)\n\t\n\treturn player_info\n\n", "description": " Get the information about a player. ", "category": "remove", "imports": ["import analyzer", "from collections import OrderedDict", "import crawler"]}, {"term": "def", "name": "get_player_transfer", "data": "def get_player_transfer(player_page, player_id):\n\t\"\"\" Get the transfers made along a player career. \"\"\"\n\tplayer_page = analyzer.cut_page('',\n\t\t\t\t\t\t\t\t  \"\", player_page)\n\n\tpages = analyzer.retrieve_in_tags('', '',\n\t\t\t\t\t\t\t\t\tplayer_page, False)\n\n\ttransfers = []\n\n\tif pages is None:\n\t\treturn pages\n\n\tfor page in pages:\n\t\tinfo = {}\n\t\tinfo['Player Id'] = player_id\n\t\tinfo['Season'] = analyzer.retrieve_in_tags(\n\t\t\t'class=\"zentriert hide-for-small\"', '', page)[0]\n\t\tinfo['Fee'] = analyzer.retrieve_in_tags('zelle-abloese', '<', page)\n\t\tinfo['Market Value'] = analyzer.retrieve_in_tags('zelle-mw', '<', page)\n\n\n\t\tclubs_name = []\n\t\tclubs_name = analyzer.retrieve_in_tags('vereinsname\">', '', page)\n\t\tprint(clubs_name)\n\t\t#print(\"len(clubs_name) = \",len(clubs_name))\n\t\t# for i in range(0,len(clubs_name)):#\n\t\t#\t print(i)\n\t\t#\t clubs_name_1[i] = analyzer.retrieve_in_tags('vereinsname\">\\n\\t\\t\\t\\t\\t\\t', '', page)\n\t\t#\t clubs_name_1[i] = analyzer.remove_tokens(clubs_name[i],['\\t', '\\n'])\n\t\t\t#print(\"\ud074\ub7fd \uc774\ub984 : \",clubs_name[i])\n\t\t\n\t\t# make a set without sorting the list\n\t\tclubs_id = list(OrderedDict.fromkeys(analyzer.retrieve_in_tags(\n\t\t\t'id=\"', '\"', page)))\n\n\t\t# The even values are the teams nickname\n\t\tinfo['Team A'], info['Team B'] = clubs_name[1], clubs_name[3]\n\n\t\tprint(\"a = \",info['Team A'])\n\t\tprint(\"b = \",info['Team B'])\n\t\tinfo['ID Team A'], info['ID Team B'] = clubs_id[0], clubs_id[1]\n\t\ttransfers.append(info)\n\n\treturn transfers\n", "description": " Get the transfers made along a player career. ", "category": "remove", "imports": ["import analyzer", "from collections import OrderedDict", "import crawler"]}], [{"term": "class", "name": "Container", "data": "class Container(tuple):\n\t\"\"\"\n\tBase class for containers.\n\n\tContainers are classes that collect semantically related Artists such as\n\tthe bars of a bar plot.\n\t\"\"\"\n\n\tdef __repr__(self):\n\t\treturn (\"<{} object of {} artists>\"\n\t\t\t\t.format(type(self).__name__, len(self)))\n\n\tdef __new__(cls, *kl, **kwargs):\n\t\treturn tuple.__new__(cls, kl[0])\n\n\tdef __init__(self, kl, label=None):\n\n\t\tself.eventson = False  # fire events only if eventson\n\t\tself._oid = 0  # an observer id\n\t\tself._propobservers = {}  # a dict from oids to funcs\n\n\t\tself._remove_method = None\n\n\t\tself.set_label(label)\n\n\tdef set_remove_method(self, f):\n\t\tself._remove_method = f\n\n\tdef remove(self):\n\t\tfor c in cbook.flatten(\n\t\t\t\tself, scalarp=lambda x: isinstance(x, martist.Artist)):\n\t\t\tif c is not None:\n\t\t\t\tc.remove()\n\n\t\tif self._remove_method:\n\t\t\tself._remove_method(self)\n\n\tdef __getstate__(self):\n\t\td = self.__dict__.copy()\n\t\t# remove the unpicklable remove method, this will get re-added on load\n\t\t# (by the axes) if the artist lives on an axes.\n\t\td['_remove_method'] = None\n\t\treturn d\n\n\tdef get_label(self):\n\t\t\"\"\"\n\t\tGet the label used for this artist in the legend.\n\t\t\"\"\"\n\t\treturn self._label\n\n\tdef set_label(self, s):\n\t\t\"\"\"\n\t\tSet the label to *s* for auto legend.\n\n\t\tACCEPTS: string or anything printable with '%s' conversion.\n\t\t\"\"\"\n\t\tif s is not None:\n\t\t\tself._label = '%s' % (s, )\n\t\telse:\n\t\t\tself._label = None\n\t\tself.pchanged()\n\n\tdef add_callback(self, func):\n\t\t\"\"\"\n\t\tAdds a callback function that will be called whenever one of\n\t\tthe :class:`Artist`'s properties changes.\n\n\t\tReturns an *id* that is useful for removing the callback with\n\t\t:meth:`remove_callback` later.\n\t\t\"\"\"\n\t\toid = self._oid\n\t\tself._propobservers[oid] = func\n\t\tself._oid += 1\n\t\treturn oid\n\n\tdef remove_callback(self, oid):\n\t\t\"\"\"\n\t\tRemove a callback based on its *id*.\n\n\t\t.. seealso::\n\n\t\t\t:meth:`add_callback`\n\t\t\t   For adding callbacks\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tdel self._propobservers[oid]\n\t\texcept KeyError:\n\t\t\tpass\n\n\tdef pchanged(self):\n\t\t\"\"\"\n\t\tFire an event when property changed, calling all of the\n\t\tregistered callbacks.\n\t\t\"\"\"\n\t\tfor oid, func in list(six.iteritems(self._propobservers)):\n\t\t\tfunc(self)\n\n\tdef get_children(self):\n\t\treturn [child for child in cbook.flatten(self) if child is not None]\n\n", "description": "\n\tBase class for containers.\n\n\tContainers are classes that collect semantically related Artists such as\n\tthe bars of a bar plot.\n\t", "category": "remove", "imports": ["from __future__ import (absolute_import, division, print_function,", "import six", "import matplotlib.cbook as cbook", "import matplotlib.artist as martist"]}, {"term": "class", "name": "BarContainer", "data": "class BarContainer(Container):\n\t\"\"\"\n\tContainer for the artists of bar plots (e.g. created by `.Axes.bar`).\n\n\tThe container can be treated as a tuple of the *patches* themselves.\n\tAdditionally, you can access these and further parameters by the\n\tattributes.\n\n\tAttributes\n\t----------\n\tpatches : list of :class:`~matplotlib.patches.Rectangle`\n\t\tThe artists of the bars.\n\n\terrorbar : None or :class:`~matplotlib.container.ErrorbarContainer`\n\t\tA container for the error bar artists if error bars are present.\n\t\t*None* otherwise.\n\n\t\"\"\"\n\n\tdef __init__(self, patches, errorbar=None, **kwargs):\n\t\tself.patches = patches\n\t\tself.errorbar = errorbar\n\t\tContainer.__init__(self, patches, **kwargs)\n\n", "description": "\n\tContainer for the artists of bar plots (e.g. created by `.Axes.bar`).\n\n\tThe container can be treated as a tuple of the *patches* themselves.\n\tAdditionally, you can access these and further parameters by the\n\tattributes.\n\n\tAttributes\n\t----------\n\tpatches : list of :class:`~matplotlib.patches.Rectangle`\n\t\tThe artists of the bars.\n\n\terrorbar : None or :class:`~matplotlib.container.ErrorbarContainer`\n\t\tA container for the error bar artists if error bars are present.\n\t\t*None* otherwise.\n\n\t", "category": "remove", "imports": ["from __future__ import (absolute_import, division, print_function,", "import six", "import matplotlib.cbook as cbook", "import matplotlib.artist as martist"]}, {"term": "class", "name": "ErrorbarContainer", "data": "class ErrorbarContainer(Container):\n\t\"\"\"\n\tContainer for the artists of error bars (e.g. created by `.Axes.errorbar`).\n\n\tThe container can be treated as the *lines* tuple itself.\n\tAdditionally, you can access these and further parameters by the\n\tattributes.\n\n\tAttributes\n\t----------\n\tlines : tuple\n\t\tTuple of ``(data_line, caplines, barlinecols)``.\n\n\t\t- data_line : :class:`~matplotlib.lines.Line2D` instance of\n\t\t  x, y plot markers and/or line.\n\t\t- caplines : tuple of :class:`~matplotlib.lines.Line2D` instances of\n\t\t  the error bar caps.\n\t\t- barlinecols : list of :class:`~matplotlib.collections.LineCollection`\n\t\t  with the horizontal and vertical error ranges.\n\n\thas_xerr, has_yerr : bool\n\t\t``True`` if the errorbar has x/y errors.\n\n\t\"\"\"\n\n\tdef __init__(self, lines, has_xerr=False, has_yerr=False, **kwargs):\n\t\tself.lines = lines\n\t\tself.has_xerr = has_xerr\n\t\tself.has_yerr = has_yerr\n\t\tContainer.__init__(self, lines, **kwargs)\n\n", "description": "\n\tContainer for the artists of error bars (e.g. created by `.Axes.errorbar`).\n\n\tThe container can be treated as the *lines* tuple itself.\n\tAdditionally, you can access these and further parameters by the\n\tattributes.\n\n\tAttributes\n\t----------\n\tlines : tuple\n\t\tTuple of ``(data_line, caplines, barlinecols)``.\n\n\t\t- data_line : :class:`~matplotlib.lines.Line2D` instance of\n\t\t  x, y plot markers and/or line.\n\t\t- caplines : tuple of :class:`~matplotlib.lines.Line2D` instances of\n\t\t  the error bar caps.\n\t\t- barlinecols : list of :class:`~matplotlib.collections.LineCollection`\n\t\t  with the horizontal and vertical error ranges.\n\n\thas_xerr, has_yerr : bool\n\t\t``True`` if the errorbar has x/y errors.\n\n\t", "category": "remove", "imports": ["from __future__ import (absolute_import, division, print_function,", "import six", "import matplotlib.cbook as cbook", "import matplotlib.artist as martist"]}, {"term": "class", "name": "StemContainer", "data": "class StemContainer(Container):\n\t\"\"\"\n\tContainer for the artists created in a :meth:`.Axes.stem` plot.\n\n\tThe container can be treated like a namedtuple ``(markerline, stemlines,\n\tbaseline)``.\n\n\tAttributes\n\t----------\n\tmarkerline :  :class:`~matplotlib.lines.Line2D`\n\t\tThe artist of the markers at the stem heads.\n\n\tstemlines : list of :class:`~matplotlib.lines.Line2D`\n\t\tThe artists of the vertical lines for all stems.\n\n\tbaseline : :class:`~matplotlib.lines.Line2D`\n\t\tThe artist of the horizontal baseline.\n\n\t\"\"\"\n\n\tdef __init__(self, markerline_stemlines_baseline, **kwargs):\n\t\tmarkerline, stemlines, baseline = markerline_stemlines_baseline\n\t\tself.markerline = markerline\n\t\tself.stemlines = stemlines\n\t\tself.baseline = baseline\n\t\tContainer.__init__(self, markerline_stemlines_baseline, **kwargs)\n", "description": "\n\tContainer for the artists created in a :meth:`.Axes.stem` plot.\n\n\tThe container can be treated like a namedtuple ``(markerline, stemlines,\n\tbaseline)``.\n\n\tAttributes\n\t----------\n\tmarkerline :  :class:`~matplotlib.lines.Line2D`\n\t\tThe artist of the markers at the stem heads.\n\n\tstemlines : list of :class:`~matplotlib.lines.Line2D`\n\t\tThe artists of the vertical lines for all stems.\n\n\tbaseline : :class:`~matplotlib.lines.Line2D`\n\t\tThe artist of the horizontal baseline.\n\n\t", "category": "remove", "imports": ["from __future__ import (absolute_import, division, print_function,", "import six", "import matplotlib.cbook as cbook", "import matplotlib.artist as martist"]}], [], [], [], [], [], [], [], [], [], [], [{"term": "def", "name": "solution", "data": "def solution(nums,track,rst):\n\tif len(nums) == len(track):\n\t\t# print(\"track is \",track)\n\t\trst.append(track)\n\t\tprint(\"rst is \",rst)\n\t\treturn track\n\tfor i in nums:\n\t\tif  i in track:\n\t\t\tcontinue\n\t\telse:\n\t\t\ttrack.append(i)\n\t\t\t# print(\"after append track is \",track)\n\t\t\tsolution(nums,track,rst)\n\t\t\ttrack.remove(i)\n\t\t\t# print(\"after remove track is \",track)\n", "description": null, "category": "remove", "imports": []}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('funding', '0039_auto_20191022_1105'),\n\t]\n\n\toperations = [\n\t\tmigrations.AlterModelOptions(\n\t\t\tname='donation',\n\t\t\toptions={'verbose_name': 'Donation', 'verbose_name_plural': 'Donations'},\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='donation',\n\t\t\tname='payout_amount',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='donation',\n\t\t\tname='payout_amount_currency',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='payout',\n\t\t\tname='amount_donated',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='payout',\n\t\t\tname='amount_donated_currency',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='payout',\n\t\t\tname='amount_matched',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='payout',\n\t\t\tname='amount_matched_currency',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='payout',\n\t\t\tname='amount_pledged',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='payout',\n\t\t\tname='amount_pledged_currency',\n\t\t),\n\t\tmigrations.AddField(\n\t\t\tmodel_name='payout',\n\t\t\tname='provider',\n\t\t\tfield=models.CharField(default='stripe', max_length=100),\n\t\t\tpreserve_default=False,\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from __future__ import unicode_literals", "from django.db import migrations, models"]}], [{"term": "class", "name": "ConductorAPI", "data": "class ConductorAPI(object):\n\t\"\"\"Client side of the conductor RPC API\n\n\tAPI version history:\n\n\t* 1.0 - Initial version.\n\t* 1.1 - Added migration_update\n\t* 1.2 - Added instance_get_by_uuid and instance_get_all_by_host\n\t* 1.3 - Added aggregate_host_add and aggregate_host_delete\n\t* 1.4 - Added migration_get\n\t* 1.5 - Added bw_usage_update\n\t* 1.6 - Added get_backdoor_port()\n\t* 1.7 - Added aggregate_get_by_host, aggregate_metadata_add,\n\t  and aggregate_metadata_delete\n\t* 1.8 - Added security_group_get_by_instance and\n\t  security_group_rule_get_by_security_group\n\t* 1.9 - Added provider_fw_rule_get_all\n\t* 1.10 - Added agent_build_get_by_triple\n\t* 1.11 - Added aggregate_get\n\t* 1.12 - Added block_device_mapping_update_or_create\n\t* 1.13 - Added block_device_mapping_get_all_by_instance\n\t* 1.14 - Added block_device_mapping_destroy\n\t* 1.15 - Added instance_get_all_by_filters and\n\t  instance_get_all_hung_in_rebooting and\n\t  instance_get_active_by_window\n\t  Deprecated instance_get_all_by_host\n\t* 1.16 - Added instance_destroy\n\t* 1.17 - Added instance_info_cache_delete\n\t* 1.18 - Added instance_type_get\n\t* 1.19 - Added vol_get_usage_by_time and vol_usage_update\n\t* 1.20 - Added migration_get_unconfirmed_by_dest_compute\n\t* 1.21 - Added service_get_all_by\n\t* 1.22 - Added ping\n\t* 1.23 - Added instance_get_all\n\t\t\t Un-Deprecate instance_get_all_by_host\n\t* 1.24 - Added instance_get\n\t* 1.25 - Added action_event_start and action_event_finish\n\t* 1.26 - Added instance_info_cache_update\n\t* 1.27 - Added service_create\n\t* 1.28 - Added binary arg to service_get_all_by\n\t* 1.29 - Added service_destroy\n\t* 1.30 - Added migration_create\n\t* 1.31 - Added migration_get_in_progress_by_host_and_node\n\t* 1.32 - Added optional node to instance_get_all_by_host\n\t* 1.33 - Added compute_node_create and compute_node_update\n\t* 1.34 - Added service_update\n\t* 1.35 - Added instance_get_active_by_window_joined\n\t* 1.36 - Added instance_fault_create\n\t* 1.37 - Added task_log_get, task_log_begin_task, task_log_end_task\n\t* 1.38 - Added service name to instance_update\n\t* 1.39 - Added notify_usage_exists\n\t* 1.40 - Added security_groups_trigger_handler and\n\t  security_groups_trigger_members_refresh\n\t  Remove instance_get_active_by_window\n\t* 1.41 - Added fixed_ip_get_by_instance, network_get,\n\t  instance_floating_address_get_all, quota_commit,\n\t  quota_rollback\n\t* 1.42 - Added get_ec2_ids, aggregate_metadata_get_by_host\n\t* 1.43 - Added compute_stop\n\t* 1.44 - Added compute_node_delete\n\t* 1.45 - Added project_id to quota_commit and quota_rollback\n\t* 1.46 - Added compute_confirm_resize\n\t* 1.47 - Added columns_to_join to instance_get_all_by_host and\n\t  instance_get_all_by_filters\n\t* 1.48 - Added compute_unrescue\n\n\t... Grizzly supports message version 1.48.  So, any changes to existing\n\tmethods in 2.x after that point should be done such that they can\n\thandle the version_cap being set to 1.48.\n\n\t* 1.49 - Added columns_to_join to instance_get_by_uuid\n\t* 1.50 - Added object_action() and object_class_action()\n\t* 1.51 - Added the 'legacy' argument to\n\t\t\t block_device_mapping_get_all_by_instance\n\t* 1.52 - Pass instance objects for compute_confirm_resize\n\t* 1.53 - Added compute_reboot\n\t* 1.54 - Added 'update_cells' argument to bw_usage_update\n\t* 1.55 - Pass instance objects for compute_stop\n\t* 1.56 - Remove compute_confirm_resize and\n\t\t\t migration_get_unconfirmed_by_dest_compute\n\t* 1.57 - Remove migration_create()\n\t* 1.58 - Remove migration_get()\n\n\t... Havana supports message version 1.58.  So, any changes to existing\n\tmethods in 1.x after that point should be done such that they can\n\thandle the version_cap being set to 1.58.\n\n\t* 1.59 - Remove instance_info_cache_update()\n\t* 1.60 - Remove aggregate_metadata_add() and aggregate_metadata_delete()\n\t* ...  - Remove security_group_get_by_instance() and\n\t\t\t security_group_rule_get_by_security_group()\n\t* 1.61 - Return deleted instance from instance_destroy()\n\t* 1.62 - Added object_backport()\n\t* 1.63 - Changed the format of values['stats'] from a dict to a JSON string\n\t\t\t in compute_node_update()\n\t* 1.64 - Added use_slave to instance_get_all_filters()\n\t\t   - Remove instance_type_get()\n\t\t   - Remove aggregate_get()\n\t\t   - Remove aggregate_get_by_host()\n\t\t   - Remove instance_get()\n\t\t   - Remove migration_update()\n\t\t   - Remove block_device_mapping_destroy()\n\n\t* 2.0  - Drop backwards compatibility\n\t\t   - Remove quota_rollback() and quota_commit()\n\t\t   - Remove aggregate_host_add() and aggregate_host_delete()\n\t\t   - Remove network_migrate_instance_start() and\n\t\t\t network_migrate_instance_finish()\n\t\t   - Remove vol_get_usage_by_time\n\n\t... Icehouse supports message version 2.0.  So, any changes to\n\texisting methods in 2.x after that point should be done such\n\tthat they can handle the version_cap being set to 2.0.\n\n\t* Remove instance_destroy()\n\t* Remove compute_unrescue()\n\t* Remove instance_get_all_by_filters()\n\t* Remove instance_get_active_by_window_joined()\n\t* Remove instance_fault_create()\n\t* Remove action_event_start() and action_event_finish()\n\t* Remove instance_get_by_uuid()\n\t* Remove agent_build_get_by_triple()\n\n\t... Juno supports message version 2.0.  So, any changes to\n\texisting methods in 2.x after that point should be done such\n\tthat they can handle the version_cap being set to 2.0.\n\n\t* 2.1  - Make notify_usage_exists() take an instance object\n\t* Remove bw_usage_update()\n\t* Remove notify_usage_exists()\n\n\t... Kilo supports message version 2.1.  So, any changes to\n\texisting methods in 2.x after that point should be done such\n\tthat they can handle the version_cap being set to 2.1.\n\n\t* Remove get_ec2_ids()\n\t* Remove service_get_all_by()\n\t* Remove service_create()\n\t* Remove service_destroy()\n\t* Remove service_update()\n\t* Remove migration_get_in_progress_by_host_and_node()\n\t* Remove aggregate_metadata_get_by_host()\n\t* Remove block_device_mapping_update_or_create()\n\t* Remove block_device_mapping_get_all_by_instance()\n\t* Remove instance_get_all_by_host()\n\t* Remove compute_node_update()\n\t* Remove compute_node_delete()\n\t* Remove security_groups_trigger_handler()\n\t* Remove task_log_get()\n\t* Remove task_log_begin_task()\n\t* Remove task_log_end_task()\n\t* Remove security_groups_trigger_members_refresh()\n\t* Remove vol_usage_update()\n\t* Remove instance_update()\n\n\t* 2.2 - Add object_backport_versions()\n\t* 2.3 - Add object_class_action_versions()\n\t* Remove compute_node_create()\n\t* Remove object_backport()\n\n\t* 3.0  - Drop backwards compatibility\n\n\t... Liberty, Mitaka, Newton, and Ocata support message version 3.0.  So,\n\tany changes to existing methods in 3.x after that point should be done such\n\tthat they can handle the version_cap being set to 3.0.\n\n\t* Remove provider_fw_rule_get_all()\n\t\"\"\"\n\n\tVERSION_ALIASES = {\n\t\t'grizzly': '1.48',\n\t\t'havana': '1.58',\n\t\t'icehouse': '2.0',\n\t\t'juno': '2.0',\n\t\t'kilo': '2.1',\n\t\t'liberty': '3.0',\n\t\t'mitaka': '3.0',\n\t\t'newton': '3.0',\n\t\t'ocata': '3.0',\n\t}\n\n\tdef __init__(self):\n\t\tsuper(ConductorAPI, self).__init__()\n\t\ttarget = messaging.Target(topic=RPC_TOPIC, version='3.0')\n\t\tversion_cap = self.VERSION_ALIASES.get(CONF.upgrade_levels.conductor,\n\t\t\t\t\t\t\t\t\t\t\t   CONF.upgrade_levels.conductor)\n\t\tserializer = objects_base.NovaObjectSerializer()\n\t\tself.client = rpc.get_client(target,\n\t\t\t\t\t\t\t\t\t version_cap=version_cap,\n\t\t\t\t\t\t\t\t\t serializer=serializer)\n\n\t# TODO(hanlind): This method can be removed once oslo.versionedobjects\n\t# has been converted to use version_manifests in remotable_classmethod\n\t# operations, which will use the new class action handler.\n\tdef object_class_action(self, context, objname, objmethod, objver,\n\t\t\t\t\t\t\targs, kwargs):\n\t\tversions = ovo_base.obj_tree_get_versions(objname)\n\t\treturn self.object_class_action_versions(context,\n\t\t\t\t\t\t\t\t\t\t\t\t objname,\n\t\t\t\t\t\t\t\t\t\t\t\t objmethod,\n\t\t\t\t\t\t\t\t\t\t\t\t versions,\n\t\t\t\t\t\t\t\t\t\t\t\t args, kwargs)\n\n\tdef object_class_action_versions(self, context, objname, objmethod,\n\t\t\t\t\t\t\t\t\t object_versions, args, kwargs):\n\t\tcctxt = self.client.prepare()\n\t\treturn cctxt.call(context, 'object_class_action_versions',\n\t\t\t\t\t\t  objname=objname, objmethod=objmethod,\n\t\t\t\t\t\t  object_versions=object_versions,\n\t\t\t\t\t\t  args=args, kwargs=kwargs)\n\n\tdef object_action(self, context, objinst, objmethod, args, kwargs):\n\t\tcctxt = self.client.prepare()\n\t\treturn cctxt.call(context, 'object_action', objinst=objinst,\n\t\t\t\t\t\t  objmethod=objmethod, args=args, kwargs=kwargs)\n\n\tdef object_backport_versions(self, context, objinst, object_versions):\n\t\tcctxt = self.client.prepare()\n\t\treturn cctxt.call(context, 'object_backport_versions', objinst=objinst,\n\t\t\t\t\t\t  object_versions=object_versions)\n\n", "description": "Client side of the conductor RPC API\n\n\tAPI version history:\n\n\t* 1.0 - Initial version.\n\t* 1.1 - Added migration_update\n\t* 1.2 - Added instance_get_by_uuid and instance_get_all_by_host\n\t* 1.3 - Added aggregate_host_add and aggregate_host_delete\n\t* 1.4 - Added migration_get\n\t* 1.5 - Added bw_usage_update\n\t* 1.6 - Added get_backdoor_port()\n\t* 1.7 - Added aggregate_get_by_host, aggregate_metadata_add,\n\t  and aggregate_metadata_delete\n\t* 1.8 - Added security_group_get_by_instance and\n\t  security_group_rule_get_by_security_group\n\t* 1.9 - Added provider_fw_rule_get_all\n\t* 1.10 - Added agent_build_get_by_triple\n\t* 1.11 - Added aggregate_get\n\t* 1.12 - Added block_device_mapping_update_or_create\n\t* 1.13 - Added block_device_mapping_get_all_by_instance\n\t* 1.14 - Added block_device_mapping_destroy\n\t* 1.15 - Added instance_get_all_by_filters and\n\t  instance_get_all_hung_in_rebooting and\n\t  instance_get_active_by_window\n\t  Deprecated instance_get_all_by_host\n\t* 1.16 - Added instance_destroy\n\t* 1.17 - Added instance_info_cache_delete\n\t* 1.18 - Added instance_type_get\n\t* 1.19 - Added vol_get_usage_by_time and vol_usage_update\n\t* 1.20 - Added migration_get_unconfirmed_by_dest_compute\n\t* 1.21 - Added service_get_all_by\n\t* 1.22 - Added ping\n\t* 1.23 - Added instance_get_all\n\t\t\t Un-Deprecate instance_get_all_by_host\n\t* 1.24 - Added instance_get\n\t* 1.25 - Added action_event_start and action_event_finish\n\t* 1.26 - Added instance_info_cache_update\n\t* 1.27 - Added service_create\n\t* 1.28 - Added binary arg to service_get_all_by\n\t* 1.29 - Added service_destroy\n\t* 1.30 - Added migration_create\n\t* 1.31 - Added migration_get_in_progress_by_host_and_node\n\t* 1.32 - Added optional node to instance_get_all_by_host\n\t* 1.33 - Added compute_node_create and compute_node_update\n\t* 1.34 - Added service_update\n\t* 1.35 - Added instance_get_active_by_window_joined\n\t* 1.36 - Added instance_fault_create\n\t* 1.37 - Added task_log_get, task_log_begin_task, task_log_end_task\n\t* 1.38 - Added service name to instance_update\n\t* 1.39 - Added notify_usage_exists\n\t* 1.40 - Added security_groups_trigger_handler and\n\t  security_groups_trigger_members_refresh\n\t  Remove instance_get_active_by_window\n\t* 1.41 - Added fixed_ip_get_by_instance, network_get,\n\t  instance_floating_address_get_all, quota_commit,\n\t  quota_rollback\n\t* 1.42 - Added get_ec2_ids, aggregate_metadata_get_by_host\n\t* 1.43 - Added compute_stop\n\t* 1.44 - Added compute_node_delete\n\t* 1.45 - Added project_id to quota_commit and quota_rollback\n\t* 1.46 - Added compute_confirm_resize\n\t* 1.47 - Added columns_to_join to instance_get_all_by_host and\n\t  instance_get_all_by_filters\n\t* 1.48 - Added compute_unrescue\n\n\t... Grizzly supports message version 1.48.  So, any changes to existing\n\tmethods in 2.x after that point should be done such that they can\n\thandle the version_cap being set to 1.48.\n\n\t* 1.49 - Added columns_to_join to instance_get_by_uuid\n\t* 1.50 - Added object_action() and object_class_action()\n\t* 1.51 - Added the 'legacy' argument to\n\t\t\t block_device_mapping_get_all_by_instance\n\t* 1.52 - Pass instance objects for compute_confirm_resize\n\t* 1.53 - Added compute_reboot\n\t* 1.54 - Added 'update_cells' argument to bw_usage_update\n\t* 1.55 - Pass instance objects for compute_stop\n\t* 1.56 - Remove compute_confirm_resize and\n\t\t\t migration_get_unconfirmed_by_dest_compute\n\t* 1.57 - Remove migration_create()\n\t* 1.58 - Remove migration_get()\n\n\t... Havana supports message version 1.58.  So, any changes to existing\n\tmethods in 1.x after that point should be done such that they can\n\thandle the version_cap being set to 1.58.\n\n\t* 1.59 - Remove instance_info_cache_update()\n\t* 1.60 - Remove aggregate_metadata_add() and aggregate_metadata_delete()\n\t* ...  - Remove security_group_get_by_instance() and\n\t\t\t security_group_rule_get_by_security_group()\n\t* 1.61 - Return deleted instance from instance_destroy()\n\t* 1.62 - Added object_backport()\n\t* 1.63 - Changed the format of values['stats'] from a dict to a JSON string\n\t\t\t in compute_node_update()\n\t* 1.64 - Added use_slave to instance_get_all_filters()\n\t\t   - Remove instance_type_get()\n\t\t   - Remove aggregate_get()\n\t\t   - Remove aggregate_get_by_host()\n\t\t   - Remove instance_get()\n\t\t   - Remove migration_update()\n\t\t   - Remove block_device_mapping_destroy()\n\n\t* 2.0  - Drop backwards compatibility\n\t\t   - Remove quota_rollback() and quota_commit()\n\t\t   - Remove aggregate_host_add() and aggregate_host_delete()\n\t\t   - Remove network_migrate_instance_start() and\n\t\t\t network_migrate_instance_finish()\n\t\t   - Remove vol_get_usage_by_time\n\n\t... Icehouse supports message version 2.0.  So, any changes to\n\texisting methods in 2.x after that point should be done such\n\tthat they can handle the version_cap being set to 2.0.\n\n\t* Remove instance_destroy()\n\t* Remove compute_unrescue()\n\t* Remove instance_get_all_by_filters()\n\t* Remove instance_get_active_by_window_joined()\n\t* Remove instance_fault_create()\n\t* Remove action_event_start() and action_event_finish()\n\t* Remove instance_get_by_uuid()\n\t* Remove agent_build_get_by_triple()\n\n\t... Juno supports message version 2.0.  So, any changes to\n\texisting methods in 2.x after that point should be done such\n\tthat they can handle the version_cap being set to 2.0.\n\n\t* 2.1  - Make notify_usage_exists() take an instance object\n\t* Remove bw_usage_update()\n\t* Remove notify_usage_exists()\n\n\t... Kilo supports message version 2.1.  So, any changes to\n\texisting methods in 2.x after that point should be done such\n\tthat they can handle the version_cap being set to 2.1.\n\n\t* Remove get_ec2_ids()\n\t* Remove service_get_all_by()\n\t* Remove service_create()\n\t* Remove service_destroy()\n\t* Remove service_update()\n\t* Remove migration_get_in_progress_by_host_and_node()\n\t* Remove aggregate_metadata_get_by_host()\n\t* Remove block_device_mapping_update_or_create()\n\t* Remove block_device_mapping_get_all_by_instance()\n\t* Remove instance_get_all_by_host()\n\t* Remove compute_node_update()\n\t* Remove compute_node_delete()\n\t* Remove security_groups_trigger_handler()\n\t* Remove task_log_get()\n\t* Remove task_log_begin_task()\n\t* Remove task_log_end_task()\n\t* Remove security_groups_trigger_members_refresh()\n\t* Remove vol_usage_update()\n\t* Remove instance_update()\n\n\t* 2.2 - Add object_backport_versions()\n\t* 2.3 - Add object_class_action_versions()\n\t* Remove compute_node_create()\n\t* Remove object_backport()\n\n\t* 3.0  - Drop backwards compatibility\n\n\t... Liberty, Mitaka, Newton, and Ocata support message version 3.0.  So,\n\tany changes to existing methods in 3.x after that point should be done such\n\tthat they can handle the version_cap being set to 3.0.\n\n\t* Remove provider_fw_rule_get_all()\n\t", "category": "remove", "imports": ["import oslo_messaging as messaging", "from oslo_serialization import jsonutils", "from oslo_versionedobjects import base as ovo_base", "import nova.conf", "from nova import exception", "from nova.i18n import _", "from nova.objects import base as objects_base", "from nova import profiler", "from nova import rpc"]}, {"term": "class", "name": "ComputeTaskAPI", "data": "class ComputeTaskAPI(object):\n\t\"\"\"Client side of the conductor 'compute' namespaced RPC API\n\n\tAPI version history:\n\n\t1.0 - Initial version (empty).\n\t1.1 - Added unified migrate_server call.\n\t1.2 - Added build_instances\n\t1.3 - Added unshelve_instance\n\t1.4 - Added reservations to migrate_server.\n\t1.5 - Added the legacy_bdm parameter to build_instances\n\t1.6 - Made migrate_server use instance objects\n\t1.7 - Do not send block_device_mapping and legacy_bdm to build_instances\n\t1.8 - Add rebuild_instance\n\t1.9 - Converted requested_networks to NetworkRequestList object\n\t1.10 - Made migrate_server() and build_instances() send flavor objects\n\t1.11 - Added clean_shutdown to migrate_server()\n\t1.12 - Added request_spec to rebuild_instance()\n\t1.13 - Added request_spec to migrate_server()\n\t1.14 - Added request_spec to unshelve_instance()\n\t1.15 - Added live_migrate_instance\n\t1.16 - Added schedule_and_build_instances\n\t1.17 - Added tags to schedule_and_build_instances()\n\t1.18 - Added request_spec to build_instances().\n\t1.19 - build_instances() now gets a 'host_lists' parameter that represents\n\t\t   potential alternate hosts for retries within a cell for each\n\t\t   instance.\n\t1.20 - migrate_server() now gets a 'host_list' parameter that represents\n\t\t   potential alternate hosts for retries within a cell.\n\t1.21 - Added cache_images()\n\t1.22 - Added confirm_snapshot_based_resize()\n\t1.23 - Added revert_snapshot_based_resize()\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tsuper(ComputeTaskAPI, self).__init__()\n\t\ttarget = messaging.Target(topic=RPC_TOPIC,\n\t\t\t\t\t\t\t\t  namespace='compute_task',\n\t\t\t\t\t\t\t\t  version='1.0')\n\t\tserializer = objects_base.NovaObjectSerializer()\n\t\tself.client = rpc.get_client(target, serializer=serializer)\n\n\tdef live_migrate_instance(self, context, instance, scheduler_hint,\n\t\t\t\t\t\t\t  block_migration, disk_over_commit, request_spec):\n\t\tkw = {'instance': instance, 'scheduler_hint': scheduler_hint,\n\t\t\t  'block_migration': block_migration,\n\t\t\t  'disk_over_commit': disk_over_commit,\n\t\t\t  'request_spec': request_spec,\n\t\t\t  }\n\t\tversion = '1.15'\n\t\tcctxt = self.client.prepare(version=version)\n\t\tcctxt.cast(context, 'live_migrate_instance', **kw)\n\n\t# TODO(melwitt): Remove the reservations parameter in version 2.0 of the\n\t# RPC API.\n\t# TODO(mriedem): Make request_spec required *and* a RequestSpec object\n\t# rather than a legacy dict in version 2.0 of the RPC API.\n\tdef migrate_server(self, context, instance, scheduler_hint, live, rebuild,\n\t\t\t\t  flavor, block_migration, disk_over_commit,\n\t\t\t\t  reservations=None, clean_shutdown=True, request_spec=None,\n\t\t\t\t  host_list=None, do_cast=False):\n\t\tkw = {'instance': instance, 'scheduler_hint': scheduler_hint,\n\t\t\t  'live': live, 'rebuild': rebuild, 'flavor': flavor,\n\t\t\t  'block_migration': block_migration,\n\t\t\t  'disk_over_commit': disk_over_commit,\n\t\t\t  'reservations': reservations,\n\t\t\t  'clean_shutdown': clean_shutdown,\n\t\t\t  'request_spec': request_spec,\n\t\t\t  'host_list': host_list,\n\t\t\t  }\n\t\tversion = '1.20'\n\t\tif not self.client.can_send_version(version):\n\t\t\tdel kw['host_list']\n\t\t\tversion = '1.13'\n\t\tif not self.client.can_send_version(version):\n\t\t\tdel kw['request_spec']\n\t\t\tversion = '1.11'\n\t\tif not self.client.can_send_version(version):\n\t\t\tdel kw['clean_shutdown']\n\t\t\tversion = '1.10'\n\t\tif not self.client.can_send_version(version):\n\t\t\tkw['flavor'] = objects_base.obj_to_primitive(flavor)\n\t\t\tversion = '1.6'\n\t\tif not self.client.can_send_version(version):\n\t\t\tkw['instance'] = jsonutils.to_primitive(\n\t\t\t\t\tobjects_base.obj_to_primitive(instance))\n\t\t\tversion = '1.4'\n\t\tcctxt = self.client.prepare(\n\t\t\tversion=version,\n\t\t\tcall_monitor_timeout=CONF.rpc_response_timeout,\n\t\t\ttimeout=CONF.long_rpc_timeout)\n\t\tif do_cast:\n\t\t\treturn cctxt.cast(context, 'migrate_server', **kw)\n\t\treturn cctxt.call(context, 'migrate_server', **kw)\n\n\tdef build_instances(self, context, instances, image, filter_properties,\n\t\t\tadmin_password, injected_files, requested_networks,\n\t\t\tsecurity_groups, block_device_mapping, legacy_bdm=True,\n\t\t\trequest_spec=None, host_lists=None):\n\t\timage_p = jsonutils.to_primitive(image)\n\t\tkwargs = {\"instances\": instances, \"image\": image_p,\n\t\t\t\t  \"filter_properties\": filter_properties,\n\t\t\t\t  \"admin_password\": admin_password,\n\t\t\t\t  \"injected_files\": injected_files,\n\t\t\t\t  \"requested_networks\": requested_networks,\n\t\t\t\t  \"security_groups\": security_groups,\n\t\t\t\t  \"request_spec\": request_spec,\n\t\t\t\t  \"host_lists\": host_lists}\n\t\tversion = '1.19'\n\t\tif not self.client.can_send_version(version):\n\t\t\tversion = '1.18'\n\t\t\tkwargs.pop(\"host_lists\")\n\t\tif not self.client.can_send_version(version):\n\t\t\tversion = '1.10'\n\t\t\tkwargs.pop(\"request_spec\")\n\t\tif not self.client.can_send_version(version):\n\t\t\tversion = '1.9'\n\t\t\tif 'instance_type' in filter_properties:\n\t\t\t\tflavor = filter_properties['instance_type']\n\t\t\t\tflavor_p = objects_base.obj_to_primitive(flavor)\n\t\t\t\tkwargs[\"filter_properties\"] = dict(filter_properties,\n\t\t\t\t\t\t\t\t\t\t\t\t   instance_type=flavor_p)\n\t\tif not self.client.can_send_version(version):\n\t\t\tversion = '1.8'\n\t\t\tnets = kwargs['requested_networks'].as_tuples()\n\t\t\tkwargs['requested_networks'] = nets\n\t\tif not self.client.can_send_version('1.7'):\n\t\t\tversion = '1.5'\n\t\t\tbdm_p = objects_base.obj_to_primitive(block_device_mapping)\n\t\t\tkwargs.update({'block_device_mapping': bdm_p,\n\t\t\t\t\t\t   'legacy_bdm': legacy_bdm})\n\n\t\tcctxt = self.client.prepare(version=version)\n\t\tcctxt.cast(context, 'build_instances', **kwargs)\n\n\tdef schedule_and_build_instances(self, context, build_requests,\n\t\t\t\t\t\t\t\t\t request_specs,\n\t\t\t\t\t\t\t\t\t image, admin_password, injected_files,\n\t\t\t\t\t\t\t\t\t requested_networks,\n\t\t\t\t\t\t\t\t\t block_device_mapping,\n\t\t\t\t\t\t\t\t\t tags=None):\n\t\tversion = '1.17'\n\t\tkw = {'build_requests': build_requests,\n\t\t\t  'request_specs': request_specs,\n\t\t\t  'image': jsonutils.to_primitive(image),\n\t\t\t  'admin_password': admin_password,\n\t\t\t  'injected_files': injected_files,\n\t\t\t  'requested_networks': requested_networks,\n\t\t\t  'block_device_mapping': block_device_mapping,\n\t\t\t  'tags': tags}\n\n\t\tif not self.client.can_send_version(version):\n\t\t\tversion = '1.16'\n\t\t\tdel kw['tags']\n\n\t\tcctxt = self.client.prepare(version=version)\n\t\tcctxt.cast(context, 'schedule_and_build_instances', **kw)\n\n\tdef unshelve_instance(self, context, instance, request_spec=None):\n\t\tversion = '1.14'\n\t\tkw = {'instance': instance,\n\t\t\t  'request_spec': request_spec\n\t\t\t  }\n\t\tif not self.client.can_send_version(version):\n\t\t\tversion = '1.3'\n\t\t\tdel kw['request_spec']\n\t\tcctxt = self.client.prepare(version=version)\n\t\tcctxt.cast(context, 'unshelve_instance', **kw)\n\n\tdef rebuild_instance(self, ctxt, instance, new_pass, injected_files,\n\t\t\timage_ref, orig_image_ref, orig_sys_metadata, bdms,\n\t\t\trecreate=False, on_shared_storage=False, host=None,\n\t\t\tpreserve_ephemeral=False, request_spec=None):\n\t\tversion = '1.12'\n\t\tkw = {'instance': instance,\n\t\t\t  'new_pass': new_pass,\n\t\t\t  'injected_files': injected_files,\n\t\t\t  'image_ref': image_ref,\n\t\t\t  'orig_image_ref': orig_image_ref,\n\t\t\t  'orig_sys_metadata': orig_sys_metadata,\n\t\t\t  'bdms': bdms,\n\t\t\t  'recreate': recreate,\n\t\t\t  'on_shared_storage': on_shared_storage,\n\t\t\t  'preserve_ephemeral': preserve_ephemeral,\n\t\t\t  'host': host,\n\t\t\t  'request_spec': request_spec,\n\t\t\t  }\n\t\tif not self.client.can_send_version(version):\n\t\t\tversion = '1.8'\n\t\t\tdel kw['request_spec']\n\t\tcctxt = self.client.prepare(version=version)\n\t\tcctxt.cast(ctxt, 'rebuild_instance', **kw)\n\n\tdef cache_images(self, ctxt, aggregate, image_ids):\n\t\tversion = '1.21'\n\t\tif not self.client.can_send_version(version):\n\t\t\traise exception.NovaException('Conductor RPC version pin does not '\n\t\t\t\t\t\t\t\t\t\t  'allow cache_images() to be called')\n\t\tcctxt = self.client.prepare(version=version)\n\t\tcctxt.cast(ctxt, 'cache_images', aggregate=aggregate,\n\t\t\t\t   image_ids=image_ids)\n\n\tdef confirm_snapshot_based_resize(\n\t\t\tself, ctxt, instance, migration, do_cast=True):\n\t\tversion = '1.22'\n\t\tif not self.client.can_send_version(version):\n\t\t\traise exception.ServiceTooOld(_('nova-conductor too old'))\n\t\tkw = {'instance': instance, 'migration': migration}\n\t\tcctxt = self.client.prepare(version=version)\n\t\tif do_cast:\n\t\t\treturn cctxt.cast(ctxt, 'confirm_snapshot_based_resize', **kw)\n\t\treturn cctxt.call(ctxt, 'confirm_snapshot_based_resize', **kw)\n\n\tdef revert_snapshot_based_resize(self, ctxt, instance, migration):\n\t\tversion = '1.23'\n\t\tif not self.client.can_send_version(version):\n\t\t\traise exception.ServiceTooOld(_('nova-conductor too old'))\n\t\tkw = {'instance': instance, 'migration': migration}\n\t\tcctxt = self.client.prepare(version=version)\n\t\treturn cctxt.cast(ctxt, 'revert_snapshot_based_resize', **kw)\n", "description": "Client side of the conductor 'compute' namespaced RPC API\n\n\tAPI version history:\n\n\t1.0 - Initial version (empty).\n\t1.1 - Added unified migrate_server call.\n\t1.2 - Added build_instances\n\t1.3 - Added unshelve_instance\n\t1.4 - Added reservations to migrate_server.\n\t1.5 - Added the legacy_bdm parameter to build_instances\n\t1.6 - Made migrate_server use instance objects\n\t1.7 - Do not send block_device_mapping and legacy_bdm to build_instances\n\t1.8 - Add rebuild_instance\n\t1.9 - Converted requested_networks to NetworkRequestList object\n\t1.10 - Made migrate_server() and build_instances() send flavor objects\n\t1.11 - Added clean_shutdown to migrate_server()\n\t1.12 - Added request_spec to rebuild_instance()\n\t1.13 - Added request_spec to migrate_server()\n\t1.14 - Added request_spec to unshelve_instance()\n\t1.15 - Added live_migrate_instance\n\t1.16 - Added schedule_and_build_instances\n\t1.17 - Added tags to schedule_and_build_instances()\n\t1.18 - Added request_spec to build_instances().\n\t1.19 - build_instances() now gets a 'host_lists' parameter that represents\n\t\t   potential alternate hosts for retries within a cell for each\n\t\t   instance.\n\t1.20 - migrate_server() now gets a 'host_list' parameter that represents\n\t\t   potential alternate hosts for retries within a cell.\n\t1.21 - Added cache_images()\n\t1.22 - Added confirm_snapshot_based_resize()\n\t1.23 - Added revert_snapshot_based_resize()\n\t", "category": "remove", "imports": ["import oslo_messaging as messaging", "from oslo_serialization import jsonutils", "from oslo_versionedobjects import base as ovo_base", "import nova.conf", "from nova import exception", "from nova.i18n import _", "from nova.objects import base as objects_base", "from nova import profiler", "from nova import rpc"]}], [{"term": "def", "name": "count_enclosed_functions", "data": "def count_enclosed_functions (source):\n\tfunc_count = 0\n\topen_brace = 0\n\tclose_brace = 0\n\tfor ch in source:\n\t\tif ch == '{':\n\t\t\topen_brace += 1\n\t\telif ch == '}':\n\t\t\tclose_brace += 1\n\t\t\tif open_brace == close_brace:\n\t\t\t\tfunc_count += 1\n\t\tif open_brace < close_brace:\n\t\t\tprint \"count_enclosed_functions : open_brace < close_brace\"\n\t\t\treturn -1\n\treturn func_count\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_function_prototype", "data": "def find_function_prototype (source, proto_name):\n\tproto_re = \"(^[a-zA-Z_ \\t]+\\s+%s[^a-zA-Z0-9_]\\s*\\([^\\)]+\\)\\s+;\\n)\" % (proto_name)\n\tproto_result = re.search (proto_re, source, re.MULTILINE | re.DOTALL)\n\tif not proto_result:\n\t\treturn None\n\tproto_text = proto_result.groups ()[0]\n\treturn proto_text\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_function_definition", "data": "def find_function_definition (source, func_name):\n\tfunc_re = \"(\\n[a-zA-Z_ \\t]+\\n%s[^a-zA-Z0-9_].* /\\* %s \\*/\\n)\" % (func_name, func_name)\n\tfunc_result = re.search (func_re, source, re.MULTILINE | re.DOTALL)\n\tif not func_result:\n\t\tsys.exit (1)\n\t\treturn None\n\tfunc_text = func_result.groups ()[0]\n\n\t# Now to check that we only have one enclosing function.\n\tfunc_count = count_enclosed_functions (func_text)\n\tif func_count != 1:\n\t\treturn None\n\treturn func_text\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_include", "data": "def find_include (source, inc_name):\n\tinc_re = \"(^#include\\s+[\\<\\\"]%s[\\\"\\>]\\s*)\" % inc_name\n\tinc_result = re.search (inc_re, source, re.MULTILINE | re.DOTALL)\n\tif not inc_result:\n\t\treturn None\n\tinc_text = inc_result.groups ()[0]\n\treturn inc_text\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_assign_statement", "data": "def find_assign_statement (source, var_name):\n\tvar_re = \"(^\\s+%s\\s*=[^;]+;)\" % var_name\n\tvar_result = re.search (var_re, source, re.MULTILINE | re.DOTALL)\n\tif not var_result:\n\t\treturn None\n\tassign_text = var_result.groups ()[0]\n\treturn assign_text\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_include", "data": "def remove_include (source, inc_name):\n\tinc_text = find_include (source, inc_name)\n\tif not inc_text:\n\t\tprint \"remove_include : include '%s' not found. Exiting.\" % inc_name\n\t\tsys.exit (1)\n\n\tsource = string.replace (source, inc_text, \"\")\n\treturn source\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_assign", "data": "def remove_assign (source, assign_name):\n\tassign_text = find_assign (source, inc_name)\n\tif not inc_text:\n\t\tprint \"remove_include : include '%s' not found. Exiting.\" % inc_name\n\t\tsys.exit (1)\n\n\tsource = string.replace (source, inc_text, \"\")\n\treturn source\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_prototype", "data": "def remove_prototype (source, proto_name):\n\tproto_text = find_function_prototype (source, proto_name)\n\tif not proto_text:\n\t\tprint \"remove_prototype : prototype '%s' not found. Exiting.\" % proto_name\n\t\tsys.exit (1)\n\n\tsource = string.replace (source, proto_text, \"\")\n\treturn source\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_function", "data": "def remove_function (source, func_name):\n\tfunc_text = find_function_definition (source, func_name)\n\tif not func_text:\n\t\tprint \"remove_function : function '%s' not found. Exiting.\" % func_name\n\t\tsys.exit (1)\n\n\tsource = string.replace (source, func_text, \"/* Function %s() removed here. */\\n\" % func_name)\n\treturn source\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_all_assignments", "data": "def remove_all_assignments (source, var):\n\tcount = 0\n\twhile 1:\n\t\tassign_text = find_assign_statement (source, var)\n\t\tif not assign_text:\n\t\t\tif count != 0:\n\t\t\t\tbreak\n\t\t\tprint \"remove_all_assignments : variable '%s' not found. Exiting.\" % var\n\t\t\tsys.exit (1)\n\n\t\tsource = string.replace (source, assign_text, \"\")\n\t\tcount += 1\n\treturn source\n\n\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_funcs_and_protos_from_file", "data": "def remove_funcs_and_protos_from_file (filename, func_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor func in func_list:\n\t\tsource_code = remove_prototype (source_code, func) ;\n\t\tsource_code = remove_function (source_code, func) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_funcs_from_file", "data": "def remove_funcs_from_file (filename, func_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor func in func_list:\n\t\tsource_code = remove_function (source_code, func) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_protos_from_file", "data": "def remove_protos_from_file (filename, func_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor func in func_list:\n\t\tsource_code = remove_prototype (source_code, func) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_includes_from_file", "data": "def remove_includes_from_file (filename, inc_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor inc in inc_list:\n\t\tsource_code = remove_include (source_code, inc) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_all_assignments_from_file", "data": "def remove_all_assignments_from_file (filename, var_list):\n\tsource_code = open (filename, 'r').read ()\n\n\tfor var in var_list:\n\t\tsource_code = remove_all_assignments (source_code, var) ;\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_comment_start_end", "data": "def remove_comment_start_end (filename, start_comment, end_comment):\n\tsource_code = open (filename, 'r').read ()\n\n\twhile 1:\n\t\tstart_index = string.find (source_code, start_comment)\n\t\tend_index = string.find (source_code, end_comment)\n\t\tif start_index < 0 or end_index < start_index:\n\t\t\tbreak\n\t\tend_index += len (end_comment)\n\t\tsource_code = source_code [:start_index-1] + source_code [end_index:] ;\n\n\topen (filename, 'w').write (source_code)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_strings_from_file", "data": "def remove_strings_from_file (filename, str_list):\n\tfile_text = open (filename, 'r').read ()\n\tfor current_str in str_list:\n\t\tfile_text = string.replace (file_text, current_str, '')\n\topen (filename, 'w').write (file_text)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "string_replace_in_file", "data": "def string_replace_in_file (filename, from_str, to_str):\n\tfile_text = open (filename, 'r').read ()\n\tfile_text = string.replace (file_text, from_str, to_str)\n\topen (filename, 'w').write (file_text)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "remove_regex_from_file", "data": "def remove_regex_from_file (filename, regex_list):\n\tfile_text = open (filename, 'r').read ()\n\tfor regex in regex_list:\n\t\tfile_text = re.sub (regex, '', file_text, re.MULTILINE | re.DOTALL)\n\topen (filename, 'w').write (file_text)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "find_configure_version", "data": "def find_configure_version (filename):\n\t# AM_INIT_AUTOMAKE(libsndfile,0.0.21pre6)\n\tfile = open (filename)\n\twhile 1:\n\t\tline = file.readline ()\n\t\tif re.search (\"AC_INIT\", line):\n\t\t\tx = re.sub (\"[^\\(]+\\(\", \"\", line)\n\t\t\tx = re.sub (\"\\).*\\n\", \"\", x)\n\t\t\tx = string.split (x, \",\")\n\t\t\tpackage = x [0]\n\t\t\tversion = x [1]\n\t\t\tbreak\n\tfile.close ()\n\t# version = re.escape (version)\n\treturn package, version\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "fix_configure_ac_file", "data": "def fix_configure_ac_file (filename):\n\tdata = open (filename, 'r').read ()\n\tdata = string.replace (data, \"AM_INIT_AUTOMAKE(libsndfile,\", \"AM_INIT_AUTOMAKE(libsndfile_lite,\", 1)\n\n\tfile = open (filename, 'w')\n\tfile.write (data)\n\tfile.close ()\n\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "make_dist_file", "data": "def make_dist_file (package, version):\n\tprint \"Making dist file.\"\n\ttar_gz_file = \"%s-%s.tar.gz\" % (package, version)\n\tif os.path.exists (tar_gz_file):\n\t\treturn\n\tif os.system (\"make dist\"):\n\t\tsys.exit (1)\n\treturn\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}, {"term": "def", "name": "delete_files", "data": "def delete_files (file_list):\n\tfor file_name in file_list:\n\t\tos.remove (file_name)\n", "description": null, "category": "remove", "imports": ["import commands, os, re, string, sys, time"]}], [], [], [{"term": "def", "name": "FORCe", "data": "def FORCe(info, original_data):\n\tnCh = len(info['chs'])\n\tdata = np.multiply(original_data, 1e6)\n\n\t\"\"\" Step 1) - 4) \"\"\"\n\twaveletname = 'sym4'\n\tcoeffitiantsLevel1 = []\n\tcoeffitiantsLevel2 = []\n\tcA2 = []\n\tcD1 = []\n\tcD2 = []\n\tmarked_ICs = np.zeros(nCh)\n\n\t\"\"\"\n\t# ''cAx'' is he array of approximation coefficient (low pass filters) and\n\t# ''cDx'' is the list of detail coefficients (high pass filters).\n\t# 2 level decomposition!\n\t\"\"\"\n\n\tfor i in range(np.size(data, 0)):\n\t\tactual_coeff = pywt.wavedec(data[i][:], waveletname, level=1)\n\t\tcA1_actual, cD1_actual = actual_coeff\n\t\tcoeffitiantsLevel1.append(actual_coeff)\n\t\tcD1.append(cD1_actual)\n\tD1 = np.array(cD1)\n\n\tfor i in range(np.size(data, 0)):\n\t\tactual_coeff = pywt.wavedec(data[i][:], waveletname, level=2)\n\t\tcA2_actual, cD2_actual, cD1_actual = actual_coeff\n\t\tcoeffitiantsLevel2.append(actual_coeff)\n\t\tcA2.append(cA2_actual)\n\t\tcD2.append(cD2_actual)\n\tA2 = np.array(cA2)\n\tD2 = np.array(cD2)\n\n\t\"\"\"\n\t# S: 2D array containing estimated source signals\n\t# A: 2D array containing mixing matrix, i.e. A.dot(S) = X\n\t\"\"\"\n\tica = FastICA(n_components=len(info['chs']), algorithm='parallel',max_iter=500,tol=1e-2)\n\tS = ica.fit(A2.T).transform(A2.T)\n\tA = ica.mixing_\n\tassert np.allclose(A2.T, np.dot(S, A.T) + ica.mean_)\n\tassert A.shape[0] == A.shape[1]\n\n\tICs_projections = np.zeros((np.size(S, 0), np.size(S, 1), np.size(S, 1)))\n\tfor i in range(np.size(S, 1)):\n\t\tactual_S = np.copy(S)\n\t\tfor j in range(np.size(S, 1)):\n\t\t\tif (j != i):\n\t\t\t\tactual_S[:, j] = 0\n\t\tactual_projection = np.dot(actual_S, A.T)\n\t\tICs_projections[:, :, i] = actual_projection\n\n\twith concurrent.futures.ThreadPoolExecutor() as executor:\n\t\ttoRemoveICs1 = executor.submit(Auto_Mutual_Information, ICs_projections, LAG_OFFSET, THRESHOLD_MAX, THRESHOLD_MIN)\n\t\ttoRemoveICs2 = executor.submit(Spiking_activity, S, THRESHOLD_COEFF)\n\t\ttoRemoveICs3 = executor.submit(Kurtosis, ICs_projections)\n\t\ttoRemoveICs4 = executor.submit(PSD, ICs_projections, info['sfreq'], DISTANCE, THRESHOLD_GAMMA)\n\t\ttoRemoveICs5 = executor.submit(Projection_STD, ICs_projections)\n\t\ttoRemoveICs6 = executor.submit(Topographic_distribution, ICs_projections, info)\n\t\ttoRemoveICs7 = executor.submit(Amplitude_thresholding, ICs_projections, THRESHOLD_AMPLITUDE, PEAK_DIFFERENCE)\n\n\tfor i in range(np.size(marked_ICs)):\n\t\t marked_ICs[i] = toRemoveICs1.result()[i] + toRemoveICs2.result()[i] + toRemoveICs3.result()[i] + toRemoveICs4.result()[i] + toRemoveICs5.result()[i] + toRemoveICs6.result()[i] + toRemoveICs7.result()[i]\n\n\t# Without paralellisation:\n\t\"\"\" Step 4).1: Auto-Mutal Informatin \"\"\"\n\t# toRemoveICs1 = Auto_Mutual_Information(ICs_projections, LAG_OFFSET, THRESHOLD_MAX, THRESHOLD_MIN, marked_ICs)\n\t# print('Marked to remove by AMI: ', toRemoveICs1)\n\n\t\"\"\" Step 4).2: Spiking activity \"\"\"\n\t# toRemoveICs2a, toRemoveICs2b = Spiking_activity(S, THRESHOLD_COEFF, marked_ICs)\n\t# print('Marked to remove by Spiking-activity: ', toRemoveICs2a)\n\t# print('Marked to remove by Spike zone coefficients: ', toRemoveICs2b)\n\n\t\"\"\" 4).3 Kurtosis \"\"\"\n\t# toRemoveICs3 = Kurtosis(ICs_projections, marked_ICs)\n\t# print('Marked to remove by Kurtosis : ', toRemoveICs3)\t# data = raw_croped.get_data(picks = chanls)\n\n\t# toRemoveICs4, toRemoveICs5 = PSD(ICs_projections, info['sfreq'], DISTANCE, THRESHOLD_GAMMA, marked_ICs)\n\t# print('Marked to remove by PSD & 1/F distribution: ', toRemoveICs4)\n\t# print('Marked to remove by Gamma frequency: ', toRemoveICs5)\n\n\t\"\"\" 4).6 Check stds of projections of ICs. \"\"\"\n\t# toRemoveICs6 = Projection_STD(ICs_projections, marked_ICs)\n\t# print('Marked to remove by Std: ', toRemoveICs6)\n\n\t\"\"\" 4).7 Topographic distribution of standard deviations \"\"\"\n\t# toRemoveICs7 = Topographic_distribution(ICs_projections, info, marked_ICs)\n\t# print('Marked to remove by topographic distribution: ', toRemoveICs7)\n\n\t\"\"\" 4).8 Remove ICs with large amplitudes \"\"\"\n\t# toRemoveICs8a, toRemoveICs8b = Amplitude_thresholding(ICs_projections, THRESHOLD_AMPLITUDE, PEAK_DIFFERENCE,\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t   marked_ICs)\n\t# print('Marked to remove by Large amplitudes A: ', toRemoveICs8a)\n\t# print('Marked to remove by Large amplitudes B: ', toRemoveICs8b)\n\n\tfor i in range(np.size(marked_ICs)):\n\t\tif (marked_ICs[i] > REMOVING_THRESHOLD):\n\t\t\tS[:, i] = 0\n\n\tclean_A2 = (ica.inverse_transform(S)).T\n\n\t\"\"\" 5) Spike Zone Thresholding \"\"\"\n\tnewD1 = Thresholding(D1, DC_checkVal, DC_adjustVal)\n\tnewA2 = Thresholding(clean_A2, AC_checkVal, AC_adjustVal)\n\tnewD2 = Thresholding(D2, DC_checkVal, DC_adjustVal)\n\n\tprint(\"Number of channel markings: \", marked_ICs)\n\n\tpre_clea_data = []\n\tfor i in range(np.size(D2, 0)):\n\t\tcoeff = [newA2[i], newD2[i], newD1[i]]\n\t\tactual_reconstuction = pywt.waverec(coeff, waveletname)\n\t\tpre_clea_data.append(actual_reconstuction)\n\n\tclean_data = np.array(pre_clea_data)\n\treturn clean_data\n", "description": " Step 1) - 4) ", "category": "remove", "imports": ["import numpy as np", "import pywt", "import concurrent.futures", "import matplotlib.pyplot as plt", "import mne", "from mne.io import read_raw_edf", "from artefact_1_AMI import *", "from artefact_2_Spiking import *", "from artefact_3_Kurtosis import *", "from artefact_4_5_PSD import *", "from artefact_6_Projection_STD import *", "from artefact_7_Topographic_distribution import *", "from artefact_8_Amplitude_thresholding import *", "from artefact_Spike_zone_thresholding import *", "# from preprocess.artefact_1_AMI import *", "# from preprocess.artefact_2_Spiking import *", "# from preprocess.artefact_3_Kurtosis import *", "# from preprocess.artefact_4_5_PSD import *", "# from preprocess.artefact_6_Projection_STD import *", "# from preprocess.artefact_7_Topographic_distribution import *", "# from preprocess.artefact_8_Amplitude_thresholding import *", "# # from preprocess.artefact_Spike_zone_thresholding import *", "# from preprocess.artefact_PSD import *", "from sklearn.decomposition import FastICA"]}, {"term": "class", "name": "classArtefactFilter:", "data": "class ArtefactFilter:\n\n\tdef offline_filter(self, epochs):\n\t\t\"\"\"Offline Faster algorithm\n\n\t\tFilters the input epochs, and saves the parameters (such as ICA weights),\n\t\tfor the possibility of online filtering\n\n\t\tParameters\n\t\t----------\n\t\tepochs : mne.Epochs\n\t\t\tThe epochs to analyze\n\n\t\tReturns\n\t\t-------\n\t\tmne.Epochs\n\t\t\tThe filtered epoch\n\t\t\"\"\"\n\t\tfor i in range(len(epochs)):\n\t\t\tepochs._data[i, ...] = FORCe(epochs.info, epochs[i].get_data()[0])[:,:-1]\n\t\treturn epochs\n\n", "description": "Offline Faster algorithm\n\n\t\tFilters the input epochs, and saves the parameters (such as ICA weights),\n\t\tfor the possibility of online filtering\n\n\t\tParameters\n\t\t----------\n\t\tepochs : mne.Epochs\n\t\t\tThe epochs to analyze\n\n\t\tReturns\n\t\t-------\n\t\tmne.Epochs\n\t\t\tThe filtered epoch\n\t\t", "category": "remove", "imports": ["import numpy as np", "import pywt", "import concurrent.futures", "import matplotlib.pyplot as plt", "import mne", "from mne.io import read_raw_edf", "from artefact_1_AMI import *", "from artefact_2_Spiking import *", "from artefact_3_Kurtosis import *", "from artefact_4_5_PSD import *", "from artefact_6_Projection_STD import *", "from artefact_7_Topographic_distribution import *", "from artefact_8_Amplitude_thresholding import *", "from artefact_Spike_zone_thresholding import *", "# from preprocess.artefact_1_AMI import *", "# from preprocess.artefact_2_Spiking import *", "# from preprocess.artefact_3_Kurtosis import *", "# from preprocess.artefact_4_5_PSD import *", "# from preprocess.artefact_6_Projection_STD import *", "# from preprocess.artefact_7_Topographic_distribution import *", "# from preprocess.artefact_8_Amplitude_thresholding import *", "# # from preprocess.artefact_Spike_zone_thresholding import *", "# from preprocess.artefact_PSD import *", "from sklearn.decomposition import FastICA"]}], [], [], [{"term": "class", "name": "MassEditingWizard", "data": "class MassEditingWizard(models.TransientModel):\n\t_name = 'mass.editing.wizard'\n\n\t@api.model\n\tdef fields_view_get(self, view_id=None, view_type='form', toolbar=False,\n\t\t\t\t\t\tsubmenu=False):\n\t\tresult =\\\n\t\t\tsuper(MassEditingWizard, self).fields_view_get(view_id=view_id,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t   view_type=view_type,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t   toolbar=toolbar,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t   submenu=submenu)\n\t\tcontext = self._context\n\t\tif context.get('mass_editing_object'):\n\t\t\tmass_obj = self.env['mass.object']\n\t\t\tediting_data = mass_obj.browse(context.get('mass_editing_object'))\n\t\t\tall_fields = {}\n\t\t\txml_form = etree.Element('form', {\n\t\t\t\t'string': tools.ustr(editing_data.name)\n\t\t\t})\n\t\t\txml_group = etree.SubElement(xml_form, 'group', {\n\t\t\t\t'colspan': '6',\n\t\t\t\t'col': '6',\n\t\t\t})\n\t\t\tetree.SubElement(xml_group, 'label', {\n\t\t\t\t'string': '',\n\t\t\t\t'colspan': '2',\n\t\t\t})\n\t\t\txml_group = etree.SubElement(xml_form, 'group', {\n\t\t\t\t'colspan': '6',\n\t\t\t\t'col': '6',\n\t\t\t})\n\t\t\tmodel_obj = self.env[context.get('active_model')]\n\t\t\tfield_info = model_obj.fields_get()\n\t\t\tfor field in editing_data.field_ids:\n\t\t\t\tif field.ttype == \"many2many\":\n\t\t\t\t\tall_fields[field.name] = field_info[field.name]\n\t\t\t\t\tall_fields[\"selection__\" + field.name] = {\n\t\t\t\t\t\t'type': 'selection',\n\t\t\t\t\t\t'string': field_info[field.name]['string'],\n\t\t\t\t\t\t'selection': [('set', 'Set'),\n\t\t\t\t\t\t\t\t\t  ('remove_m2m', 'Remove'),\n\t\t\t\t\t\t\t\t\t  ('add', 'Add')]\n\t\t\t\t\t}\n\t\t\t\t\txml_group = etree.SubElement(xml_group, 'group', {\n\t\t\t\t\t\t'colspan': '6',\n\t\t\t\t\t\t'col': '6',\n\t\t\t\t\t})\n\t\t\t\t\tetree.SubElement(xml_group, 'separator', {\n\t\t\t\t\t\t'string': field_info[field.name]['string'],\n\t\t\t\t\t\t'colspan': '6',\n\t\t\t\t\t})\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': \"selection__\" + field.name,\n\t\t\t\t\t\t'colspan': '6',\n\t\t\t\t\t\t'nolabel': '1'\n\t\t\t\t\t})\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': field.name,\n\t\t\t\t\t\t'colspan': '6',\n\t\t\t\t\t\t'nolabel': '1',\n\t\t\t\t\t\t'attrs': (\"{'invisible': [('selection__\" +\n\t\t\t\t\t\t\t\t  field.name + \"', '=', 'remove_m2m')]}\"),\n\t\t\t\t\t})\n\t\t\t\telif field.ttype == \"one2many\":\n\t\t\t\t\tall_fields[\"selection__\" + field.name] = {\n\t\t\t\t\t\t'type': 'selection',\n\t\t\t\t\t\t'string': field_info[field.name]['string'],\n\t\t\t\t\t\t'selection': [('set', 'Set'), ('remove', 'Remove')],\n\t\t\t\t\t}\n\t\t\t\t\tall_fields[field.name] = {\n\t\t\t\t\t\t'type': field.ttype,\n\t\t\t\t\t\t'string': field.field_description,\n\t\t\t\t\t\t'relation': field.relation,\n\t\t\t\t\t}\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': \"selection__\" + field.name,\n\t\t\t\t\t\t'colspan': '4',\n\t\t\t\t\t})\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': field.name,\n\t\t\t\t\t\t'colspan': '6',\n\t\t\t\t\t\t'nolabel': '1',\n\t\t\t\t\t\t'attrs': (\"{'invisible':[('selection__\" +\n\t\t\t\t\t\t\t\t  field.name + \"', '=', 'remove_o2m')]}\"),\n\t\t\t\t\t})\n\t\t\t\telif field.ttype == \"many2one\":\n\t\t\t\t\tall_fields[\"selection__\" + field.name] = {\n\t\t\t\t\t\t'type': 'selection',\n\t\t\t\t\t\t'string': field_info[field.name]['string'],\n\t\t\t\t\t\t'selection': [('set', 'Set'), ('remove', 'Remove')],\n\t\t\t\t\t}\n\t\t\t\t\tall_fields[field.name] = {\n\t\t\t\t\t\t'type': field.ttype,\n\t\t\t\t\t\t'string': field.field_description,\n\t\t\t\t\t\t'relation': field.relation,\n\t\t\t\t\t}\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': \"selection__\" + field.name,\n\t\t\t\t\t\t'colspan': '2',\n\t\t\t\t\t})\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': field.name,\n\t\t\t\t\t\t'nolabel': '1',\n\t\t\t\t\t\t'colspan': '4',\n\t\t\t\t\t\t'attrs': (\"{'invisible':[('selection__\" +\n\t\t\t\t\t\t\t\t  field.name + \"', '=', 'remove')]}\"),\n\t\t\t\t\t})\n\t\t\t\telif field.ttype == \"char\":\n\t\t\t\t\tall_fields[\"selection__\" + field.name] = {\n\t\t\t\t\t\t'type': 'selection',\n\t\t\t\t\t\t'string': field_info[field.name]['string'],\n\t\t\t\t\t\t'selection': [('set', 'Set'), ('remove', 'Remove')],\n\t\t\t\t\t}\n\t\t\t\t\tall_fields[field.name] = {\n\t\t\t\t\t\t'type': field.ttype,\n\t\t\t\t\t\t'string': field.field_description,\n\t\t\t\t\t\t'size': field.size or 256,\n\t\t\t\t\t}\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': \"selection__\" + field.name,\n\t\t\t\t\t\t'colspan': '2',\n\t\t\t\t\t})\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': field.name,\n\t\t\t\t\t\t'nolabel': '1',\n\t\t\t\t\t\t'attrs': (\"{'invisible':[('selection__\" +\n\t\t\t\t\t\t\t\t  field.name + \"','=','remove')]}\"),\n\t\t\t\t\t\t'colspan': '4',\n\t\t\t\t\t})\n\t\t\t\telif field.ttype == 'selection':\n\t\t\t\t\tall_fields[\"selection__\" + field.name] = {\n\t\t\t\t\t\t'type': 'selection',\n\t\t\t\t\t\t'string': field_info[field.name]['string'],\n\t\t\t\t\t\t'selection': [('set', 'Set'), ('remove', 'Remove')]\n\t\t\t\t\t}\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': \"selection__\" + field.name,\n\t\t\t\t\t\t'colspan': '2',\n\t\t\t\t\t})\n\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t'name': field.name,\n\t\t\t\t\t\t'nolabel': '1',\n\t\t\t\t\t\t'colspan': '4',\n\t\t\t\t\t\t'attrs': (\"{'invisible':[('selection__\" +\n\t\t\t\t\t\t\t\t  field.name + \"', '=', 'remove')]}\"),\n\t\t\t\t\t})\n\t\t\t\t\tall_fields[field.name] = {\n\t\t\t\t\t\t'type': field.ttype,\n\t\t\t\t\t\t'string': field.field_description,\n\t\t\t\t\t\t'selection': field_info[field.name]['selection'],\n\t\t\t\t\t}\n\t\t\t\telse:\n\t\t\t\t\tall_fields[field.name] = {\n\t\t\t\t\t\t'type': field.ttype,\n\t\t\t\t\t\t'string': field.field_description,\n\t\t\t\t\t}\n\t\t\t\t\tall_fields[\"selection__\" + field.name] = {\n\t\t\t\t\t\t'type': 'selection',\n\t\t\t\t\t\t'string': field_info[field.name]['string'],\n\t\t\t\t\t\t'selection': [('set', 'Set'), ('remove', 'Remove')]\n\t\t\t\t\t}\n\t\t\t\t\tif field.ttype == 'text':\n\t\t\t\t\t\txml_group = etree.SubElement(xml_group, 'group', {\n\t\t\t\t\t\t\t'colspan': '6',\n\t\t\t\t\t\t\t'col': '6',\n\t\t\t\t\t\t})\n\t\t\t\t\t\tetree.SubElement(xml_group, 'separator', {\n\t\t\t\t\t\t\t'string': all_fields[field.name]['string'],\n\t\t\t\t\t\t\t'colspan': '6',\n\t\t\t\t\t\t})\n\t\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t\t'name': \"selection__\" + field.name,\n\t\t\t\t\t\t\t'colspan': '6',\n\t\t\t\t\t\t\t'nolabel': '1',\n\t\t\t\t\t\t})\n\t\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t\t'name': field.name,\n\t\t\t\t\t\t\t'colspan': '6',\n\t\t\t\t\t\t\t'nolabel': '1',\n\t\t\t\t\t\t\t'attrs': (\"{'invisible':[('selection__\" +\n\t\t\t\t\t\t\t\t\t  field.name + \"','=','remove')]}\"),\n\t\t\t\t\t\t})\n\t\t\t\t\telse:\n\t\t\t\t\t\tall_fields[\"selection__\" + field.name] = {\n\t\t\t\t\t\t\t'type': 'selection',\n\t\t\t\t\t\t\t'string': field_info[field.name]['string'],\n\t\t\t\t\t\t\t'selection': [('set', 'Set'), ('remove', 'Remove')]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t\t'name': \"selection__\" + field.name,\n\t\t\t\t\t\t\t'colspan': '2',\n\t\t\t\t\t\t})\n\t\t\t\t\t\tetree.SubElement(xml_group, 'field', {\n\t\t\t\t\t\t\t'name': field.name,\n\t\t\t\t\t\t\t'nolabel': '1',\n\t\t\t\t\t\t\t'attrs': (\"{'invisible':[('selection__\" +\n\t\t\t\t\t\t\t\t\t  field.name + \"','=','remove')]}\"),\n\t\t\t\t\t\t\t'colspan': '4',\n\t\t\t\t\t\t})\n\t\t\t# Patch fields with required extra data\n\t\t\tfor field in all_fields.values():\n\t\t\t\tfield.setdefault(\"views\", {})\n\t\t\tetree.SubElement(xml_form, 'separator', {\n\t\t\t\t'string': '',\n\t\t\t\t'colspan': '6',\n\t\t\t\t'col': '6',\n\t\t\t})\n\t\t\txml_group3 = etree.SubElement(xml_form, 'footer', {})\n\t\t\tetree.SubElement(xml_group3, 'button', {\n\t\t\t\t'string': 'Apply',\n\t\t\t\t'class': 'btn-primary',\n\t\t\t\t'type': 'object',\n\t\t\t\t'name': 'action_apply',\n\t\t\t})\n\t\t\tetree.SubElement(xml_group3, 'button', {\n\t\t\t\t'string': 'Close',\n\t\t\t\t'class': 'btn-default',\n\t\t\t\t'special': 'cancel',\n\t\t\t})\n\t\t\troot = xml_form.getroottree()\n\t\t\tresult['arch'] = etree.tostring(root)\n\t\t\tresult['fields'] = all_fields\n\t\treturn result\n\n\t@api.model\n\tdef create(self, vals):\n\t\tif (self._context.get('active_model') and\n\t\t\t\tself._context.get('active_ids')):\n\t\t\tmodel_obj = self.env[self._context.get('active_model')]\n\t\t\tvalues = {}\n\t\t\tfor key, val in vals.items():\n\t\t\t\tif key.startswith('selection_'):\n\t\t\t\t\tsplit_key = key.split('__', 1)[1]\n\t\t\t\t\tif val == 'set':\n\t\t\t\t\t\tvalues.update({split_key: vals.get(split_key, False)})\n\t\t\t\t\telif val == 'remove':\n\t\t\t\t\t\tvalues.update({split_key: False})\n\t\t\t\t\telif val == 'remove_m2m':\n\t\t\t\t\t\tvalues.update({split_key: [(5, 0, [])]})\n\t\t\t\t\telif val == 'add':\n\t\t\t\t\t\tm2m_list = []\n\t\t\t\t\t\tfor m2m_id in vals.get(split_key, False)[0][2]:\n\t\t\t\t\t\t\tm2m_list.append((4, m2m_id))\n\t\t\t\t\t\tvalues.update({split_key: m2m_list})\n\t\t\tif values:\n\t\t\t\tmodel_obj.browse(self._context.get('active_ids')).write(values)\n\t\treturn super(MassEditingWizard, self).create({})\n\n\t@api.multi\n\tdef action_apply(self):\n\t\treturn {'type': 'ir.actions.act_window_close'}\n\n\tdef read(self, fields, load='_classic_read'):\n\t\t\"\"\" Without this call, dynamic fields build by fields_view_get()\n\t\t\tgenerate a log warning, i.e.:\n\t\t\todoo.models:mass.editing.wizard.read() with unknown field 'myfield'\n\t\t\todoo.models:mass.editing.wizard.read()\n\t\t\t\twith unknown field 'selection__myfield'\n\t\t\"\"\"\n\t\treal_fields = fields\n\t\tif fields:\n\t\t\t# We remove fields which are not in _fields\n\t\t\treal_fields = [x for x in fields if x in self._fields]\n\t\treturn super(MassEditingWizard, self).read(real_fields, load=load)\n", "description": " Without this call, dynamic fields build by fields_view_get()\n\t\t\tgenerate a log warning, i.e.:\n\t\t\todoo.models:mass.editing.wizard.read() with unknown field 'myfield'\n\t\t\todoo.models:mass.editing.wizard.read()\n\t\t\t\twith unknown field 'selection__myfield'\n\t\t", "category": "remove", "imports": ["from lxml import etree", "import odoo.tools as tools", "from odoo import api, models"]}], [], [], [{"term": "class", "name": "TransformEosDataset", "data": "class TransformEosDataset(FairseqDataset):\n\t\"\"\"A :class:`~fairseq.data.FairseqDataset` wrapper that appends/prepends/strips EOS.\n\n\tNote that the transformation is applied in :func:`collater`.\n\n\tArgs:\n\t\tdataset (~fairseq.data.FairseqDataset): dataset to wrap\n\t\teos (int): index of the end-of-sentence symbol\n\t\tappend_eos_to_src (bool, optional): append EOS to the end of src\n\t\tremove_eos_from_src (bool, optional): remove EOS from the end of src\n\t\tappend_eos_to_tgt (bool, optional): append EOS to the end of tgt\n\t\tremove_eos_from_tgt (bool, optional): remove EOS from the end of tgt\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tdataset,\n\t\teos,\n\t\tappend_eos_to_src=False,\n\t\tremove_eos_from_src=False,\n\t\tappend_eos_to_tgt=False,\n\t\tremove_eos_from_tgt=False,\n\t\thas_target=True,\n\t):\n\t\tif not isinstance(dataset, FairseqDataset):\n\t\t\traise ValueError(\"dataset must be an instance of FairseqDataset\")\n\t\tif append_eos_to_src and remove_eos_from_src:\n\t\t\traise ValueError(\"cannot combine append_eos_to_src and remove_eos_from_src\")\n\t\tif append_eos_to_tgt and remove_eos_from_tgt:\n\t\t\traise ValueError(\"cannot combine append_eos_to_tgt and remove_eos_from_tgt\")\n\n\t\tself.dataset = dataset\n\t\tself.eos = torch.LongTensor([eos])\n\t\tself.append_eos_to_src = append_eos_to_src\n\t\tself.remove_eos_from_src = remove_eos_from_src\n\t\tself.append_eos_to_tgt = append_eos_to_tgt\n\t\tself.remove_eos_from_tgt = remove_eos_from_tgt\n\t\tself.has_target = has_target\n\n\t\t# precompute how we should adjust the reported sizes\n\t\tself._src_delta = 0\n\t\tself._src_delta += 1 if append_eos_to_src else 0\n\t\tself._src_delta -= 1 if remove_eos_from_src else 0\n\t\tself._tgt_delta = 0\n\t\tself._tgt_delta += 1 if append_eos_to_tgt else 0\n\t\tself._tgt_delta -= 1 if remove_eos_from_tgt else 0\n\n\t\tself._checked_src = False\n\t\tself._checked_tgt = False\n\n\tdef _check_src(self, src, expect_eos):\n\t\tif not self._checked_src:\n\t\t\tassert (src[-1] == self.eos[0]) == expect_eos\n\t\t\tself._checked_src = True\n\n\tdef _check_tgt(self, tgt, expect_eos):\n\t\tif self.has_target and not self._checked_tgt:\n\t\t\tassert (tgt[-1] == self.eos[0]) == expect_eos\n\t\t\tself._checked_tgt = True\n\n\tdef __getitem__(self, index):\n\t\treturn self.dataset[index]\n\n\tdef __len__(self):\n\t\treturn len(self.dataset)\n\n\tdef collater(self, samples):\n\t\tdef transform(item):\n\t\t\tif self.append_eos_to_src:\n\t\t\t\tself.eos = self.eos.to(device=item[\"source\"].device)\n\t\t\t\tself._check_src(item[\"source\"], expect_eos=False)\n\t\t\t\titem[\"source\"] = torch.cat([item[\"source\"], self.eos])\n\t\t\tif self.remove_eos_from_src:\n\t\t\t\tself.eos = self.eos.to(device=item[\"source\"].device)\n\t\t\t\tself._check_src(item[\"source\"], expect_eos=True)\n\t\t\t\titem[\"source\"] = item[\"source\"][:-1]\n\t\t\tif self.append_eos_to_tgt:\n\t\t\t\tself.eos = self.eos.to(device=item[\"target\"].device)\n\t\t\t\tself._check_tgt(item[\"target\"], expect_eos=False)\n\t\t\t\titem[\"target\"] = torch.cat([item[\"target\"], self.eos])\n\t\t\tif self.remove_eos_from_tgt:\n\t\t\t\tself.eos = self.eos.to(device=item[\"target\"].device)\n\t\t\t\tself._check_tgt(item[\"target\"], expect_eos=True)\n\t\t\t\titem[\"target\"] = item[\"target\"][:-1]\n\t\t\treturn item\n\n\t\tsamples = list(map(transform, samples))\n\t\treturn self.dataset.collater(samples)\n\n\tdef num_tokens(self, index):\n\t\treturn self.dataset.num_tokens(index)\n\n\tdef size(self, index):\n\t\tif self.has_target:\n\t\t\tsrc_len, tgt_len = self.dataset.size(index)\n\t\t\treturn (src_len + self._src_delta, tgt_len + self._tgt_delta)\n\t\telse:\n\t\t\treturn self.dataset.size(index)\n\n\tdef ordered_indices(self):\n\t\t# NOTE: we assume that the ordering does not change based on the\n\t\t# addition or removal of eos\n\t\treturn self.dataset.ordered_indices()\n\n\t@property\n\tdef supports_prefetch(self):\n\t\treturn getattr(self.dataset, \"supports_prefetch\", False)\n\n\tdef prefetch(self, indices):\n\t\treturn self.dataset.prefetch(indices)\n", "description": "A :class:`~fairseq.data.FairseqDataset` wrapper that appends/prepends/strips EOS.\n\n\tNote that the transformation is applied in :func:`collater`.\n\n\tArgs:\n\t\tdataset (~fairseq.data.FairseqDataset): dataset to wrap\n\t\teos (int): index of the end-of-sentence symbol\n\t\tappend_eos_to_src (bool, optional): append EOS to the end of src\n\t\tremove_eos_from_src (bool, optional): remove EOS from the end of src\n\t\tappend_eos_to_tgt (bool, optional): append EOS to the end of tgt\n\t\tremove_eos_from_tgt (bool, optional): remove EOS from the end of tgt\n\t", "category": "remove", "imports": ["import torch", "from . import FairseqDataset"]}], [{"term": "def", "name": "callback", "data": "def callback(ch, method, properties, body):\n\tchain = str(body).split(\",\")\n\temail_not_remove = chain[0]\n\tphone_not_remove = chain[1]\n\tmessage_not_remove = chain[2]\n\temail_remove = email_not_remove.translate({ord(i): None for i in \"b'\"})\n\tphone_remove =  phone_not_remove.translate({ord(i): None for i in \"'\"})\n\tmessage_remove =  message_not_remove.translate({ord(i): None for i in \"'\"})\n\tnotification_manager = Notification_manager(email_remove,phone_remove,message_remove)\n\tnotification_manager.sms_ad()\n", "description": null, "category": "remove", "imports": ["import pika", "from notification_manager import Notification_manager", "import os"]}], [{"term": "class", "name": "RemoveContactNotificationProtocolEntity", "data": "class RemoveContactNotificationProtocolEntity(ContactNotificationProtocolEntity):\n\t'''\n\t\n \n\n\t'''\n\n\tdef __init__(self, _id,  _from, timestamp, notify, offline, contactJid):\n\t\tsuper(RemoveContactNotificationProtocolEntity, self).__init__(_id, _from, timestamp, notify, offline)\n\t\tself.setData(contactJid)\n\n\tdef setData(self, jid):\n\t\tself.contactJid = jid\n\n\tdef toProtocolTreeNode(self):\n\t\tnode = super(RemoveContactNotificationProtocolEntity, self).toProtocolTreeNode()\n\t\tremoveNode = ProtocolTreeNode(\"remove\", {\"jid\": self.contactJid}, None, None)\n\t\tnode.addChild(removeNode)\n\t\treturn node\n\n\t@staticmethod\n\tdef fromProtocolTreeNode(node):\n\t\tentity = ContactNotificationProtocolEntity.fromProtocolTreeNode(node)\n\t\tentity.__class__ = RemoveContactNotificationProtocolEntity\n\t\tremoveNode = node.getChild(\"remove\")\n\t\tentity.setData(removeNode.getAttributeValue(\"jid\"))\n", "description": null, "category": "remove", "imports": ["from yowsup.structs import ProtocolTreeNode", "from .notification_contact import ContactNotificationProtocolEntity"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('ral', '0005_ituser_lastname'),\n\t]\n\n\toperations = [\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='bio',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='company_description',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='company_size',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='contact_links',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='extra_info',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='github_link',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='industry',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='institution_name',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='is_company',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='languages',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='linkedin_link',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='memberships',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='other_website_links',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='profile_image',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='resume',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='undergraduate_year',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='university',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='verified',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='volunteering_experience',\n\t\t),\n\t\tmigrations.RemoveField(\n\t\t\tmodel_name='ituser',\n\t\t\tname='work_experience',\n\t\t),\n\t]\n", "description": null, "category": "remove", "imports": ["from django.db import migrations"]}], [], [], [], [], [], [], [], [], [], [], [], [], [{"term": "class", "name": "ReplayBuffer", "data": "class ReplayBuffer(object):\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "__init__", "data": "  def __init__(self, max_size):\n\tself.max_size = max_size\n\tself.cur_size = 0\n\tself.buffer = {}\n\tself.init_length = 0\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "__len__", "data": "  def __len__(self):\n\treturn self.cur_size\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "seed_buffer", "data": "  def seed_buffer(self, episodes):\n\tself.init_length = len(episodes)\n\tself.add(episodes, np.ones(self.init_length))\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "add", "data": "  def add(self, episodes, *args):\n\t\"\"\"Add episodes to buffer.\"\"\"\n\tidx = 0\n\twhile self.cur_size < self.max_size and idx < len(episodes):\n\t  self.buffer[self.cur_size] = episodes[idx]\n\t  self.cur_size += 1\n\t  idx += 1\n\n\tif idx < len(episodes):\n\t  remove_idxs = self.remove_n(len(episodes) - idx)\n\t  for remove_idx in remove_idxs:\n\t\tself.buffer[remove_idx] = episodes[idx]\n\t\tidx += 1\n\n\tassert len(self.buffer) == self.cur_size\n", "description": "Add episodes to buffer.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "remove_n", "data": "  def remove_n(self, n):\n\t\"\"\"Get n items for removal.\"\"\"\n\t# random removal\n\tidxs = random.sample(xrange(self.init_length, self.cur_size), n)\n\treturn idxs\n", "description": "Get n items for removal.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "get_batch", "data": "  def get_batch(self, n):\n\t\"\"\"Get batch of episodes to train on.\"\"\"\n\t# random batch\n\tidxs = random.sample(xrange(self.cur_size), n)\n\treturn [self.buffer[idx] for idx in idxs], None\n", "description": "Get batch of episodes to train on.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "update_last_batch", "data": "  def update_last_batch(self, delta):\n\tpass\n\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "class", "name": "PrioritizedReplayBuffer", "data": "class PrioritizedReplayBuffer(ReplayBuffer):\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "__init__", "data": "  def __init__(self, max_size, alpha=0.2,\n\t\t\t   eviction_strategy='rand'):\n\tself.max_size = max_size\n\tself.alpha = alpha\n\tself.eviction_strategy = eviction_strategy\n\tassert self.eviction_strategy in ['rand', 'fifo', 'rank']\n\tself.remove_idx = 0\n\n\tself.cur_size = 0\n\tself.buffer = {}\n\tself.priorities = np.zeros(self.max_size)\n\tself.init_length = 0\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "__len__", "data": "  def __len__(self):\n\treturn self.cur_size\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "add", "data": "  def add(self, episodes, priorities, new_idxs=None):\n\t\"\"\"Add episodes to buffer.\"\"\"\n\tif new_idxs is None:\n\t  idx = 0\n\t  new_idxs = []\n\t  while self.cur_size < self.max_size and idx < len(episodes):\n\t\tself.buffer[self.cur_size] = episodes[idx]\n\t\tnew_idxs.append(self.cur_size)\n\t\tself.cur_size += 1\n\t\tidx += 1\n\n\t  if idx < len(episodes):\n\t\tremove_idxs = self.remove_n(len(episodes) - idx)\n\t\tfor remove_idx in remove_idxs:\n\t\t  self.buffer[remove_idx] = episodes[idx]\n\t\t  new_idxs.append(remove_idx)\n\t\t  idx += 1\n\telse:\n\t  assert len(new_idxs) == len(episodes)\n\t  for new_idx, ep in zip(new_idxs, episodes):\n\t\tself.buffer[new_idx] = ep\n\n\tself.priorities[new_idxs] = priorities\n\tself.priorities[0:self.init_length] = np.max(\n\t\tself.priorities[self.init_length:])\n\n\tassert len(self.buffer) == self.cur_size\n\treturn new_idxs\n", "description": "Add episodes to buffer.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "remove_n", "data": "  def remove_n(self, n):\n\t\"\"\"Get n items for removal.\"\"\"\n\tassert self.init_length + n <= self.cur_size\n\n\tif self.eviction_strategy == 'rand':\n\t  # random removal\n\t  idxs = random.sample(xrange(self.init_length, self.cur_size), n)\n\telif self.eviction_strategy == 'fifo':\n\t  # overwrite elements in cyclical fashion\n\t  idxs = [\n\t\t  self.init_length +\n\t\t  (self.remove_idx + i) % (self.max_size - self.init_length)\n\t\t  for i in xrange(n)]\n\t  self.remove_idx = idxs[-1] + 1 - self.init_length\n\telif self.eviction_strategy == 'rank':\n\t  # remove lowest-priority indices\n\t  idxs = np.argpartition(self.priorities, n)[:n]\n\n\treturn idxs\n", "description": "Get n items for removal.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "sampling_distribution", "data": "  def sampling_distribution(self):\n\tp = self.priorities[:self.cur_size]\n\tp = np.exp(self.alpha * (p - np.max(p)))\n\tnorm = np.sum(p)\n\tif norm > 0:\n\t  uniform = 0.0\n\t  p = p / norm * (1 - uniform) + 1.0 / self.cur_size * uniform\n\telse:\n\t  p = np.ones(self.cur_size) / self.cur_size\n\treturn p\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "get_batch", "data": "  def get_batch(self, n):\n\t\"\"\"Get batch of episodes to train on.\"\"\"\n\tp = self.sampling_distribution()\n\tidxs = np.random.choice(self.cur_size, size=int(n), replace=False, p=p)\n\tself.last_batch = idxs\n\treturn [self.buffer[idx] for idx in idxs], p[idxs]\n", "description": "Get batch of episodes to train on.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "update_last_batch", "data": "  def update_last_batch(self, delta):\n\t\"\"\"Update last batch idxs with new priority.\"\"\"\n\tself.priorities[self.last_batch] = np.abs(delta)\n\tself.priorities[0:self.init_length] = np.max(\n\t\tself.priorities[self.init_length:])\n", "description": "Update last batch idxs with new priority.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}], [{"term": "class", "name": "ReplayBuffer", "data": "class ReplayBuffer(object):\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "__init__", "data": "  def __init__(self, max_size):\n\tself.max_size = max_size\n\tself.cur_size = 0\n\tself.buffer = {}\n\tself.init_length = 0\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "__len__", "data": "  def __len__(self):\n\treturn self.cur_size\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "seed_buffer", "data": "  def seed_buffer(self, episodes):\n\tself.init_length = len(episodes)\n\tself.add(episodes, np.ones(self.init_length))\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "add", "data": "  def add(self, episodes, *args):\n\t\"\"\"Add episodes to buffer.\"\"\"\n\tidx = 0\n\twhile self.cur_size < self.max_size and idx < len(episodes):\n\t  self.buffer[self.cur_size] = episodes[idx]\n\t  self.cur_size += 1\n\t  idx += 1\n\n\tif idx < len(episodes):\n\t  remove_idxs = self.remove_n(len(episodes) - idx)\n\t  for remove_idx in remove_idxs:\n\t\tself.buffer[remove_idx] = episodes[idx]\n\t\tidx += 1\n\n\tassert len(self.buffer) == self.cur_size\n", "description": "Add episodes to buffer.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "remove_n", "data": "  def remove_n(self, n):\n\t\"\"\"Get n items for removal.\"\"\"\n\t# random removal\n\tidxs = random.sample(xrange(self.init_length, self.cur_size), n)\n\treturn idxs\n", "description": "Get n items for removal.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "get_batch", "data": "  def get_batch(self, n):\n\t\"\"\"Get batch of episodes to train on.\"\"\"\n\t# random batch\n\tidxs = random.sample(xrange(self.cur_size), n)\n\treturn [self.buffer[idx] for idx in idxs], None\n", "description": "Get batch of episodes to train on.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "update_last_batch", "data": "  def update_last_batch(self, delta):\n\tpass\n\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "class", "name": "PrioritizedReplayBuffer", "data": "class PrioritizedReplayBuffer(ReplayBuffer):\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "__init__", "data": "  def __init__(self, max_size, alpha=0.2,\n\t\t\t   eviction_strategy='rand'):\n\tself.max_size = max_size\n\tself.alpha = alpha\n\tself.eviction_strategy = eviction_strategy\n\tassert self.eviction_strategy in ['rand', 'fifo', 'rank']\n\tself.remove_idx = 0\n\n\tself.cur_size = 0\n\tself.buffer = {}\n\tself.priorities = np.zeros(self.max_size)\n\tself.init_length = 0\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "__len__", "data": "  def __len__(self):\n\treturn self.cur_size\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "add", "data": "  def add(self, episodes, priorities, new_idxs=None):\n\t\"\"\"Add episodes to buffer.\"\"\"\n\tif new_idxs is None:\n\t  idx = 0\n\t  new_idxs = []\n\t  while self.cur_size < self.max_size and idx < len(episodes):\n\t\tself.buffer[self.cur_size] = episodes[idx]\n\t\tnew_idxs.append(self.cur_size)\n\t\tself.cur_size += 1\n\t\tidx += 1\n\n\t  if idx < len(episodes):\n\t\tremove_idxs = self.remove_n(len(episodes) - idx)\n\t\tfor remove_idx in remove_idxs:\n\t\t  self.buffer[remove_idx] = episodes[idx]\n\t\t  new_idxs.append(remove_idx)\n\t\t  idx += 1\n\telse:\n\t  assert len(new_idxs) == len(episodes)\n\t  for new_idx, ep in zip(new_idxs, episodes):\n\t\tself.buffer[new_idx] = ep\n\n\tself.priorities[new_idxs] = priorities\n\tself.priorities[0:self.init_length] = np.max(\n\t\tself.priorities[self.init_length:])\n\n\tassert len(self.buffer) == self.cur_size\n\treturn new_idxs\n", "description": "Add episodes to buffer.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "remove_n", "data": "  def remove_n(self, n):\n\t\"\"\"Get n items for removal.\"\"\"\n\tassert self.init_length + n <= self.cur_size\n\n\tif self.eviction_strategy == 'rand':\n\t  # random removal\n\t  idxs = random.sample(xrange(self.init_length, self.cur_size), n)\n\telif self.eviction_strategy == 'fifo':\n\t  # overwrite elements in cyclical fashion\n\t  idxs = [\n\t\t  self.init_length +\n\t\t  (self.remove_idx + i) % (self.max_size - self.init_length)\n\t\t  for i in xrange(n)]\n\t  self.remove_idx = idxs[-1] + 1 - self.init_length\n\telif self.eviction_strategy == 'rank':\n\t  # remove lowest-priority indices\n\t  idxs = np.argpartition(self.priorities, n)[:n]\n\n\treturn idxs\n", "description": "Get n items for removal.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "sampling_distribution", "data": "  def sampling_distribution(self):\n\tp = self.priorities[:self.cur_size]\n\tp = np.exp(self.alpha * (p - np.max(p)))\n\tnorm = np.sum(p)\n\tif norm > 0:\n\t  uniform = 0.0\n\t  p = p / norm * (1 - uniform) + 1.0 / self.cur_size * uniform\n\telse:\n\t  p = np.ones(self.cur_size) / self.cur_size\n\treturn p\n", "description": null, "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "get_batch", "data": "  def get_batch(self, n):\n\t\"\"\"Get batch of episodes to train on.\"\"\"\n\tp = self.sampling_distribution()\n\tidxs = np.random.choice(self.cur_size, size=int(n), replace=False, p=p)\n\tself.last_batch = idxs\n\treturn [self.buffer[idx] for idx in idxs], p[idxs]\n", "description": "Get batch of episodes to train on.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}, {"term": "def", "name": "update_last_batch", "data": "  def update_last_batch(self, delta):\n\t\"\"\"Update last batch idxs with new priority.\"\"\"\n\tself.priorities[self.last_batch] = np.abs(delta)\n\tself.priorities[0:self.init_length] = np.max(\n\t\tself.priorities[self.init_length:])\n", "description": "Update last batch idxs with new priority.", "category": "remove", "imports": ["import random", "import numpy as np", "from six.moves import xrange"]}], [], [], [], [], [], [], [], [], [], []]