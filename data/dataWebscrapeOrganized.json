[[{"term": "def", "name": "get_cursor", "data": "def get_cursor(file_name):\n\t\"\"\" Connects and returns a cursor to an sqlite output file\n\n\tParameters\n\t----------\n\tfile_name: str\n\t\tname of the sqlite file\n\n\tReturns\n\t-------\n\tsqlite cursor\n\t\"\"\"\n\tcon = sql.connect(file_name)\n\tcon.row_factory = sql.Row\n\treturn con.cursor()\n\n", "description": " Connects and returns a cursor to an sqlite output file\n\n\tParameters\n\t----------\n\tfile_name: str\n\t\tname of the sqlite file\n\n\tReturns\n\t-------\n\tsqlite cursor\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "import_pris", "data": "def import_pris(pris_link):\n\t\"\"\" Opens pris_csv using Pandas. Adds Latitude and Longitude\n\tcolumns\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\n\tReturns\n\t-------\n\tpris: pd.Dataframe\n\t\tpris database\n\t\"\"\"\n\tpris = pd.read_csv(pris_link,\n\t\t\t\t\t   delimiter=',',\n\t\t\t\t\t   encoding='iso-8859-1'\n\t\t\t\t\t   )\n\tpris.insert(13, 'Latitude', np.nan)\n\tpris.insert(14, 'Longitude', np.nan)\n\tpris = pris.replace(np.nan, '')\n\treturn pris\n\n", "description": " Opens pris_csv using Pandas. Adds Latitude and Longitude\n\tcolumns\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\n\tReturns\n\t-------\n\tpris: pd.Dataframe\n\t\tpris database\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "import_webscrape_data", "data": "def import_webscrape_data(scrape_link):\n\t\"\"\" Returns sqlite content of webscrape by performing an\n\tsqlite query\n\n\tParameters\n\t----------\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tcoords: sqlite cursor\n\t\tsqlite cursor containing webscrape data\n\t\"\"\"\n\tcur = get_cursor(scrape_link)\n\tcoords = cur.execute(\"SELECT name, long, lat FROM reactors_coordinates\")\n\treturn coords\n\n", "description": " Returns sqlite content of webscrape by performing an\n\tsqlite query\n\n\tParameters\n\t----------\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tcoords: sqlite cursor\n\t\tsqlite cursor containing webscrape data\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "get_edge_cases", "data": "def get_edge_cases():\n\t\"\"\" Returns a dictionary of edge cases that fuzzywuzzy is\n\tunable to catch. This could be because PRIS database stores\n\treactor names and Webscrape database fetches power plant names,\n\tor because PRIS reactor names are abbreviated.\n\n\tParameters\n\t----------\n\n\tReturns\n\t-------\n\tothers: dict\n\t\tdictionary of edge cases with \"key=pris_reactor_name, and\n\t\tvalue=webscrape_plant_name\"\n\t\"\"\"\n\tothers = {'OHI-': '\u014ci',\n\t\t\t  'ASCO-': 'Asc\u00f3',\n\t\t\t  'ROVNO-': 'Rivne',\n\t\t\t  'SHIN-KORI-': 'Kori',\n\t\t\t  'ANO-': 'Arkansas One',\n\t\t\t  'HANBIT-': 'Yeonggwang',\n\t\t\t  'FERMI-': 'Enrico Fermi',\n\t\t\t  'BALTIC-': 'Kaliningrad',\n\t\t\t  'COOK-': 'Donald C. Cook',\n\t\t\t  'HATCH-': 'Edwin I. Hatch',\n\t\t\t  'HARRIS-': 'Shearon Harris',\n\t\t\t  'SHIN-WOLSONG-': 'Wolseong',\n\t\t\t  'ST. ALBAN-': 'Saint-Alban',\n\t\t\t  'LASALLE-': 'LaSalle County',\n\t\t\t  'SUMMER-': 'Virgil C. Summer',\n\t\t\t  'FARLEY-': 'Joseph M. Farley',\n\t\t\t  'ST. LAURENT ': 'Saint-Laurent',\n\t\t\t  'HADDAM NECK': 'Connecticut Yankee',\n\t\t\t  'HIGASHI DORI-1 (TOHOKU)': 'Higashid\u014dri',\n\t\t\t  }\n\treturn others\n\n", "description": " Returns a dictionary of edge cases that fuzzywuzzy is\n\tunable to catch. This could be because PRIS database stores\n\treactor names and Webscrape database fetches power plant names,\n\tor because PRIS reactor names are abbreviated.\n\n\tParameters\n\t----------\n\n\tReturns\n\t-------\n\tothers: dict\n\t\tdictionary of edge cases with \"key=pris_reactor_name, and\n\t\tvalue=webscrape_plant_name\"\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "sanitize_webscrape_name", "data": "def sanitize_webscrape_name(name):\n\t\"\"\" Sanitizes webscrape powerplant names by removing unwanted\n\tstrings (listed in blacklist), applying lower case, and deleting\n\ttrailing whitespace.\n\n\tParameters\n\t----------\n\tname: str\n\t\twebscrape plant name\n\n\tReturns\n\t-------\n\tname: str\n\t\tsanitized name for use with fuzzywuzzy\n\t\"\"\"\n\tblacklist = ['nuclear', 'power',\n\t\t\t\t 'plant', 'generating',\n\t\t\t\t 'station', 'reactor', 'atomic',\n\t\t\t\t 'energy', 'center', 'electric']\n\tname = name.lower()\n\tfor blacklisted in blacklist:\n\t\tname = name.replace(blacklisted, '')\n\tname = name.strip()\n\tname = ' '.join(name.split())\n\treturn name\n\n", "description": " Sanitizes webscrape powerplant names by removing unwanted\n\tstrings (listed in blacklist), applying lower case, and deleting\n\ttrailing whitespace.\n\n\tParameters\n\t----------\n\tname: str\n\t\twebscrape plant name\n\n\tReturns\n\t-------\n\tname: str\n\t\tsanitized name for use with fuzzywuzzy\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "merge_coordinates", "data": "def merge_coordinates(pris_link, scrape_link):\n\t\"\"\" Merges webscrape data with pris data performed by string\n\tcomparison of reactor names from pris and webscrape. Returns\n\tupdated pris database with coordinates.\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\t\"\"\"\n\tothers = get_edge_cases()\n\tpris = import_pris(pris_link)\n\tcoords = import_webscrape_data(scrape_link)\n\tfor web in coords:\n\t\tfor idx, prs in pris.iterrows():\n\t\t\twebscrape_name = sanitize_webscrape_name(web['name'])\n\t\t\tpris_name = prs[1].lower()\n\t\t\tif fuzz.ratio(webscrape_name, pris_name) > 64:\n\t\t\t\tprs[13] = web['lat']\n\t\t\t\tprs[14] = web['long']\n\t\t\telse:\n\t\t\t\tfor other in others.keys():\n\t\t\t\t\tedge_case_key = other.lower()\n\t\t\t\t\tedge_case_value = others[other].lower()\n\t\t\t\t\tif (fuzz.ratio(pris_name, edge_case_key) > 80 and\n\t\t\t\t\t\t\tfuzz.ratio(webscrape_name, edge_case_value) > 75):\n\t\t\t\t\t\tprs[13] = web['lat']\n\t\t\t\t\t\tprs[14] = web['long']\n\treturn pris\n\n", "description": " Merges webscrape data with pris data performed by string\n\tcomparison of reactor names from pris and webscrape. Returns\n\tupdated pris database with coordinates.\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "save_output", "data": "def save_output(pris):\n\t\"\"\" Saves updated PRIS database as 'reactors_pris_2016.csv'\n\n\tParameters\n\t----------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\n\tReturns\n\t-------\n\n\t\"\"\"\n\tpris.to_csv('reactors_pris_2016.csv',\n\t\t\t\tindex=False,\n\t\t\t\tsep=',',\n\t\t\t\t)\n\n", "description": " Saves updated PRIS database as 'reactors_pris_2016.csv'\n\n\tParameters\n\t----------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\n\tReturns\n\t-------\n\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "main", "data": "def main(pris_link, scrape_link):\n\t\"\"\" Calls all required functions to merge PRIS and webscrape\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\n\t\"\"\"\n\tpris = merge_coordinates(pris_link, scrape_link)\n\tsave_output(pris)\n\n", "description": " Calls all required functions to merge PRIS and webscrape\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}], [], [], [], [{"term": "def", "name": "fetch_dataframe", "data": "def fetch_dataframe() -> pd.DataFrame():\n\t\"\"\"Returns the dataframe accessed on SQL\n\n\tArgs:\n\n\tReturns:\n\t\t(pd.DataFrame) : Tweets dataframe from host.engine\n\t\"\"\"\n\ttry:\n\t\tdf = pd.read_csv('tweets.csv', sep='\\t', names=[\"id\", \"user_id\", \"username\", \"date\", \"tweet\"])\n\texcept:\n\t\tdf = pd.DataFrame()\n\treturn df\n\n", "description": "Returns the dataframe accessed on SQL\n\n\tArgs:\n\n\tReturns:\n\t\t(pd.DataFrame) : Tweets dataframe from host.engine\n\t", "category": "webscraping", "imports": ["import twint", "import pandas as pd", "from pytz import timezone", "from datetime import datetime as dt", "from datetime import timedelta", "from src.topic_search import topic_for_search"]}, {"term": "class", "name": "classClassTwitterScrape:", "data": "class ClassTwitterScrape:\n\tdef __init__(self):\n\t\tself.webscrape()\n\n\tdef webscrape(self) -> None:\n\t\t\"\"\"Webscrapes\n\n\t\tArgs:\n\t\t\tNone\n\n\t\tReturns:\n\t\t\tscrape_df (pd.DataFrame): scraped dataframe with keep columns\n\t\t\"\"\"\n\n\t\t# create the config\n\t\tc = twint.Config()\n\n\t\t# defining the topic to be searched\n\t\tc.Search = topic_for_search\n\n\t\t# Time\n\t\teu_paris = timezone(\"Europe/Paris\")  # set Paris timezone\n\t\tweek_ago = (dt.now(eu_paris) - timedelta(days=7)).strftime(\n\t\t\t\"%Y-%m-%d\"\n\t\t)  # today minus 7 days\n\n\t\tc.Since = week_ago  # researches up to a week ago until now\n\n\t\tc.Limit = 1000  # number of Tweets to scrape\n\t\tc.Lang = \"fr\"  # search for french text\n\t\tc.Pandas = True\n\t\tc.Hide_output = True\n\n\t\ttwint.run.Search(c)\n\n\t\twebscrape_df = twint.output.panda.Tweets_df\n\t\twebscrape_df['date'] = pd.to_datetime(webscrape_df['date'])\n\t\twebscrape_df['tweet'] = webscrape_df['tweet'].map(lambda x: x.encode('unicode-escape').decode('utf-8')) # TODO: fix the accents and french language stuff\n\t\twebscrape_df['username'] = webscrape_df['username'].map(lambda x: x.encode('unicode-escape').decode('utf-8'))\n\t\tkeep_columns = [\"id\", \"user_id\", \"username\", \"date\", \"tweet\"]\n\n\t\twebscrape_df = webscrape_df[keep_columns]\n\t\tfile_name = open('tweets.csv', 'a+')\n\t\twebscrape_df.to_csv(file_name, sep='\\t', header=False)\n\n\n", "description": "Webscrapes\n\n\t\tArgs:\n\t\t\tNone\n\n\t\tReturns:\n\t\t\tscrape_df (pd.DataFrame): scraped dataframe with keep columns\n\t\t", "category": "webscraping", "imports": ["import twint", "import pandas as pd", "from pytz import timezone", "from datetime import datetime as dt", "from datetime import timedelta", "from src.topic_search import topic_for_search"]}], [{"term": "def", "name": "main", "data": "def main():\n\t# run therapist_webscrape_basic.py\n\ttherapist_webscrape_basic.main()\n\n\t# run psy_webscrape_basic.py\n\tpsy_webscrape_basic.main()\n\n\t# run therapist_webscrape_moreinfo.py\n\ttherapist_webscrape_moreinfo.main()\n\n\t# run psy_webscrape_moreinfo.py\n\tpsy_webscrape_moreinfo.main()\n", "description": null, "category": "webscraping", "imports": ["import therapist_webscrape_basic", "import therapist_webscrape_moreinfo", "import psy_webscrape_basic", "import psy_webscrape_moreinfo"]}], [], [{"term": "def", "name": "main", "data": "def main():\n\tdata_config = json.load(open('config/data-params.json'))\n\tmodel_config = json.load(open('config/model-params.json'))\n\tweight_config = json.load(open('config/weight-params.json'))\n\twebscrape_config = json.load(open('config/webscrape-params.json'))\n\twebsite_config = json.load(open('config/website-params.json'))\n\treport_config = json.load(open('config/report-params.json'))\n\ttest_config = json.load(open('config/test-params.json'))\n\n\tos.system('git submodule update --init')\n\t\n\t# Getting the target\n\t# If no target is given, then run 'website'\n\tif len(sys.argv) == 1:\n\t\ttargets = 'website'\n\telse:\n\t\ttargets = sys.argv[1]\n\t\t\n\tif 'data' in targets:\n\t\tconvert_txt(**data_config)\n\tif 'autophrase' in targets:\n\t\tautophrase(data_config['outdir'], data_config['pdfname'], model_config['outdir'], model_config['filename'])\n\tif 'weight' in targets:\n\t\ttry:\n\t\t\tunique_key = '_' + sys.argv[2]\n\t\t\tchange_weight(unique_key, **weight_config)\n\t\texcept:\n\t\t\tchange_weight(unique_key='', **weight_config)\n\tif 'webscrape' in targets:\n\t\ttry:\n\t\t\tunique_key = '_' +  sys.argv[2]\n\t\t\twebscrape(unique_key, **webscrape_config)\n\t\texcept:\n\t\t\twebscrape(unique_key='', **webscrape_config)\n\tif 'report' in targets:\n\t\tconvert_report(report_config['experiment_in_path'], report_config['experiment_out_path'])\n\t\tconvert_report(report_config['analysis_in_path'], report_config['analysis_out_path'])\n\tif 'website' in targets:\n\t\tactivate_website(**website_config)\n\tif 'test' in targets:\n\t\tconvert_txt(test_config['indir'], data_config['outdir'], test_config['pdfname'],)\n\t\tautophrase(data_config['outdir'], test_config['pdfname'], model_config['outdir'], model_config['filename'])\n\t\tchange_weight(unique_key='', **weight_config)\n\t\twebscrape(unique_key='', **webscrape_config)\n\t\tconvert_report(report_config['experiment_in_path'], report_config['experiment_out_path'])\n\t\tconvert_report(report_config['analysis_in_path'], report_config['analysis_out_path'])\n\treturn\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import json", "from etl import convert_txt", "from model import autophrase", "from weight_phrases import change_weight", "from webscrape import webscrape", "from website import activate_website", "from utils import convert_report"]}], [{"term": "class", "name": "classparameters:", "data": "class parameters:\n\tseason_year = '2022'\n\tteam_array = [\n\t\t'BOS', 'BRK', 'TOR', 'PHI',\n\t\t'NYK', 'MIN', 'OKC', 'DEN',\n\t\t'POR', 'UTA', 'CHI', 'CLE',\n\t\t'DET', 'IND', 'MIL', 'LAL',\n\t\t'LAC', 'GSW', 'PHO', 'SAC',\n\t\t'WAS', 'ATL', 'ORL', 'MIA',\n\t\t'CHO', 'MEM', 'NOP', 'HOU',\n\t\t'SAS', 'DAL'\n\t]\n\toffence_variables_for_average = [\n\t\t'FG', '3P', '2P',\n\t\t'FT', 'ORB'\n\t]\n\tdefence_variables_for_average = [\n\t\t'STL', 'BLK', 'DRB',\n\t\t'PF', 'TOV'\n\t]\n\tteam_name_abbrevations = {\n\t'Celtics':'BOS',\n\t'Nets':'BRK',\n\t'Raptors':'TOR',\n\t'76ers':'PHI',\n\t'Knicks':'NYK',\n\t'Timberwolves':'MIN',\n\t'Thunder':'OKC',\n\t'Nuggets':'DEN',\n\t'Trail Blazers':'POR',\n\t'Jazz':'UTA',\n\t'Bulls':'CHI',\n\t'Cavaliers':'CLE',\n\t'Pistons':'DET',\n\t'Pacers':'IND',\n\t'Bucks':'MIL',\n\t'Lakers':'LAL',\n\t'Clippers':'LAC',\n\t'Warriors':'GSW',\n\t'Suns':'PHO',\n\t'Kings':'SAC',\n\t'Wizards':'WAS',\n\t'Hawks':'ATL',\n\t'Magic':'ORL',\n\t'Heat':'MIA',\n\t'Hornets':'CHO',\n\t'Grizzlies':'MEM',\n\t'Pelicans':'NOP',\n\t'Rockets':'HOU',\n\t'Spurs':'SAS',\n\t'Mavericks':'DAL'\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from utils import *"]}, {"term": "class", "name": "classoffence_defence_functions:", "data": "class offence_defence_functions:\n\t@staticmethod\n\tdef offence_dataset(df):\n\t\tdf = df.loc[0:12, parameters.offence_variables_for_average]\n\t\treturn df\n\t@staticmethod\n\tdef defence_dataset(df):\n\t\tdf = df.loc[0:12, parameters.defence_variables_for_average]\n\t\treturn df\n\t@staticmethod\n\tdef offence_function(FG, ThreeP, TwoP, FT, ORB):\n\t\tfinal_result = (0.1 * FG) + (0.5 * ThreeP) + (0.25 * TwoP) + \\\n\t\t\t\t\t   + (0.1 * FT) + (0.05 * ORB)\n\t\treturn final_result\n\t@staticmethod\n\tdef defence_function(Stl, Blk, Drb, Pf, Tov):\n\t\tfinal_result = (0.2 * Stl) + (0.1 * Blk) + \\\n\t\t\t\t\t   (0.1 * Drb) - ((0.3 * Pf) + (0.3 * Tov))\n\t\treturn final_result\n\t@staticmethod\n\tdef offence_defence_combo(team_number):\n\t\ttotal_team_score = offence_defence_functions.offence_function(\n\t\t\tnormalization(team_dataset_offence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t  parameters.offence_variables_for_average[0]),\n\t\t\tnormalization(team_dataset_offence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t  parameters.offence_variables_for_average[1]),\n\t\t\tnormalization(team_dataset_offence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t  parameters.offence_variables_for_average[2]),\n\t\t\tnormalization(team_dataset_offence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t  parameters.offence_variables_for_average[3]),\n\t\t\tnormalization(team_dataset_offence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t  parameters.offence_variables_for_average[4])) + \\\n\t\t\t\t\t\t   offence_defence_functions.defence_function(\n\t\t\t\t\t\t\t   normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t\t\t\t\t\t parameters.defence_variables_for_average[0]),\n\t\t\t\t\t\t\t   normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t\t\t\t\t\t parameters.defence_variables_for_average[1]),\n\t\t\t\t\t\t\t   normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t\t\t\t\t\t parameters.defence_variables_for_average[2]),\n\t\t\t\t\t\t\t   normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t\t\t\t\t\t parameters.defence_variables_for_average[3]),\n\t\t\t\t\t\t\t   normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n\t\t\t\t\t\t\t\t\t\t\t parameters.defence_variables_for_average[4]))\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from utils import *"]}, {"term": "class", "name": "classwebscrape_functions:", "data": "class webscrape_functions:\n\ttodays_date = date.today().strftime(\"%Y%m%d\")\n\trandom_date_for_testing = '20211013'\n\tSchedule = pd.read_html('https://www.cbssports.com/nba/scoreboard/' + todays_date + '/')\n\tSchedule.pop(15)\n\tdef webscrape_dataset_function(team_array, season_year):\n\t\tdf = pd.read_html(\n\t\t\t'https://www.basketball-reference.com/teams/' + team_array + '/' + season_year + '.html#advanced')\n\t\tteam_table_number = 1\n\t\tdf = pd.DataFrame(df[team_table_number])\n\t\tdf = injury_function(df)\n\t\tdf = df.loc[0:12, ['FG', '3P', '3PA', '2P', '2PA', 'FT', 'FTA', 'ORB',\n\t\t\t\t\t\t   'DRB', 'STL', 'BLK', 'PF', 'TOV']].fillna(0)\n\t\treturn df\n\t@staticmethod\n\tdef concat_schedule_teams():\n\t\ttodays_scheduled_teams = list()\n\t\tappend_scheduled_teams = list()\n\t\tfor i in range(0, len(webscrape_functions.Schedule)):\n\t\t\tif i % 3 == 0:\n\t\t\t\ttodays_scheduled_teams.append(webscrape_functions.Schedule[i])\n\t\tfor k in range(0, len(todays_scheduled_teams)):\n\t\t\tappend_scheduled_teams.append(todays_scheduled_teams[k].T.iloc[0])\n\t\ttodays_scheduled_teams_df = pd.DataFrame(append_scheduled_teams)\n\t\ttodays_scheduled_teams_df.columns = ['Away', 'Home']\n\t\ttodays_scheduled_teams_df_away_no_numbers = pd.DataFrame(\n\t\t\ttodays_scheduled_teams_df['Away'].str[:-3]).reset_index(drop=True)\n\t\ttodays_scheduled_teams_df_home_no_numbers = pd.DataFrame(\n\t\t\ttodays_scheduled_teams_df['Home'].str[:-3]).reset_index(drop=True)\n\t\tfinal_schedule_df = pd.merge(todays_scheduled_teams_df_away_no_numbers['Away'].map(parameters.team_name_abbrevations),\n\t\t\t\t\t\t\t\t\t todays_scheduled_teams_df_home_no_numbers['Home'].map(parameters.team_name_abbrevations),\n\t\t\t\t\t\t\t\t\t left_index=True, right_index=True)\n\t\treturn final_schedule_df\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from utils import *"]}, {"term": "def", "name": "normalization", "data": "def normalization(df,variable):\n\tvariable_normalzied = float(np.sum(((df.loc[:, [variable]] - pd.DataFrame.mean(df.loc[:, [variable]])) /\n\t\t\t\t\t\t\t\t\t\tpd.DataFrame.std(df.loc[:, [variable]]))))\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from utils import *"]}, {"term": "def", "name": "injury_function", "data": "def injury_function(dataset):\n\tdataset = dataset[~dataset.isin(Injury_df)]\n\tdataset.dropna(subset=[('Unnamed: 1')], inplace = True)\n\treturn dataset\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from utils import *"]}], [{"term": "def", "name": "main", "data": "def main():\n\tprisen = Pris()\n\tprisen.currentPris = Webscrape.webScrape()\n\tprisen.forjePris = prisen.currentPris\n\tpb = Pushbullet(\"skriv inn api key her\")\n\n\twhile Webscrape.kjorProg:\n\t\tif prisen.forjePris != prisen.currentPris:\n\t\t\tWebscrape.kjorProg = False\n\t\t\tprint(Webscrape.kjorProg)\n\t\t\tprint(\"Pris er oppdatert:\")\n\t\telse:\n\t\t\tprisen.forjePris = prisen.currentPris\n\t\t\tprisen.currentPris = Webscrape.webScrape()\n\t\t\tprint(prisen.currentPris)\n\t\t\tschedule.run_pending()\n\t\t\ttime.sleep(14400) #sleep 4 timer kj\u00f8r igjen.\n\tif Webscrape.kjorProg is False:\n\t\tpush = pb.push_note(\"Prisendring\", \"Prisen er n\u00e5 {} \".format(str(prisen.currentPris)))\n", "description": null, "category": "webscraping", "imports": ["import schedule", "import time", "from pushbullet import Pushbullet", "from Pris import Pris", "from WebScrape import Webscrape"]}], [{"term": "class", "name": "classoffence_defence:", "data": "class offence_defence:\n\tdef webscrape_link(team, season_year):\n\t\tLink = 'https://www.baseball-reference.com/teams/' + team + '/' + season_year + '.shtml#team_batting'\n\t\treturn Link\n\tdef batting_df(web_link):\n\t\tbatting_Link = pd.read_html(web_link)\n\t\tbatting_Df = pd.DataFrame(batting_Link[(len(batting_Link) - 2)])\n\t\tbatting_Df = batting_Df.loc[0:len(batting_Df), ['R', 'H', '2B', '3B', 'RBI', 'SB']]\n\t\tbatting_Df = batting_Df.drop(labels=range(21, len(batting_Df)),\n\t\t\t\t\t\t\t\t\t axis=0)\n\t\tbatting_Df = batting_Df.loc[(batting_Df['R'] != 'R')]\n\t\tbatting_Df = batting_Df.dropna()\n\t\treturn batting_Df\n\tdef pitching_df(web_link):\n\t\tpitching_Link = pd.read_html(web_link)\n\t\tpitching_Df = pd.DataFrame(pitching_Link[(len(pitching_Link) - 1)])\n\t\tpitching_Df = pitching_Df.loc[0:len(pitching_Df), ['R', 'H', 'ER', 'HR', 'BB', 'SO']]\n\t\tpitching_Df = pitching_Df.drop(labels=range(21, len(pitching_Df)),\n\t\t\t\t\t\t\t\t\t   axis=0)\n\t\tpitching_Df = pitching_Df.loc[(pitching_Df['H'] != 'H')]\n\t\tpitching_Df = pitching_Df.dropna()\n\t\treturn pitching_Df\n", "description": null, "category": "webscraping", "imports": ["# This script is to import the data", "from utils import *"]}, {"term": "class", "name": "classtodays_games:", "data": "class todays_games:\n\ttodays_date = date.today().strftime(\"%Y%m%d\")\n\tlink_to_todays_games = pd.read_html('https://www.cbssports.com/mlb/schedule/' + todays_date)\n\t@staticmethod\n\tdef home_teams():\n\t\ttodays_home_teams_df = todays_games.link_to_todays_games[0]['Home'].map(team_abbrev)\n\t\treturn todays_home_teams_df\n\t@staticmethod\n\tdef away_teams():\n\t\ttodays_away_teams_df = todays_games.link_to_todays_games[0]['Away'].map(team_abbrev)\n\t\treturn todays_away_teams_df\n", "description": null, "category": "webscraping", "imports": ["# This script is to import the data", "from utils import *"]}, {"term": "class", "name": "classteams:", "data": "class teams:\n\tclass pitching:\n\t\t# Arizona = ARI\n\t\tARI = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[0],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Tampa Bay = TBR\n\t\tTBR = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[1],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Houston = HOU\n\t\tHOU = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[2],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# NY Yankees = NYY\n\t\tNYY = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[3],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Chicago White Sox = CHW\n\t\tCHW = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[4],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Boston = BOS\n\t\tBOS = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[5],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Oakland = OAK\n\t\tOAK = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[6],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Seattle = SEA\n\t\tSEA = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[7],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Toronto = TOR\n\t\tTOR = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[8],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Cleveland = CLE\n\t\tCLE = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[9],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# LA Angels = LAA\n\t\tLAA = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[10],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Detroit = DET\n\t\tDET = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[11],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Kansas City = KCR\n\t\tKCR = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[12],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Minnesota = MIN\n\t\tMIN = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[13],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Texas = TEX\n\t\tTEX = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[14],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Baltimore = BAL\n\t\tBAL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[15],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# SF Giants = SFG\n\t\tSFG = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[16],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# LA Dodgers = LAD\n\t\tLAD = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[17],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Milwauke = MIL\n\t\tMIL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[18],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Cincinnati = CIN\n\t\tCIN = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[19],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Atlanta = ATL\n\t\tATL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[20],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# San Diego = SDP\n\t\tSDP = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[21],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# St. Louis\n\t\tSTL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[22],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Philadelphia = PHI\n\t\tPHI = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[23],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# NY Mets = NYM\n\t\tNYM = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[24],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Colorado = COL\n\t\tCOL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[25],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Washington = WSN\n\t\tWSN = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[26],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Chicago Cubs = CHC\n\t\tCHC = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[27],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Miami = MIA\n\t\tMIA = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[28],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n\t\t# Pittsburgh = PIT\n\t\tPIT = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[29],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  season_year=year_selected))\n", "description": null, "category": "webscraping", "imports": ["# This script is to import the data", "from utils import *"]}], [{"term": "def", "name": "webscrape_data_manipulation_function", "data": "def webscrape_data_manipulation_function(team_array_position):\n\tlatest_season = pd.read_html('https://www.pro-football-reference.com/teams/'\n\t\t\t\t\t\t\t\t + str.lower(team_array_for_website[team_array_position]) + '/'\n\t\t\t\t\t\t\t\t + str(season_year) + '.htm#passing')[1]\n\tlatest_season_minus_1 = pd.read_html('https://www.pro-football-reference.com/teams/'\n\t\t\t\t\t\t\t\t\t\t + str.lower(team_array_for_website[team_array_position])\n\t\t\t\t\t\t\t\t\t\t + '/' + str(season_year-1) + '.htm#passing')[1]\n\tlatest_season_minus_2 = pd.read_html('https://www.pro-football-reference.com/teams/'\n\t\t\t\t\t\t\t\t\t\t + str.lower(team_array_for_website[team_array_position])\n\t\t\t\t\t\t\t\t\t\t + '/' + str(season_year-2) + '.htm#passing')[1]\n\tlatest_season_minus_3 = pd.read_html('https://www.pro-football-reference.com/teams/'\n\t\t\t\t\t\t\t\t\t\t + str.lower(team_array_for_website[team_array_position])\n\t\t\t\t\t\t\t\t\t\t + '/' + str(season_year-3) + '.htm#passing')[1]\n\tlatest_season_minus_4 = pd.read_html('https://www.pro-football-reference.com/teams/'\n\t\t\t\t\t\t\t\t\t\t + str.lower(team_array_for_website[team_array_position])\n\t\t\t\t\t\t\t\t\t\t + '/' + str(season_year-4) + '.htm#passing')[1]\n\tdf = pd.concat([pd.DataFrame(latest_season_minus_4), pd.DataFrame(latest_season_minus_3),\n\t\t\t\t\tpd.DataFrame(latest_season_minus_2), pd.DataFrame(latest_season_minus_1),\n\t\t\t\t\tpd.DataFrame(latest_season)])\n\tdf = df.get(('Score', 'Tm'))\n\tdf = pd.DataFrame(df).dropna().reset_index(drop=True)\n\treturn df\n", "description": null, "category": "webscraping", "imports": ["from utils import *"]}, {"term": "def", "name": "normalization", "data": "def normalization(df):\n\tnormalized_values = np.log(df.shift(1) / df)\n\tnormalized_values = normalized_values.dropna()\n\tnormalized_values = normalized_values.replace([np.inf, -np.inf], 0)\n\treturn normalized_values\n", "description": null, "category": "webscraping", "imports": ["from utils import *"]}], [{"term": "class", "name": "TestWebscrapeTrails", "data": "class TestWebscrapeTrails(unittest.TestCase):\n\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(TestWebscrapeTrails, self).__init__(*args, **kwargs)\n\t\tself.webscrape = WebscrapeTrails()\n\t\t\n\tdef test_make_tables(self):\n\t\t\"\"\"\n\t\tGIVEN a URL for trails at a resort\n\n\t\tWHEN trails are webscraped and processed\n\n\t\tTHEN test if data structure is a DataFrame\n\t\t\tand if specified columns exist in DataFrame\n\t\t\"\"\"\n\t\n\t\tTEST_URL = \"https://jollyturns.com/resort/united-states-of-america/beaver-creek-resort/skiruns-green\"\n\t\t\n\t\tdf_trails = self.webscrape.make_tables(URL=TEST_URL)\n\n\t\tself.assertIsInstance(df_trails, pd.core.frame.DataFrame)\n\n\t\ttest_cols = ['Trail Name', 'Bottom Elev (ft)', 'Top Elev (ft)', 'Vertical Drop (ft)', 'Length (mi)', 'URL']\n\n\t\tself.assertTrue(all([col in df_trails.columns for col in test_cols]))\n\t\n\tdef test_rename_resorts(self):\n\t\t\"\"\"\n\t\tGIVEN a Pandas DataFrame of trail data\n\n\t\tWHEN resorts are renamed based on the URL\n\n\t\tTHEN test if data structure is a DataFrame\n\t\t\tand Resort column is in DataFrame\n\t\t\tand tested resort name is Beaver Creek\n\t\t\"\"\"\n\n\t\tdf_trails = pd.DataFrame({'Trail Name': {0: '\\xa0 Anderson Way ', 1: '\\xa0 Bear Paw '},\n\t\t\t\t\t\t\t\t  'Bottom Elev (ft)': {0: '8025 ft', 1: '8501 ft'},\n\t\t\t\t\t\t\t\t  'Top Elev (ft)': {0: '8238 ft', 1: '8547 ft'},\n\t\t\t\t\t\t\t\t  'Vertical Drop (ft)': {0: '213 ft', 1: '43 ft'},\n\t\t\t\t\t\t\t\t  'Length (mi)': {0: '0.73 mi', 1: '0.17 mi'},\n\t\t\t\t\t\t\t\t  'URL': {0: 'https://jollyturns.com/resort/united-states-of-america/beaver-creek-resort/skiruns-green',\n\t\t\t\t\t\t\t\t\t\t  1: 'https://jollyturns.com/resort/united-states-of-america/beaver-creek-resort/skiruns-green'}})\n\t\t\n\t\tdf_trails = self.webscrape.rename_resorts(df=df_trails)\n\t\t\n\t\tself.assertIsInstance(df_trails, pd.core.frame.DataFrame)\n\n\t\tself.assertTrue(\"Resort\" in list(df_trails.columns))\n\t\tself.assertTrue(all(df_trails[\"Resort\"] == \"Beaver Creek\"))\n", "description": "\n\t\tGIVEN a URL for trails at a resort\n\n\t\tWHEN trails are webscraped and processed\n\n\t\tTHEN test if data structure is a DataFrame\n\t\t\tand if specified columns exist in DataFrame\n\t\t", "category": "webscraping", "imports": ["import unittest", "from src.webscrape_trails import WebscrapeTrails", "import pandas as pd", "import numpy as np"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(airline):\n\t\"\"\" Input: airline: text used to go to reviews website for particular airline\n\tOutput: reviews: list of HTML segments that contains all relevant review information \"\"\"\n\treviews = []\n\tbrowser = webd.Chrome(executable_path=\"C:/chromeDriver/chromedriver.exe\")\n\n\tfor page_num in range(1, 50):\n\t\turl = f'http://www.airlinequality.com/airline-reviews/{airline}/page/{page_num}/?sortby=post_date%3ADesc&pagesize=100'\n\t\tbrowser.get(url)\n\t\ttime.sleep(5)\n\t\thtml = browser.page_source\n\t\tsoup = BeautifulSoup(html, 'html.parser')\n\t\tif soup.select('div.col-content article article.list-item'):\n\t\t\treviews.append(soup.select('div.col-content article article.list-item'))\n\t\telse:\n\t\t\tbreak\n\n\treturn reviews\n\n", "description": " Input: airline: text used to go to reviews website for particular airline\n\tOutput: reviews: list of HTML segments that contains all relevant review information ", "category": "webscraping", "imports": ["import time", "from collections import defaultdict", "import bs4", "import pandas as pd", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "from sqlalchemy import create_engine", "import pymysql", "import sentiment_analysis"]}, {"term": "def", "name": "parse_review", "data": "def parse_review(review: bs4.element.Tag) -> dict:\n\t\"\"\"\n\tInput: review: HTML segment that contains all relevant review information\n\tOutput: d: dictionary of relevant review information\n\t\"\"\"\n\n\td = {}\n\tif review.select_one(\"div.rating-10 span\"):\n\t\td['rating'] = int(review.select_one(\"div.rating-10 span\").text)\n\td['headline'] = review.select_one(\"h2.text_header\").text\n\ttry:\n\t\td['country'] = review.select_one('h3.text_sub_header').text\\\n\t\t\t.replace(')', '(').split('(')[1]\n\texcept IndexError:\n\t\td['country'] = 'None'\n\td['body'] = review.select_one(\"div.text_content\").text.strip()\n\trows = review.select('tr')\n\tfor row in rows:\n\t\tif row.select('td')[1].attrs['class'][0] == 'review-rating-stars':\n\t\t\tfor x in row.select('span'):\n\t\t\t\ttry:\n\t\t\t\t\tif x.attrs['class'] == ['star', 'fill']:\n\t\t\t\t\t\tnum = int(x.text)\n\t\t\t\t\t\td[row.td.attrs['class'][1]] = num\n\t\t\t\texcept KeyError:\n\t\t\t\t\tcontinue\n\t\telse:\n\t\t\td[row.td.attrs['class'][1]] = row.select('td')[1].text\n\treturn d\n\n", "description": "\n\tInput: review: HTML segment that contains all relevant review information\n\tOutput: d: dictionary of relevant review information\n\t", "category": "webscraping", "imports": ["import time", "from collections import defaultdict", "import bs4", "import pandas as pd", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "from sqlalchemy import create_engine", "import pymysql", "import sentiment_analysis"]}, {"term": "def", "name": "webscrape_manager", "data": "def webscrape_manager(airline_list):\n\t\"\"\"\n\tInput: airline_list: list of airline names as strings\n\tOutput: webscrape_info_dict: dictionary with keys as airline names and values as webscraped html code\n\t\"\"\"\n\twebscrape_info_dict = {}\n\n\tfor airline in airline_list:\n\t\twebscrape_info_dict[airline] = webscrape(airline)\n\n\treturn webscrape_info_dict\n\n", "description": "\n\tInput: airline_list: list of airline names as strings\n\tOutput: webscrape_info_dict: dictionary with keys as airline names and values as webscraped html code\n\t", "category": "webscraping", "imports": ["import time", "from collections import defaultdict", "import bs4", "import pandas as pd", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "from sqlalchemy import create_engine", "import pymysql", "import sentiment_analysis"]}, {"term": "def", "name": "review_parser", "data": "def review_parser(airline_list, webscrape_info_dict):\n\t\"\"\"\n\tInput: airline_list: list of airline names as strings\n\twebscrape_info_dict: dictionary with keys as airline names and values as webscraped html code\n\tOutput: airline_dict: dictionary with keys as airline names and values as their respective reviews\n\t\"\"\"\n\tairline_dict = defaultdict(list)\n\ti = 0\n\tfor reviews in webscrape_info_dict.values():\n\t\tfor review in reviews:\n\t\t\tfor r in review:\n\t\t\t\tairline_dict[airline_list[i]].append(parse_review(r))\n\t\ti += 1\n\treturn airline_dict\n\n", "description": "\n\tInput: airline_list: list of airline names as strings\n\twebscrape_info_dict: dictionary with keys as airline names and values as webscraped html code\n\tOutput: airline_dict: dictionary with keys as airline names and values as their respective reviews\n\t", "category": "webscraping", "imports": ["import time", "from collections import defaultdict", "import bs4", "import pandas as pd", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "from sqlalchemy import create_engine", "import pymysql", "import sentiment_analysis"]}, {"term": "def", "name": "copy_to_sql", "data": "def copy_to_sql(airline_list, airline_dict, engine):\n\t\"\"\"\n\tInput: airline_list: list of airline names as strings\n\tairline_dict: dictionary with keys as airline names and values as their respective reviews\n\tengine: directory to SQL database to store webscraped review data\n\t\"\"\"\n\tfor airline in airline_list:\n\t\tpd.DataFrame(airline_dict[airline]).to_sql(airline.split('-')[0], con=engine)\n\n", "description": "\n\tInput: airline_list: list of airline names as strings\n\tairline_dict: dictionary with keys as airline names and values as their respective reviews\n\tengine: directory to SQL database to store webscraped review data\n\t", "category": "webscraping", "imports": ["import time", "from collections import defaultdict", "import bs4", "import pandas as pd", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "from sqlalchemy import create_engine", "import pymysql", "import sentiment_analysis"]}, {"term": "def", "name": "getReviews", "data": "def getReviews(airline, engine):\n\tairline_list = [airline]\n\twebscrape_info_dict = webscrape_manager(airline_list)\n\tairline_dict = review_parser(airline_list, webscrape_info_dict)\n", "description": null, "category": "webscraping", "imports": ["import time", "from collections import defaultdict", "import bs4", "import pandas as pd", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "from sqlalchemy import create_engine", "import pymysql", "import sentiment_analysis"]}], [], [{"term": "def", "name": "magicId", "data": "def magicId (content):\n\tif content is not None:\n\t\tfrom magic_lib import Magic\n\t\tm = Magic()\n\t\treturn m.from_buffer(content)\n\telse: pass\n", "description": null, "category": "webscraping", "imports": ["import requests", "import json", "import datetime", "from html_scraper import scrape_url", "\t\tfrom magic_lib import Magic"]}, {"term": "def", "name": "whatTheFile", "data": "def whatTheFile (uri):\n\tif uri is not None:\n\t\turi = \"https://arweave.net/\" + uri\n\t\tdata = requests.get(uri)\n\t\treturn magicId(data.content)\n\telse: pass\n", "description": null, "category": "webscraping", "imports": ["import requests", "import json", "import datetime", "from html_scraper import scrape_url", "\t\tfrom magic_lib import Magic"]}, {"term": "def", "name": "runArweaveAPI", "data": "def runArweaveAPI():\n\twith open('height.txt', 'r') as f:\n\t\tlast_block = f.readlines()[-1]\n\n\turl = 'https://arweave.net/graphql'\n\n\theight = int(last_block)+1\n\tquery = \"\"\"query {\n\t\ttransactions(block: {min: \"\"\"+str(height)+\"\"\", max: \"\"\"+str(height)+\"\"\"}) {\n\t\t\tedges {\n\t\t\t\tnode {\n\t\t\t\t\tid,\n\t\t\t\t\tdata {\n\t\t\t\t\t\tsize\n\t\t\t\t\t\ttype\n\t\t\t\t\t},\n\t\t\t\t\ttags {\n\t\t\t\t\t\tname,\n\t\t\t\t\t\tvalue\n\t\t\t\t\t},\n\t\t\t\t\tblock {\n\t\t\t\t\t\tid\n\t\t\t\t\t\ttimestamp\n\t\t\t\t\t\theight\n\t\t\t\t\t\tprevious\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\"\"\"\n\tresponse = requests.post(url, json={'query': query})\n\tprint(\"arweave_api response received {}\".format(str(response.status_code)))\n\tjson_data = json.loads(str(response.text))[\"data\"][\"transactions\"][\"edges\"]\n\tarweave_api_data = []\n\tfor i in json_data:\n\t\t# print(\"working through each tx: {} of {}\".format(i, str(len(json_data))))\n\t\ttx_id = i[\"node\"][\"id\"]\n\t\ttimestamp = i[\"node\"][\"block\"][\"timestamp\"]\n\t\tfiletype = whatTheFile(tx_id)\n\n\t\tdata = {\"timestamp\": str(datetime.datetime.fromtimestamp(timestamp)),\n\t\t\t\t\"tx_id\": str(tx_id),\n\t\t\t\t\"block_height\": str(i[\"node\"][\"block\"][\"height\"]),\n\t\t\t\t\"file_type\": str(filetype),\n\t\t\t\t\"data_type\": str(i[\"node\"][\"data\"][\"type\"]),\n\t\t\t\t\"data_size\": str(i[\"node\"][\"data\"][\"size\"])\n\t\t\t\t}\n\t\tfor tag in i[\"node\"][\"tags\"]:\n\t\t\tif \"_\" in str(tag[\"name\"]):\n\t\t\t\tname = str(tag[\"name\"]).replace(\":\",\"_\")\n\t\t\telse:\n\t\t\t\tname = tag[\"name\"]\n\t\t\tdata[tag[\"name\"]] = str(tag[\"value\"])\n\n\t\t# # try:\n\t\tif \"text\" in str(filetype):\n\t\t\twebscrape = scrape_url(tx_id)\n\t\t\t# data[\"named_entities\"] = str(webscrape[\"named_entities\"])\n\t\t\t# data[\"source_language\"] = webscrape[\"source_language\"]\n\t\t\t# data[\"source_text\"] = webscrape[\"source_text\"]\n\t\t\tdata[\"source_text\"] = webscrape[\"source_text\"]\n\t\t\t# # except:\n\t\t\t# #\t pass\n\t\t\tprint(\"done with webscrape stuff\")\n\t\tarweave_api_data.append(data)\n\tprint(\"about to open height.txt to write new height\")\n\twith open('height.txt', 'a') as f:\n\t\tf.write(\"\\n\"+str(height))\n\tprint(\"about to return arweave_api_data\")\n\t# return json.dumps(arweave_api_data, indent=4)\n\treturn arweave_api_data\n", "description": "query {\n\t\ttransactions(block: {min: ", "category": "webscraping", "imports": ["import requests", "import json", "import datetime", "from html_scraper import scrape_url", "\t\tfrom magic_lib import Magic"]}], [{"term": "def", "name": "init_connection", "data": "def init_connection():\n\tconn = psycopg2.connect(\n\t\thost=\"localhost\", database=\"mydb\", user=\"zaki\", password=\"password123\"\n\t)\n\tconn.autocommit = True\n\treturn conn\n\n", "description": null, "category": "webscraping", "imports": ["import psycopg2", "from database.methods import return_user"]}, {"term": "def", "name": "create_schema", "data": "def create_schema(conn):\n\t\"\"\"After manually creating and setting up psql database, run this\"\"\"\n\tcur = conn.cursor()\n\tcur.execute(\"CREATE SCHEMA IF NOT EXISTS webscrape AUTHORIZATION zaki\")\n\tcur.close()\n\treturn\n\n", "description": "After manually creating and setting up psql database, run this", "category": "webscraping", "imports": ["import psycopg2", "from database.methods import return_user"]}, {"term": "def", "name": "create_tables", "data": "def create_tables(conn):\n\t\"\"\"look into storing passwords, and its hashes\"\"\"\n\tcur = conn.cursor()\n\t# cur.execute(\"CREATE TABLE IF NOT EXISTS webscrape.test (name text, age integer)\")\n\tcur.execute(\n\t\t\"CREATE TABLE IF NOT EXISTS webscrape.users (\"\n\t\t\"id serial,\"\n\t\t\"email text,\"\n\t\t\"password text,\"\n\t\t\"PRIMARY KEY (id),\"\n\t\t\"UNIQUE (email)\"\n\t\t\")\"\n\t)\n\tcur.execute(\n\t\t\"CREATE TABLE IF NOT EXISTS webscrape.items(\"\n\t\t\"asin text,\"\n\t\t\"title text,\"\n\t\t\"current_amount numeric(6,2),\"\n\t\t\"currency text,\"\n\t\t\"PRIMARY KEY (asin)\"\n\t\t\")\"\n\t)\n\tcur.execute(\n\t\t\"CREATE TABLE IF NOT EXISTS webscrape.alerts (\"\n\t\t\"asin text REFERENCES webscrape.items (asin),\"\n\t\t\"id serial REFERENCES webscrape.users (id),\"\n\t\t\"target_amount numeric(6, 2)\"\n\t\t\")\"\n\t)\n\tcur.execute(\n\t\t\"CREATE TABLE IF NOT EXISTS webscrape.price_history (\"\n\t\t\"asin text REFERENCES webscrape.items (asin),\"\n\t\t\"date date,\"\n\t\t\"amount numeric(6, 2),\"\n\t\t\"currency text\"\n\t\t\")\"\n\t)\n\n\tcur.close()\n\treturn\n\n", "description": "look into storing passwords, and its hashes", "category": "webscraping", "imports": ["import psycopg2", "from database.methods import return_user"]}, {"term": "def", "name": "get_alerts_to_email", "data": "def get_alerts_to_email(conn):\n\tcur = conn.cursor()\n\t# three table join\n\tcur.execute(\n\t\t\"\"\"\n\tSELECT users.email, alerts.asin, items.title, items.current_amount, alerts.target_amount\n\tFROM webscrape.users\n\tINNER JOIN webscrape.alerts ON users.id=alerts.id\n\tINNER JOIN webscrape.items ON alerts.asin=items.asin\n\tWHERE items.current_amount > alerts.target_amount\n\t\"\"\"\n\t)\n\t# for person in full_list:\n\t#\t (email, asin, title, current_amount, target_amount) = person\n\t#\t print(current_amount)\n\n\treturn cur.fetchall()\n\n", "description": "\n\tSELECT users.email, alerts.asin, items.title, items.current_amount, alerts.target_amount\n\tFROM webscrape.users\n\tINNER JOIN webscrape.alerts ON users.id=alerts.id\n\tINNER JOIN webscrape.items ON alerts.asin=items.asin\n\tWHERE items.current_amount > alerts.target_amount\n\t", "category": "webscraping", "imports": ["import psycopg2", "from database.methods import return_user"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(airline):\n\t\"\"\"\n\tINPUT:\n\tairline: text used to go to reviews website for particular airline\n\n\tOUTPUT:\n\treviews: list of HTML segments that contains all relevant review\n\t\tinformation\n\t\"\"\"\n\treviews = []\n\tbrowser = webd.Chrome()\n\n\tfor page_num in range(1, 50):\n\t\turl = f'http://www.airlinequality.com/airline-reviews/{airline}/page/{page_num}/?sortby=post_date%3ADesc&pagesize=100'\n\t\tbrowser.get(url)\n\t\ttime.sleep(5)\n\t\thtml = browser.page_source\n\t\tsoup = BeautifulSoup(html, 'html.parser')\n\t\tif soup.select('div.col-content article article.list-item'):\n\t\t\treviews.append(soup.select('div.col-content article article.list-item'))\n\t\telse:\n\t\t\tbreak\n\n\treturn reviews\n\n", "description": "\n\tINPUT:\n\tairline: text used to go to reviews website for particular airline\n\n\tOUTPUT:\n\treviews: list of HTML segments that contains all relevant review\n\t\tinformation\n\t", "category": "webscraping", "imports": ["from sqlalchemy import create_engine", "from collections import defaultdict", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "import pandas as pd", "import selenium", "import time", "import bs4"]}, {"term": "def", "name": "parse_review", "data": "def parse_review(review: bs4.element.Tag) -> dict:\n\t\"\"\"\n\tINPUT:\n\treview: HTML segment that contains all relevant review information\n\n\tOUTPUT:\n\td: dictionary of relevant review information\n\t\"\"\"\n\n\td = {}\n\tif review.select_one(\"div.rating-10 span\"):\n\t\td['rating'] = int(review.select_one(\"div.rating-10 span\").text)\n\td['headline'] = review.select_one(\"h2.text_header\").text\n\ttry:\n\t\td['country'] = review.select_one('h3.text_sub_header').text\\\n\t\t\t.replace(')', '(').split('(')[1]\n\texcept IndexError:\n\t\td['country'] = 'None'\n\td['body'] = review.select_one(\"div.text_content\").text.strip()\n\trows = review.select('tr')\n\tfor row in rows:\n\t\tif row.select('td')[1].attrs['class'][0] == 'review-rating-stars':\n\t\t\tfor x in row.select('span'):\n\t\t\t\ttry:\n\t\t\t\t\tif x.attrs['class'] == ['star', 'fill']:\n\t\t\t\t\t\tnum = int(x.text)\n\t\t\t\t\t\td[row.td.attrs['class'][1]] = num\n\t\t\t\texcept KeyError:\n\t\t\t\t\tcontinue\n\t\telse:\n\t\t\td[row.td.attrs['class'][1]] = row.select('td')[1].text\n\treturn d\n\n", "description": "\n\tINPUT:\n\treview: HTML segment that contains all relevant review information\n\n\tOUTPUT:\n\td: dictionary of relevant review information\n\t", "category": "webscraping", "imports": ["from sqlalchemy import create_engine", "from collections import defaultdict", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "import pandas as pd", "import selenium", "import time", "import bs4"]}, {"term": "def", "name": "webscrape_manager", "data": "def webscrape_manager(airline_list):\n\t\"\"\"\n\tINPUT:\n\tairline_list: list of airline names as strings\n\n\tOUTPUT:\n\twebscrape_info_dict: dictionary with keys as airline names and values as\n\t\twebscraped html code\n\t\"\"\"\n\twebscrape_info_dict = {}\n\n\tfor airline in airline_list:\n\t\twebscrape_info_dict[airline] = webscrape(airline)\n\n\treturn webscrape_info_dict\n\n", "description": "\n\tINPUT:\n\tairline_list: list of airline names as strings\n\n\tOUTPUT:\n\twebscrape_info_dict: dictionary with keys as airline names and values as\n\t\twebscraped html code\n\t", "category": "webscraping", "imports": ["from sqlalchemy import create_engine", "from collections import defaultdict", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "import pandas as pd", "import selenium", "import time", "import bs4"]}, {"term": "def", "name": "review_parser", "data": "def review_parser(airline_list, webscrape_info_dict):\n\t\"\"\"\n\tINPUT:\n\tairline_list: list of airline names as strings\n\twebscrape_info_dict: dictionary with keys as airline names and values as\n\t\twebscraped html code\n\n\tOUTPUT:\n\tairline_dict: dictionary with keys as airline names and values as their\n\t\trespective reviews\n\t\"\"\"\n\tairline_dict = defaultdict(list)\n\ti = 0\n\tfor reviews in webscrape_info_dict.values():\n\t\tfor review in reviews:\n\t\t\tfor r in review:\n\t\t\t\tairline_dict[airline_list[i]].append(parse_review(r))\n\t\ti += 1\n\treturn airline_dict\n\n", "description": "\n\tINPUT:\n\tairline_list: list of airline names as strings\n\twebscrape_info_dict: dictionary with keys as airline names and values as\n\t\twebscraped html code\n\n\tOUTPUT:\n\tairline_dict: dictionary with keys as airline names and values as their\n\t\trespective reviews\n\t", "category": "webscraping", "imports": ["from sqlalchemy import create_engine", "from collections import defaultdict", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "import pandas as pd", "import selenium", "import time", "import bs4"]}, {"term": "def", "name": "copy_to_sql", "data": "def copy_to_sql(airline_list, airline_dict, engine):\n\t\"\"\"\n\tINPUT:\n\tairline_list: list of airline names as strings\n\tairline_dict: dictionary with keys as airline names and values as their\n\t\trespective reviews\n\tengine: directory to SQL database to store webscraped review data\n\n\tOUTPUT:\n\tNone\n\t\"\"\"\n\tfor airline in airline_list:\n\t\tpd.DataFrame(airline_dict[airline]).to_sql(airline.split('-')[0],\n\t\t\t\t\t\t\t\t\t\t\t\t   con=engine)\n\n", "description": "\n\tINPUT:\n\tairline_list: list of airline names as strings\n\tairline_dict: dictionary with keys as airline names and values as their\n\t\trespective reviews\n\tengine: directory to SQL database to store webscraped review data\n\n\tOUTPUT:\n\tNone\n\t", "category": "webscraping", "imports": ["from sqlalchemy import create_engine", "from collections import defaultdict", "import selenium.webdriver as webd", "from bs4 import BeautifulSoup", "import pandas as pd", "import selenium", "import time", "import bs4"]}], [{"term": "def", "name": "home", "data": "def home():\n\treturn render_template(\"index.html\")\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template, request", "import json", "import Webscrape"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\tmessage = request.get_json(force=True)\n\t\n\tWebscrape.webscrape(message['search'])\n\tresponse = json.load(open('product_info.json', 'r'))\n\n\treturn jsonify(response)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template, request", "import json", "import Webscrape"]}, {"term": "def", "name": "not_found", "data": "def not_found(e):\n\treturn render_template(\"404.html\")\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template, request", "import json", "import Webscrape"]}], [], [], [], [{"term": "def", "name": "home", "data": "def home():\n\treturn render_template(\"index.html\")\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template, request", "import json", "import Webscrape"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\tmessage = request.get_json(force=True)\n\tWebscrape.webscrape(message['search'])\n\tresponse = json.load(open('product_info.json', 'r'))\n\treturn jsonify(response)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template, request", "import json", "import Webscrape"]}, {"term": "def", "name": "not_found", "data": "def not_found(e):\n\treturn render_template(\"404.html\")\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template, request", "import json", "import Webscrape"]}], [], [], [{"term": "def", "name": "ncdefon_message", "data": "async def on_message(message):\n\t# we do not want the bot to reply to itself\n\tif message.author == client.user:\n\t\treturn\n\n\tif message.content.startswith('!hello'):\n\t\tmsg = 'Hello {0.author.mention}'.format(message)\n\t\t# await message.channel.send(msg)\n\t\ttestFile = File('options.csv')\n\t\tawait message.channel.send(file = testFile)\n\n\telif message.content.startswith('!recordanewmixtape'):\n\t\tmsg = 'No {0.author.mention}, you do it'.format(message)\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!tickers'):\n\t\tcmd = message.content\n\t\tcmd = cmd[9:]\n\t\tprint('tickers command ticker: ' + cmd)\n\t\ttry:\n\t\t\ttickers = WebScrape.printTickers(cmd)\n\t\t\tmsg = ''\n\t\t\tfor tick in tickers:\n\t\t\t\tmsg = msg + tick + '\\n'\n\t\texcept:\n\t\t\tmsg = 'Not a valid option\\nLook at !listOptions'\n\t\t\t\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!setOption'):\n\t\tcmd = message.content\n\t\tcmdParts = cmd.split()\n\t\tWebScrape.setOption(cmdParts[1], cmdParts[2])\n\t\tmsg = '{0.author.mention} new option added '.format(message) + cmdParts[2]\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!removeOption'):\n\t\tcmd = message.content\n\t\tcmdParts = cmd.split()\n\t\tWebScrape.removeOption(cmdParts[1])\n\t\tmsg = '{0.author.mention} option was removed '.format(message) + cmdParts[1]\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!listOptions'):\n\t\toptions = WebScrape.getOptions()\n\t\tmsg = ''\n\t\tif options == 'There are no current options setup use !setOptions to set a new option':\n\t\t\tmsg = options\n\t\telse:\n\t\t\tfor opt in options:\n\t\t\t\tmsg = msg + opt + '\\n'\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!stock'):\n\t\tcmd = message.content\n\t\tcmdParts = cmd.split()\n\t\tGenerateGraph.makeGraph(cmdParts[1])\n\t\tgraphFile = File('stock_information.html')\n\t\tawait message.channel.send(file = graphFile)\n\n\n\telif message.content.startswith('!help'):\n\t\tmsg = 'Remember to seperate each part of the command with a space\\n'\n\t\tmsg = msg + '!setOption  \\n'\n\t\tmsg = msg + '!removeOption \\n'\n\t\tmsg = msg + '!listOptions\\n'\n\t\tmsg = msg + '!tickers \\n'\n\t\tmsg = msg + '!stock \\n'\n\t\tawait message.channel.send(msg)\n", "description": null, "category": "webscraping", "imports": ["import discord", "import json", "import WebScrape", "import GenerateGraph", "from discord import File"]}, {"term": "def", "name": "ncdefon_ready", "data": "async def on_ready():\n\tprint('Logged in as')\n\tprint(client.user.name)\n\tprint(client.user.id)\n\tprint('------')\n", "description": null, "category": "webscraping", "imports": ["import discord", "import json", "import WebScrape", "import GenerateGraph", "from discord import File"]}], [{"term": "def", "name": "get_cursor", "data": "def get_cursor(file_name):\n\t\"\"\" Connects and returns a cursor to an sqlite output file\n\n\tParameters\n\t----------\n\tfile_name: str\n\t\tname of the sqlite file\n\n\tReturns\n\t-------\n\tsqlite cursor3\n\t\"\"\"\n\tcon = sql.connect(file_name)\n\tcon.row_factory = sql.Row\n\treturn con.cursor()\n\n", "description": " Connects and returns a cursor to an sqlite output file\n\n\tParameters\n\t----------\n\tfile_name: str\n\t\tname of the sqlite file\n\n\tReturns\n\t-------\n\tsqlite cursor3\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "import_pris", "data": "def import_pris(pris_link):\n\t\"\"\" Opens pris_csv using Pandas. Adds Latitude and Longitude\n\tcolumns\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\n\tReturns\n\t-------\n\tpris: pd.Dataframe\n\t\tpris database\n\t\"\"\"\n\tpris = pd.read_csv(pris_link,\n\t\t\t\t\t   delimiter=',',\n\t\t\t\t\t   encoding='iso-8859-1'\n\t\t\t\t\t   )\n\tpris.insert(13, 'Latitude', np.nan)\n\tpris.insert(14, 'Longitude', np.nan)\n\tpris = pris.replace(np.nan, '')\n\treturn pris\n\n", "description": " Opens pris_csv using Pandas. Adds Latitude and Longitude\n\tcolumns\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\n\tReturns\n\t-------\n\tpris: pd.Dataframe\n\t\tpris database\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "import_webscrape_data", "data": "def import_webscrape_data(scrape_link):\n\t\"\"\" Returns sqlite content of webscrape by performing an\n\tsqlite query\n\n\tParameters\n\t----------\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tcoords: sqlite cursor\n\t\tsqlite cursor containing webscrape data\n\t\"\"\"\n\tcur = get_cursor(scrape_link)\n\tcoords = cur.execute(\"SELECT name, long, lat FROM reactors_coordinates\")\n\treturn coords\n\n", "description": " Returns sqlite content of webscrape by performing an\n\tsqlite query\n\n\tParameters\n\t----------\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tcoords: sqlite cursor\n\t\tsqlite cursor containing webscrape data\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "get_edge_cases", "data": "def get_edge_cases():\n\t\"\"\" Returns a dictionary of edge cases that fuzzywuzzy is\n\tunable to catch. This could be because PRIS database stores\n\treactor names and Webscrape database fetches power plant names,\n\tor because PRIS reactor names are abbreviated.\n\n\tParameters\n\t----------\n\n\tReturns\n\t-------\n\tothers: dict\n\t\tdictionary of edge cases with \"key=pris_reactor_name, and\n\t\tvalue=webscrape_plant_name\"\n\t\"\"\"\n\tothers = {'OHI-': '\u014ci',\n\t\t\t  'ASCO-': 'Asc\u00f3',\n\t\t\t  'ROVNO-': 'Rivne',\n\t\t\t  'SHIN-KORI-': 'Kori',\n\t\t\t  'ANO-': 'Arkansas One',\n\t\t\t  'HANBIT-': 'Yeonggwang',\n\t\t\t  'FERMI-': 'Enrico Fermi',\n\t\t\t  'BALTIC-': 'Kaliningrad',\n\t\t\t  'COOK-': 'Donald C. Cook',\n\t\t\t  'HATCH-': 'Edwin I. Hatch',\n\t\t\t  'HARRIS-': 'Shearon Harris',\n\t\t\t  'SHIN-WOLSONG-': 'Wolseong',\n\t\t\t  'ST. ALBAN-': 'Saint-Alban',\n\t\t\t  'LASALLE-': 'LaSalle County',\n\t\t\t  'ZAPOROZHYE-': 'Zaporizhzhya',\n\t\t\t  'ROBINSON-': 'H. B. Robinson',\n\t\t\t  'SUMMER-': 'Virgil C. Summer',\n\t\t\t  'FARLEY-': 'Joseph M. Farley',\n\t\t\t  'ST. LAURENT ': 'Saint-Laurent',\n\t\t\t  'HADDAM NECK': 'Connecticut1 Yankee',\n\t\t\t  'FITZPATRICK': 'James A. FitzPatrick',\n\t\t\t  'HIGASHI DORI-1 (TOHOKU)': 'Higashid\u014dri',\n\t\t\t  }\n\treturn others\n\n", "description": " Returns a dictionary of edge cases that fuzzywuzzy is\n\tunable to catch. This could be because PRIS database stores\n\treactor names and Webscrape database fetches power plant names,\n\tor because PRIS reactor names are abbreviated.\n\n\tParameters\n\t----------\n\n\tReturns\n\t-------\n\tothers: dict\n\t\tdictionary of edge cases with \"key=pris_reactor_name, and\n\t\tvalue=webscrape_plant_name\"\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "sanitize_webscrape_name", "data": "def sanitize_webscrape_name(name):\n\t\"\"\" Sanitizes webscrape powerplant names by removing unwanted\n\tstrings (listed in blacklist), applying lower case, and deleting\n\ttrailing whitespace.\n\n\tParameters\n\t----------\n\tname: str\n\t\twebscrape plant name\n\n\tReturns\n\t-------\n\tname: str\n\t\tsanitized name for use with fuzzywuzzy\n\t\"\"\"\n\tblacklist = ['nuclear', 'power',\n\t\t\t\t 'plant', 'generating',\n\t\t\t\t 'station', 'reactor', 'atomic',\n\t\t\t\t 'energy', 'center', 'electric']\n\tname = name.lower()\n\tfor blacklisted in blacklist:\n\t\tname = name.replace(blacklisted, '')\n\tname = name.strip()\n\tname = ' '.join(name.split())\n\treturn name\n\n", "description": " Sanitizes webscrape powerplant names by removing unwanted\n\tstrings (listed in blacklist), applying lower case, and deleting\n\ttrailing whitespace.\n\n\tParameters\n\t----------\n\tname: str\n\t\twebscrape plant name\n\n\tReturns\n\t-------\n\tname: str\n\t\tsanitized name for use with fuzzywuzzy\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "sanitize_pris_name", "data": "def sanitize_pris_name(name):\n\tpris_name = name.lower()\n\tif pris_name.find('-') != -1 and is_int(pris_name[-1]):\n\t\tif pris_name[pris_name.find('-') + 1:].find('-') != -1:\n\t\t\tidx = pris_name.find('-')\n\t\t\tidx += pris_name[pris_name.find('-') + 1:].find('-')\n\t\t\tpris_name = pris_name[:idx]\n\t\telse:\n\t\t\tpris_name = pris_name[:pris_name.find('-')]\n\treturn pris_name\n\n", "description": null, "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "is_int", "data": "def is_int(str):\n\t\"\"\" Checks if input string is a number rather than a letter\n\n\tParameters\n\t----------\n\tstr: str\n\t\tstring to test\n\n\tReturns\n\t-------\n\tanswer: bool\n\t\treturns True if string is a number; False if string is not\n\t\"\"\"\n\tanswer = False\n\ttry:\n\t\tint(str)\n\texcept ValueError:\n\t\treturn answer\n\tanswer = True\n\treturn answer\n\n", "description": " Checks if input string is a number rather than a letter\n\n\tParameters\n\t----------\n\tstr: str\n\t\tstring to test\n\n\tReturns\n\t-------\n\tanswer: bool\n\t\treturns True if string is a number; False if string is not\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "merge_coordinates", "data": "def merge_coordinates(pris_link, scrape_link):\n\t\"\"\" Obtains coordinates from webscrape.sqlite and\n\twrites them to matching reactors in PRIS reactor file.\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath and name of pris reactor text file\n\tscrape: str\n\t\tpath and name of webscrape sqlite file\n\n\tReturns\n\t-------\n\tnull\n\t\tWrites pris text file with coordinates\n\t\"\"\"\n\tothers = get_edge_cases()\n\tpris = import_pris(pris_link)\n\tcoords = import_webscrape_data(scrape_link)\n\tfor web in coords:\n\t\tfor i, prs in pris.iterrows():\n\t\t\twebscrape_name = sanitize_webscrape_name(web['name'])\n\t\t\tpris_name = sanitize_pris_name(prs[1])\n\t\t\tif fuzz.ratio(webscrape_name, pris_name) > 78:\n\t\t\t\tprs[13] = web['lat']\n\t\t\t\tprs[14] = web['long']\n\t\t\telse:\n\t\t\t\tfor other in others.keys():\n\t\t\t\t\tedge_case_key = other.lower()\n\t\t\t\t\tedge_case_value = others[other].lower()\n\t\t\t\t\tif fuzz.ratio(pris_name, edge_case_key) > 80:\n\t\t\t\t\t\tif fuzz.ratio(webscrape_name, edge_case_value) > 75:\n\t\t\t\t\t\t\tprs[13] = web['lat']\n\t\t\t\t\t\t\tprs[14] = web['long']\n\tpris.to_csv('reactors_pris_2016.csv', index=False, sep=',')\n\n", "description": " Obtains coordinates from webscrape.sqlite and\n\twrites them to matching reactors in PRIS reactor file.\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath and name of pris reactor text file\n\tscrape: str\n\t\tpath and name of webscrape sqlite file\n\n\tReturns\n\t-------\n\tnull\n\t\tWrites pris text file with coordinates\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "import_csv", "data": "def import_csv(in_csv, delimit):\n\t\"\"\" Imports contents of a csv text file to a list of\n\tlists.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath and name of input csv file\n\tdelimit: str\n\t\tdelimiter of the csv file\n\n\tReturns\n\t-------\n\tdata_list: list\n\t\tlist of lists containing the csv data\n\t\"\"\"\n\twith open(in_csv, encoding='utf-8') as source:\n\t\tsourcereader = csv.reader(source, delimiter=delimit)\n\t\tdata_list = []\n\t\tfor row in sourcereader:\n\t\t\tdata_list.append(row)\n\treturn data_list\n\n", "description": " Imports contents of a csv text file to a list of\n\tlists.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath and name of input csv file\n\tdelimit: str\n\t\tdelimiter of the csv file\n\n\tReturns\n\t-------\n\tdata_list: list\n\t\tlist of lists containing the csv data\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "load_template", "data": "def load_template(in_template):\n\t\"\"\" Returns a jinja2 template from file.\n\n\tParameters\n\t---------\n\tin_template: str\n\t\tpath and name of jinja2 template\n\n\tReturns\n\t-------\n\toutput_template: jinja template object\n\t\"\"\"\n\twith open(in_template, 'r') as default:\n\t\toutput_template = jinja2.Template(default.read())\n\treturn output_template\n\n", "description": " Returns a jinja2 template from file.\n\n\tParameters\n\t---------\n\tin_template: str\n\t\tpath and name of jinja2 template\n\n\tReturns\n\t-------\n\toutput_template: jinja template object\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "get_composition_fresh", "data": "def get_composition_fresh(in_list, burnup):\n\t\"\"\" Returns a dictionary of isotope and composition (in mass fraction)\n\tusing vision_recipes for fresh UOX fuel.\n\n\tParameters\n\t---------\n\tin_list: list\n\t\tlist containing vision_recipes\n\tburnup: int\n\t\tburnup\n\n\tReturns\n\t-------\n\tdata_dict: dict\n\t\tdictionary with key=[isotope],\n\t\tand value=[composition]\n\t\"\"\"\n\tdata_dict = {}\n\tfor i in range(len(in_list)):\n\t\tif i > 1:\n\t\t\tif burnup == 33:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][1])})\n\t\t\telif burnup == 51:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][3])})\n\t\t\telse:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][5])})\n\treturn data_dict\n\n", "description": " Returns a dictionary of isotope and composition (in mass fraction)\n\tusing vision_recipes for fresh UOX fuel.\n\n\tParameters\n\t---------\n\tin_list: list\n\t\tlist containing vision_recipes\n\tburnup: int\n\t\tburnup\n\n\tReturns\n\t-------\n\tdata_dict: dict\n\t\tdictionary with key=[isotope],\n\t\tand value=[composition]\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "get_composition_spent", "data": "def get_composition_spent(in_list, burnup):\n\t\"\"\" Returns a dictionary of isotope and composition (in mass fraction)\n\tusing vision_recipes for spent nuclear fuel\n\n\tParameters\n\t---------\n\tin_list: list\n\t\tlist containing vision_recipes data\n\tburnup: int\n\t\tburnup\n\n\tReturns\n\t-------\n\tdata_dict: dict\n\t\tdictionary with key=[isotope],\n\t\tand value=[composition]\n\t\"\"\"\n\tdata_dict = {}\n\tfor i in range(len(in_list)):\n\t\tif i > 1:\n\t\t\tif burnup == 33:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][2])})\n\t\t\telif burnup == 51:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][4])})\n\t\t\telse:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][6])})\n\treturn data_dict\n\n", "description": " Returns a dictionary of isotope and composition (in mass fraction)\n\tusing vision_recipes for spent nuclear fuel\n\n\tParameters\n\t---------\n\tin_list: list\n\t\tlist containing vision_recipes data\n\tburnup: int\n\t\tburnup\n\n\tReturns\n\t-------\n\tdata_dict: dict\n\t\tdictionary with key=[isotope],\n\t\tand value=[composition]\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "write_recipes", "data": "def write_recipes(fresh_dict, spent_dict, in_template, burnup, region):\n\t\"\"\" Renders jinja template using fresh and spent fuel composition.\n\n\tParameters\n\t---------\n\tfresh_dict: dict\n\t\tdictionary with key=[isotope], and\n\t\tvalue=[composition] for fresh UOX\n\tspent_dict: dict\n\t\tdictionary with key=[isotope], and\n\t\tvalue=[composition] for spent fuel\n\tin_template: jinja template object\n\t\tjinja template object to be rendered\n\tburnup: int\n\t\tamount of burnup\n\n\tReturns\n\t-------\n\tnull\n\t\tgenerates recipe files for cyclus.\n\t\"\"\"\n\tout_path = 'cyclus/input/' + region + '/recipes/'\n\tpathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n\trendered = in_template.render(fresh=fresh_dict,\n\t\t\t\t\t\t\t\t  spent=spent_dict)\n\twith open(out_path + '/uox_' + str(burnup) + '.xml', 'w') as output:\n\t\toutput.write(rendered)\n\n", "description": " Renders jinja template using fresh and spent fuel composition.\n\n\tParameters\n\t---------\n\tfresh_dict: dict\n\t\tdictionary with key=[isotope], and\n\t\tvalue=[composition] for fresh UOX\n\tspent_dict: dict\n\t\tdictionary with key=[isotope], and\n\t\tvalue=[composition] for spent fuel\n\tin_template: jinja template object\n\t\tjinja template object to be rendered\n\tburnup: int\n\t\tamount of burnup\n\n\tReturns\n\t-------\n\tnull\n\t\tgenerates recipe files for cyclus.\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "produce_recipes", "data": "def produce_recipes(in_csv, recipe_template, burnup):\n\t\"\"\" Generates commodity composition xml input for cyclus.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath and name of recipe file\n\trecipe_template: str\n\t\tpath and name of recipe template\n\tburnup: int\n\t\tamount of burnup\n\n\tReturns\n\t-------\n\tnull\n\t\tGenerates commodity composition xml input for cyclus.\n\t\"\"\"\n\trecipe = import_csv(in_csv, ',')\n\twrite_recipes(get_composition_fresh(recipe, burnup),\n\t\t\t\t  get_composition_spent(recipe, burnup),\n\t\t\t\t  load_template(recipe_template), burnup)\n\n", "description": " Generates commodity composition xml input for cyclus.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath and name of recipe file\n\trecipe_template: str\n\t\tpath and name of recipe template\n\tburnup: int\n\t\tamount of burnup\n\n\tReturns\n\t-------\n\tnull\n\t\tGenerates commodity composition xml input for cyclus.\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "confirm_deployment", "data": "def confirm_deployment(date_str, capacity):\n\t\"\"\" Confirms if reactor is to be deployed for CYCLUS by\n\tchecking if the capacity > 400 and if the commercial date\n\tis a proper date format.\n\n\tParameters\n\t----------\n\tdate_str: str\n\t\t\tthe commercial date string from PRIS data file\n\tcapacity: str\n\t\t\tcapacity in MWe from RPIS data file\n\n\tReturns\n\t-------\n\tis_deployed: bool\n\t\t\tdetermines whether the reactor will be deployed\n\t\t\tin CYCLUS\n\t\"\"\"\n\tis_deployed = False\n\tif len(date_str) > 4 and float(capacity) > 400:\n\t\ttry:\n\t\t\tdate.parse(date_str)\n\t\t\tis_deployed = True\n\t\texcept:\n\t\t\tpass\n\treturn is_deployed\n\n", "description": " Confirms if reactor is to be deployed for CYCLUS by\n\tchecking if the capacity > 400 and if the commercial date\n\tis a proper date format.\n\n\tParameters\n\t----------\n\tdate_str: str\n\t\t\tthe commercial date string from PRIS data file\n\tcapacity: str\n\t\t\tcapacity in MWe from RPIS data file\n\n\tReturns\n\t-------\n\tis_deployed: bool\n\t\t\tdetermines whether the reactor will be deployed\n\t\t\tin CYCLUS\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "select_region", "data": "def select_region(in_list, region):\n\t\"\"\" Returns a list of reactors that will be deployed for\n\tCYCLUS by checking the capacity and commercial date\n\n\tParameters\n\t----------\n\tin_list: list\n\t\t\timported csv file in list format\n\tregion: str\n\t\t\tname of the region\n\n\tReturns\n\t-------\n\treactor_list: list\n\t\t\tlist of reactors from PRIS\n\t\"\"\"\n\tASIA = {'IRAN', 'JAPAN', 'KAZAKHSTAN',\n\t\t\t'BANGLADESH', 'CHINA', 'INDIA',\n\t\t\t'UNITED ARAB EMIRATES', 'VIETNAM',\n\t\t\t'PAKISTAN', 'PHILIPPINES', 'SOUTH KOREA'\n\t\t\t}\n\tUNITED_STATES = {'UNITED STATES'}\n\tSOUTH_AMERICA = {'ARGENTINA', 'BRAZIL'}\n\tNORTH_AMERICA = {'CANADA', 'MEXICO', 'UNITED STATES'}\n\tEUROPE = {'UKRAINE', 'UNITED KINGDOM',\n\t\t\t  'POLAND', 'ROMANIA', 'RUSSIA',\n\t\t\t  'BELARUS', 'BELGIUM', 'BULGARIA',\n\t\t\t  'GERMANY', 'ITALY', 'NETHERLANDS',\n\t\t\t  'SWEDEN', 'SWITZERLAND', 'TURKEY',\n\t\t\t  'SLOVENIA', 'SOVIET UNION', 'SPAIN',\n\t\t\t  'CZECHOSLOVAKIA', 'FINLAND', 'FRANCE'\n\t\t\t  }\n\tAFRICA = {'EGYPT', 'MOROCCO', 'SOUTH AFRICA', 'TUNISIA'}\n\tALL = (SOUTH_AMERICA | NORTH_AMERICA |\n\t\t   EUROPE | ASIA | AFRICA | UNITED_STATES)\n\tregions = {'ASIA': ASIA,\n\t\t\t   'AFRICA': AFRICA,\n\t\t\t   'EUROPE': EUROPE,\n\t\t\t   'SOUTH_AMERICA': SOUTH_AMERICA,\n\t\t\t   'NORTH_AMERICA': NORTH_AMERICA,\n\t\t\t   'UNITED_STATES': UNITED_STATES,\n\t\t\t   'ALL': ALL}\n\tif region.upper() not in regions.keys():\n\t\traise ValueError(region + 'is not a valid region')\n\treactor_list = []\n\tfor row in in_list:\n\t\tcountry = row[0]\n\t\tif country.upper() in regions[region.upper()]:\n\t\t\tcapacity = row[3]\n\t\t\tstart_date = row[10]\n\t\t\tif confirm_deployment(start_date, capacity):\n\t\t\t\treactor_list.append(row)\n\treturn reactor_list\n\n", "description": " Returns a list of reactors that will be deployed for\n\tCYCLUS by checking the capacity and commercial date\n\n\tParameters\n\t----------\n\tin_list: list\n\t\t\timported csv file in list format\n\tregion: str\n\t\t\tname of the region\n\n\tReturns\n\t-------\n\treactor_list: list\n\t\t\tlist of reactors from PRIS\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "get_lifetime", "data": "def get_lifetime(in_row):\n\t\"\"\" Calculates the lifetime of a reactor using first\n\tcommercial date and shutdown date. Defaults to 720 months\n\tif shutdown date is not available.\n\n\tParameters\n\t----------\n\tin_row: list\n\t\tsingle row from PRIS data that contains reactor\n\t\tinformation\n\n\tReturns\n\t-------\n\tlifetime: int\n\t\tlifetime of reactor\n\t\"\"\"\n\tcomm_date = in_row[10]\n\tshutdown_date = in_row[11]\n\tif not shutdown_date.strip():\n\t\treturn 720\n\telse:\n\t\tn_days_month = 365.0 / 12\n\t\tdelta = (date.parse(shutdown_date) - date.parse(comm_date)).days\n\t\treturn int(delta / n_days_month)\n\n", "description": " Calculates the lifetime of a reactor using first\n\tcommercial date and shutdown date. Defaults to 720 months\n\tif shutdown date is not available.\n\n\tParameters\n\t----------\n\tin_row: list\n\t\tsingle row from PRIS data that contains reactor\n\t\tinformation\n\n\tReturns\n\t-------\n\tlifetime: int\n\t\tlifetime of reactor\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "write_reactors", "data": "def write_reactors(in_list, out_path, reactor_template):\n\t\"\"\" Renders CYCAMORE::reactor specifications using jinja2.\n\n\tParameters\n\t----------\n\tin_list: list\n\t\tlist containing PRIS data\n\tout_path: str\n\t\toutput path for reactor files\n\treactor_template: str\n\t\tpath to reactor template\n\n\tReturns\n\t-------\n\tnull\n\t\twrites xml files with CYCAMORE::reactor config\n\t\"\"\"\n\tif out_path[-1] != '/':\n\t\tout_path += '/'\n\tpathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n\treactor_template = load_template(reactor_template)\n\tfor row in in_list:\n\t\tcapacity = float(row[3])\n\t\tif capacity >= 400:\n\t\t\tname = row[1].replace(' ', '_')\n\t\t\tassem_per_batch = 0\n\t\t\tassem_no = 0\n\t\t\tassem_size = 0\n\t\t\treactor_type = row[2]\n\t\t\tlatitude = row[13] if row[13] != '' else 0\n\t\t\tlongitude = row[14] if row[14] != '' else 0\n\t\t\tif reactor_type in ['BWR', 'ESBWR']:\n\t\t\t\tassem_no = 732\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 138000 / assem_no\n\t\t\telif reactor_type in ['GCR', 'HWGCR']:  # Need batch number\n\t\t\t\tassem_no = 324\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 114000 / assem_no\n\t\t\telif reactor_type == 'HTGR':  # Need batch number\n\t\t\t\tassem_no = 3944\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 39000 / assem_no\n\t\t\telif reactor_type == 'PHWR':\n\t\t\t\tassem_no = 390\n\t\t\t\tassem_per_batch = int(assem_no / 45)\n\t\t\t\tassem_size = 80000 / assem_no\n\t\t\telif reactor_type == 'VVER':  # Need batch number\n\t\t\t\tassem_no = 312\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 41500 / assem_no\n\t\t\telif reactor_type == 'VVER-1200':  # Need batch number\n\t\t\t\tassem_no = 163\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 80000 / assem_no\n\t\t\telse:\n\t\t\t\tassem_no = 241\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 103000 / assem_no\n\t\t\tconfig = reactor_template.render(name=name,\n\t\t\t\t\t\t\t\t\t\t\t lifetime=get_lifetime(row),\n\t\t\t\t\t\t\t\t\t\t\t assem_size=assem_size,\n\t\t\t\t\t\t\t\t\t\t\t n_assem_core=assem_no,\n\t\t\t\t\t\t\t\t\t\t\t n_assem_batch=assem_per_batch,\n\t\t\t\t\t\t\t\t\t\t\t power_cap=row[3],\n\t\t\t\t\t\t\t\t\t\t\t lon=longitude,\n\t\t\t\t\t\t\t\t\t\t\t lat=latitude)\n\t\t\twith open(out_path + name.replace(' ', '_') + '.xml',\n\t\t\t\t\t  'w') as output:\n\t\t\t\toutput.write(config)\n\n", "description": " Renders CYCAMORE::reactor specifications using jinja2.\n\n\tParameters\n\t----------\n\tin_list: list\n\t\tlist containing PRIS data\n\tout_path: str\n\t\toutput path for reactor files\n\treactor_template: str\n\t\tpath to reactor template\n\n\tReturns\n\t-------\n\tnull\n\t\twrites xml files with CYCAMORE::reactor config\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "obtain_reactors", "data": "def obtain_reactors(in_csv, region, reactor_template):\n\t\"\"\" Writes xml files for individual reactors in a given\n\tregion.\n\n\tParameters\n\t----------\n\tin_csv: str\n\t\tcsv file name\n\tregion: str\n\t\tregion name\n\treactor_template: str\n\t\tpath to CYCAMORE::reactor config template file\n\n\tReturns\n\t-------\n\tnull\n\t\tWrites xml files for individual reactors in region.\n\t\"\"\"\n\tin_data = import_csv(in_csv, ',')\n\treactor_list = select_region(in_data, region)\n\tout_path = 'cyclus/input/' + region + '/reactors'\n\twrite_reactors(reactor_list, out_path, reactor_template)\n\n", "description": " Writes xml files for individual reactors in a given\n\tregion.\n\n\tParameters\n\t----------\n\tin_csv: str\n\t\tcsv file name\n\tregion: str\n\t\tregion name\n\treactor_template: str\n\t\tpath to CYCAMORE::reactor config template file\n\n\tReturns\n\t-------\n\tnull\n\t\tWrites xml files for individual reactors in region.\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "write_deployment", "data": "def write_deployment(in_dict, out_path, deployinst_template,\n\t\t\t\t\t inclusions_template):\n\t\"\"\" Renders jinja template using dictionary of reactor name and buildtime.\n\tOutputs an xml file that uses xinclude to include the reactor xml files\n\tlocated in cyclus_input/reactors.\n\n\tParameters\n\t---------\n\tin_dict: dictionary\n\t\tdictionary with key=[reactor name], and value=[buildtime]\n\tout_path: str\n\t\toutput path for files\n\tdeployinst_template: str\n\t\tpath to deployinst template\n\tinclusions_template: str\n\t\tpath to inclusions template\n\n\tReturns\n\t-------\n\tnull\n\t\tgenerates input files that have deployment and xml inclusions\n\t\"\"\"\n\tif out_path[-1] != '/':\n\t\tout_path += '/'\n\tpathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n\tdeployinst_template = load_template(deployinst_template)\n\tinclusions_template = load_template(inclusions_template)\n\tcountry_list = {value[0] for value in in_dict.values()}\n\tfor nation in country_list:\n\t\ttemp_dict = {}\n\t\tfor reactor in in_dict.keys():\n\t\t\tif in_dict[reactor][0].upper() == nation.upper():\n\t\t\t\ttemp_dict.update({reactor: in_dict[reactor][1]})\n\t\tpathlib.Path(out_path + nation.replace(' ', '_') +\n\t\t\t\t\t '/').mkdir(parents=True, exist_ok=True)\n\t\tdeployinst = deployinst_template.render(reactors=temp_dict)\n\t\twith open(out_path + nation.replace(' ', '_') +\n\t\t\t\t  '/deployinst.xml', 'w') as output1:\n\t\t\toutput1.write(deployinst)\n\tinclusions = inclusions_template.render(reactors=in_dict)\n\twith open(out_path + 'inclusions.xml', 'w') as output2:\n\t\toutput2.write(inclusions)\n\n", "description": " Renders jinja template using dictionary of reactor name and buildtime.\n\tOutputs an xml file that uses xinclude to include the reactor xml files\n\tlocated in cyclus_input/reactors.\n\n\tParameters\n\t---------\n\tin_dict: dictionary\n\t\tdictionary with key=[reactor name], and value=[buildtime]\n\tout_path: str\n\t\toutput path for files\n\tdeployinst_template: str\n\t\tpath to deployinst template\n\tinclusions_template: str\n\t\tpath to inclusions template\n\n\tReturns\n\t-------\n\tnull\n\t\tgenerates input files that have deployment and xml inclusions\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "get_buildtime", "data": "def get_buildtime(in_list, start_year, path_list):\n\t\"\"\" Calculates the buildtime required for reactor\n\tdeployment in months.\n\n\tParameters\n\t----------\n\tin_list: list\n\t\tlist of reactors\n\tstart_year: int\n\t\tstarting year of simulation\n\tpath_list: list\n\t\tlist of paths to reactor files\n\n\tReturns\n\t-------\n\tbuildtime_dict: dict\n\t\tdictionary with key=[name of reactor], and\n\t\tvalue=[set of country and buildtime]\n\t\"\"\"\n\tbuildtime_dict = {}\n\tfor row in in_list:\n\t\tcomm_date = date.parse(row[10])\n\t\tstart_date = [comm_date.year, comm_date.month, comm_date.day]\n\t\tdelta = ((start_date[0] - int(start_year)) * 12 +\n\t\t\t\t (start_date[1]) +\n\t\t\t\t round(start_date[2] / (365.0 / 12)))\n\t\tfor index, reactor in enumerate(path_list):\n\t\t\tname = row[1].replace(' ', '_')\n\t\t\tcountry = row[0]\n\t\t\tfile_name = (reactor.replace(\n\t\t\t\tos.path.dirname(path_list[index]), '')).replace('/', '')\n\t\t\tif (name + '.xml' == file_name):\n\t\t\t\tbuildtime_dict.update({name: (country, delta)})\n\treturn buildtime_dict\n\n", "description": " Calculates the buildtime required for reactor\n\tdeployment in months.\n\n\tParameters\n\t----------\n\tin_list: list\n\t\tlist of reactors\n\tstart_year: int\n\t\tstarting year of simulation\n\tpath_list: list\n\t\tlist of paths to reactor files\n\n\tReturns\n\t-------\n\tbuildtime_dict: dict\n\t\tdictionary with key=[name of reactor], and\n\t\tvalue=[set of country and buildtime]\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "deploy_reactors", "data": "def deploy_reactors(in_csv, region, start_year, deployinst_template,\n\t\t\t\t\tinclusions_template, reactors_path, deployment_path):\n\t\"\"\" Generates xml files that specify the reactors that will be included\n\tin a CYCLUS simulation.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath to pris reactor database\n\tregion: str\n\t\tregion name\n\tstart_year: int\n\t\tstarting year of simulation\n\tdeployinst_template: str\n\t\tpath to deployinst template\n\tinclusions_template: str\n\t\tpath to inclusions template\n\treactors_path: str\n\t\tpath containing reactor files\n\tdeployment_path: str\n\t\toutput path for deployinst xml\n\n\tReturns\n\t-------\n\tbuildtime_dict: dict\n\t\tdictionary with key=[name of reactor], and\n\t\tvalue=[set of country and buildtime]\n\t\"\"\"\n\tlists = []\n\tif reactors_path[-1] != '/':\n\t\treactors_path += '/'\n\tfor files in os.listdir(reactors_path):\n\t\tlists.append(reactors_path + files)\n\tin_data = import_csv(in_csv, ',')\n\treactor_list = select_region(in_data, region)\n\tbuildtime = get_buildtime(reactor_list, start_year, lists)\n\twrite_deployment(buildtime, deployment_path, deployinst_template,\n\t\t\t\t\t inclusions_template)\n\treturn buildtime\n\n", "description": " Generates xml files that specify the reactors that will be included\n\tin a CYCLUS simulation.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath to pris reactor database\n\tregion: str\n\t\tregion name\n\tstart_year: int\n\t\tstarting year of simulation\n\tdeployinst_template: str\n\t\tpath to deployinst template\n\tinclusions_template: str\n\t\tpath to inclusions template\n\treactors_path: str\n\t\tpath containing reactor files\n\tdeployment_path: str\n\t\toutput path for deployinst xml\n\n\tReturns\n\t-------\n\tbuildtime_dict: dict\n\t\tdictionary with key=[name of reactor], and\n\t\tvalue=[set of country and buildtime]\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}, {"term": "def", "name": "render_cyclus", "data": "def render_cyclus(cyclus_template, region, in_dict, out_path):\n\t\"\"\" Renders final CYCLUS input file with xml base, and institutions\n\tfor each country\n\n\tParameters\n\t----------\n\tcyclus_template: str\n\t\tpath to CYCLUS input file template\n\tregion: str\n\t\tregion chosen for CYCLUS simulation\n\tin_dict: dictionary\n\t\tin_dict should be buildtime_dict from get_buildtime function\n\tout_path: str\n\t\toutput path for CYCLUS input file\n\toutput_name:\n\n\tReturns\n\t-------\n\tnull\n\t\twrites CYCLUS input file in out_path\n\t\"\"\"\n\tif out_path[-1] != '/':\n\t\tout_path += '/'\n\tcyclus_template = load_template(cyclus_template)\n\tcountry_list = {value[0].replace(' ', '_') for value in in_dict.values()}\n\trendered = cyclus_template.render(countries=country_list,\n\t\t\t\t\t\t\t\t\t  base_dir=os.path.abspath(out_path) + '/')\n\twith open(out_path + region + '.xml', 'w') as output:\n\t\toutput.write(rendered)\n", "description": " Renders final CYCLUS input file with xml base, and institutions\n\tfor each country\n\n\tParameters\n\t----------\n\tcyclus_template: str\n\t\tpath to CYCLUS input file template\n\tregion: str\n\t\tregion chosen for CYCLUS simulation\n\tin_dict: dictionary\n\t\tin_dict should be buildtime_dict from get_buildtime function\n\tout_path: str\n\t\toutput path for CYCLUS input file\n\toutput_name:\n\n\tReturns\n\t-------\n\tnull\n\t\twrites CYCLUS input file in out_path\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit):", "\trecipe = import_csv(in_csv, ',')", "\t\t\timported csv file in list format", "\tin_data = import_csv(in_csv, ',')", "\tin_data = import_csv(in_csv, ',')"]}], [{"term": "def", "name": "ncdefon_message", "data": "async def on_message(message):\n\t# we do not want the bot to reply to itself\n\tif message.author == client.user:\n\t\treturn\n\n\tif message.content.startswith('!hello'):\n\t\tmsg = 'Hello {0.author.mention}'.format(message)\n\t\t# await message.channel.send(msg)\n\t\ttestFile = File('options.csv')\n\t\tawait message.channel.send(file = testFile)\n\n\telif message.content.startswith('!recordanewmixtape'):\n\t\tmsg = 'No {0.author.mention}, you do it'.format(message)\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!tickers'):\n\t\tcmd = message.content\n\t\tcmd = cmd[9:]\n\t\tprint('tickers command ticker: ' + cmd)\n\t\ttry:\n\t\t\ttickers = WebScrape.printTickers(cmd)\n\t\t\tmsg = ''\n\t\t\tfor tick in tickers:\n\t\t\t\tmsg = msg + tick + '\\n'\n\t\texcept:\n\t\t\tmsg = 'Not a valid option\\nLook at !listOptions'\n\t\t\t\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!setOption'):\n\t\tcmd = message.content\n\t\tcmdParts = cmd.split()\n\t\tWebScrape.setOption(cmdParts[1], cmdParts[2])\n\t\tmsg = '{0.author.mention} new option added '.format(message) + cmdParts[2]\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!removeOption'):\n\t\tcmd = message.content\n\t\tcmdParts = cmd.split()\n\t\tWebScrape.removeOption(cmdParts[1])\n\t\tmsg = '{0.author.mention} option was removed '.format(message) + cmdParts[1]\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!listOptions'):\n\t\toptions = WebScrape.getOptions()\n\t\tmsg = ''\n\t\tif options == 'There are no current options setup use !setOptions to set a new option':\n\t\t\tmsg = options\n\t\telse:\n\t\t\tfor opt in options:\n\t\t\t\tmsg = msg + opt + '\\n'\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!stock'):\n\t\tcmd = message.content\n\t\tcmdParts = cmd.split()\n\t\tGenerateGraph.makeGraph(cmdParts[1])\n\t\tgraphFile = File('stock_information.html')\n\t\tawait message.channel.send(file = graphFile)\n\n\n\telif message.content.startswith('!help'):\n\t\tmsg = 'Remember to seperate each part of the command with a space\\n'\n\t\tmsg = msg + '!setOption  \\n'\n\t\tmsg = msg + '!removeOption \\n'\n\t\tmsg = msg + '!listOptions\\n'\n\t\tmsg = msg + '!tickers \\n'\n\t\tmsg = msg + '!stock \\n'\n\t\tawait message.channel.send(msg)\n", "description": null, "category": "webscraping", "imports": ["import discord", "import json", "import WebScrape", "import GenerateGraph", "from discord import File"]}, {"term": "def", "name": "ncdefon_ready", "data": "async def on_ready():\n\tprint('Logged in as')\n\tprint(client.user.name)\n\tprint(client.user.id)\n\tprint('------')\n", "description": null, "category": "webscraping", "imports": ["import discord", "import json", "import WebScrape", "import GenerateGraph", "from discord import File"]}], [{"term": "def", "name": "initMain", "data": "def initMain():\n\t\"\"\"\n\tInitializes all data needed for the main function including\n\tpreprocessing, schedules, and wikipedia related objects\n\t\"\"\"\n\tif True: # TODO: have a condition so it only webscrapes when needed\n\t\tcompleteWebscrape()\n\n\t# Get DF and related info for wikipedia\n\twikiRet = getDF()\n\n\t# Get DF and related info for schedules\n\tschDf = {}\n\tdf_files = glob.glob(\"data/*.csv\")\n\tfor file in df_files:\n\t\tdf_name = re.split(\"\\\\\\\\|/|\\.\", file)[1]\n\t\tschDf[df_name] = pd.read_csv(file, encoding=\"ISO-8859-1\")\n\twith open(\"data/keywords.json\") as json_file:\n\t\tschDict = json.load(json_file)\n\n\t# initialize objects and variables\n\tp = Preprocessor(schDict)\n\tschedulesProcessor = Processor(schDf)\n\n\treturn p, wikiRet, schedulesProcessor\n", "description": "\n\tInitializes all data needed for the main function including\n\tpreprocessing, schedules, and wikipedia related objects\n\t", "category": "webscraping", "imports": ["from wikipedia import webscrapeWikipedia, getResponse, getDF", "from schedules_query_processor import Processor", "from classification import Preprocessor", "import json", "import glob", "import pandas as pd", "import re"]}, {"term": "def", "name": "completeWebscrape", "data": "def completeWebscrape():\n\twebscrapeWikipedia()\n\t# TODO add webscrape schedules in this function\n", "description": null, "category": "webscraping", "imports": ["from wikipedia import webscrapeWikipedia, getResponse, getDF", "from schedules_query_processor import Processor", "from classification import Preprocessor", "import json", "import glob", "import pandas as pd", "import re"]}, {"term": "def", "name": "handleQuery", "data": "def handleQuery(query, p, schP, wikiRet):\n\tqp = p.preprocessAndClassifyQuery(query)\n\tresp = \"\"\n\tif qp[\"classification\"] == \"schedules\":\n\t\tresp = schP.getResponse(qp)\n\t\tif resp == \"\":\n\t\t\t# If schedules can't find it, try wikipedia\n\t\t\tresp = getResponse(wikiRet[0], wikiRet[1], wikiRet[2], qp)\n\telif qp[\"classification\"] == \"wikipedia\":\n\t\tresp = getResponse(wikiRet[0], wikiRet[1], wikiRet[2], qp)\n\tif resp == \"\":\n\t\tresp = \"Sorry I don't know how to respond to that.\"\n\treturn resp\n", "description": null, "category": "webscraping", "imports": ["from wikipedia import webscrapeWikipedia, getResponse, getDF", "from schedules_query_processor import Processor", "from classification import Preprocessor", "import json", "import glob", "import pandas as pd", "import re"]}], [], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(Resource):\n\tdef get(self):\n\t\treturn jsonify(webscrape.get_special_menu_json())\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify", "from flask_restful import Resource, Api", "import webscrape"]}], [], [{"term": "def", "name": "main", "data": "def main():\n\tif request.method == 'POST':\n\t\tnewsub = request.form['subreddit']\n\t\t#print(newsub)\n\t\tdata = runWebscrape(newsub)\n\t\ttitle = get_title(data, 0)\n\t\turl = get_link(data, 0)\n\t\tsentiment = np.round(get_sentiment(data),5)\n\t\tsub_link = \"https://www.reddit.com/r/\" + newsub\n\t\t#exists = check_url(sub_link)\n\t\t#if not exists:\n\t\t#\tsub_link =  \"https://www.reddit.com/r/worldnews/\"\n\t\texists = True\n\t\treturn render_template(\"index.html\", headline = title, link = url, sentiment = sentiment, subreddit = newsub, sub_link= sub_link)\n\telse:\n\t\t#a = get database info\n\t\tdata = runWebscrape(\"WorldNews\")\n\t\t#print(data.to_string())\n\t\ttitle = get_title(data, 0)\n\t\turl = get_link(data, 0)\n\t\tsentiment = np.round(get_sentiment(data), 5)\n\t\t#hlOneImg = data.iloc[0]['title']\n\t\t#hlTwoImg = data.iloc[1]['title']\n\t\t#imgscr.main(hlOneImg);\n\t\t#imgscr.main(hlTwoImg);\n\t\treturn render_template('index.html', headline = title, link = url, sentiment = sentiment, subreddit=\"WorldNews\", sub_link = \"https://www.reddit.com/r/worldnews/\", exists=True)\n\n", "description": null, "category": "webscraping", "imports": ["from flask import (", "from werkzeug.exceptions import abort", "from werkzeug.utils import secure_filename", "import pandas as pd", "import numpy as np", "import webscrape_script as wbscr", "import image_script as imgscr", "import nlanguage_script as langscr", "import requests", "#import urlib2"]}, {"term": "def", "name": "method", "data": "def method():\n\treturn render_template('method.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import (", "from werkzeug.exceptions import abort", "from werkzeug.utils import secure_filename", "import pandas as pd", "import numpy as np", "import webscrape_script as wbscr", "import image_script as imgscr", "import nlanguage_script as langscr", "import requests", "#import urlib2"]}, {"term": "def", "name": "runWebscrape", "data": "def runWebscrape( subreddit ):\n\tdict = wbscr.main(subreddit)\n\treturn dict\n", "description": null, "category": "webscraping", "imports": ["from flask import (", "from werkzeug.exceptions import abort", "from werkzeug.utils import secure_filename", "import pandas as pd", "import numpy as np", "import webscrape_script as wbscr", "import image_script as imgscr", "import nlanguage_script as langscr", "import requests", "#import urlib2"]}, {"term": "def", "name": "runLangScript", "data": "def runLangScript( phrase ):\n\treturn langscr.main( phrase )\n", "description": null, "category": "webscraping", "imports": ["from flask import (", "from werkzeug.exceptions import abort", "from werkzeug.utils import secure_filename", "import pandas as pd", "import numpy as np", "import webscrape_script as wbscr", "import image_script as imgscr", "import nlanguage_script as langscr", "import requests", "#import urlib2"]}, {"term": "def", "name": "get_sentiment", "data": "def get_sentiment( data ):\n\tlst = [runLangScript(get_title(data, index)) for index in range(8)]\n\treturn np.average(lst)\n", "description": null, "category": "webscraping", "imports": ["from flask import (", "from werkzeug.exceptions import abort", "from werkzeug.utils import secure_filename", "import pandas as pd", "import numpy as np", "import webscrape_script as wbscr", "import image_script as imgscr", "import nlanguage_script as langscr", "import requests", "#import urlib2"]}, {"term": "def", "name": "get_title", "data": "def get_title(data, index):\n\tnew_data = data['title']\n\ttitle = str(new_data[index])\n\treturn title\n", "description": null, "category": "webscraping", "imports": ["from flask import (", "from werkzeug.exceptions import abort", "from werkzeug.utils import secure_filename", "import pandas as pd", "import numpy as np", "import webscrape_script as wbscr", "import image_script as imgscr", "import nlanguage_script as langscr", "import requests", "#import urlib2"]}, {"term": "def", "name": "get_link", "data": "def get_link(data, index):\n\tnew_data = data['url']\n\turl = str(new_data[index])\n\treturn url\n", "description": null, "category": "webscraping", "imports": ["from flask import (", "from werkzeug.exceptions import abort", "from werkzeug.utils import secure_filename", "import pandas as pd", "import numpy as np", "import webscrape_script as wbscr", "import image_script as imgscr", "import nlanguage_script as langscr", "import requests", "#import urlib2"]}, {"term": "def", "name": "check_url", "data": "def check_url(link):\n\trequest = requests.get(link)\n\tif request.status_code == 200:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "description": null, "category": "webscraping", "imports": ["from flask import (", "from werkzeug.exceptions import abort", "from werkzeug.utils import secure_filename", "import pandas as pd", "import numpy as np", "import webscrape_script as wbscr", "import image_script as imgscr", "import nlanguage_script as langscr", "import requests", "#import urlib2"]}], [{"term": "def", "name": "get_airbus_value", "data": "def get_airbus_value():\n\t#Get beautiful soup objects\n\tairbus = WebScrape('https://markets.businessinsider.com/stocks/airbus-stock?op=1', {\n\t\t'Access-Control-Allow-Origin': '*',\n\t\t'Access-Control-Allow-Methods': 'GET',\n\t\t'Access-Control-Allow-Headers': 'Content-Type',\n\t\t'Access-Control-Max-Age': '3600',\n\t\t'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n\t\t})\n\tairbusPrice = airbus.scraper.find('span', class_=\"price-section__current-value\")\n\tairbusHoldings = float(airbusPrice.text)\n\t#Convert to AUD\n\teur_audConverter = WebScrape('https://www.x-rates.com/calculator/?from=EUR&to=AUD&amount=1', headers)\n\teuro = eur_audConverter.scraper.find('span', class_ ='ccOutputRslt').text\n\trates = re.findall(r'\\d+\\.\\d+', euro)\n\tairbusAUDTotal = float(rates[0]) * float(airbusHoldings)\n\tAirAmt = PortHoldings.objects.get(nameFund=\"Airbus\")\n\tAirValue = float(AirAmt.numHoldings) * float(airbusAUDTotal)\n\n\treturn AirAmt, AirValue\n", "description": null, "category": "webscraping", "imports": ["from concurrent.futures import ThreadPoolExecutor", "from webbrowser import get", "import re", "import requests", "from .models import PortHoldings", "from .web_scrape import WebScrape"]}, {"term": "def", "name": "get_etherium_value", "data": "def get_etherium_value():\n\tetherium = WebScrape('https://www.independentreserve.com/market/eth', headers)\n\tethPrice = etherium.scraper.find('span', class_=\"currency-value__amount\")\n\tEthAmt = PortHoldings.objects.get(nameFund=\"Etherium\")\n\tEthValue = float(EthAmt.numHoldings) * float(ethPrice.text.replace(',','').strip('$'))\n\n\treturn EthAmt, EthValue\n", "description": null, "category": "webscraping", "imports": ["from concurrent.futures import ThreadPoolExecutor", "from webbrowser import get", "import re", "import requests", "from .models import PortHoldings", "from .web_scrape import WebScrape"]}, {"term": "def", "name": "get_bitcoin_value", "data": "def get_bitcoin_value():\n\tbitcoin = WebScrape('https://www.independentreserve.com/market/btc', headers)\n\tbitcoinPrice = bitcoin.scraper.find('span', class_=\"currency-value__amount\")\n\tBitAmt = PortHoldings.objects.get(nameFund=\"Bitcoin\")\n\tBitValue = float(BitAmt.numHoldings) * float(bitcoinPrice.text.replace(',','').strip('$'))\n\t\n\treturn BitAmt, BitValue\n", "description": null, "category": "webscraping", "imports": ["from concurrent.futures import ThreadPoolExecutor", "from webbrowser import get", "import re", "import requests", "from .models import PortHoldings", "from .web_scrape import WebScrape"]}, {"term": "def", "name": "get_VDHG_value", "data": "def get_VDHG_value():\n\tvdhg = WebScrape('https://www.google.com/finance/quote/VDHG:ASX', headers)\n\t#Scrape stock price value from page\n\tvdhgPrice = vdhg.scraper.find('div', class_=\"YMlKec fxKbKc\")\n\t#Calculate total price\n\tVDHGAmt = PortHoldings.objects.get(nameFund=\"VDHG\", institution=\"CommSec\")\n\tVDHGValue = float(VDHGAmt.numHoldings) * float(vdhgPrice.text.replace(',','').strip('$'))\n\n\treturn VDHGAmt, VDHGValue\n", "description": null, "category": "webscraping", "imports": ["from concurrent.futures import ThreadPoolExecutor", "from webbrowser import get", "import re", "import requests", "from .models import PortHoldings", "from .web_scrape import WebScrape"]}, {"term": "def", "name": "get_VESG_value", "data": "def get_VESG_value():\n\tvesg = WebScrape('https://www.google.com/finance/quote/VESG:ASX', headers)\n\tvesgPrice = vesg.scraper.find('div', class_=\"YMlKec fxKbKc\")\n\tVESGAmt = PortHoldings.objects.get(nameFund=\"VESG\")\n\tVESGValue = float(VESGAmt.numHoldings) * float(vesgPrice.text.replace(',','').strip('$'))\n\n\treturn VESGAmt, VESGValue\n", "description": null, "category": "webscraping", "imports": ["from concurrent.futures import ThreadPoolExecutor", "from webbrowser import get", "import re", "import requests", "from .models import PortHoldings", "from .web_scrape import WebScrape"]}, {"term": "def", "name": "get_vanguard_value", "data": "def get_vanguard_value():\n\tvanguard = WebScrape('https://www.morningstar.com.au/Fund/FundReportPrint/5402', headers)\n\tvanguardPrice = vanguard.scraper.find_all('span', class_=\"YMWpadright\")\n\tVanguardAmt = PortHoldings.objects.get(institution=\"Vanguard\")\n\tVanguardValue = float(VanguardAmt.numHoldings) * float(vanguardPrice[4].text.replace(',','').strip('$'))\n\n\treturn VanguardAmt, VanguardValue\n", "description": null, "category": "webscraping", "imports": ["from concurrent.futures import ThreadPoolExecutor", "from webbrowser import get", "import re", "import requests", "from .models import PortHoldings", "from .web_scrape import WebScrape"]}, {"term": "def", "name": "get_hbar_value", "data": "def get_hbar_value():\n\thbar = WebScrape('https://coinmarketcap.com/currencies/hedera/hbar/aud/', headers)\n\thbarPrice = hbar.scraper.find('div', class_=\"priceValue\")\n\tHbarAmt = PortHoldings.objects.get(nameFund=\"Hbar\")\n\tHbarValue = float(HbarAmt.numHoldings) * float(hbarPrice.text.replace(',','').strip('$'))\n\n\treturn HbarAmt, HbarValue\n", "description": null, "category": "webscraping", "imports": ["from concurrent.futures import ThreadPoolExecutor", "from webbrowser import get", "import re", "import requests", "from .models import PortHoldings", "from .web_scrape import WebScrape"]}, {"term": "def", "name": "scrapeData", "data": "def scrapeData():\t\n\n\t# Using ThreadPoolExecutor so all the webscraping functions can run at once, drastically increasing render time\n\twith ThreadPoolExecutor(max_workers=10) as executor:\n\t\tf1 = executor.submit(get_airbus_value)\n\t\tf2 = executor.submit(get_etherium_value)\n\t\tf3 = executor.submit(get_VESG_value)\n\t\tf4 = executor.submit(get_VDHG_value)\n\t\tf5 = executor.submit(get_bitcoin_value)\n\t\tf6 = executor.submit(get_vanguard_value)\n\t\tf7 = executor.submit(get_hbar_value)\n\n\tAirAmt, AirValue = f1.result()\n\tEthAmt, EthValue = f2.result()\n\tVESGAmt, VESGValue = f3.result()\n\tVDHGAmt, VDHGValue = f4.result()\n\tBitAmt, BitValue = f5.result()\n\tVanguardAmt, VanguardValue = f6.result()\n\tHbarAmt, HbarValue = f7.result()\n\n\tholdingsDatabase = PortHoldings.objects.all().values()\n\toutput = \"\"\n\tfor x in holdingsDatabase:\n\t\toutput += x['nameFund']\t\t\n\n\ttotalValue = (AirValue + EthValue + BitValue + VanguardValue + VESGValue + VDHGValue + HbarValue)\n\n\tholdings = {'funds':[{'fundName': VESGAmt.nameFund, \"value\": VESGValue},\n\t\t\t\t\t{'fundName': VDHGAmt.nameFund, \"value\": VDHGValue},\n\t\t\t\t\t{'fundName': EthAmt.nameFund, \"value\": EthValue},\n\t\t\t\t\t{'fundName': BitAmt.nameFund, \"value\": BitValue},\n\t\t\t\t\t{'fundName': AirAmt.nameFund, \"value\": AirValue},\n\t\t\t\t\t{'fundName': VanguardAmt.institution, \"value\": VanguardValue},\n\t\t\t\t\t{'fundName': HbarAmt.nameFund, \"value\": HbarValue},]\n\t\t\t\t} \n\treturn holdings, totalValue, holdingsDatabase\n", "description": null, "category": "webscraping", "imports": ["from concurrent.futures import ThreadPoolExecutor", "from webbrowser import get", "import re", "import requests", "from .models import PortHoldings", "from .web_scrape import WebScrape"]}], [], [], [], [{"term": "def", "name": "response_and_soup", "data": "def response_and_soup(url_link):\n\tresp = requests.get(url_link)\n\tsoup = BeautifulSoup(resp.text, 'html.parser')\n\treturn soup\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pprint", "import sys"]}, {"term": "def", "name": "get_link", "data": "def get_link(soup):\n\treturn soup.select('.storylink')\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pprint", "import sys"]}, {"term": "def", "name": "get_subtext", "data": "def get_subtext(soup):\n\treturn soup.select('.subtext')\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pprint", "import sys"]}, {"term": "def", "name": "sort_stories_by_votes", "data": "def sort_stories_by_votes(hnlist):\n\treturn sorted(hnlist, key=lambda k: k['votes'], reverse=True)\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pprint", "import sys"]}, {"term": "def", "name": "create_custom_fn", "data": "def create_custom_fn(links, subtext):\n\thn = []\n\tfor idx, item in enumerate(links):\n\t\ttitle = links[idx].getText()\n\t\thref = links[idx].get('href', None)\n\t\tvote = subtext[idx].select('.score')\n\t\tif len(vote):\n\t\t\tpoints = int(vote[0].getText().replace(' points', ''))\n\t\t\tif points > 99:\n\t\t\t\thn.append({'title': title, 'votes': points, 'link': href})\n\treturn sort_stories_by_votes(hn)\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pprint", "import sys"]}, {"term": "def", "name": "get_mega_links_and_subtext", "data": "def get_mega_links_and_subtext(urls):\n\tmega_link = []\n\tmega_subtext = []\n\tfor url in urls:\n\t\tmega_link.extend(get_link(response_and_soup(url)))\n\t\tmega_subtext.extend(get_subtext(response_and_soup(url)))\n\tpprint.pprint(create_custom_fn(mega_link, mega_subtext))\n\tprint(\"DONE !!!\")\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pprint", "import sys"]}, {"term": "def", "name": "input_urls", "data": "def input_urls():\n\turls_to_webscrape = []\n\turl = 'https://' + input('Please enter the url to scrape from: ')\n\turls_to_webscrape.append(url)\n\twhile True:\n\t\tmore = input('Do you have more links? y/n: ').lower()\n\t\tif more == 'n':\n\t\t\tbreak\n\t\telif more == 'y':\n\t\t\turl = 'https://' + input('Please enter the url to scrape from: ')\n\t\t\turls_to_webscrape.append(url)\n\t\telse:\n\t\t\tprint('Invalid input! Try again')\n\n\treturn get_mega_links_and_subtext(urls_to_webscrape)\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pprint", "import sys"]}], [{"term": "def", "name": "home", "data": "def home(request):\n\tform = UserLoginForm(data=request.POST or None)\n\tif request.method == 'POST':\n\t\tif form.is_valid():\n\t\t\tform.save()\n\t\t\treturn redirect('Account')\n\n\tcontext = {\n\t\t'form': form\n\t}\n\treturn render(request, 'Resellers_MarketWatch/MarketWatch_home.html', context)\n\n", "description": null, "category": "webscraping", "imports": ["from django.http import Http404", "from django.shortcuts import render, redirect", "from .forms import WebscrapeForm, UserLoginForm", "from .models import WebScrape"]}, {"term": "def", "name": "account", "data": "def account(request):\n\tform = WebscrapeForm(data=request.POST or None)\n\tif request.method == 'POST':\n\t\tprint('Method is POST')\n\t\tif form.is_valid():\n\t\t\tprint('Form is valid')\n\t\t\tform.save()\n\t\t\treturn redirect('Listview')\n\n\tcontext = {\n\t\t'form': form\n\t}\n\treturn render(request, 'Resellers_MarketWatch/AccountPage.html', context)\n\n", "description": null, "category": "webscraping", "imports": ["from django.http import Http404", "from django.shortcuts import render, redirect", "from .forms import WebscrapeForm, UserLoginForm", "from .models import WebScrape"]}, {"term": "def", "name": "all_webscrape", "data": "def all_webscrape(request):\n\tdataset = WebScrape.WebScrape_db.all()\n\tcontext = {\n\t\t'dataset': dataset\n\t}\n\treturn render(request, 'Resellers_MarketWatch/Listview.html', context)\n\n", "description": null, "category": "webscraping", "imports": ["from django.http import Http404", "from django.shortcuts import render, redirect", "from .forms import WebscrapeForm, UserLoginForm", "from .models import WebScrape"]}, {"term": "def", "name": "detailsview", "data": "def detailsview(request, pk):\n\ttry:\n\t\tdata = WebScrape.WebScrape_db.get(id=pk)\n\texcept WebScrape.DoesNotExist:\n\t\traise Http404('Data does not exist')\n\n", "description": null, "category": "webscraping", "imports": ["from django.http import Http404", "from django.shortcuts import render, redirect", "from .forms import WebscrapeForm, UserLoginForm", "from .models import WebScrape"]}], [], [{"term": "def", "name": "ncdefon_message", "data": "async def on_message(message):\r\n\r\n\tif message.author == client.user:\r\n\t\treturn\r\n\t\r\n\t#message.content = message.content.lower()\r\n\r\n\tif message.content.lower() == (f'$hello'):\r\n\t\tawait message.channel.send(\"Hello nye\")\r\n\r\n\tif message.content.lower() == (f'$livemiko'):\r\n\t\tif WebScrape.is_liveYT(\"https://www.youtube.com/channel/UC-hM6YJuNYVAmUWxeIr9FeA/live\") == True:\r\n\t\t\tawait message.channel.send(\"Nye :cherry_blossom: \\n https://www.youtube.com/channel/UC-hM6YJuNYVAmUWxeIr9FeA/live\")\r\n\t\telse:\r\n\t\t\tawait message.channel.send(\"No nye :cry: \")\r\n\r\n\tif message.content.lower() == (f\"$help\"):\r\n\t\tfor command in command_list:\r\n\t\t\tawait message.channel.send(\"$\" + command + \": \" + command_list[command])\r\n\t\r\n\tif (f\"apex legends\") in message.content.lower():\r\n\t\t\tawait message.add_reaction(u\"\\U0001F92E\")\r\n\t\r\n\tif \"$malfav\" in message.content.lower():\r\n\t\tmsg = message.content.split(\" \")\r\n\t\tif(len(msg) == 3):\r\n\t\t\tif message.content.split(\" \")[2].lower() == \"character\":\r\n\t\t\t\tcharacters = WebScrape.getUserFavoriteCharacter(message.content.split(\" \")[1])\r\n\r\n\t\t\t\tcount=0\r\n\t\t\t\tprintString= \"**\" + msg[1] + \"'s** favorite characters: \\n\" + \"---------------------\\n\"\r\n\t\t\t\tfor character in characters:\r\n\t\t\t\t\tcount+=1\r\n\t\t\t\t\tif count==1:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + character + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n\t\t\t\t\t\tcontinue \r\n\t\t\t\t\tif count==2:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + character + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tif count==3:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + character + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tprintString+=str(count) + \". \" + character + \"\\n\"\r\n\t\t\t\tawait message.channel.send(printString)\r\n\t\t\t\tif count==0:\r\n\t\t\t\t\tawait message.channel.send(\"This user doesn't have a favorite character!\")\r\n\r\n\t\t\telif message.content.split(\" \")[2] == \"anime\":\r\n\t\t\t\tanimes = WebScrape.getUserFavoriteAnimes(message.content.split(\" \")[1])\r\n\r\n\t\t\t\tcount=0\r\n\t\t\t\tprintString= \"**\" + msg[1] + \"'s** favorite animes: \\n\" \"---------------------\\n\"\r\n\t\t\t\tfor anime in animes:\r\n\t\t\t\t\tcount+=1\r\n\t\t\t\t\tif count==1:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + anime + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n\t\t\t\t\t\tcontinue \r\n\t\t\t\t\tif count==2:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + anime + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tif count==3:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + anime + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tprintString+=str(count) + \". \" + anime + \"\\n\"\r\n\t\t\t\tawait message.channel.send(printString)\r\n\t\t\t\tif count==0:\r\n\t\t\t\t\tawait message.channel.send(\"This user doesn't have a favorite anime!\")\r\n\r\n\t\t\telif message.content.split(\" \")[2] == \"manga\":\r\n\t\t\t\tmangas = WebScrape.getUserFavoriteMangas(message.content.split(\" \")[1])\r\n\r\n\t\t\t\tcount=0\r\n\t\t\t\tprintString= \"**\" + msg[1] + \"'s** favorite mangas: \\n\" \"---------------------\\n\"\r\n\t\t\t\tfor manga in mangas:\r\n\t\t\t\t\tcount+=1\r\n\t\t\t\t\tif count==1:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + manga + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n\t\t\t\t\t\tcontinue \r\n\t\t\t\t\tif count==2:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + manga + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tif count==3:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + manga + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tprintString+=str(count) + \". \" + manga + \"\\n\"\r\n\t\t\t\tawait message.channel.send(printString)\r\n\t\t\t\tif count==0:\r\n\t\t\t\t\tawait message.channel.send(\"This user doesn't have a favorite manga!\")\r\n\t\t\t\r\n\t\t\telif message.content.split(\" \")[2] == \"people\":\r\n\t\t\t\tpeople = WebScrape.getUserFavoritePeople(message.content.split(\" \")[1])\r\n\r\n\t\t\t\tcount=0\r\n\t\t\t\tprintString= \"**\" + msg[1] + \"'s** favorite people: \\n\" + \"---------------------\\n\"\r\n\t\t\t\tfor person in people:\r\n\t\t\t\t\tcount+=1\r\n\t\t\t\t\tif count==1:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + person + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n\t\t\t\t\t\tcontinue \r\n\t\t\t\t\tif count==2:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + person + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tif count==3:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + person + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tprintString+=str(count) + \". \" + person + \"\\n\"\r\n\t\t\t\tawait message.channel.send(printString)\r\n\t\t\t\tif count==0:\r\n\t\t\t\t\tawait message.channel.send(\"This user doesn't have a favorite person!\")\r\n\t\t\t\r\n\t\t\telif message.content.split(\" \")[2] == \"producer\":\r\n\t\t\t\tproducers = WebScrape.getUserFavoriteCompanies(message.content.split(\" \")[1])\r\n\r\n\t\t\t\tcount=0\r\n\t\t\t\tprintString= \"**\" + msg[1] + \"'s** favorite producers: \\n\" + \"---------------------\\n\"\r\n\t\t\t\tfor producer in producers:\r\n\t\t\t\t\tcount+=1\r\n\t\t\t\t\tif count==1:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + producer + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n\t\t\t\t\t\tcontinue \r\n\t\t\t\t\tif count==2:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + producer + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tif count==3:\r\n\t\t\t\t\t\tprintString+=str(count) + \". \" + producer + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\tprintString+=str(count) + \". \" + producer + \"\\n\"\r\n\t\t\t\tawait message.channel.send(printString)\r\n\t\t\t\tif count==0:\r\n\t\t\t\t\tawait message.channel.send(\"This user doesn't have a favorite producer!\")\r\n\t\t\r\n\t\telif(len(msg) == 2):\r\n\t\t\teverything = WebScrape.getUserTopFavorites(message.content.split(\" \")[1])\r\n\t\t\tfor entry in everything:\r\n\t\t\t\tawait message.channel.send(entry)\r\n\t\telse:\r\n\t\t\tawait message.channel.send(\"Correct Syntax: $malfav (username) (category)\")\r\n\t\r\n\tif f\"$malscore\" in message.content.lower():\r\n\t\tmsg = message.content.split(\" \")\r\n\t\tusername = msg[1]\r\n\t\tanime = msg[2:]\r\n\t\tanimeName = \"\"\r\n\t\tfor word in anime:\r\n\t\t\tif anime.index(word) != len(anime)-1:\r\n\t\t\t\tanimeName = animeName + word + \" \"\r\n\t\t\telse:\r\n\t\t\t\tanimeName = animeName + word\r\n\t\t\r\n\t\tanimeList = WebScrape.getUserScoreAnime(username, animeName)[0]\r\n\t\tscores = WebScrape.getUserScoreAnime(username, animeName)[1]\r\n\t\tstatus = WebScrape.getUserScoreAnime(username, animeName)[2]\r\n\r\n\t\tawait message.channel.send(\"Searching **\" + username + \"'s** list for \\\"\" + animeName + \"\\\"... \\n\")\r\n\r\n\t\tif len(animeList) == 0:\r\n\t\t\tawait message.channel.send(\"**\" + username + \"** does not have a rating for \\\"\" + animeName + \"\\\"\")\r\n\r\n\t\telse:\r\n\t\t\toutput = \"\"\r\n\t\t\tcounter=0\r\n\t\t\tfor entry in animeList:\r\n\r\n\t\t\t\tif status[counter] == '2':\r\n\t\t\t\t\tif scores[counter] == '0':\r\n\t\t\t\t\t\toutput = output + \":white_check_mark:  : \" + entry + \" : **\" + \"-\" + \"**\\n\"\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\toutput = output + \":white_check_mark:  : \" + entry + \" : **\" + scores[counter] + \"**\\n\"\r\n\t\t\t\t\t\r\n\r\n\t\t\t\telif status[counter] == '3':\r\n\t\t\t\t\tif scores[counter] == '0':\r\n\t\t\t\t\t\toutput = output + \"\u00e2\u0153\u2039 : \" + entry + \" : **\" + \"-\" + \"**\\n\"\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\toutput = output + \"\u00e2\u0153\u2039 : \" + entry + \" : **\" + scores[counter] + \"**\\n\"\r\n\r\n\t\t\t\telif status[counter] == '4':\r\n\t\t\t\t\tif scores[counter] == '0':\r\n\t\t\t\t\t\toutput = output + \"\u00f0\u0178\u203a\u2018 : \" + entry + \" : **\" + \"-\" + \"**\\n\"\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\toutput = output + \"\u00f0\u0178\u203a\u2018 : \" + entry + \" : **\" + scores[counter] + \"**\\n\"\r\n\r\n\t\t\t\tcounter+=1\r\n\t\t\t\r\n\t\t\tawait message.channel.send(output)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "import os\r", "import WebScrape\r", "from dotenv import load_dotenv\r"]}, {"term": "def", "name": "ncdefon_ready", "data": "async def on_ready():\r\n\tprint(f'{client.user} is online nye')\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "import os\r", "import WebScrape\r", "from dotenv import load_dotenv\r"]}], [{"term": "def", "name": "create_directory", "data": "def create_directory(directory_name, xlsx_name):\n\tdirname = os.path.dirname(__file__)\n\tpath = os.path.join(dirname, directory_name)\n\tfilename = os.path.join(path, xlsx_name + '.xlsx')\n\tif not os.path.exists(path):\n\t\tos.mkdir(path)\n\treturn(filename)\n", "description": null, "category": "webscraping", "imports": ["from utils import *"]}, {"term": "class", "name": "classposition_functions:", "data": "class position_functions:\n\tdef offence_function(df):\n\t\toffence = 0.65 * (np.sum(df.loc[0]) + 0.35 * (np.sum(df.loc[1])))\n\t\treturn offence\n\n\tdef defence_function(df):\n\t\tdefence = 0.35 * (np.sum(df.loc[0])) + 0.25 * (np.sum(df.loc[1])) + 0.4 * (np.sum(df.loc[2]))\n\t\treturn defence\n\n\tdef goalie_function(df):\n\t\tgoalie = np.sum(df)\n", "description": null, "category": "webscraping", "imports": ["from utils import *"]}, {"term": "class", "name": "classdataframe_manipulation:", "data": "class dataframe_manipulation:\n\tdef offence_dataframe(df, offence_defence_table_number):\n\t\tteam_df = df[offence_defence_table_number].loc[:len(df[offence_defence_table_number]).__index__()-1,\n\t\t\t\t  [('Scoring', 'G'), ('Unnamed: 17_level_0', 'S')]]\n\t\treturn team_df\n\n\tdef defence_dataframe(df, offence_defence_table_number):\n\t\tdefence_df = pd.DataFrame(df[offence_defence_table_number])\n\t\tteam_defence_df = defence_df.loc[:len(df[offence_defence_table_number]).__index__()-1,\n\t\t\t\t\t\t  [('Unnamed: 24_level_0', 'BLK'), ('Unnamed: 25_level_0', 'HIT'),\n\t\t\t\t\t\t   ('Unnamed: 26_level_0', 'FOW'), ('Unnamed: 27_level_0', 'FOL')]]\n\t\tteam_defence_df['Unnamed: 26_level_0', 'XXX'] = team_defence_df['Unnamed: 26_level_0', 'FOW'] -\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tteam_defence_df['Unnamed: 27_level_0', 'FOL']\n\t\tteam_defence_df.drop(columns=[('Unnamed: 26_level_0', 'FOW'), ('Unnamed: 27_level_0', 'FOL')])\n\t\treturn team_defence_df\n\n\tdef goalie_dataframe(df, goalie_table_number):\n\t\tteam_goalie_df = df[goalie_table_number].loc[0:2, [('Goalie Stats', 'GA')]]\n\t\treturn team_goalie_df\n\n\tdef concat_teams(dictionary):\n\t\tfor i in list(dictionary.keys()):\n\t\t\tdf = pd.concat(dictionary.get(i))\n\t\t\treturn df\n", "description": null, "category": "webscraping", "imports": ["from utils import *"]}, {"term": "def", "name": "webscrape_function", "data": "def webscrape_function(team_name, season_year):\n\tdf = pd.read_html('https://www.hockey-reference.com/teams/' + team_name + '/' + season_year + '.html')\n", "description": null, "category": "webscraping", "imports": ["from utils import *"]}, {"term": "def", "name": "normalization_function", "data": "def normalization_function(df):\n\tnormalized_value = ( df - pd.DataFrame.mean(df) ) / pd.DataFrame.std(df)\n", "description": null, "category": "webscraping", "imports": ["from utils import *"]}, {"term": "class", "name": "classtodays_games:", "data": "class todays_games:\n\ttodays_date = date.today().strftime(\"%Y%m%d\")\n\tlink_to_todays_games = pd.read_html('https://www.cbssports.com/nhl/schedule/' + todays_date)\n\t@staticmethod\n\tdef home_teams():\n\t\ttodays_home_teams_df = todays_games.link_to_todays_games[0]['Home'].map(team_abbrev)\n\t\treturn todays_home_teams_df\n\t@staticmethod\n\tdef away_teams():\n\t\ttodays_away_teams_df = todays_games.link_to_todays_games[0]['Away'].map(team_abbrev)\n\t\treturn todays_away_teams_df\n\n", "description": null, "category": "webscraping", "imports": ["from utils import *"]}], [], [{"term": "def", "name": "return_user", "data": "def return_user(conn, email):\n\tcur = conn.cursor()\n\tcur.execute(\"SELECT * FROM webscrape.users WHERE email = %s\", (email,))\n\treturn cur.fetchone()\n\n", "description": null, "category": "webscraping", "imports": ["from database.data_classes import Alerts, Items, Price_History, User"]}, {"term": "def", "name": "add_user", "data": "def add_user(conn, person: User):\n\t\"\"\"add user to db\"\"\"\n\tcur = conn.cursor()\n\tcur.execute(\n\t\t\"INSERT INTO webscrape.users (email, password) VALUES (%s, %s)\",\n\t\t(person.email, person.password),\n\t)\n\tcur.close()\n\treturn\n\n", "description": "add user to db", "category": "webscraping", "imports": ["from database.data_classes import Alerts, Items, Price_History, User"]}, {"term": "def", "name": "add_item", "data": "def add_item(conn, item: Items):\n\tcur = conn.cursor()\n\tcur.execute(\n\t\t\"INSERT INTO webscrape.items (asin, title, current_amount, currency) VALUES (%s, %s, %s, %s)\",\n\t\t(item.asin, item.title, item.current_amount, item.currency),\n\t)\n\tcur.close()\n\treturn\n\n", "description": null, "category": "webscraping", "imports": ["from database.data_classes import Alerts, Items, Price_History, User"]}, {"term": "def", "name": "add_alert", "data": "def add_alert(conn, alert: Alerts):\n\t\"\"\"add alert to db\"\"\"\n\tcur = conn.cursor()\n\tcur.execute(\n\t\t\"INSERT INTO webscrape.alerts (asin, id, target_amount) VALUES (%s, %s, %s)\",\n\t\t(alert.asin, alert.id, alert.target_amount),\n\t)\n\tcur.close()\n\treturn\n\n", "description": "add alert to db", "category": "webscraping", "imports": ["from database.data_classes import Alerts, Items, Price_History, User"]}, {"term": "def", "name": "add_price_history", "data": "def add_price_history(conn, ph: Price_History):\n\t\"\"\"add ph to db\"\"\"\n\tcur = conn.cursor()\n\tcur.execute(\n\t\t\"INSERT INTO webscrape.price_history (asin, date, amount, currency) VALUES (%s, %s, %s, %s)\",\n\t\t(ph.asin, ph.date, ph.amount, ph.currency),\n\t)\n\tcur.close()\n\treturn\n\n", "description": "add ph to db", "category": "webscraping", "imports": ["from database.data_classes import Alerts, Items, Price_History, User"]}, {"term": "def", "name": "list_of_unique_asins", "data": "def list_of_unique_asins(conn):\n\t# the asins have empty space, caused by the psql table defintions\n\tcur = conn.cursor()\n\tcur.execute(\"SELECT DISTINCT asin FROM webscrape.alerts\")\n\tx = [i[0] for i in cur.fetchall()]\n\tcur.close()\n\treturn x\n\n", "description": null, "category": "webscraping", "imports": ["from database.data_classes import Alerts, Items, Price_History, User"]}, {"term": "def", "name": "print_table", "data": "def print_table(conn):\n\tcur = conn.cursor()\n\tcur.execute(\"SELECT * FROM webscrape.alerts\")\n\ta = cur.fetchall()\n\tprint(a)\n\tcur.close()\n\treturn\n\n", "description": null, "category": "webscraping", "imports": ["from database.data_classes import Alerts, Items, Price_History, User"]}], [{"term": "class", "name": "RecipeSearchTerm", "data": "class RecipeSearchTerm(FlaskForm):\n\tsearch_term = StringField(\n\t\t'Search Term', \n\t\tvalidators=[DataRequired()]\n\t)\n\n\timage_format = SelectField(\"Choose an option\", choices=[(\"none\", \"None\"), (\"grayscale\", \"Grayscale\"), (\"negative\", \"Negative\"), (\"sephia\", \"Sephia\"), (\"thumbnail\", \"Thumbnail\"), (\"winter\", \"Winter\")])\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "run_webscrape", "data": "def run_webscrape():\n\twebscrape.webscrape_function()\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "store_search_term", "data": "def store_search_term(token):\n\tsearch_term = token.lower().split()\n\treturn search_term\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "preprocess", "data": "def preprocess(): \n\tfor recipe in website_recipe_info: \n\t\trecipes.append({})\n\t\trecipes[-1]['title'] = recipe['title']\n\t\trecipes[-1]['recipe_url'] = recipe['recipe_url']\n\t\t# split tag into separate words and convert words to lower case \n\t\trecipes[-1]['tags'] = recipe['tags'].lower().split()\n\t\trecipes[-1]['image_url'] = recipe['image_url']\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "search_for_recipe_matches", "data": "def search_for_recipe_matches(search_term): \n\t# empty array so recipes that matched previous search term aren't included\n\tmatched_recipes.clear()\n\t# used to check if a recipe has already been added into matched_recipes\n\talready_matched_recipe_titles = []\n\n\t# for every word in the search_term, go through each recipe's tags and check if the word matches any word in the \n\t# tag. If there is a match and the recipe has not already been added, add it to matched_recipes\n\tfor word in search_term: \n\t\tfor recipe in recipes: \n\t\t\tfor tag in recipe['tags']: \n\t\t\t\tif (tag == word and recipe['title'] not in already_matched_recipe_titles):\n\t\t\t\t\tmatched_recipes.append({})\n\t\t\t\t\talready_matched_recipe_titles.append(recipe['title'])\n\t\t\t\t\tmatched_recipes[-1]['title'] = recipe['title']\n\t\t\t\t\tmatched_recipes[-1]['recipe_url'] = recipe['recipe_url']\n\t\t\t\t\tmatched_recipes[-1]['tags'] = recipe['tags']\n\t\t\t\t\tmatched_recipes[-1]['image_url'] = recipe['image_url']\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "apply_grayscale", "data": "def apply_grayscale(image_name, recipe_title): \n\tim = Image.open(image_name)\n\tgrayscale_list = [ ( (a[0]+a[1]+a[2])//3, ) * 3\n\t\t\t\t  for a in im.getdata() ]\n\tim.putdata(grayscale_list)\n\tim.save(\"static/images/grayscale/\" + recipe_title + \".jpg\")\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "apply_negative", "data": "def apply_negative(image_name, recipe_title): \n\tim = Image.open(image_name)\n\tnegative_list = [(255 - p[0], 255 - p[1], 255 - p[2]) for p in im.getdata()]\n\tim.putdata(negative_list)\n\tim.save(\"static/images/negative/\" + recipe_title + \".jpg\")\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "apply_thumbnail", "data": "def apply_thumbnail(image_name, recipe_title):\n\tsource = Image.open(image_name)\n\tw,h = source.width, source.height\n\ttarget = Image.new('RGB', (w, h), 'rosybrown')\n\n\ttarget_x = 0\n\tfor source_x in range(0, source.width, 2):\n\t\ttarget_y = 0\n\t\tfor source_y in range(0, source.height, 2):\n\t\t\tpixel = source.getpixel((source_x, source_y))\n\t\t\ttarget.putpixel((target_x, target_y), pixel)\n\t\t\ttarget_y += 1\n\t\ttarget_x += 1\n\ttarget.save(\"static/images/thumbnail/\" + recipe_title + \".jpg\")\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "apply_sephia", "data": "def apply_sephia(image_name, recipe_title): \n\tim = Image.open(image_name)\n\tsepia_list = [(255 + pixel[0], pixel[1], pixel[2])\n\t\t\t\t\tfor pixel in im.getdata()]\n\tim.putdata(sepia_list)\n\tim.save(\"static/images/sephia/\" + recipe_title + \".jpg\")\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "apply_winter", "data": "def apply_winter(image_name, recipe_title): \n\timage_winter = cv2.imread(image_name,cv2.IMREAD_GRAYSCALE)\n\timage_remap = cv2.applyColorMap(image_winter, cv2.COLORMAP_WINTER)\n\tcv2.imwrite(\"static/images/winter/\" + recipe_title + \".jpg\", image_remap)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "apply_filter", "data": "def apply_filter(image_filter):\n\t# Index to index into matched recipes :3c\n\ti = 0\n\tpprint(matched_recipes)\n\t\n\tfor recipe in matched_recipes:\n\t\tif image_filter == \"grayscale\":\n\t\t\tmatched_recipes[i][\"image_url\"] = \"static/images/grayscale/\" + recipe['title'] + \".jpg\"\n\t\t\n\t\tif image_filter == \"negative\":\n\t\t\tmatched_recipes[i][\"image_url\"] = \"static/images/negative/\" + recipe['title'] + \".jpg\"\n\n\t\tif image_filter == \"sephia\":\n\t\t\tmatched_recipes[i][\"image_url\"] = \"static/images/sephia/\" + recipe['title'] + \".jpg\"\n\n\t\tif image_filter == \"winter\":\n\t\t\tmatched_recipes[i][\"image_url\"] = \"static/images/winter/\" + recipe['title'] + \".jpg\"\n\n\t\tif image_filter == \"thumbnail\":\n\t\t\tmatched_recipes[i][\"image_url\"] = \"static/images/thumbnail/\" + recipe['title'] + \".jpg\"\n\n\t\ti += 1\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "create_filter_images", "data": "def create_filter_images(): \n\tindex = 0 \n\tfor recipe in recipes:\n\t\tprint(\"inside of matched_recipes loop\")\n\t\t#print(recipe)\n\t\timage_name = \"static/images/\" + recipe[\"title\"] + \".jpg\"\n\t\tcurrent_image_url = recipe[\"image_url\"]\n\t\t# retrieve the image and save it\n\t\turllib.request.urlretrieve(current_image_url, image_name)\n\t\tim = Image.open(image_name)\n\t\t# filters\n\t\tapply_grayscale(image_name, recipe[\"title\"])\n\t\tapply_negative(image_name, recipe[\"title\"])\n\t\tapply_thumbnail(image_name, recipe['title'])\n\t\tapply_sephia(image_name, recipe['title'])\n\t\tapply_winter(image_name, recipe['title'])\n\t\tindex += 1\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "index", "data": "def index():\n\tpreprocess()\n\tform = RecipeSearchTerm()\n\t# when the user hits submit we will grab out the information we need\n\tif form.validate_on_submit():\n\t\tsearch_term = store_search_term(form.search_term.data)\n\t\timage_filter = form.image_format.data\n\t\tprint(image_filter)\n\t\tsearch_for_recipe_matches(search_term)\n\t\t# create_filter_images()   \n\t\tapply_filter(image_filter)\n\t\treturn redirect('/result')\n\treturn render_template('index.html', form=form)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}, {"term": "def", "name": "vp", "data": "def vp():\n\tpprint(matched_recipes)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, flash, redirect", "from flask_bootstrap import Bootstrap", "from flask_wtf import FlaskForm", "from wtforms import StringField, SubmitField, SelectField", "from wtforms.validators import DataRequired", "from webscrape_recipe_file import website_recipe_info", "from pprint import pprint", "import urllib.request\t   # for saving images", "from PIL import Image", "import cv2", "import webscrape\t\t\t\t\t\t# webscrape"]}], [], [{"term": "def", "name": "make_directory", "data": "def make_directory():\r\n\t# # dir = os.path.join(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n\t# # if not os.path.exists(dir):\r\n\t# #\t os.mkdir(dir)\r\n\tdir_move = os.path.join(\"C:\\\\Users\\\\asha1\\\\Previous\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n\tif not os.path.exists(dir_move):\r\n\t\tos.mkdir(dir_move)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "old_file_move", "data": "def old_file_move():\r\n\tfiles = []\r\n\tfilespath = []\r\n\tcountloop = int(0)\r\n\tpath = os.path.join(\"C:\\\\Users\\\\asha1\\\\WebScrape\")\r\n\tdst = os.path.join(\"C:\\\\Users\\\\asha1\\\\Previous\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n\r\n\tfor r, d, f in os.walk(path):\r\n\t\tfor file in f:\r\n\t\t\tif 'glassdoor' in file or 'indeed' in file or \"coherence\" in file:\r\n\t\t\t\tfiles.append(file)\r\n\t\t\t\tfilespath.append(r)\r\n\r\n\tfor f in files:\r\n\t\tsrc = str(filespath[countloop])\r\n\t\tshutil.move(os.path.join(src,f), os.path.join(dst,f))\r\n\t\tcountloop = countloop + 1\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "web_scrape_indeed", "data": "def web_scrape_indeed():\r\n\tfor page in range(0, 3):\r\n\t\tprint(\"New Page\")\r\n\t\tresponse = get(url_indeed[page])\r\n\t\thtml_soup = BeautifulSoup(response.text, 'html.parser')\r\n\t\t# print(html_soup.prettify())\r\n\r\n\t\t# Finds all the elements on the page\r\n\t\tstar_containers = html_soup.find_all('div', class_='cmp-ReviewRating-text')\r\n\t\treview_containers = html_soup.find_all('div', class_='cmp-Review-text')\r\n\t\ttitle_containers = html_soup.find_all('div', class_='cmp-Review-title')\r\n\t\t# pros_containers = html_soup.find_all('div', class_='cmp-ReviewProsCons-prosText')\r\n\t\t# cons_containers = html_soup.find_all('div', class_='cmp-ReviewProsCons-consText')\r\n\t\tposition_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n\t\tgeneral_review = html_soup.find_all('div', class_='cmp-Review-content')\r\n\t\t# location_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n\t\t# date_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n\r\n\t\t#Test to see if the containers are getting the proper amount of reviews\r\n\t\tprint(len(star_containers))\r\n\t\tprint(len(review_containers))\r\n\t\tprint(len(title_containers))\r\n\t\tprint(len(position_containers))\r\n\t\tprint(len(general_review))\r\n\t\t# print(len(cons_containers))\r\n\t\t# print(len(location_containers))\r\n\t\t# print(len(date_containers))\r\n\r\n\t\t# Adds each element on the page to list in order\r\n\t\tfor i in range(0, len(title_containers) - 1):\r\n\t\t\tfirst_star = star_containers[i].text\r\n\t\t\tstars.append(first_star)\r\n\r\n\t\t\tfirst_review_text = review_containers[i].span.span.text\r\n\t\t\tdescription.append(first_review_text)\r\n\r\n\t\t\tfirst_title_text = title_containers[i].text\r\n\t\t\ttitle.append(first_title_text)\r\n\r\n\t\t\tfirst_position_text = position_containers[i].text\r\n\t\t\tposition.append(first_position_text)\r\n\r\n\t\t\ttry:\r\n\t\t\t\tpros_containers = general_review[i].find('div', class_='cmp-ReviewProsCons-prosText')\r\n\t\t\t\tfirst_pros_text = pros_containers.span.text\r\n\t\t\t\tpros.append(first_pros_text)\r\n\t\t\texcept Exception as e:\r\n\t\t\t\tpros.append(\"N/A\")\r\n\r\n\t\t\ttry:\r\n\t\t\t\tcons_containers = general_review[i].find('div', class_='cmp-ReviewProsCons-consText')\r\n\t\t\t\tfirst_cons_text = cons_containers.span.text\r\n\t\t\t\tcons.append(first_cons_text)\r\n\t\t\texcept Exception as e:\r\n\t\t\t\tcons.append(\"N/A\")\r\n\t\t\t# first_location_text = location_containers[i].a.text.\r\n\t\t\t# location.append(first_location_text)\r\n\t\t\t# #\r\n\t\t\t# first_date_text = date_containers[i].a.text\r\n\t\t\t# date.append(first_date_text)\r\n\t# Creates dataframe for all given elements and exports to excel\r\n\ttest_df = pd.DataFrame({'Stars': stars, 'Title': title, 'Description': description, 'Position, Date, Location': position, \"Pros\": pros, \"Cons\": cons})\r\n\t\t\t\t\t\t\t # 'Date': date, 'Location': location})\r\n\t# test_df.head(10)\r\n\t# print(test_df.head(10))\r\n\r\n\t# test_df.loc[:, 'Stars'] = test_df['Stars'].str[0:3]\r\n\r\n\texport_csv = test_df.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeed.csv')\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "email_attachments", "data": "def email_attachments():\r\n\tif os.path.exists(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\Email.xlsm\"):\r\n\t\txl = win32com.client.Dispatch(\"Excel.Application\")\r\n\t\txl.Workbooks.Open(os.path.abspath(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\Email.xlsm\"), ReadOnly=1)\r\n\t\txl.Application.Run(\"Send_the_Email\")\r\n\t\tdel xl\r\n\telse:\r\n\t\tprint(\"Path doesn't exist\")\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "csv_convert", "data": "def csv_convert(path, file_name):\r\n\tdf = pd.read_csv(path + file_name)\r\n\tsaved_column_pros = df.pros\r\n\tsaved_column_cons = df.cons\r\n\tsaved_column_MainText = df.MainText\r\n\tres = pd.DataFrame([], [])\r\n\tres = res.append(saved_column_pros)\r\n\tres = res.append(saved_column_cons)\r\n\tres = res.append(saved_column_MainText)\r\n\tprint(Counter(\" \".join(df['pros']).split()).most_common(100))\r\n\tres.to_csv(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoortext\"  + \".csv\")\r\n\tprint(res)\r\n\treturn 0\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_glassdoor", "data": "def most_common_glassdoor():\r\n\ttop_N = 60\r\n\tdf = pd.read_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoor'  + str(x.year) + str(x.month) + str(x.day) + '.csv',\r\n\t\t\t\t\t usecols=['pros', 'cons', 'MainText'])\r\n\r\n\ttxt = df.pros.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.cons.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.MainText.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\r\n\twords = nltk.tokenize.word_tokenize(txt)\r\n\tword_dist = nltk.FreqDist(words)\r\n\r\n\tstopwords = nltk.corpus.stopwords.words('english')\r\n\twords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords)\r\n\r\n\trslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\r\n\t\t\t\t\t\tcolumns=['Word', 'Frequency']).set_index('Word')\r\n\tprint(rslt)\r\n\trslt.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoorcommon'  + str(x.year) + str(x.month) + str(x.day) + '.csv')\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_glassdoor_text", "data": "def most_common_glassdoor_text(texts):\r\n\t# text_counter = collections.Counter(texts)\r\n\t#\t # Common = text_counter.most_common(300)\r\n\tCommon = Counter(chain.from_iterable(texts)).most_common(60)\r\n\tprint(Common)\r\n\tdf = pd.DataFrame(Common, columns=['Common Words in Glassdoor','Amount of Occurences'])\r\n\tdf.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoorcommonwords'  +  '.csv')\r\n\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_indeed", "data": "def most_common_indeed():\r\n\ttop_N = 60\r\n\tdf = pd.read_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeed'  + str(x.year) + str(x.month) + str(x.day) + '.csv',\r\n\t\t\t\t\t usecols=['Description', 'Title'])\r\n\r\n\ttxt = df.Description.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.Title.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\r\n\twords = nltk.tokenize.word_tokenize(txt)\r\n\tword_dist = nltk.FreqDist(words)\r\n\r\n\tstopwords = nltk.corpus.stopwords.words('english')\r\n\twords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords)\r\n\r\n\trslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\r\n\t\t\t\t\t\tcolumns=['Word', 'Frequency']).set_index('Word')\r\n\tprint(rslt)\r\n\trslt.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeedcommon'  + str(x.year) + str(x.month) + str(x.day) + '.csv')\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_indeed_text", "data": "def most_common_indeed_text(texts):\r\n\tCommon = Counter(chain.from_iterable(texts)).most_common(60)\r\n\tprint(Common)\r\n\tdf = pd.DataFrame(Common, columns=['Common Words in Indeed','Number of Occurences'])\r\n\tdf.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeedcommonwords'  +  '.csv')\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_wordcloud_indeed", "data": "def most_common_wordcloud_indeed(texts):\r\n\ttext = \"\"\r\n\tfor i in texts:\r\n\t\tprint (i)\r\n\t\ttext = text + str(i)\r\n\twordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"black\").generate(text)\r\n\tplt.figure()\r\n\tplt.imshow(wordcloud, interpolation= \"bilinear\")\r\n\tplt.axis(\"off\")\r\n\t# plt.show()\r\n\t# wordcloud.to_file(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n\tplt.savefig(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n\tCAA_logo = np.array(Image.open(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_logo.png\"))\r\n\t# CAA_logo = CAA_logo.reshape((CAA_logo.shape[0], CAA_logo.shape[1]), order='F')\r\n\ttransformed_CAA_logo = np.ndarray((CAA_logo.shape[0],CAA_logo.shape[1]), np.int32)\r\n\r\n\tfor i in range(len(CAA_logo)):\r\n\t\ttransformed_CAA_logo[i] = list(map(transform_format, CAA_logo[i]))\r\n\tprint(transformed_CAA_logo)\r\n\twc = WordCloud(background_color=\"black\", max_words=500, mask=CAA_logo, contour_width=3, contour_color=\"blue\")\r\n\twc.generate(text)\r\n\twc.to_file(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_transformed_indeed.png\")\r\n\tplt.figure(figsize=[20,10])\r\n\tplt.imshow(wc, interpolation=\"bilinear\")\r\n\tplt.axis(\"off\")\r\n\t# plt.show()\r\n\treturn 0\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_wordcloud_glassdoor", "data": "def most_common_wordcloud_glassdoor(texts):\r\n\t# Extracts all the text information into a string variable\r\n\ttext = \"\"\r\n\tfor i in texts:\r\n\t\tprint(i)\r\n\t\ttext = text + str(i)\r\n\t# Creates the wordcloud\r\n\twordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\").generate(text)\r\n\tplt.figure()\r\n\tplt.imshow(wordcloud, interpolation=\"bilinear\")\r\n\tplt.axis(\"off\")\r\n\t# plt.show()\r\n\t# wordcloud.to_file(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n\t# Saves the wordcloud\r\n\tplt.savefig(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforglassdoor.png\")\r\n\r\n\t# Creates a logo in the shape of a CAA logo\r\n\t# Opens the CAA logo and takes its shape\r\n\tCAA_logo = np.array(Image.open(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_logo.png\"))\r\n\ttransformed_CAA_logo = np.ndarray((CAA_logo.shape[0], CAA_logo.shape[1]), np.int32)\r\n\r\n\t# Changes the array values of the array to the value 255\r\n\tfor i in range(len(CAA_logo)):\r\n\t\ttransformed_CAA_logo[i] = list(map(transform_format, CAA_logo[i]))\r\n\tprint(transformed_CAA_logo)\r\n\r\n\t# Creates the wordclud in the CAA logo\r\n\twc = WordCloud(background_color=\"black\", max_words=500, mask=CAA_logo, contour_width=3, contour_color=\"blue\")\r\n\twc.generate(text)\r\n\t# Saves it to the file\r\n\twc.to_file(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_transformed_glassdoor.png\")\r\n\t# Creates the dimensions\r\n\tplt.figure(figsize=[20, 10])\r\n\tplt.imshow(wc, interpolation=\"bilinear\")\r\n\t# Makes sure that it has no axises\r\n\tplt.axis(\"off\")\r\n\treturn 0\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "transform_format", "data": "def transform_format(val):\r\n\treturn int(255)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "load_data", "data": "def load_data(path, file_name):\r\n\tprint(\"load_data\")\r\n\tdocuments_list = []\r\n\ttitles = []\r\n\twith open(os.path.join(path, file_name), \"r\", encoding='utf8', errors='ignore') as fin:\r\n\t\tfor line in fin.readlines():\r\n\t\t\ttext = line.strip()\r\n\t\t\tdocuments_list.append(text)\r\n\tprint(len(documents_list))\r\n\ttitles.append(text[0:min(len(text), 250)])\r\n\treturn documents_list, titles\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "load_data_glassdoor", "data": "def load_data_glassdoor(path, file_name):\r\n\tprint(\"load data\")\r\n\tdocuments_list = []\r\n\ttitles = []\r\n\twith open(os.path.join(path, file_name), \"r\", encoding='utf8', errors='ignore') as fin:\r\n\t\tfor line in fin.readlines():\r\n\t\t\ttext = line.strip()\r\n\t\t\tdocuments_list.append(text)\r\n\tprint(len(documents_list))\r\n\ttitles.append(text[0:min(len(text), 250)])\r\n\treturn documents_list, titles\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "preprocess_data", "data": "def preprocess_data(doc_set):\r\n\tpunctuations = '!()-[]{};:\"\\,<>./?@#$%^&*_~'\r\n\tprint(\"preprocess data\")\r\n\ttokenizer = RegexpTokenizer(r'\\w+')\r\n\t# create English stop words list\r\n\ten_stop = set(stopwords.words('english'))\r\n\ten_punctuation = set(string.punctuation)\r\n\t# Create p_stemmer of class PorterStemmer\r\n\tp_stemmer = PorterStemmer()\r\n\t# list for tokenized documents in loop\r\n\ttexts = []\r\n\t# loop through document list\r\n\tfor i in doc_set:\r\n\t\t# clean and tokenize document string\r\n\t\traw = i.lower()\r\n\t\tfor x in raw:\r\n\t\t\tif x in punctuations:\r\n\t\t\t\traw = raw.replace(x,\"\")\r\n\t\ttokens = tokenizer.tokenize(raw)\r\n\t\t# remove stop words from tokens\r\n\t\tstopped_tokens = [i for i in tokens if i not in en_stop]\r\n\t\t# stem tokens\r\n\t\tstemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\r\n\t\t# add tokens to list\r\n\t\ttexts.append(stemmed_tokens)\r\n\treturn texts\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "negative_words", "data": "def negative_words():\r\n\tpunctuations = '!()-[]{};:\"\\,<>./?@#$%^&*_~'\r\n\ttokenizer = RegexpTokenizer(r'\\w+')\r\n\ten_stop = set(stopwords.words('english'))\r\n\ten_punctuation = set(string.punctuation)\r\n\tp_stemmer = PorterStemmer()\r\n\ttexts = []\r\n\tnegative_word_list = []\r\n\ttry:\r\n\t\twith open(\"negative_words.txt\", \"r\", encoding='utf8', errors='ignore') as fin:\r\n\t\t\tfor line in fin.readlines():\r\n\t\t\t\tline = line.replace(\"\\n\", \" \")\r\n\t\t\t\tnegative_word_list.append(line)\r\n\texcept Exception as e:\r\n\t\tprint(e)\r\n\tfor i in negative_word_list:\r\n\t\t# clean and tokenize document string\r\n\t\traw = i.lower()\r\n\t\tfor x in raw:\r\n\t\t\tif x in punctuations:\r\n\t\t\t\traw = raw.replace(x,\"\")\r\n\t\ttokens = tokenizer.tokenize(raw)\r\n\t\t# remove stop words from tokens\r\n\t\tstopped_tokens = [i for i in tokens if i not in en_stop]\r\n\t\t# stem tokens\r\n\t\tstemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\r\n\t\t# add tokens to list\r\n\t\ttexts.append(stemmed_tokens)\r\n\tfor text in texts:\r\n\t\tfor text_indiv in text:\r\n\t\t\tprint(text_indiv + \"TEXT_INDIVIDUAL\")\r\n\treturn texts\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "negative_word_list_compare", "data": "def negative_word_list_compare(negative_word_list, doc_clean):\r\n\tdoc_clean_neg = []\r\n\tprint(\"does it work\")\r\n\tfor i in doc_clean:\r\n\t\tprint(i)\r\n\t\tfor x in i:\r\n\t\t\tprint(x)\r\n\t\t\tfor neg_array in negative_word_list:\r\n\t\t\t\tfor neg_word in neg_array:\r\n\t\t\t\t\tif x == neg_word:\r\n\t\t\t\t\t\tdoc_clean_neg.append(x)\r\n\treturn doc_clean_neg\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "negative_word_common", "data": "def negative_word_common(doc_clean_neg):\r\n\tCommon = Counter(doc_clean_neg).most_common(30)\r\n\tdf = pd.DataFrame(Common, columns = [\"Most common negative words\", \"Number of Occurences of Words\"])\r\n\tdf.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\negativewords.csv\", index=False)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "prepare_corpus", "data": "def prepare_corpus(doc_clean):\r\n\tprint(\"print corpus\")\r\n\t\"\"\"\r\n\t  Input  : clean document\r\n\t  Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\r\n\t  Output : term dictionary and Document Term Matrix\r\n\t  \"\"\"\r\n\t# Creating the term dictionary of our courpus, where every unique term is assigned an index.\r\n\tdictionary = corpora.Dictionary(doc_clean)\r\n\t# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\r\n\tdoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\r\n\t# generate LDA model\r\n\treturn dictionary, doc_term_matrix\r\n", "description": "\r\n\t  Input  : clean document\r\n\t  Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\r\n\t  Output : term dictionary and Document Term Matrix\r\n\t  ", "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "create_gensim_lsa_model", "data": "def create_gensim_lsa_model(doc_clean, number_of_topics, words):\r\n\t\"\"\"\r\n\t\tInput  : clean document, number of topics and number of words associated with each topic\r\n\t\tPurpose: create LSA model using gensim\r\n\t\tOutput : return LSA model\r\n\t\t\"\"\"\r\n\tdictionary, doc_term_matrix = prepare_corpus(doc_clean)\r\n\t# generate LSA model\r\n\tlsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)  # train model\r\n\tprint(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\r\n\treturn lsamodel\r\n", "description": "\r\n\t\tInput  : clean document, number of topics and number of words associated with each topic\r\n\t\tPurpose: create LSA model using gensim\r\n\t\tOutput : return LSA model\r\n\t\t", "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "compute_coherence_values", "data": "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean,  stop, start, step):\r\n\r\n\t# Input : dictionary : Gensim dictionary\r\n\t#\t\t corpus: gensim corpus\r\n\t#\t\t texts: list of input texts\r\n\t#\t\t stop: max num of topics\r\n\t# Purpse : Compute c_v coherence for different number of topics\r\n\t# Output: model_list : List of LSA topic models\r\n\t#\t\t coherence_values : Coherence values corresponding to the lDA model with respective numbers\r\n\r\n\tcoherence_values = []\r\n\tmodel_list = []\r\n\tfor num_topics in range(start, stop, step):\r\n\t\tmodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)\r\n\t\tmodel_list.append(model)\r\n\t\tcoherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\r\n\t\tcoherence_values.append(coherencemodel.get_coherence())\r\n\treturn model_list, coherence_values\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LSA_indeed_model_to_csv", "data": "def LSA_indeed_model_to_csv(model_list):\r\n\t# df = pd.DataFrame(data={\"Topic Modelling Indeed\":[model_list]})\r\n\t# df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelindeed\"  +  \".csv\", sep=\" \", index=False)\r\n\tdf = pd.DataFrame.from_records(model_list)\r\n\tdf.columns = [\"Topic Modelling Indeed\", \"col 2\"]\r\n\tdf.applymap(str)\r\n\tdf['Topic Modelling Indeed'] = df['Topic Modelling Indeed'].astype(str)\r\n\t# df['Topic Modelling Indeed'].apply(str)\r\n\t# Removes coherence scores from the dataframe\r\n\tprint(df.dtypes)\r\n\tdf['Topic Modelling Indeed'] = df['Topic Modelling Indeed'].str.replace('\\d+', ' ')\r\n\tdf['col 2'] = df['col 2'].str.replace('\\d+', ' ')\r\n\tdf['col 2'] = df['col 2'].str.replace(r'[^\\w\\s]+', '')\r\n\t# df.transpose()\r\n\tdf.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelindeed\"  +  \".csv\", sep=\" \", index=False)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LSA_glassdoor_model_to_csv", "data": "def LSA_glassdoor_model_to_csv(model_list):\r\n\tdf = pd.DataFrame.from_records(model_list)\r\n\tdf.columns = [\"Topic Modelling Glassdoor\", \"col 2\"]\r\n\tdf.applymap(str)\r\n\tdf['Topic Modelling Glassdoor'] = df['Topic Modelling Glassdoor'].astype(str)\r\n\t# Removes coherence scores from the dataframe\r\n\tdf['Topic Modelling Glassdoor'] = df['Topic Modelling Glassdoor'].str.replace('\\d+', ' ')\r\n\tdf['col 2'] = df['col 2'].str.replace('\\d+', ' ')\r\n\t# Removes the punctuation from the dataframe\r\n\tdf['col 2'] = df['col 2'].str.replace(r'[^\\w\\s]+', '')\r\n\tdf.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelglassdoor\"  +\".csv\", sep=\" \", index=False)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LDA_model_indeed", "data": "def LDA_model_indeed(dictionaryy, doc_term_matrixx, doc_clean):\r\n\texclude = '!()-[]{};:\"\\,<>./?@#$%^&*_~+*'\r\n\tlda_model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrixx,\r\n\t\t\t\t\t\t\t\t\t\t\t\tid2word=dictionaryy, per_word_topics=True, num_topics = 5)\r\n\t\t\t\t\t\t\t\t\t\t\t\t# ,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# num_topics=6,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# random_state=100,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# update_every=1,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# chunksize=100,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# passes=10,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# alpha='auto',\r\n\t\t\t\t\t\t\t\t\t\t\t\t# per_word_topics=True)\r\n\tpprint(lda_model.print_topics())\r\n\tdoc_lda = lda_model[doc_term_matrixx]\r\n\tcoherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, dictionary=dictionaryy, coherence='c_v')\r\n\tcoherence_lda = coherence_model_lda.get_coherence()\r\n\tprint('\\nCoherence Score: ', coherence_lda)\r\n\traw = lda_model.print_topics()\r\n\t# try:\r\n\t#\t raw = ''.join(ch for ch in x if ch not in exclude)\r\n\t# except:\r\n\t#\t pass\r\n\tdf = pd.DataFrame(raw)\r\n\tdf.columns = [\"Topic Nummber\", \"Topic Words\"]\r\n\tdf.applymap(str)\r\n\tdf['Topic Words'] = df['Topic Words'].astype(str)\r\n\t# Gets rid of all the numbers\r\n\tdf['Topic Words'] = df['Topic Words'].str.replace('\\d+', '')\r\n\t# Gets rid of all the punctuation from the topic model\r\n\tdf['Topic Words'] = df['Topic Words'].str.replace(r'[^\\w\\s]+', '')\r\n\tprint(df.head(10))\r\n\tdf.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\coherencescoresindeed\" + \".csv\", sep=\" \", index=False)\r\n\t# mallet_path = 'C:\\\\Users\\\\asha1\\\\AppData\\\\Local\\\\Temp\\\\mallet-2.0.8.zip'  # update this path\r\n\t# ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrixx, num_topics=20, id2word=dictionaryy)\r\n\t#\r\n\t# pprint(ldamallet.show_topics(formatted=False))\r\n\t#\r\n\t# coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=doc_clean, dictionary=dictionaryy,\r\n\t#\t\t\t\t\t\t\t\t\t\t\tcoherence='c_v')\r\n\t# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\r\n\t# print('\\nCoherence Score: ', coherence_ldamallet)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LDA_model_glassdoor", "data": "def LDA_model_glassdoor(dictionaryy, doc_term_matrixx, doc_clean):\r\n\t# Generates the lda model\r\n\tlda_model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrixx,\r\n\t\t\t\t\t\t\t\t\t\t\t\tid2word=dictionaryy, per_word_topics=True, num_topics = 5)\r\n\t\t\t\t\t\t\t\t\t\t\t\t# ,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# num_topics=6,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# random_state=100,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# update_every=1,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# chunksize=100,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# passes=10,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# alpha='auto',\r\n\t\t\t\t\t\t\t\t\t\t\t\t# per_word_topics=True)\r\n\tpprint(lda_model.print_topics())\r\n\tdoc_lda = lda_model[doc_term_matrixx]\r\n\tcoherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, dictionary=dictionaryy, coherence='c_v')\r\n\tcoherence_lda = coherence_model_lda.get_coherence()\r\n\tprint('Coherence Score:', coherence_lda)\r\n\tdf = pd.DataFrame(lda_model.print_topics())\r\n\t# df.columns = [\"Topic Modelling Indeed\", \" \"]\r\n\t# df['Coherence Score'] = df['Coherence Score'].str.replace('\\dt+', '')\r\n\t# df['Topic Words'] = df['Topic Words'].replace('\\d+', '')\r\n\tdf.columns = [\"Topic Nummber\", \"Topic Words\"]\r\n\tdf.applymap(str)\r\n\t# Converts the entire dataframe to an object so it can be interpreted as a string\r\n\tdf['Topic Words'] = df['Topic Words'].astype(str)\r\n\t# Gets rid of all the numbers within the string\r\n\tdf['Topic Words'] = df['Topic Words'].str.replace('\\d+', '')\r\n\t# Gets rid of all the punctuation within the dataframe (easier for user to read)\r\n\tdf['Topic Words'] = df['Topic Words'].str.replace(r'[^\\w\\s]+', '')\r\n\t# Prints first 10 topics generated by the model\r\n\tprint(df.head(10))\r\n\t# Makes the dataframe go to a csv file\r\n\tdf.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\coherencescoresglassdoor\" + \".csv\", sep=\" \", index=False)\r\n\t# mallet_path = 'C:\\\\Users\\\\asha1\\\\AppData\\\\Local\\\\Temp\\\\mallet-2.0.8.zip'  # update this path\r\n\t# ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrixx, num_topics=20, id2word=dictionaryy)\r\n\t#\r\n\t# pprint(ldamallet.show_topics(formatted=False))\r\n\t#\r\n\t# coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=doc_clean, dictionary=dictionaryy,\r\n\t#\t\t\t\t\t\t\t\t\t\t\tcoherence='c_v')\r\n\t# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\r\n\t# print('\\nCoherence Score: ', coherence_ldamallet)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "plot_graph", "data": "def plot_graph(doc_clean, start, stop, step):\r\n\tdictionary, doc_term_matrix = prepare_corpus(doc_clean)\r\n\tmodel_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start, step)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "scrape", "data": "def scrape(field, review, author):\r\n\r\n\tdef scrape_date(review):\r\n\t\treturn review.find_element_by_class_name(\"date\").text\r\n\r\n\tdef scrape_emp_title(review):\r\n\t\tif 'Anonymous Employee' not in review.text:\r\n\t\t\ttry:\r\n\t\t\t\tres = author.find_element_by_class_name(\r\n\t\t\t\t\t'authorJobTitle').text.split('-')[1]\r\n\t\t\texcept Exception:\r\n\t\t\t\tres = np.nan\r\n\t\telse:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_location(review):\r\n\t\ttry:\r\n\t\t\tres = author.find_element_by_class_name(\r\n\t\t\t'authorLocation').text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_status(review):\r\n\t\ttry:\r\n\t\t\tres = author.text.split('-')[0]\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_rev_title(review):\r\n\t\ttry:\r\n\t\t\tres = review.find_element_by_class_name('summary').text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_years(review):\r\n\t\ttry:\r\n\t\t\tfirst_par = review.find_element_by_class_name(\r\n\t\t\t'reviewBodyCell').text\r\n\t\t\tres = first_par\r\n\t\texcept:\r\n\t\t\tprint(\"doesn't work\")\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_helpful(review):\r\n\t\ttry:\r\n\t\t\thelpful = review.find_element_by_class_name('helpfulCount')\r\n\t\t\tres = helpful[helpful.find('(') + 1: -1]\r\n\t\texcept Exception:\r\n\t\t\tres = 0\r\n\t\treturn res\r\n\r\n\tdef expand_show_more(section):\r\n\t\ttry:\r\n\t\t\tmore_content = section.find_element_by_class_name('moreContent')\r\n\t\t\tmore_link = more_content.find_element_by_class_name('moreLink')\r\n\t\t\tmore_link.click()\r\n\t\texcept Exception:\r\n\t\t\tpass\r\n\r\n\tdef scrape_pros(review):\r\n\t\ttry:\r\n\t\t\tpros = review.find_element_by_css_selector(\"p.mt-0.mb-xsm.v2__EIReviewDetailsV2__bodyColor.v2__EIReviewDetailsV2__lineHeightLarge\")\r\n\t\t\texpand_show_more(pros)\r\n\t\t\tres = pros.text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_cons(review):\r\n\t\ttry:\r\n\t\t\tcons = review.find_elements_by_css_selector(\r\n\t\t\t\t\"p.mt-0.mb-xsm.v2__EIReviewDetailsV2__bodyColor.v2__EIReviewDetailsV2__lineHeightLarge\")[1]\r\n\t\t\texpand_show_more(cons)\r\n\t\t\tres = cons.text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_advice(review):\r\n\t\ttry:\r\n\t\t\tadvice = review.find_element_by_class_name('adviceMgmt')\r\n\t\t\texpand_show_more(advice)\r\n\t\t\tres = advice.text.replace('\\nShow Less', '')\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_overall_rating(review):\r\n\t\ttry:\r\n\t\t\tratings = review.find_element_by_class_name('gdStars')\r\n\t\t\toverall = ratings.find_element_by_class_name(\r\n\t\t\t\t'rating').find_element_by_class_name('value-title')\r\n\t\t\tres = overall.get_attribute('title')\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef _scrape_subrating(i):\r\n\t\ttry:\r\n\t\t\tratings = review.find_element_by_class_name('gdStars')\r\n\t\t\tsubratings = ratings.find_element_by_class_name(\r\n\t\t\t\t'subRatings').find_element_by_tag_name('ul')\r\n\t\t\tthis_one = subratings.find_elements_by_tag_name('li')[i]\r\n\t\t\tres = this_one.find_element_by_class_name(\r\n\t\t\t\t'gdBars').get_attribute('title')\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_work_life_balance(review):\r\n\t\treturn _scrape_subrating(0)\r\n\r\n\tdef scrape_culture_and_values(review):\r\n\t\treturn _scrape_subrating(1)\r\n\r\n\tdef scrape_career_opportunities(review):\r\n\t\treturn _scrape_subrating(2)\r\n\r\n\tdef scrape_comp_and_benefits(review):\r\n\t\treturn _scrape_subrating(3)\r\n\r\n\tdef scrape_senior_management(review):\r\n\t\treturn _scrape_subrating(4)\r\n\r\n\tdef scrape_maintext(review):\r\n\t\ttry:\r\n\t\t\tmaintext = review.find_element_by_class_name('mainText')\r\n\t\t\tres = maintext.text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\t# All the functions for scraping within a list so that they are easier to call at once\r\n\tfuncs = [\r\n\t\tscrape_date,\r\n\t\tscrape_emp_title,\r\n\t\tscrape_location,\r\n\t\tscrape_status,\r\n\t\tscrape_rev_title,\r\n\t\tscrape_helpful,\r\n\t\tscrape_pros,\r\n\t\tscrape_cons,\r\n\t\tscrape_maintext,\r\n\t\tscrape_overall_rating,\r\n\t\tscrape_work_life_balance,\r\n\t\tscrape_culture_and_values,\r\n\t\tscrape_career_opportunities,\r\n\t\tscrape_comp_and_benefits,\r\n\t\tscrape_senior_management\r\n\t]\r\n\r\n\t# Calls all the functions for scraping and collects into a variavle for one review\r\n\tfdict = dict((s, f) for (s, f) in zip(SCHEMA, funcs))\r\n\r\n\treturn fdict[field](review)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "extract_from_page", "data": "def extract_from_page():\r\n\r\n\t# Extracts all the reviews from the webpages\r\n\tdef extract_review(review):\r\n\t\ttime.sleep(2)\r\n\t\tauthor = review.find_element_by_css_selector('span.authorInfo')\r\n\r\n\t\tres = {}\r\n\t\tfor field in SCHEMA:\r\n\t\t\tres[field] = scrape(field, review, author)\r\n\t\t\ttime.sleep(0.1)\r\n\r\n\t\tprint(\"Extracting review\")\r\n\t\tassert set(res.keys()) == set(SCHEMA)\r\n\t\treturn res\r\n\r\n\t# Creates the pandas dataframe, and inputs the columns from the SCHEMA.py file\r\n\tres = pd.DataFrame([], columns=SCHEMA)\r\n\r\n\treviews = browser.find_elements_by_class_name('empReview')\r\n\t# Extracts all the reviews on the page and increases index length of array for each review scraped on page\r\n\tfor review in reviews:\r\n\t\tdata = extract_review(review)\r\n\t\tres.loc[idx[0]] = data\r\n\t\tidx[0] = idx[0] + 1\r\n\t\tprint(idx[0])\r\n\t\tprint(\"index length\")\r\n\r\n\tprint(\"Done extracting from page\")\r\n\t#Arguments not passed for max and min date, but it would check and stop the process if not within bounds of dates passsed\r\n\tif args.max_date and \\\r\n\t\t(pd.to_datetime(res['date']).max() > args.max_date) or \\\r\n\t\t\targs.min_date and \\\r\n\t\t\t(pd.to_datetime(res['date']).min() < args.min_date):\r\n\t\tdate_limit_reached[0] = True\r\n\r\n\treturn res\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "more_pages", "data": "def more_pages():\r\n\tnext_ = browser.find_element_by_css_selector('li.pagination__PaginationStyle__next')\r\n\tprint(\"Found li tag\")\r\n\ttry:\r\n\t\tnext_.find_element_by_tag_name('a')\r\n\t\tprint(\"Element is found\")\r\n\t\treturn True\r\n\texcept selenium.common.exceptions.NoSuchElementException:\r\n\t\treturn False\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "go_to_next_page", "data": "def go_to_next_page():\r\n\tnext_ = browser.find_element_by_css_selector('li.pagination__PaginationStyle__next a')\r\n\tbrowser.get(next_.get_attribute('href'))\r\n\tpage[0] = page[0] + 1\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "no_reviews", "data": "def no_reviews():\r\n\treturn False\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "navigate_to_reviews", "data": "def navigate_to_reviews():\r\n\r\n\tbrowser.get(args.url)\r\n\ttime.sleep(1)\r\n\r\n\tif no_reviews():\r\n\t\treturn False\r\n\r\n\tprint(\"Navigating to reviews\")\r\n\ttime.sleep(1)\r\n\r\n\treturn True\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "sign_in", "data": "def sign_in():\r\n\t# logger.info(f'Signing in to {args.username}')\r\n\r\n\turl = 'https://www.glassdoor.ca/profile/login_input.htm?userOriginHook=HEADER_SIGNIN_LINK'\r\n\tbrowser.get(url)\r\n\r\n\temail_field = browser.find_element_by_name('username')\r\n\tpassword_field = browser.find_element_by_name('password')\r\n\tsubmit_btn = browser.find_element_by_xpath('//button[@type=\"submit\"]')\r\n\r\n\temail_field.send_keys(args.username)\r\n\tpassword_field.send_keys(args.password)\r\n\tsubmit_btn.click()\r\n\r\n\ttime.sleep(1)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "get_browser", "data": "def get_browser():\r\n\tchrome_options = wd.ChromeOptions()\r\n\t# if args.headless:\r\n\tchrome_options.add_argument('--headless')\r\n\tchrome_options.add_argument('log-level=3')\r\n\tbrowser = wd.Chrome(options=chrome_options)\r\n\treturn browser\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "get_current_page", "data": "def get_current_page():\r\n\tpaging_control = browser.find_element_by_class_name('pagingControls')\r\n\tcurrent = int(paging_control.find_element_by_xpath(\r\n\t\t'//ul//li[contains\\\r\n\t\t(concat(\\' \\',normalize-space(@class),\\' \\'),\\' current \\')]\\\r\n\t\t//span[contains(concat(\\' \\',\\\r\n\t\tnormalize-space(@class),\\' \\'),\\' disabled \\')]')\r\n\t\t.text.replace(',', ''))\r\n\treturn current\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "verify_date_sorting", "data": "def verify_date_sorting():\r\n\tascending = urllib.parse.parse_qs(\r\n\t\targs.url)['sort.ascending'] == ['true']\r\n\r\n\tif args.min_date and ascending:\r\n\t\traise Exception(\r\n\t\t\t'min_date required reviews to be sorted DESCENDING by date.')\r\n\telif args.max_date and not ascending:\r\n\t\traise Exception(\r\n\t\t\t'max_date requires reviews to be sorted ASCENDING by date.')\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "web_scrape_glassdoor", "data": "def web_scrape_glassdoor():\r\n\tres = pd.DataFrame([], columns=SCHEMA)\r\n\tsign_in()\r\n\r\n\tif not args.start_from_url:\r\n\t\treviews_exist = navigate_to_reviews()\r\n\t\tif not reviews_exist:\r\n\t\t\treturn\r\n\telif args.max_date or args.min_date:\r\n\t\tverify_date_sorting()\r\n\t\tbrowser.get(args.url)\r\n\t\tpage[0] = get_current_page()\r\n\t\tprint(f'Starting from page {page[0]:,}.')\r\n\t\ttime.sleep(1)\r\n\telse:\r\n\t\tbrowser.get(args.url)\r\n\t\tpage[0] = get_current_page()\r\n\t\tprint(f'Starting from page {page[0]:,}.')\r\n\t\ttime.sleep(1)\r\n\ttime.sleep(4)\r\n\treviews_df = extract_from_page()\r\n\tres = res.append(reviews_df)\r\n\tcount = int(0)\r\n\r\n\twhile more_pages() and len(res) < args.limit:\r\n\t\t# not date_limit_reached[0]:\r\n\t\ttime.sleep(2)\r\n\t\tgo_to_next_page()\r\n\t\ttime.sleep(12)\r\n\t\treviews_df = extract_from_page()\r\n\t\tres = res.append(reviews_df)\r\n\t\tprint(len(res))\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LSA_indeed", "data": "def LSA_indeed():\r\n\tprint(\"hi\")\r\n\tnumber_of_topics = 4\r\n\twords = 8\r\n\ttext_info, title_info = load_data('C:\\\\Users\\\\asha1\\\\WebScrape', 'indeed' + '.csv')\r\n\tprint(text_info)\r\n\tclean_text = preprocess_data(text_info)\r\n\tprint(clean_text)\r\n\tprep_dict, prep_matrix = prepare_corpus(clean_text)\r\n\tprint(prep_dict)\r\n\tprint(\"1\")\r\n\tprint(prep_matrix)\r\n\tnew_model = create_gensim_lsa_model(clean_text, 5, 10)\r\n\tprint(new_model)\r\n\tnew_list, num_topics = compute_coherence_values(prep_dict, prep_matrix, clean_text, 2, 10, 1)\r\n\tprint(new_list)\r\n\tprint(\"In between\")\r\n\t# plot_graph(clean_text, 2, 10, 1)\r\n\t# most_common_indeed()\r\n\tcommon = most_common_indeed_text(clean_text)\r\n\tprint(common)\r\n\tmost_common_wordcloud_indeed(clean_text)\r\n\tprint(\"LSA model\")\r\n\tprint(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n\tLSA_indeed_model_to_csv(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n\tLDA_model_indeed(prep_dict, prep_matrix, clean_text)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LSA_glassdoor", "data": "def LSA_glassdoor():\r\n\tnumber_of_topics = 5\r\n\twords = 10\r\n\tcsv_convert('C:\\\\Users\\\\asha1\\\\WebScrape\\\\', 'glassdoor' + '.csv')\r\n\ttext_info, title_info = load_data_glassdoor('C:\\\\Users\\\\asha1\\\\WebScrape', 'glassdoortext' + '.csv')\r\n\tprint(text_info)\r\n\tclean_text = preprocess_data(text_info)\r\n\tprint(clean_text)\r\n\tprep_dict, prep_matrix = prepare_corpus(clean_text)\r\n\tprint(prep_dict)\r\n\tprint(\"1\")\r\n\tprint(prep_matrix)\r\n\tnew_model = create_gensim_lsa_model(clean_text, 5, 10)\r\n\tprint(new_model)\r\n\tnew_list, num_topics = compute_coherence_values(prep_dict, prep_matrix, clean_text, 5, 10, 1)\r\n\tprint(new_list)\r\n\tprint(new_model)\r\n\tmost_common_wordcloud_glassdoor(clean_text)\r\n\tLSA_glassdoor_model_to_csv(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n\tcommon = most_common_glassdoor_text(clean_text)\r\n\tprint(common)\r\n\tLDA_model_glassdoor(prep_dict, prep_matrix, clean_text)\r\n\r\n\t# plot_graph(clean_text, 2, 10, 1)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "run_the_program", "data": "def run_the_program():\r\n\t# Brings old files to old directory and allows space for new files to exist in current directoryis\r\n\tmake_directory()\r\n\t# #\r\n\t# # Webscrapes indeed and glassdoor respectively and saves dataframe to csv file\r\n\tweb_scrape_indeed()\r\n\ttime.sleep(3)\r\n\tweb_scrape_glassdoor()\r\n\t#\r\n\t# # Runs the latent semantic analysis\r\n\tLSA_indeed()\r\n\tLSA_glassdoor()\r\n\r\n\t# # Emails the files to Recruitment\r\n\temail_attachments()\r\n\told_file_move()\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}], [{"term": "class", "name": "classDb:", "data": "class Db:\n\n\tdef __init__(self):\n\t\t# AUTHENTICATION\n\t\tload_dotenv()\n\t\tself.dbname = os.getenv('DB_NAME')\n\t\tself.user = os.getenv('DB_USERNAME')\n\t\tself.password = os.getenv('DB_PASSWORD')\n\t\tself.host = os.getenv('HOST')\n\t\n\tdef connect(self):\n\t\tself.conn = psycopg2.connect(\"dbname={0} user={1} password={2} host={3}\".format(self.dbname, self.user, self.password, self.host))\n\t\tself.cur = self.conn.cursor()\n\t\t\n\tdef create_table(self, school: str):\n\n\t# https://stackoverflow.com/questions/19812597/postgresql-string-escaping-settings\n\t# https://stackoverflow.com/questions/41396195/what-is-the-difference-between-single-quotes-and-double-quotes-in-postgresql\n\t# backticks will not work\n\t# table name should use underscore, but uses double quotes in case\n\t\tcmd = \"CREATE TABLE IF NOT EXISTS \\\"{0}\\\" ( \\\n\t\t\t\tname\tvarchar(10), \\\n\t\t\t\tdepartment  varchar(5), \\\n\t\t\t\tcourse  varchar(5), \\\n\t\t\t\tprof\tvarchar(30), \\\n\t\t\t\tdifficulty  real, \\\n\t\t\t\tsize\tint \\\n\t\t\t\t);\".format(school)\n\n\t\tself.cur.execute(cmd)\n\tdef execute(self, cmd):\n\t\tself.cur.execute(cmd)\n\t\n\tdef close(self):\n\t\tself.conn.commit()\n\t\tself.cur.close()\n\t\tself.conn.close()\n", "description": null, "category": "webscraping", "imports": ["import os", "from dotenv import load_dotenv", "import psycopg2"]}], [{"term": "def", "name": "make_directory", "data": "def make_directory():\r\n\t# # dir = os.path.join(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n\t# # if not os.path.exists(dir):\r\n\t# #\t os.mkdir(dir)\r\n\tdir_move = os.path.join(\"C:\\\\Users\\\\asha1\\\\Previous\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n\tif not os.path.exists(dir_move):\r\n\t\tos.mkdir(dir_move)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "old_file_move", "data": "def old_file_move():\r\n\tfiles = []\r\n\tfilespath = []\r\n\tcountloop = int(0)\r\n\tpath = os.path.join(\"C:\\\\Users\\\\asha1\\\\WebScrape\")\r\n\tdst = os.path.join(\"C:\\\\Users\\\\asha1\\\\Previous\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n\r\n\tfor r, d, f in os.walk(path):\r\n\t\tfor file in f:\r\n\t\t\tif 'glassdoor' in file or 'indeed' in file or \"coherence\" in file:\r\n\t\t\t\tfiles.append(file)\r\n\t\t\t\tfilespath.append(r)\r\n\r\n\tfor f in files:\r\n\t\tsrc = str(filespath[countloop])\r\n\t\tshutil.move(os.path.join(src,f), os.path.join(dst,f))\r\n\t\tcountloop = countloop + 1\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "web_scrape_indeed", "data": "def web_scrape_indeed():\r\n\tfor page in range(0, 3):\r\n\t\tprint(\"New Page\")\r\n\t\tresponse = get(url_indeed[page])\r\n\t\thtml_soup = BeautifulSoup(response.text, 'html.parser')\r\n\t\t# print(html_soup.prettify())\r\n\r\n\t\t# Finds all the elements on the page\r\n\t\tstar_containers = html_soup.find_all('div', class_='cmp-ReviewRating-text')\r\n\t\treview_containers = html_soup.find_all('div', class_='cmp-Review-text')\r\n\t\ttitle_containers = html_soup.find_all('div', class_='cmp-Review-title')\r\n\t\t# pros_containers = html_soup.find_all('div', class_='cmp-ReviewProsCons-prosText')\r\n\t\t# cons_containers = html_soup.find_all('div', class_='cmp-ReviewProsCons-consText')\r\n\t\tposition_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n\t\tgeneral_review = html_soup.find_all('div', class_='cmp-Review-content')\r\n\t\t# location_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n\t\t# date_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n\r\n\t\t#Test to see if the containers are getting the proper amount of reviews\r\n\t\tprint(len(star_containers))\r\n\t\tprint(len(review_containers))\r\n\t\tprint(len(title_containers))\r\n\t\tprint(len(position_containers))\r\n\t\tprint(len(general_review))\r\n\t\t# print(len(cons_containers))\r\n\t\t# print(len(location_containers))\r\n\t\t# print(len(date_containers))\r\n\r\n\t\t# Adds each element on the page to list in order\r\n\t\tfor i in range(0, len(title_containers) - 1):\r\n\t\t\tfirst_star = star_containers[i].text\r\n\t\t\tstars.append(first_star)\r\n\r\n\t\t\tfirst_review_text = review_containers[i].span.span.text\r\n\t\t\tdescription.append(first_review_text)\r\n\r\n\t\t\tfirst_title_text = title_containers[i].text\r\n\t\t\ttitle.append(first_title_text)\r\n\r\n\t\t\tfirst_position_text = position_containers[i].text\r\n\t\t\tposition.append(first_position_text)\r\n\r\n\t\t\ttry:\r\n\t\t\t\tpros_containers = general_review[i].find('div', class_='cmp-ReviewProsCons-prosText')\r\n\t\t\t\tfirst_pros_text = pros_containers.span.text\r\n\t\t\t\tpros.append(first_pros_text)\r\n\t\t\texcept Exception as e:\r\n\t\t\t\tpros.append(\"N/A\")\r\n\r\n\t\t\ttry:\r\n\t\t\t\tcons_containers = general_review[i].find('div', class_='cmp-ReviewProsCons-consText')\r\n\t\t\t\tfirst_cons_text = cons_containers.span.text\r\n\t\t\t\tcons.append(first_cons_text)\r\n\t\t\texcept Exception as e:\r\n\t\t\t\tcons.append(\"N/A\")\r\n\t\t\t# first_location_text = location_containers[i].a.text.\r\n\t\t\t# location.append(first_location_text)\r\n\t\t\t# #\r\n\t\t\t# first_date_text = date_containers[i].a.text\r\n\t\t\t# date.append(first_date_text)\r\n\t# Creates dataframe for all given elements and exports to excel\r\n\ttest_df = pd.DataFrame({'Stars': stars, 'Title': title, 'Description': description, 'Position, Date, Location': position, \"Pros\": pros, \"Cons\": cons})\r\n\t\t\t\t\t\t\t # 'Date': date, 'Location': location})\r\n\t# test_df.head(10)\r\n\t# print(test_df.head(10))\r\n\r\n\t# test_df.loc[:, 'Stars'] = test_df['Stars'].str[0:3]\r\n\r\n\texport_csv = test_df.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeed.csv')\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "email_attachments", "data": "def email_attachments():\r\n\tif os.path.exists(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\Email.xlsm\"):\r\n\t\txl = win32com.client.Dispatch(\"Excel.Application\")\r\n\t\txl.Workbooks.Open(os.path.abspath(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\Email.xlsm\"), ReadOnly=1)\r\n\t\txl.Application.Run(\"Send_the_Email\")\r\n\t\tdel xl\r\n\telse:\r\n\t\tprint(\"Path doesn't exist\")\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "csv_convert", "data": "def csv_convert(path, file_name):\r\n\tdf = pd.read_csv(path + file_name)\r\n\tsaved_column_pros = df.pros\r\n\tsaved_column_cons = df.cons\r\n\tsaved_column_MainText = df.MainText\r\n\tres = pd.DataFrame([], [])\r\n\tres = res.append(saved_column_pros)\r\n\tres = res.append(saved_column_cons)\r\n\tres = res.append(saved_column_MainText)\r\n\tprint(Counter(\" \".join(df['pros']).split()).most_common(100))\r\n\tres.to_csv(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoortext\"  + \".csv\")\r\n\tprint(res)\r\n\treturn 0\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_glassdoor", "data": "def most_common_glassdoor():\r\n\ttop_N = 60\r\n\tdf = pd.read_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoor'  + str(x.year) + str(x.month) + str(x.day) + '.csv',\r\n\t\t\t\t\t usecols=['pros', 'cons', 'MainText'])\r\n\r\n\ttxt = df.pros.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.cons.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.MainText.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\r\n\twords = nltk.tokenize.word_tokenize(txt)\r\n\tword_dist = nltk.FreqDist(words)\r\n\r\n\tstopwords = nltk.corpus.stopwords.words('english')\r\n\twords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords)\r\n\r\n\trslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\r\n\t\t\t\t\t\tcolumns=['Word', 'Frequency']).set_index('Word')\r\n\tprint(rslt)\r\n\trslt.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoorcommon'  + str(x.year) + str(x.month) + str(x.day) + '.csv')\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_glassdoor_text", "data": "def most_common_glassdoor_text(texts):\r\n\t# text_counter = collections.Counter(texts)\r\n\t#\t # Common = text_counter.most_common(300)\r\n\tCommon = Counter(chain.from_iterable(texts)).most_common(60)\r\n\tprint(Common)\r\n\tdf = pd.DataFrame(Common, columns=['Common Words in Glassdoor','Amount of Occurences'])\r\n\tdf.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoorcommonwords'  +  '.csv')\r\n\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_indeed", "data": "def most_common_indeed():\r\n\ttop_N = 60\r\n\tdf = pd.read_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeed'  + str(x.year) + str(x.month) + str(x.day) + '.csv',\r\n\t\t\t\t\t usecols=['Description', 'Title'])\r\n\r\n\ttxt = df.Description.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.Title.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\r\n\twords = nltk.tokenize.word_tokenize(txt)\r\n\tword_dist = nltk.FreqDist(words)\r\n\r\n\tstopwords = nltk.corpus.stopwords.words('english')\r\n\twords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords)\r\n\r\n\trslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\r\n\t\t\t\t\t\tcolumns=['Word', 'Frequency']).set_index('Word')\r\n\tprint(rslt)\r\n\trslt.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeedcommon'  + str(x.year) + str(x.month) + str(x.day) + '.csv')\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_indeed_text", "data": "def most_common_indeed_text(texts):\r\n\tCommon = Counter(chain.from_iterable(texts)).most_common(60)\r\n\tprint(Common)\r\n\tdf = pd.DataFrame(Common, columns=['Common Words in Indeed','Number of Occurences'])\r\n\tdf.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeedcommonwords'  +  '.csv')\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_wordcloud_indeed", "data": "def most_common_wordcloud_indeed(texts):\r\n\ttext = \"\"\r\n\tfor i in texts:\r\n\t\tprint (i)\r\n\t\ttext = text + str(i)\r\n\twordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"black\").generate(text)\r\n\tplt.figure()\r\n\tplt.imshow(wordcloud, interpolation= \"bilinear\")\r\n\tplt.axis(\"off\")\r\n\t# plt.show()\r\n\t# wordcloud.to_file(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n\tplt.savefig(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n\tCAA_logo = np.array(Image.open(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_logo.png\"))\r\n\t# CAA_logo = CAA_logo.reshape((CAA_logo.shape[0], CAA_logo.shape[1]), order='F')\r\n\ttransformed_CAA_logo = np.ndarray((CAA_logo.shape[0],CAA_logo.shape[1]), np.int32)\r\n\r\n\tfor i in range(len(CAA_logo)):\r\n\t\ttransformed_CAA_logo[i] = list(map(transform_format, CAA_logo[i]))\r\n\tprint(transformed_CAA_logo)\r\n\twc = WordCloud(background_color=\"black\", max_words=500, mask=CAA_logo, contour_width=3, contour_color=\"blue\")\r\n\twc.generate(text)\r\n\twc.to_file(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_transformed_indeed.png\")\r\n\tplt.figure(figsize=[20,10])\r\n\tplt.imshow(wc, interpolation=\"bilinear\")\r\n\tplt.axis(\"off\")\r\n\t# plt.show()\r\n\treturn 0\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "most_common_wordcloud_glassdoor", "data": "def most_common_wordcloud_glassdoor(texts):\r\n\t# Extracts all the text information into a string variable\r\n\ttext = \"\"\r\n\tfor i in texts:\r\n\t\tprint(i)\r\n\t\ttext = text + str(i)\r\n\t# Creates the wordcloud\r\n\twordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\").generate(text)\r\n\tplt.figure()\r\n\tplt.imshow(wordcloud, interpolation=\"bilinear\")\r\n\tplt.axis(\"off\")\r\n\t# plt.show()\r\n\t# wordcloud.to_file(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n\t# Saves the wordcloud\r\n\tplt.savefig(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforglassdoor.png\")\r\n\r\n\t# Creates a logo in the shape of a CAA logo\r\n\t# Opens the CAA logo and takes its shape\r\n\tCAA_logo = np.array(Image.open(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_logo.png\"))\r\n\ttransformed_CAA_logo = np.ndarray((CAA_logo.shape[0], CAA_logo.shape[1]), np.int32)\r\n\r\n\t# Changes the array values of the array to the value 255\r\n\tfor i in range(len(CAA_logo)):\r\n\t\ttransformed_CAA_logo[i] = list(map(transform_format, CAA_logo[i]))\r\n\tprint(transformed_CAA_logo)\r\n\r\n\t# Creates the wordclud in the CAA logo\r\n\twc = WordCloud(background_color=\"black\", max_words=500, mask=CAA_logo, contour_width=3, contour_color=\"blue\")\r\n\twc.generate(text)\r\n\t# Saves it to the file\r\n\twc.to_file(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_transformed_glassdoor.png\")\r\n\t# Creates the dimensions\r\n\tplt.figure(figsize=[20, 10])\r\n\tplt.imshow(wc, interpolation=\"bilinear\")\r\n\t# Makes sure that it has no axises\r\n\tplt.axis(\"off\")\r\n\treturn 0\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "transform_format", "data": "def transform_format(val):\r\n\treturn int(255)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "load_data", "data": "def load_data(path, file_name):\r\n\tprint(\"load_data\")\r\n\tdocuments_list = []\r\n\ttitles = []\r\n\twith open(os.path.join(path, file_name), \"r\", encoding='utf8', errors='ignore') as fin:\r\n\t\tfor line in fin.readlines():\r\n\t\t\ttext = line.strip()\r\n\t\t\tdocuments_list.append(text)\r\n\tprint(len(documents_list))\r\n\ttitles.append(text[0:min(len(text), 250)])\r\n\treturn documents_list, titles\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "load_data_glassdoor", "data": "def load_data_glassdoor(path, file_name):\r\n\tprint(\"load data\")\r\n\tdocuments_list = []\r\n\ttitles = []\r\n\twith open(os.path.join(path, file_name), \"r\", encoding='utf8', errors='ignore') as fin:\r\n\t\tfor line in fin.readlines():\r\n\t\t\ttext = line.strip()\r\n\t\t\tdocuments_list.append(text)\r\n\tprint(len(documents_list))\r\n\ttitles.append(text[0:min(len(text), 250)])\r\n\treturn documents_list, titles\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "preprocess_data", "data": "def preprocess_data(doc_set):\r\n\tpunctuations = '!()-[]{};:\"\\,<>./?@#$%^&*_~'\r\n\tprint(\"preprocess data\")\r\n\ttokenizer = RegexpTokenizer(r'\\w+')\r\n\t# create English stop words list\r\n\ten_stop = set(stopwords.words('english'))\r\n\ten_punctuation = set(string.punctuation)\r\n\t# Create p_stemmer of class PorterStemmer\r\n\tp_stemmer = PorterStemmer()\r\n\t# list for tokenized documents in loop\r\n\ttexts = []\r\n\t# loop through document list\r\n\tfor i in doc_set:\r\n\t\t# clean and tokenize document string\r\n\t\traw = i.lower()\r\n\t\tfor x in raw:\r\n\t\t\tif x in punctuations:\r\n\t\t\t\traw = raw.replace(x,\"\")\r\n\t\ttokens = tokenizer.tokenize(raw)\r\n\t\t# remove stop words from tokens\r\n\t\tstopped_tokens = [i for i in tokens if i not in en_stop]\r\n\t\t# stem tokens\r\n\t\tstemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\r\n\t\t# add tokens to list\r\n\t\ttexts.append(stemmed_tokens)\r\n\treturn texts\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "negative_words", "data": "def negative_words():\r\n\tnegative_word_list = []\r\n\ttry:\r\n\t\twith open(\"negative_words.txt\") as f:\r\n\t\t\tfor line in fin.readLines():\r\n\t\t\t\tword = text.strip()\r\n\t\t\t\tnegative_word_list.append(word)\r\n\tfor word in negative_word_list:\r\n\t\tprint(word)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "negative_word_compare", "data": "def negative_word_compare(doc_clean):\r\n\treturn 0\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "prepare_corpus", "data": "def prepare_corpus(doc_clean):\r\n\tprint(\"print corpus\")\r\n\t\"\"\"\r\n\t  Input  : clean document\r\n\t  Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\r\n\t  Output : term dictionary and Document Term Matrix\r\n\t  \"\"\"\r\n\t# Creating the term dictionary of our courpus, where every unique term is assigned an index.\r\n\tdictionary = corpora.Dictionary(doc_clean)\r\n\t# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\r\n\tdoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\r\n\t# generate LDA model\r\n\treturn dictionary, doc_term_matrix\r\n", "description": "\r\n\t  Input  : clean document\r\n\t  Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\r\n\t  Output : term dictionary and Document Term Matrix\r\n\t  ", "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "create_gensim_lsa_model", "data": "def create_gensim_lsa_model(doc_clean, number_of_topics, words):\r\n\t\"\"\"\r\n\t\tInput  : clean document, number of topics and number of words associated with each topic\r\n\t\tPurpose: create LSA model using gensim\r\n\t\tOutput : return LSA model\r\n\t\t\"\"\"\r\n\tdictionary, doc_term_matrix = prepare_corpus(doc_clean)\r\n\t# generate LSA model\r\n\tlsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)  # train model\r\n\tprint(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\r\n\treturn lsamodel\r\n", "description": "\r\n\t\tInput  : clean document, number of topics and number of words associated with each topic\r\n\t\tPurpose: create LSA model using gensim\r\n\t\tOutput : return LSA model\r\n\t\t", "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "compute_coherence_values", "data": "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean,  stop, start, step):\r\n\r\n\t# Input : dictionary : Gensim dictionary\r\n\t#\t\t corpus: gensim corpus\r\n\t#\t\t texts: list of input texts\r\n\t#\t\t stop: max num of topics\r\n\t# Purpse : Compute c_v coherence for different number of topics\r\n\t# Output: model_list : List of LSA topic models\r\n\t#\t\t coherence_values : Coherence values corresponding to the lDA model with respective numbers\r\n\r\n\tcoherence_values = []\r\n\tmodel_list = []\r\n\tfor num_topics in range(start, stop, step):\r\n\t\tmodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)\r\n\t\tmodel_list.append(model)\r\n\t\tcoherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\r\n\t\tcoherence_values.append(coherencemodel.get_coherence())\r\n\treturn model_list, coherence_values\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LSA_indeed_model_to_csv", "data": "def LSA_indeed_model_to_csv(model_list):\r\n\t# df = pd.DataFrame(data={\"Topic Modelling Indeed\":[model_list]})\r\n\t# df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelindeed\"  +  \".csv\", sep=\" \", index=False)\r\n\tdf = pd.DataFrame.from_records(model_list)\r\n\tdf.columns = [\"Topic Modelling Indeed\", \"col 2\"]\r\n\tdf.applymap(str)\r\n\tdf['Topic Modelling Indeed'] = df['Topic Modelling Indeed'].astype(str)\r\n\t# df['Topic Modelling Indeed'].apply(str)\r\n\t# Removes coherence scores from the dataframe\r\n\tprint(df.dtypes)\r\n\tdf['Topic Modelling Indeed'] = df['Topic Modelling Indeed'].str.replace('\\d+', ' ')\r\n\tdf['col 2'] = df['col 2'].str.replace('\\d+', ' ')\r\n\tdf['col 2'] = df['col 2'].str.replace(r'[^\\w\\s]+', '')\r\n\t# df.transpose()\r\n\tdf.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelindeed\"  +  \".csv\", sep=\" \", index=False)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LSA_glassdoor_model_to_csv", "data": "def LSA_glassdoor_model_to_csv(model_list):\r\n\tdf = pd.DataFrame.from_records(model_list)\r\n\tdf.columns = [\"Topic Modelling Glassdoor\", \"col 2\"]\r\n\tdf.applymap(str)\r\n\tdf['Topic Modelling Glassdoor'] = df['Topic Modelling Glassdoor'].astype(str)\r\n\t# Removes coherence scores from the dataframe\r\n\tdf['Topic Modelling Glassdoor'] = df['Topic Modelling Glassdoor'].str.replace('\\d+', ' ')\r\n\tdf['col 2'] = df['col 2'].str.replace('\\d+', ' ')\r\n\t# Removes the punctuation from the dataframe\r\n\tdf['col 2'] = df['col 2'].str.replace(r'[^\\w\\s]+', '')\r\n\tdf.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelglassdoor\"  +\".csv\", sep=\" \", index=False)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LDA_model_indeed", "data": "def LDA_model_indeed(dictionaryy, doc_term_matrixx, doc_clean):\r\n\texclude = '!()-[]{};:\"\\,<>./?@#$%^&*_~+*'\r\n\tlda_model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrixx,\r\n\t\t\t\t\t\t\t\t\t\t\t\tid2word=dictionaryy, per_word_topics=True, num_topics = 5)\r\n\t\t\t\t\t\t\t\t\t\t\t\t# ,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# num_topics=6,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# random_state=100,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# update_every=1,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# chunksize=100,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# passes=10,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# alpha='auto',\r\n\t\t\t\t\t\t\t\t\t\t\t\t# per_word_topics=True)\r\n\tpprint(lda_model.print_topics())\r\n\tdoc_lda = lda_model[doc_term_matrixx]\r\n\tcoherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, dictionary=dictionaryy, coherence='c_v')\r\n\tcoherence_lda = coherence_model_lda.get_coherence()\r\n\tprint('\\nCoherence Score: ', coherence_lda)\r\n\traw = lda_model.print_topics()\r\n\t# try:\r\n\t#\t raw = ''.join(ch for ch in x if ch not in exclude)\r\n\t# except:\r\n\t#\t pass\r\n\tdf = pd.DataFrame(raw)\r\n\tdf.columns = [\"Topic Nummber\", \"Topic Words\"]\r\n\tdf.applymap(str)\r\n\tdf['Topic Words'] = df['Topic Words'].astype(str)\r\n\t# Gets rid of all the numbers\r\n\tdf['Topic Words'] = df['Topic Words'].str.replace('\\d+', '')\r\n\t# Gets rid of all the punctuation from the topic model\r\n\tdf['Topic Words'] = df['Topic Words'].str.replace(r'[^\\w\\s]+', '')\r\n\tprint(df.head(10))\r\n\tdf.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\coherencescoresindeed\" + \".csv\", sep=\" \", index=False)\r\n\t# mallet_path = 'C:\\\\Users\\\\asha1\\\\AppData\\\\Local\\\\Temp\\\\mallet-2.0.8.zip'  # update this path\r\n\t# ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrixx, num_topics=20, id2word=dictionaryy)\r\n\t#\r\n\t# pprint(ldamallet.show_topics(formatted=False))\r\n\t#\r\n\t# coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=doc_clean, dictionary=dictionaryy,\r\n\t#\t\t\t\t\t\t\t\t\t\t\tcoherence='c_v')\r\n\t# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\r\n\t# print('\\nCoherence Score: ', coherence_ldamallet)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LDA_model_glassdoor", "data": "def LDA_model_glassdoor(dictionaryy, doc_term_matrixx, doc_clean):\r\n\t# Generates the lda model\r\n\tlda_model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrixx,\r\n\t\t\t\t\t\t\t\t\t\t\t\tid2word=dictionaryy, per_word_topics=True, num_topics = 5)\r\n\t\t\t\t\t\t\t\t\t\t\t\t# ,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# num_topics=6,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# random_state=100,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# update_every=1,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# chunksize=100,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# passes=10,\r\n\t\t\t\t\t\t\t\t\t\t\t\t# alpha='auto',\r\n\t\t\t\t\t\t\t\t\t\t\t\t# per_word_topics=True)\r\n\tpprint(lda_model.print_topics())\r\n\tdoc_lda = lda_model[doc_term_matrixx]\r\n\tcoherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, dictionary=dictionaryy, coherence='c_v')\r\n\tcoherence_lda = coherence_model_lda.get_coherence()\r\n\tprint('Coherence Score:', coherence_lda)\r\n\tdf = pd.DataFrame(lda_model.print_topics())\r\n\t# df.columns = [\"Topic Modelling Indeed\", \" \"]\r\n\t# df['Coherence Score'] = df['Coherence Score'].str.replace('\\dt+', '')\r\n\t# df['Topic Words'] = df['Topic Words'].replace('\\d+', '')\r\n\tdf.columns = [\"Topic Nummber\", \"Topic Words\"]\r\n\tdf.applymap(str)\r\n\t# Converts the entire dataframe to an object so it can be interpreted as a string\r\n\tdf['Topic Words'] = df['Topic Words'].astype(str)\r\n\t# Gets rid of all the numbers within the string\r\n\tdf['Topic Words'] = df['Topic Words'].str.replace('\\d+', '')\r\n\t# Gets rid of all the punctuation within the dataframe (easier for user to read)\r\n\tdf['Topic Words'] = df['Topic Words'].str.replace(r'[^\\w\\s]+', '')\r\n\t# Prints first 10 topics generated by the model\r\n\tprint(df.head(10))\r\n\t# Makes the dataframe go to a csv file\r\n\tdf.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\coherencescoresglassdoor\" + \".csv\", sep=\" \", index=False)\r\n\t# mallet_path = 'C:\\\\Users\\\\asha1\\\\AppData\\\\Local\\\\Temp\\\\mallet-2.0.8.zip'  # update this path\r\n\t# ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrixx, num_topics=20, id2word=dictionaryy)\r\n\t#\r\n\t# pprint(ldamallet.show_topics(formatted=False))\r\n\t#\r\n\t# coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=doc_clean, dictionary=dictionaryy,\r\n\t#\t\t\t\t\t\t\t\t\t\t\tcoherence='c_v')\r\n\t# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\r\n\t# print('\\nCoherence Score: ', coherence_ldamallet)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "plot_graph", "data": "def plot_graph(doc_clean, start, stop, step):\r\n\tdictionary, doc_term_matrix = prepare_corpus(doc_clean)\r\n\tmodel_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start, step)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "scrape", "data": "def scrape(field, review, author):\r\n\r\n\tdef scrape_date(review):\r\n\t\treturn review.find_element_by_class_name(\"date\").text\r\n\r\n\tdef scrape_emp_title(review):\r\n\t\tif 'Anonymous Employee' not in review.text:\r\n\t\t\ttry:\r\n\t\t\t\tres = author.find_element_by_class_name(\r\n\t\t\t\t\t'authorJobTitle').text.split('-')[1]\r\n\t\t\texcept Exception:\r\n\t\t\t\tres = np.nan\r\n\t\telse:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_location(review):\r\n\t\ttry:\r\n\t\t\tres = author.find_element_by_class_name(\r\n\t\t\t'authorLocation').text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_status(review):\r\n\t\ttry:\r\n\t\t\tres = author.text.split('-')[0]\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_rev_title(review):\r\n\t\ttry:\r\n\t\t\tres = review.find_element_by_class_name('summary').text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_years(review):\r\n\t\ttry:\r\n\t\t\tfirst_par = review.find_element_by_class_name(\r\n\t\t\t'reviewBodyCell').text\r\n\t\t\tres = first_par\r\n\t\texcept:\r\n\t\t\tprint(\"doesn't work\")\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_helpful(review):\r\n\t\ttry:\r\n\t\t\thelpful = review.find_element_by_class_name('helpfulCount')\r\n\t\t\tres = helpful[helpful.find('(') + 1: -1]\r\n\t\texcept Exception:\r\n\t\t\tres = 0\r\n\t\treturn res\r\n\r\n\tdef expand_show_more(section):\r\n\t\ttry:\r\n\t\t\tmore_content = section.find_element_by_class_name('moreContent')\r\n\t\t\tmore_link = more_content.find_element_by_class_name('moreLink')\r\n\t\t\tmore_link.click()\r\n\t\texcept Exception:\r\n\t\t\tpass\r\n\r\n\tdef scrape_pros(review):\r\n\t\ttry:\r\n\t\t\tpros = review.find_element_by_css_selector(\"p.mt-0.mb-xsm.v2__EIReviewDetailsV2__bodyColor.v2__EIReviewDetailsV2__lineHeightLarge\")\r\n\t\t\texpand_show_more(pros)\r\n\t\t\tres = pros.text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_cons(review):\r\n\t\ttry:\r\n\t\t\tcons = review.find_elements_by_css_selector(\r\n\t\t\t\t\"p.mt-0.mb-xsm.v2__EIReviewDetailsV2__bodyColor.v2__EIReviewDetailsV2__lineHeightLarge\")[1]\r\n\t\t\texpand_show_more(cons)\r\n\t\t\tres = cons.text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_advice(review):\r\n\t\ttry:\r\n\t\t\tadvice = review.find_element_by_class_name('adviceMgmt')\r\n\t\t\texpand_show_more(advice)\r\n\t\t\tres = advice.text.replace('\\nShow Less', '')\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_overall_rating(review):\r\n\t\ttry:\r\n\t\t\tratings = review.find_element_by_class_name('gdStars')\r\n\t\t\toverall = ratings.find_element_by_class_name(\r\n\t\t\t\t'rating').find_element_by_class_name('value-title')\r\n\t\t\tres = overall.get_attribute('title')\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef _scrape_subrating(i):\r\n\t\ttry:\r\n\t\t\tratings = review.find_element_by_class_name('gdStars')\r\n\t\t\tsubratings = ratings.find_element_by_class_name(\r\n\t\t\t\t'subRatings').find_element_by_tag_name('ul')\r\n\t\t\tthis_one = subratings.find_elements_by_tag_name('li')[i]\r\n\t\t\tres = this_one.find_element_by_class_name(\r\n\t\t\t\t'gdBars').get_attribute('title')\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\tdef scrape_work_life_balance(review):\r\n\t\treturn _scrape_subrating(0)\r\n\r\n\tdef scrape_culture_and_values(review):\r\n\t\treturn _scrape_subrating(1)\r\n\r\n\tdef scrape_career_opportunities(review):\r\n\t\treturn _scrape_subrating(2)\r\n\r\n\tdef scrape_comp_and_benefits(review):\r\n\t\treturn _scrape_subrating(3)\r\n\r\n\tdef scrape_senior_management(review):\r\n\t\treturn _scrape_subrating(4)\r\n\r\n\tdef scrape_maintext(review):\r\n\t\ttry:\r\n\t\t\tmaintext = review.find_element_by_class_name('mainText')\r\n\t\t\tres = maintext.text\r\n\t\texcept Exception:\r\n\t\t\tres = np.nan\r\n\t\treturn res\r\n\r\n\t# All the functions for scraping within a list so that they are easier to call at once\r\n\tfuncs = [\r\n\t\tscrape_date,\r\n\t\tscrape_emp_title,\r\n\t\tscrape_location,\r\n\t\tscrape_status,\r\n\t\tscrape_rev_title,\r\n\t\tscrape_helpful,\r\n\t\tscrape_pros,\r\n\t\tscrape_cons,\r\n\t\tscrape_maintext,\r\n\t\tscrape_overall_rating,\r\n\t\tscrape_work_life_balance,\r\n\t\tscrape_culture_and_values,\r\n\t\tscrape_career_opportunities,\r\n\t\tscrape_comp_and_benefits,\r\n\t\tscrape_senior_management\r\n\t]\r\n\r\n\t# Calls all the functions for scraping and collects into a variavle for one review\r\n\tfdict = dict((s, f) for (s, f) in zip(SCHEMA, funcs))\r\n\r\n\treturn fdict[field](review)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "extract_from_page", "data": "def extract_from_page():\r\n\r\n\t# Extracts all the reviews from the webpages\r\n\tdef extract_review(review):\r\n\t\ttime.sleep(2)\r\n\t\tauthor = review.find_element_by_css_selector('span.authorInfo')\r\n\r\n\t\tres = {}\r\n\t\tfor field in SCHEMA:\r\n\t\t\tres[field] = scrape(field, review, author)\r\n\t\t\ttime.sleep(0.1)\r\n\r\n\t\tprint(\"Extracting review\")\r\n\t\tassert set(res.keys()) == set(SCHEMA)\r\n\t\treturn res\r\n\r\n\t# Creates the pandas dataframe, and inputs the columns from the SCHEMA.py file\r\n\tres = pd.DataFrame([], columns=SCHEMA)\r\n\r\n\treviews = browser.find_elements_by_class_name('empReview')\r\n\t# Extracts all the reviews on the page and increases index length of array for each review scraped on page\r\n\tfor review in reviews:\r\n\t\tdata = extract_review(review)\r\n\t\tres.loc[idx[0]] = data\r\n\t\tidx[0] = idx[0] + 1\r\n\t\tprint(idx[0])\r\n\t\tprint(\"index length\")\r\n\r\n\tprint(\"Done extracting from page\")\r\n\t#Arguments not passed for max and min date, but it would check and stop the process if not within bounds of dates passsed\r\n\tif args.max_date and \\\r\n\t\t(pd.to_datetime(res['date']).max() > args.max_date) or \\\r\n\t\t\targs.min_date and \\\r\n\t\t\t(pd.to_datetime(res['date']).min() < args.min_date):\r\n\t\tdate_limit_reached[0] = True\r\n\r\n\treturn res\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "more_pages", "data": "def more_pages():\r\n\tnext_ = browser.find_element_by_css_selector('li.pagination__PaginationStyle__next')\r\n\tprint(\"Found li tag\")\r\n\ttry:\r\n\t\tnext_.find_element_by_tag_name('a')\r\n\t\tprint(\"Element is found\")\r\n\t\treturn True\r\n\texcept selenium.common.exceptions.NoSuchElementException:\r\n\t\treturn False\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "go_to_next_page", "data": "def go_to_next_page():\r\n\tnext_ = browser.find_element_by_css_selector('li.pagination__PaginationStyle__next a')\r\n\tbrowser.get(next_.get_attribute('href'))\r\n\tpage[0] = page[0] + 1\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "no_reviews", "data": "def no_reviews():\r\n\treturn False\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "navigate_to_reviews", "data": "def navigate_to_reviews():\r\n\r\n\tbrowser.get(args.url)\r\n\ttime.sleep(1)\r\n\r\n\tif no_reviews():\r\n\t\treturn False\r\n\r\n\tprint(\"Navigating to reviews\")\r\n\ttime.sleep(1)\r\n\r\n\treturn True\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "sign_in", "data": "def sign_in():\r\n\t# logger.info(f'Signing in to {args.username}')\r\n\r\n\turl = 'https://www.glassdoor.ca/profile/login_input.htm?userOriginHook=HEADER_SIGNIN_LINK'\r\n\tbrowser.get(url)\r\n\r\n\temail_field = browser.find_element_by_name('username')\r\n\tpassword_field = browser.find_element_by_name('password')\r\n\tsubmit_btn = browser.find_element_by_xpath('//button[@type=\"submit\"]')\r\n\r\n\temail_field.send_keys(args.username)\r\n\tpassword_field.send_keys(args.password)\r\n\tsubmit_btn.click()\r\n\r\n\ttime.sleep(1)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "get_browser", "data": "def get_browser():\r\n\tchrome_options = wd.ChromeOptions()\r\n\t# if args.headless:\r\n\tchrome_options.add_argument('--headless')\r\n\tchrome_options.add_argument('log-level=3')\r\n\tbrowser = wd.Chrome(options=chrome_options)\r\n\treturn browser\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "get_current_page", "data": "def get_current_page():\r\n\tpaging_control = browser.find_element_by_class_name('pagingControls')\r\n\tcurrent = int(paging_control.find_element_by_xpath(\r\n\t\t'//ul//li[contains\\\r\n\t\t(concat(\\' \\',normalize-space(@class),\\' \\'),\\' current \\')]\\\r\n\t\t//span[contains(concat(\\' \\',\\\r\n\t\tnormalize-space(@class),\\' \\'),\\' disabled \\')]')\r\n\t\t.text.replace(',', ''))\r\n\treturn current\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "verify_date_sorting", "data": "def verify_date_sorting():\r\n\tascending = urllib.parse.parse_qs(\r\n\t\targs.url)['sort.ascending'] == ['true']\r\n\r\n\tif args.min_date and ascending:\r\n\t\traise Exception(\r\n\t\t\t'min_date required reviews to be sorted DESCENDING by date.')\r\n\telif args.max_date and not ascending:\r\n\t\traise Exception(\r\n\t\t\t'max_date requires reviews to be sorted ASCENDING by date.')\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "web_scrape_glassdoor", "data": "def web_scrape_glassdoor():\r\n\tres = pd.DataFrame([], columns=SCHEMA)\r\n\tsign_in()\r\n\r\n\tif not args.start_from_url:\r\n\t\treviews_exist = navigate_to_reviews()\r\n\t\tif not reviews_exist:\r\n\t\t\treturn\r\n\telif args.max_date or args.min_date:\r\n\t\tverify_date_sorting()\r\n\t\tbrowser.get(args.url)\r\n\t\tpage[0] = get_current_page()\r\n\t\tprint(f'Starting from page {page[0]:,}.')\r\n\t\ttime.sleep(1)\r\n\telse:\r\n\t\tbrowser.get(args.url)\r\n\t\tpage[0] = get_current_page()\r\n\t\tprint(f'Starting from page {page[0]:,}.')\r\n\t\ttime.sleep(1)\r\n\ttime.sleep(4)\r\n\treviews_df = extract_from_page()\r\n\tres = res.append(reviews_df)\r\n\tcount = int(0)\r\n\r\n\twhile more_pages() and len(res) < args.limit:\r\n\t\t# not date_limit_reached[0]:\r\n\t\ttime.sleep(2)\r\n\t\tgo_to_next_page()\r\n\t\ttime.sleep(12)\r\n\t\treviews_df = extract_from_page()\r\n\t\tres = res.append(reviews_df)\r\n\t\tprint(len(res))\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LSA_indeed", "data": "def LSA_indeed():\r\n\tprint(\"hi\")\r\n\tnumber_of_topics = 4\r\n\twords = 8\r\n\ttext_info, title_info = load_data('C:\\\\Users\\\\asha1\\\\WebScrape', 'indeed' + '.csv')\r\n\tprint(text_info)\r\n\tclean_text = preprocess_data(text_info)\r\n\tprint(clean_text)\r\n\tprep_dict, prep_matrix = prepare_corpus(clean_text)\r\n\tprint(prep_dict)\r\n\tprint(\"1\")\r\n\tprint(prep_matrix)\r\n\tnew_model = create_gensim_lsa_model(clean_text, 5, 10)\r\n\tprint(new_model)\r\n\tnew_list, num_topics = compute_coherence_values(prep_dict, prep_matrix, clean_text, 2, 10, 1)\r\n\tprint(new_list)\r\n\tprint(\"In between\")\r\n\t# plot_graph(clean_text, 2, 10, 1)\r\n\t# most_common_indeed()\r\n\tcommon = most_common_indeed_text(clean_text)\r\n\tprint(common)\r\n\tmost_common_wordcloud_indeed(clean_text)\r\n\tprint(\"LSA model\")\r\n\tprint(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n\tLSA_indeed_model_to_csv(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n\tLDA_model_indeed(prep_dict, prep_matrix, clean_text)\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "LSA_glassdoor", "data": "def LSA_glassdoor():\r\n\tnumber_of_topics = 5\r\n\twords = 10\r\n\tcsv_convert('C:\\\\Users\\\\asha1\\\\WebScrape\\\\', 'glassdoor' + '.csv')\r\n\ttext_info, title_info = load_data_glassdoor('C:\\\\Users\\\\asha1\\\\WebScrape', 'glassdoortext' + '.csv')\r\n\tprint(text_info)\r\n\tclean_text = preprocess_data(text_info)\r\n\tprint(clean_text)\r\n\tprep_dict, prep_matrix = prepare_corpus(clean_text)\r\n\tprint(prep_dict)\r\n\tprint(\"1\")\r\n\tprint(prep_matrix)\r\n\tnew_model = create_gensim_lsa_model(clean_text, 5, 10)\r\n\tprint(new_model)\r\n\tnew_list, num_topics = compute_coherence_values(prep_dict, prep_matrix, clean_text, 5, 10, 1)\r\n\tprint(new_list)\r\n\tprint(new_model)\r\n\tmost_common_wordcloud_glassdoor(clean_text)\r\n\tLSA_glassdoor_model_to_csv(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n\tcommon = most_common_glassdoor_text(clean_text)\r\n\tprint(common)\r\n\tLDA_model_glassdoor(prep_dict, prep_matrix, clean_text)\r\n\r\n\t# plot_graph(clean_text, 2, 10, 1)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}, {"term": "def", "name": "run_the_program", "data": "def run_the_program():\r\n\t# Brings old files to old directory and allows space for new files to exist in current directoryis\r\n\tmake_directory()\r\n\t# #\r\n\t# # Webscrapes indeed and glassdoor respectively and saves dataframe to csv file\r\n\tweb_scrape_indeed()\r\n\ttime.sleep(3)\r\n\tweb_scrape_glassdoor()\r\n\t#\r\n\t# # Runs the latent semantic analysis\r\n\tLSA_indeed()\r\n\tLSA_glassdoor()\r\n\r\n\t# # Emails the files to Recruitment\r\n\temail_attachments()\r\n\told_file_move()\r\n", "description": null, "category": "webscraping", "imports": ["from argparse import ArgumentParser\r", "import argparse\r", "from bs4 import BeautifulSoup\r", "from collections import Counter\r", "import csv\r", "import datetime as dt\r", "from datetime import datetime\r", "import gensim\r", "from gensim.utils import simple_preprocess\r", "from gensim.models import CoherenceModel\r", "from gensim.models.coherencemodel import CoherenceModel\r", "from gensim import corpora as corpora\r", "from gensim.models import LsiModel\r", "import json\r", "import logging\r", "import logging.config\r", "from itertools import chain\r", "import matplotlib.pyplot as plt\r", "import matplotlib\r", "import nltk\r", "from nltk import word_tokenize\r", "from nltk.tokenize import RegexpTokenizer\r", "from nltk.corpus import stopwords\r", "from nltk.stem.porter import PorterStemmer\r", "import numpy as np\r", "import os.path\r", "import os\r", "import pandas as pd\r", "from PIL import Image\r", "import PIL\r", "from pprint import pprint\r", "import pyLDAvis\r", "import pyLDAvis.gensim\r", "import re\r", "import requests\r", "from requests import get\r", "from schema import SCHEMA\r", "import selenium\r", "from selenium.webdriver import Chrome\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium import webdriver as wd\r", "import shutil\r", "import smtplib\r", "import string\r", "from string import Template\r", "import wordcloud\r", "import time\r", "import unittest\r", "from urllib.request import urlopen\r", "import urllib\r", "import win32com.client\r", "from wordcloud import WordCloud, ImageColorGenerator\r", "from wordcloud import STOPWORDS\r", "import xlsxwriter\r", "# import pywin32\r"]}], [{"term": "def", "name": "get_ip", "data": "def get_ip():\n\te.delete(0, END)\n\tx = socket.gethostbyname(socket.gethostname())\n\te.insert(0, str(x))\n\treturn\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "import socket, platform, subprocess, os, sys", "import requests, bs4"]}, {"term": "def", "name": "get_telnet", "data": "def get_telnet():\n\tports = [80, 443]\n\tx = os.system(\"telnet \" + str(\"www.icbc.com.cn\") + \" \" + str(ports[0]))\n\te.insert(0, str(x))\n\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "import socket, platform, subprocess, os, sys", "import requests, bs4"]}, {"term": "def", "name": "get_hostname", "data": "def get_hostname():\n\te.delete(0, END)\n\tx = socket.gethostname()\n\te.insert(0, str(x))\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "import socket, platform, subprocess, os, sys", "import requests, bs4"]}, {"term": "def", "name": "get_ping", "data": "def get_ping():\n\turl = \"www.icbc.com.cn\"\n\tx = os.system(\"ping \" + url )\n\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "import socket, platform, subprocess, os, sys", "import requests, bs4"]}, {"term": "def", "name": "get_trace", "data": "def get_trace():\n\turl = \"www.icbc.com.cn\"\n\tx = os.system(\"tracert \" + url)\n\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "import socket, platform, subprocess, os, sys", "import requests, bs4"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\tres = requests.get('http://www.icbc.com.cn')\n\tsoup = bs4.BeautifulSoup(res.content, 'html.parser')\n\n\theaders = res.headers\n\tstatus = res.status_code\n\tx = soup.find(class_=\"main\")\n\n\tif len(headers) == 3 and status == 200:\n\t\ttext = x.get_text()\n\t\tprint(\"VPN = down, Bank = Up\")\n\t\tprint(text)\n\telif len(headers) == 3 and status != 200:\n\t\ttext = x.get_text()\n\t\tprint(\"VPN = down, Bank = Down\")\n\t\tprint(text)\n\telif len(headers) > 3 and status != 200:\n\t\tprint(\"VPN = Up, Bank = Down\")\n\telse:\n\t\tprint(\"VPN = Up, Bank = Up\")\n\n\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "import socket, platform, subprocess, os, sys", "import requests, bs4"]}], [{"term": "def", "name": "setup", "data": "def setup(bot)\r\n\tbot.add_cog(webscrape())\r\n", "description": null, "category": "webscraping", "imports": ["from .webscrape import webscrape\r"]}], [], [{"term": "def", "name": "cursor", "data": "def cursor(file_name):\n\t\"\"\" Connects and returns a cursor to an sqlite output file\n\n\tParameters\n\t----------\n\tfile_name: str\n\t\tname of the sqlite file\n\n\tReturns\n\t-------\n\tsqlite cursor\n\t\"\"\"\n\tcon = sql.connect(file_name)\n\tcon.row_factory = sql.Row\n\treturn con.cursor()\n\n", "description": " Connects and returns a cursor to an sqlite output file\n\n\tParameters\n\t----------\n\tfile_name: str\n\t\tname of the sqlite file\n\n\tReturns\n\t-------\n\tsqlite cursor\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "import_pris", "data": "def import_pris(pris_link):\n\t\"\"\" Opens pris_csv using Pandas. Adds Latitude and Longitude\n\tcolumns\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\n\tReturns\n\t-------\n\tpris: pd.Dataframe\n\t\tpris database\n\t\"\"\"\n\tpris = pd.read_csv(pris_link,\n\t\t\t\t\t   delimiter=',',\n\t\t\t\t\t   encoding='iso-8859-1'\n\t\t\t\t\t   )\n\tpris.insert(13, 'Latitude', np.nan)\n\tpris.insert(14, 'Longitude', np.nan)\n\tpris = pris.replace(np.nan, '')\n\treturn pris\n\n", "description": " Opens pris_csv using Pandas. Adds Latitude and Longitude\n\tcolumns\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\n\tReturns\n\t-------\n\tpris: pd.Dataframe\n\t\tpris database\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "import_webscrape_data", "data": "def import_webscrape_data(scrape_link):\n\t\"\"\" Returns sqlite content of webscrape by performing an\n\tsqlite query\n\n\tParameters\n\t----------\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tcoords: sqlite cursor\n\t\tsqlite cursor containing webscrape data\n\t\"\"\"\n\tcur = cursor(scrape_link)\n\tcoords = cur.execute(\"SELECT name, long, lat FROM reactors_coordinates\")\n\treturn coords\n\n", "description": " Returns sqlite content of webscrape by performing an\n\tsqlite query\n\n\tParameters\n\t----------\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tcoords: sqlite cursor\n\t\tsqlite cursor containing webscrape data\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "edge_cases", "data": "def edge_cases():\n\t\"\"\" Returns a dictionary of edge cases that fuzzywuzzy is\n\tunable to catch. This could be because PRIS database stores\n\treactor names and Webscrape database fetches power plant names,\n\tor because PRIS reactor names are abbreviated.\n\n\tParameters\n\t----------\n\n\tReturns\n\t-------\n\tothers: dict\n\t\tdictionary of edge cases with \"key=pris_reactor_name, and\n\t\tvalue=webscrape_plant_name\"\n\t\"\"\"\n\tpris_edge_cases = {'OHI-': '\u014ci',\n\t\t\t  'ASCO-': 'Asc\u00f3',\n\t\t\t  'ROVNO-': 'Rivne',\n\t\t\t  'SHIN-KORI-': 'Kori',\n\t\t\t  'ANO-': 'Arkansas One',\n\t\t\t  'HANBIT-': 'Yeonggwang',\n\t\t\t  'FERMI-': 'Enrico Fermi',\n\t\t\t  'BALTIC-': 'Kaliningrad',\n\t\t\t  'COOK-': 'Donald C. Cook',\n\t\t\t  'HATCH-': 'Edwin I. Hatch',\n\t\t\t  'HARRIS-': 'Shearon Harris',\n\t\t\t  'SHIN-WOLSONG-': 'Wolseong',\n\t\t\t  'ST. ALBAN-': 'Saint-Alban',\n\t\t\t  'LASALLE-': 'LaSalle County',\n\t\t\t  'SUMMER-': 'Virgil C. Summer',\n\t\t\t  'FARLEY-': 'Joseph M. Farley',\n\t\t\t  'ST. LAURENT ': 'Saint-Laurent',\n\t\t\t  'HADDAM NECK': 'Connecticut Yankee',\n\t\t\t  'HIGASHI DORI-1 (TOHOKU)': 'Higashid\u014dri',\n\t\t\t  }\n\treturn pris_edge_cases\n\n", "description": " Returns a dictionary of edge cases that fuzzywuzzy is\n\tunable to catch. This could be because PRIS database stores\n\treactor names and Webscrape database fetches power plant names,\n\tor because PRIS reactor names are abbreviated.\n\n\tParameters\n\t----------\n\n\tReturns\n\t-------\n\tothers: dict\n\t\tdictionary of edge cases with \"key=pris_reactor_name, and\n\t\tvalue=webscrape_plant_name\"\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "sanitize_webscrape_name", "data": "def sanitize_webscrape_name(name):\n\t\"\"\" Sanitizes webscrape powerplant names by removing unwanted\n\tstrings (listed in blacklist), applying lower case, and deleting\n\ttrailing whitespace.\n\n\tParameters\n\t----------\n\tname: str\n\t\twebscrape plant name\n\n\tReturns\n\t-------\n\tname: str\n\t\tsanitized name for use with fuzzywuzzy\n\t\"\"\"\n\tblacklist = ['nuclear', 'power',\n\t\t\t\t 'plant', 'generating',\n\t\t\t\t 'station', 'reactor', 'atomic',\n\t\t\t\t 'energy', 'center', 'electric']\n\tname = name.lower()\n\tfor blacklisted in blacklist:\n\t\tname = name.replace(blacklisted, '')\n\tname = name.strip()\n\tname = ' '.join(name.split())\n\treturn name\n\n", "description": " Sanitizes webscrape powerplant names by removing unwanted\n\tstrings (listed in blacklist), applying lower case, and deleting\n\ttrailing whitespace.\n\n\tParameters\n\t----------\n\tname: str\n\t\twebscrape plant name\n\n\tReturns\n\t-------\n\tname: str\n\t\tsanitized name for use with fuzzywuzzy\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "merge_coordinates", "data": "def merge_coordinates(pris_link, scrape_link):\n\t\"\"\" Merges webscrape data with pris data performed by string\n\tcomparison of reactor names from pris and webscrape. Returns\n\tupdated pris database with coordinates.\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\t\"\"\"\n\tothers = edge_cases()\n\tpris = import_pris(pris_link)\n\tcoords = import_webscrape_data(scrape_link)\n\tfor web in coords:\n\t\tfor idx, prs in pris.iterrows():\n\t\t\twebscrape_name = sanitize_webscrape_name(web['name'])\n\t\t\tpris_name = prs[1].lower()\n\t\t\tif fuzz.ratio(webscrape_name, pris_name) > 64:\n\t\t\t\tprs[13] = web['lat']\n\t\t\t\tprs[14] = web['long']\n\t\t\telse:\n\t\t\t\tfor other in others.keys():\n\t\t\t\t\tedge_case_key = other.lower()\n\t\t\t\t\tedge_case_value = others[other].lower()\n\t\t\t\t\tif (fuzz.ratio(pris_name, edge_case_key) > 80 and\n\t\t\t\t\t\t\tfuzz.ratio(webscrape_name, edge_case_value) > 75):\n\t\t\t\t\t\tprs[13] = web['lat']\n\t\t\t\t\t\tprs[14] = web['long']\n\treturn pris\n\n", "description": " Merges webscrape data with pris data performed by string\n\tcomparison of reactor names from pris and webscrape. Returns\n\tupdated pris database with coordinates.\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "save_output", "data": "def save_output(pris):\n\t\"\"\" Saves updated PRIS database as 'reactors_pris_2016.csv'\n\n\tParameters\n\t----------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\n\tReturns\n\t-------\n\n\t\"\"\"\n\tpris.to_csv('reactors_pris_2016.csv',\n\t\t\t\tindex=False,\n\t\t\t\tsep=',',\n\t\t\t\t)\n\n", "description": " Saves updated PRIS database as 'reactors_pris_2016.csv'\n\n\tParameters\n\t----------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\n\tReturns\n\t-------\n\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}, {"term": "def", "name": "main", "data": "def main(pris_link, scrape_link):\n\t\"\"\" Calls all required functions to merge PRIS and webscrape\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\n\t\"\"\"\n\tpris = merge_coordinates(pris_link, scrape_link)\n\tsave_output(pris)\n\n", "description": " Calls all required functions to merge PRIS and webscrape\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\n\t", "category": "webscraping", "imports": ["from fuzzywuzzy import fuzz", "import numpy as np", "import pandas as pd", "import sqlite3 as sql", "import sys", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)"]}], [{"term": "def", "name": "setProfile", "data": "def setProfile( Profile):\n\tp.money = Profile.money\n\tp.id = Profile.id\n\tp.totalcoins = Profile.totalcoins\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "login", "data": "def login():\n\treturn render_template('login.html')\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "newPass", "data": "def newPass():\n\tnewPass=request.form['newpassword']\n\tconn = pymysql.connect(host=\"localhost\",\n\t\t\t\t\t\t   user=DB_USERNAME,\n\t\t\t\t\t\t   passwd=DB_PASSWORD,\n\t\t\t\t\t\t   db=DB_NAME,\n\t\t\t\t\t\t   port=3306)\n\tcursor = conn.cursor(pymysql.cursors.DictCursor)\n\tsql = \"UPDATE `users` set password =  (`password`) where id = (`id`) VALUES (%s, %s)\"\n\ttry:\n\t\tcursor.execute(sql,(newPass,p.id))\n\n\t\tconn.commit()\n\texcept:\n\t\tconn.rollback()\n\tconn.close()\n\treturn render_template('login.html')\n\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "returnProfile", "data": "def returnProfile():\n\tid = p.id\n\tconn = pymysql.connect(host=\"localhost\",\n\t\t\t\t\t\t   user=DB_USERNAME,\n\t\t\t\t\t\t   passwd=DB_PASSWORD,\n\t\t\t\t\t\t   db=DB_NAME,\n\t\t\t\t\t\t   port=3306)\n\tcursor = conn.cursor(pymysql.cursors.DictCursor)\n\n\tcursor.execute('SELECT * FROM profile WHERE id = %s ', id)\n\n\taccount = cursor.fetchone()\n\tprofile = Profile(account['id'], account['money'], account['totalcoins'])\n\tg.profile = profile\n\tcursor.execute('SELECT * FROM users WHERE id = %s ', id)\n\n\taccount1 = cursor.fetchone()\n\tuser = User(account1['id'], account1['email'], account1['password'])\n\tg.user = user\n\n\treturn render_template('profile.html')\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "profile", "data": "def profile():\n\tid = p.id\n\tconn = pymysql.connect(host=\"localhost\",\n\t\t\t\t\t\t   user=DB_USERNAME,\n\t\t\t\t\t\t   passwd=DB_PASSWORD,\n\t\t\t\t\t\t   db=DB_NAME,\n\t\t\t\t\t\t   port=3306)\n\tcursor = conn.cursor(pymysql.cursors.DictCursor)\n\n\tcursor.execute('SELECT * FROM profile WHERE id = %s ', id)\n\n\taccount = cursor.fetchone()\n\tprofile = Profile(account['id'], account['money'], account['totalcoins'])\n\tg.profile = profile\n\tcursor.execute('SELECT * FROM users WHERE id = %s ', id)\n\n\taccount1 = cursor.fetchone()\n\tuser = User(account1['id'], account1['email'], account1['password'])\n\tg.user = user\n\n\treturn render_template('profile.html')\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "homePage", "data": "def homePage():\n\tglobal etherium\n\tglobal bitcoin\n\tglobal cat\n\tglobal book\n\tglobal mv\n\tglobal cel\n\tglobal dodge\n\tglobal Tether\n\tglobal sand\n\n\tid = p.id\n\tconn = pymysql.connect(host=\"localhost\",\n\t\t\t\t\t\t   user=DB_USERNAME,\n\t\t\t\t\t\t   passwd=DB_PASSWORD,\n\t\t\t\t\t\t   db=DB_NAME,\n\t\t\t\t\t\t   port=3306)\n\tcursor = conn.cursor(pymysql.cursors.DictCursor)\n\n\tcursor.execute('SELECT * FROM profile WHERE id = %s ', id)\n\n\taccount = cursor.fetchone()\n\tcursor.execute('SELECT * FROM users WHERE id = %s ', id)\n\n\taccount1 = cursor.fetchone()\n\tprofile = Profile(account['id'], account['money'], account['totalcoins'])\n\tg.profile = profile\n\temail2 = \"\"\n\tfor i in range(0, len(account1['email'])):\n\t\tif (account1['email'][i] == '@'):\n\t\t\temail2 = account1['email'][:i]\n\tuser = User(account['id'], email2, \"hd\")\n\tg.user = user\n\tcoinList = []\n\tCoin = bitcoinMonitor()\n\ttime.sleep(2.5)\n\tCoin1 = EteriumMonitor()\n\ttime.sleep(2.5)\n\n\tCoin2 = DodgecoinMonitor()\n\ttime.sleep(2.5)\n\n\tCoin3 = TetherMonitor()\n\ttime.sleep(2.5)\n\n\tCoin4 = CatGirlMonitor()\n\ttime.sleep(2.5)\n\n\tCoin5 = CelsiusMonitor()\n\ttime.sleep(2.5)\n\n\tCoin6 = BitbookMonitor()\n\ttime.sleep(2.5)\n\n\tCoin7 = SandboxMonitor()\n\ttime.sleep(2.5)\n\n\tCoin8 = M7v2Monitor()\n\ttime.sleep(2.5)\n\n\tCoin.type = typeTrim(Coin.type)\n\tCoin1.type = typeTrim(Coin1.type)\n\tCoin2.type = typeTrim(Coin2.type)\n\tCoin3.type = typeTrim(Coin3.type)\n\tCoin4.type = typeTrim(Coin4.type)\n\tCoin5.type = typeTrim(Coin5.type)\n\tCoin6.type = \"BitBook\"\n\tCoin7.type = \"The Sandbox\"\n\tCoin8.type = \"M7v2\"\n\tsand = Coin7.price\n\tbook = Coin6.price\n\tmv = Coin8.price\n\tbitcoin = Coin.price\n\tetherium = Coin1.price\n\tdodge = Coin2.price\n\tTether = Coin3.price\n\tcat = Coin4.price\n\tcel = Coin5.price\n\n\tcoinList.append(Coin)\n\tcoinList.append(Coin1)\n\tcoinList.append(Coin2)\n\tcoinList.append(Coin3)\n\tcoinList.append(Coin4)\n\tcoinList.append(Coin5)\n\tcoinList.append(Coin6)\n\tcoinList.append(Coin7)\n\tcoinList.append(Coin8)\n\tprint(\"here\")\n\n\treturn render_template('home.html')\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "after_request", "data": "def after_request(response):\n\tresponse.headers[\"Cache-Control\"] = \"no-cache, no-store, must-revalidate\"\n\treturn response\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "purchase", "data": "def purchase():\n\n\tif request.method == 'POST':\n\t\tid = p.id\n\t\tmoney = 0\n\t\tpurchaseTotal = request.form['purchaseAmt']\n\t\tcryptoCoin = request.form['coin']\n\t\tconn = pymysql.connect(host=\"localhost\",\n\t\t\t\t\t\t\t   user=DB_USERNAME,\n\t\t\t\t\t\t\t   passwd=DB_PASSWORD,\n\t\t\t\t\t\t\t   db=DB_NAME,\n\t\t\t\t\t\t\t   port=3306)\n\t\tcursor = conn.cursor(pymysql.cursors.DictCursor)\n\t\tcursor.execute('SELECT * FROM profile WHERE id = %s ', id)\n\t\taccount = cursor.fetchone()\n\t\tprofile = Profile(account['id'], account['money'], account['totalcoins'])\n\t\tpurchaseTotal = float(purchaseTotal)\n\t\tprofile.totalcoins = float(profile.totalcoins)\n\t\tnewCoins = float(purchaseTotal+profile.totalcoins)\n\t\tif cryptoCoin == \"Bitcoin\":\n\t\t\tmoney = bitcoin\n\t\tif cryptoCoin == \"Ethereum\":\n\t\t\tmoney = etherium\n\t\tif cryptoCoin == \"Dodgecoin\":\n\t\t\tmoney = dodge\n\t\tif cryptoCoin == \"Tether\":\n\t\t\tmoney = Tether\n\t\tif cryptoCoin == \"CatGirl\":\n\t\t\tmoney = cat\n\t\tif cryptoCoin == \"Celsius\":\n\t\t\tmoney = cel\n\t\tif cryptoCoin == \"Bitbook\":\n\t\t\tmoney = book\n\t\tif cryptoCoin == \"Sandbox\":\n\t\t\tmoney = sand\n\t\tif cryptoCoin == \"M7v2\":\n\t\t\tmoney = mv\n\t\tz = money.replace(\"$\", \"\")\n\t\tm = z.replace(\",\",\"\")\n\t\tmoneySpent = float(m)\n\t\tmoneySpent = moneySpent * purchaseTotal\n\n\t\tif int(profile.money) - float(moneySpent) < 0:\n\t\t\treturn render_template('Error.html')\n\t\tif float(moneySpent) > int(profile.money):\n\t\t\treturn render_template('Error.html')\n\t\telse:\n\t\t\tprofile.money = float(profile.money - moneySpent)\n\t\t\tcursor.execute('UPDATE profile set totalcoins = %s where id = %s', (newCoins, id))\n\t\t\tconn.commit()\n\t\t\tsql = \"INSERT INTO `crypto` (`id`, `cost`, `type`) VALUES (%s, %s, %s)\"\n\n\t\t\tcursor.execute(sql, (id, moneySpent, cryptoCoin))\n\n\t\t\tconn.commit()\n\t\t\tcursor.execute('UPDATE profile set money = %s where id = %s', (profile.money, id))\n\t\t\tconn.commit()\n\n\n\t\t\tcursor.execute('SELECT * FROM users WHERE id = %s ', id)\n\t\t\taccount1 = cursor.fetchone()\n\t\t\tuser = User(account1['id'], account1['email'], account1['password'])\n\t\t\tprofile = Profile(account['id'], account['money'], account['totalcoins'])\n\t\t\tuser = User(id, user.email, user.password)\n\t\t\temail2 = \"\"\n\t\t\tfor i in range(0, len(user.email)):\n\t\t\t\tif (user.email[i] == '@'):\n\t\t\t\t\temail2 = user.email[:i]\n\n\t\t\tuser.email = email2\n\t\t\tg.user = user\n\t\t\tg.profile = profile\n\t\t\treturn render_template('success.html')\n\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "home", "data": "def home():\n\tglobal etherium\n\tglobal bitcoin\n\tglobal cat\n\tglobal book\n\tglobal mv\n\tglobal cel\n\tglobal dodge\n\tglobal Tether\n\tglobal sand\n\temail = request.form['email']\n\tpassword = request.form['password']\n\tconn = pymysql.connect(host=\"localhost\",\n\t\t\t\t\t\t   user=DB_USERNAME,\n\t\t\t\t\t\t   passwd=DB_PASSWORD,\n\t\t\t\t\t\t   db=DB_NAME,\n\t\t\t\t\t\t   port=3306)\n\tcursor = conn.cursor(pymysql.cursors.DictCursor)\n\n\tcursor.execute('SELECT * FROM users WHERE email = %s AND password = %s', (email, password,))\n\tacc = cursor.fetchone()\n\tif request.method == 'POST' and acc :\n\t\tg.profile = p\n\t\temail = request.form['email']\n\t\tpassword = request.form['password']\n\t\tuser = User(0, email, password)\n\t\temail2 = \"\"\n\t\tfor i in range(0, len(user.email)):\n\t\t\tif (user.email[i] == '@'):\n\t\t\t\temail2 = user.email[:i]\n\n\t\tuser.email = email2\n\t\tg.user = user\n\n\t\tconn = pymysql.connect(host=\"localhost\",\n\t\t\t\t\t\t\t   user=DB_USERNAME,\n\t\t\t\t\t\t\t   passwd=DB_PASSWORD,\n\t\t\t\t\t\t\t   db=DB_NAME,\n\t\t\t\t\t\t\t   port=3306)\n\t\tcursor = conn.cursor(pymysql.cursors.DictCursor)\n\n\t\tcursor.execute('SELECT * FROM users WHERE email = %s AND password = %s', (email, password,))\n\n\t\taccount = cursor.fetchone()\n\t\tcursor.execute('SELECT * FROM profile WHERE id = %s ', (account['id']))\n\n\t\taccount1 = cursor.fetchone()\n\t\tprof = Profile(account1['id'], account1['money'], account1['totalcoins'])\n\t\tg.profile = prof\n\t\tsetProfile(prof)\n\t\tcoinList = []\n\t\tCoin = bitcoinMonitor()\n\t\ttime.sleep(2.5)\n\t\tCoin1 = EteriumMonitor()\n\t\ttime.sleep(2.5)\n\n\n\t\tCoin2 = DodgecoinMonitor()\n\t\ttime.sleep(2.5)\n\n\t\tCoin3 = TetherMonitor()\n\t\ttime.sleep(2.5)\n\n\t\tCoin4 = CatGirlMonitor()\n\t\ttime.sleep(2.5)\n\n\t\tCoin5 = CelsiusMonitor()\n\t\ttime.sleep(2.5)\n\n\t\tCoin6 = BitbookMonitor()\n\t\ttime.sleep(2.5)\n\n\t\tCoin7 = SandboxMonitor()\n\t\ttime.sleep(2.5)\n\n\t\tCoin8 = M7v2Monitor()\n\t\ttime.sleep(2.5)\n\n\n\t\tCoin.type = typeTrim(Coin.type)\n\t\tCoin1.type = typeTrim(Coin1.type)\n\t\tCoin2.type = typeTrim(Coin2.type)\n\t\tCoin3.type = typeTrim(Coin3.type)\n\t\tCoin4.type = typeTrim(Coin4.type)\n\t\tCoin5.type = typeTrim(Coin5.type)\n\t\tCoin6.type = \"BitBook\"\n\t\tCoin7.type = \"The Sandbox\"\n\t\tCoin8.type = \"M7v2\"\n\t\tsand = Coin7.price\n\t\tbook = Coin6.price\n\t\tmv = Coin8.price\n\t\tbitcoin = Coin.price\n\t\tetherium = Coin1.price\n\t\tdodge = Coin2.price\n\t\tTether = Coin3.price\n\t\tcat = Coin4.price\n\t\tcel = Coin5.price\n\n\n\n\n\n\n\t\tcoinList.append(Coin)\n\t\tcoinList.append(Coin1)\n\t\tcoinList.append(Coin2)\n\t\tcoinList.append(Coin3)\n\t\tcoinList.append(Coin4)\n\t\tcoinList.append(Coin5)\n\t\tcoinList.append(Coin6)\n\t\tcoinList.append(Coin7)\n\t\tcoinList.append(Coin8)\n\n\n\t\tif account:\n\t\t\tsession['loggedin'] = True\n\t\t\tsession['email'] = account['email']\n\t\t\treturn render_template(\"home.html\", len=len(coinList), coinList=coinList)\n\t\telse:\n\t\t\tmsg = 'Incorrect username/password!'\n\t\t\treturn render_template('login.html', msg=msg)\n\telse:\n\t\treturn render_template('login.html', msg='')\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "priceBitcoin", "data": "def priceBitcoin():\n\tw = Webscrape()\n\tprice = w.priceBitcoin()\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "priceEthe", "data": "def priceEthe():\n\tw = Webscrape()\n\tprice = w.priceEterium()\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "priceCat", "data": "def priceCat():\n\tw = Webscrape()\n\tprice = w.priceCatGirl()\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "priceCel", "data": "def priceCel():\n\tw = Webscrape()\n\tprice = w.priceCelsius()\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "priceMv", "data": "def priceMv():\n\tw = Webscrape()\n\tprice = w.priceM7v2()\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "priceThe", "data": "def priceThe():\n\tw = Webscrape()\n\tprice = w.priceTether()\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "priceSand", "data": "def priceSand():\n\tw = Webscrape()\n\tprice = w.priceSandbox()\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "priceBook", "data": "def priceBook():\n\tw = Webscrape()\n\tprice = w.priceBitbook()\n\treturn price\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "priceDodge", "data": "def priceDodge():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tprice = w.priceDodgecoin()\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "bitcoinMonitor", "data": "def bitcoinMonitor():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tx = w.priceTrackBitcoin()\n\tcoin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n\tprint(\"here\")\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "EteriumMonitor", "data": "def EteriumMonitor():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tx = w.priceTrackEthereum()\n\tcoin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "DodgecoinMonitor", "data": "def DodgecoinMonitor():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tx = w.priceTrackDodgecoin()\n\tcoin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "TetherMonitor", "data": "def TetherMonitor():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tx = w.priceTrackTether()\n\tcoin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "CatGirlMonitor", "data": "def CatGirlMonitor():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tx = w.priceTrackCatGirl()\n\tcoin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "CelsiusMonitor", "data": "def CelsiusMonitor():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tx = w.priceTrackCelsius()\n\tcoin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "BitbookMonitor", "data": "def BitbookMonitor():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tx = w.priceTrackBitbook()\n\tcoin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "SandboxMonitor", "data": "def SandboxMonitor():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tx = w.priceTrackSandbox()\n\tcoin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "M7v2Monitor", "data": "def M7v2Monitor():\n\tw = Webscrape(0,\"none\",\"none\",0,\"none\")\n\tx = w.priceTrackM7v2()\n\tcoin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n\tprint(\"final\")\n\treturn coin\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "typeTrim", "data": "def typeTrim(type):\n\tcapitalCount = 0\n\tnewType = \"\"\n\tfor i in type:\n\t\tif i.isupper():\n\t\t\tcapitalCount = capitalCount + 1\n\t\tif capitalCount > 1:\n\t\t\tbreak\n\t\tnewType = newType + i\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "register", "data": "def register():\n\tif request.method == 'POST':\n\n\t\temail = request.form['email']\n\t\tpassword = request.form['password']\n\n\t\tconn = pymysql.connect(host = \"localhost\",\n\t\t\t\t\t\t\t   user=DB_USERNAME,\n\t\t\t\t\t\t\t   passwd=DB_PASSWORD,\n\t\t\t\t\t\t\t   db=DB_NAME,\n\t\t\t\t\t\t\t   port=3306)\n\t\tcursor = conn.cursor(pymysql.cursors.DictCursor)\n\t\tcursor.execute('SELECT * FROM users WHERE email = %s AND password = %s', (email, password,))\n\t\tcursor.execute('SELECT * FROM users WHERE email = %s ', email)\n\t\taccount1 = cursor.fetchone()\n\t\tif account1:\n\n\t\t\treturn render_template('login.html')\n\n\t\tsql = \"INSERT INTO `users` (`email`, `password`) VALUES (%s, %s)\"\n\n\n\n\t\tcursor.execute(sql, (email, password))\n\n\t\tconn.commit()\n\t\tcursor.execute('SELECT * FROM users WHERE email = %s AND password = %s', (email, password,))\n\t\taccount = cursor.fetchone()\n\t\tid = account['id']\n\t\tmoney = 10000\n\t\ttotalcoins = 0\n\t\tprofile = Profile(id,money,totalcoins)\n\t\tsetProfile(profile)\n\t\tsql2 = \"INSERT INTO `profile` (id,money, totalcoins) VALUES (%s,%s, %s)\"\n\t\tcursor.execute(sql2, (id, money, totalcoins))\n\n\t\tconn.commit()\n\t\treturn redirect('/')\n\n\telse:\n\t\treturn render_template('register.html')\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "logout", "data": "def logout():\n\tif request.method == 'POST':\n\n\t\tflash('You were logged out.')\n\n\t\treturn render_template('login.html')\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "bad_request", "data": "def bad_request(error):\n\treturn make_response(jsonify({'error': 'Bad request'}), 400)\n\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}, {"term": "def", "name": "not_found", "data": "def not_found(error):\n\treturn make_response(jsonify({'error': 'Not found'}), 404)\n", "description": null, "category": "webscraping", "imports": ["from urllib import response", "from time import sleep", "from flask import Flask, jsonify, abort, request, make_response, url_for, session, flash", "from flask import render_template, redirect, g", "from flask_session import Session", "from src.paperTrade.webScraper import Webscrape", "import pymysql", "from src.paperTrade.DTO.profileDTO import Profile", "from src.paperTrade.DTO.userDTO import User", "from src.paperTrade.DTO.coinDTO import CoinDTO", "import os", "import time", "import datetime", "import json", "# import MySQLdb", "# from flask_mysqldb import MySQL"]}], [{"term": "def", "name": "buildFullURL", "data": "def buildFullURL(provider, pageNumber):\n\tmaxYear = str(datetime.datetime.now().year + 1)\n\tbaseSiteURL = ParseData.getFromConfig(\"URLs\", \"baseSiteURL\")\n\n\tbaseURL = baseSiteURL + provider\n\tretUrl = baseURL + '?min-rating=0&min-year=1920&max-year=' + maxYear + '&order=title&originals=0&page=' + pageNumber\n\treturn retUrl\n\n", "description": null, "category": "webscraping", "imports": ["import traceback", "import urllib", "import uuid", "import requests", "import datetime", "import SetData", "import WebScrape", "import DataBase", "import ParseData", "import main as Main"]}, {"term": "def", "name": "buildJsonResults", "data": "def buildJsonResults(cardBodies, provider):\n\tfor pt in cardBodies:\n\t\ttitleName = str(pt)\n\t\ttry:\n\t\t\tif 'tab-content' in titleName:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tstripTitle = titleName.split('>')[3].lstrip().split('<')[0].rstrip()  # Gets the Clean Title. Example: \"This Title\"\n\t\texcept Exception:\n\t\t\ttraceback.print_exc()\n\t\t\tstripTitle = 'No Title Available'\n\n\t\ttry:\n\t\t\tstripYear = titleName.split('>')[6].lstrip().split('<')[0].rstrip()  # Gets the Title Year. Example: \"1996\"\n\t\texcept Exception:\n\t\t\ttraceback.print_exc()\n\t\t\tstripYear = 'No Year Available'\n\n\t\t# Gets the Title Stream Service Link. Example Link Title: \"this-title-name\"\n\t\tif provider == '':  # Netflix\n\t\t\ttry:\n\t\t\t\tsRawTitle = str(titleName.split('/')[2].lstrip().split('<')[0].rstrip())\n\t\t\t\tsStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n\t\t\texcept Exception:\n\t\t\t\ttraceback.print_exc()\n\t\t\t\tsStreamLink = 'No Link Available'\n\t\t\t\ttitleType = \"\"\n\t\telif provider == 'disney-plus/':\n\t\t\ttry:\n\t\t\t\tsRawTitle = str(titleName.split('/')[3].lstrip().split('<')[0].rstrip())\n\t\t\t\tsStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n\t\t\texcept Exception:\n\t\t\t\ttraceback.print_exc()\n\t\t\t\tsStreamLink = 'No Link Available'\n\t\t\t\ttitleType = \"\"\n\t\telif provider == 'hulu/':\n\t\t\ttry:\n\t\t\t\tsRawTitle = str(titleName.split('/')[3].lstrip().split('<')[0].rstrip())\n\t\t\t\tsStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n\t\t\texcept Exception:\n\t\t\t\ttraceback.print_exc()\n\t\t\t\tsStreamLink = 'No Link Available'\n\t\t\t\ttitleType = \"\"\n\t\telif provider == 'hbo-max/':\n\t\t\ttry:\n\t\t\t\tsRawTitle = str(titleName.split('/')[3].lstrip().split('<')[0].rstrip())\n\t\t\t\tsStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n\t\t\texcept Exception:\n\t\t\t\ttraceback.print_exc()\n\t\t\t\tsStreamLink = 'No Link Available'\n\t\t\t\ttitleType = \"\"\n\n\t\telif provider == 'amazon-prime-video/':\n\t\t\ttry:\n\t\t\t\tsRawTitle = str(titleName.split('/')[3].lstrip().split('<')[0].rstrip())\n\t\t\t\tsStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n\t\t\texcept Exception:\n\t\t\t\ttraceback.print_exc()\n\t\t\t\tsStreamLink = 'No Link Available'\n\t\t\t\ttitleType = \"\"\n\t\telse:\n\t\t\tsStreamLink = 'No Link Available'\n\t\t\ttitleType = \"\"\n\n\t\tMain.titleNumber += 1\n\t\tprint(str(Main.titleNumber) + '.) ' + SetData.getProviderNames(provider) + ' - ' + stripTitle)\n\t\tif stripTitle != '':\n\t\t\tif stripYear == '':\n\t\t\t\tstripYear = ' '\n\t\t\tbuildDbInsert(stripTitle, stripYear, sStreamLink, provider, titleType)\n\t\telse:\n\t\t\tprint('Title: ' + stripTitle + ', Year: ' + stripYear + ', Link: ' + sStreamLink + ', Provider: ' + provider)\n\t\t\tprint(titleName)\n\t\t\tprint(cardBodies)\n\t\t\tprint(\"buildJsonResults() Catch\")\n\n", "description": null, "category": "webscraping", "imports": ["import traceback", "import urllib", "import uuid", "import requests", "import datetime", "import SetData", "import WebScrape", "import DataBase", "import ParseData", "import main as Main"]}, {"term": "def", "name": "buildDbInsert", "data": "def buildDbInsert(title, year, link, provider, titleType):\n\tservice = SetData.getProviderNames(provider)\n\tnewID = uuid.uuid4()\n\tif DataBase.titleExists(title, year):\n\t\treturn\n\telse:\n\t\tFullAPIresults = buildAPICall(title, year, titleType)\n\t\tdict_parsedAPIforResults = ParseData.parseAPIresults(FullAPIresults, title, titleType)\n\t\tstr_Plot = ParseData.parseDictData(dict_parsedAPIforResults, \"overview\")\n\t\tstr_TmdbID = ParseData.parseDictData(dict_parsedAPIforResults, 'id')\n\t\tstr_PosterURL = buildPosterURL(dict_parsedAPIforResults)\n\t\tDataBase.insertIntoDB(newID, title, year, service, link, str_PosterURL, str_Plot, str_TmdbID)\n\t\t# print(\"buildDbInsert() Catch\")\n\n", "description": null, "category": "webscraping", "imports": ["import traceback", "import urllib", "import uuid", "import requests", "import datetime", "import SetData", "import WebScrape", "import DataBase", "import ParseData", "import main as Main"]}, {"term": "def", "name": "buildAPICall", "data": "def buildAPICall(title, year, titleType):\n\ttry:\n\t\tqueryTitle = urllib.parse.quote(title)\n\t\tAPIkey = ParseData.getFromConfig(\"API\", \"APIkey\")\n\n\t\trequestURL = 'https://api.themoviedb.org/3/search/' + titleType + '?api_key=' + APIkey + '&query=' + queryTitle + '&year=' + year\n\t\tresponse = requests.get(requestURL)\n\t\tAPIresults = response.json()\n\t\treturn APIresults\n\texcept Exception:\n\t\ttraceback.print_exc()\n\t\tprint(\"buildAPICall() Catch\")\n\n", "description": null, "category": "webscraping", "imports": ["import traceback", "import urllib", "import uuid", "import requests", "import datetime", "import SetData", "import WebScrape", "import DataBase", "import ParseData", "import main as Main"]}, {"term": "def", "name": "buildPosterURL", "data": "def buildPosterURL(parsedAPIdict):\n\tposterURL = ''\n\ttry:\n\t\tAPIkey = ParseData.getFromConfig(\"API\", \"APIkey\")\n\n\t\trequestURL = 'https://api.themoviedb.org/3/configuration?api_key=' + APIkey\n\t\tresponse = requests.get(requestURL)\n\t\tAPIresults = response.json()\n\n\t\tposterDict = ParseData.parseDictData(APIresults, 'images')\n\t\tposterBaseURL = ParseData.parseDictData(posterDict, 'base_url')\n\t\tposterPath = ParseData.parseDictData(parsedAPIdict, 'poster_path')\n\t\tif posterPath == {}:\n\t\t\tposterURL = \"Null\"\n\t\telse:\n\t\t\tposterURL = posterBaseURL + 'original' + posterPath\n\texcept Exception:\n\t\ttraceback.print_exc()\n\t\tprint(\"buildPosterURL() Catch\")\n\treturn posterURL\n", "description": null, "category": "webscraping", "imports": ["import traceback", "import urllib", "import uuid", "import requests", "import datetime", "import SetData", "import WebScrape", "import DataBase", "import ParseData", "import main as Main"]}], [{"term": "class", "name": "classJobs:", "data": "class Jobs:\n\tcompanyName = None\n\tdesignation = None\n\tqualification = None\n\tlocation = None\n\tlink = None\n\tlastDate = None\n\n\tdef __init__(self):\n\t\tself.companyName = []\n\t\tself.designation = []\n\t\tself.qualification = []\n\t\tself.lastDate = []\n\t\tself.location = []\n\t\tself.link = []\n\n\tdef getBlocks(self):\n\t\tpage = requests.get(url)\n\t\tparser = BeautifulSoup(page.content, \"html.parser\")\n\t\tblocks = parser.find('div', id=\"all-jobs-append\")\n\t\tself.getDetails(blocks)\n\n\tdef getDetails(self, block):\n\t\tfor name in block.find_all('span', itemprop=\"name\"):\n\t\t\tself.companyName.append(name.get_text())\n\t\tfor designation in block.find_all('div', class_=\"\"):\n\t\t\tself.designation.append(designation.get_text())\n\t\tfor location in block.find_all('span', class_=\"job-location\"):\n\t\t\tself.location.append(location.get_text())\n\t\tfor qualification in block.find_all('span', class_=\"qualifications\"):\n\t\t\tself.qualification.append(qualification.get_text())\n\t\tfor lastDate in block.find_all('span', class_=\"padding-left-4\"):\n\t\t\tself.lastDate.append(lastDate.get_text())\n\t\tfor link in block.find_all('a',\n\t\t\t\t\t\t\t\t   attrs={'href': re.compile('^https://'), 'class': re.compile('^view-apply-button')}):\n\t\t\tself.link.append(link.get('href'))\n\t\t# print(self.companyName)\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "import re", "from bs4 import BeautifulSoup", "import pandas as pd", "import csv"]}, {"term": "class", "name": "Database", "data": "class Database(Jobs):\n\tdef putDataIntoDatabase(self, Jobs):\n\t\tjobs = {\n\t\t\t'Company': Jobs.companyName,\n\t\t\t'Designation': Jobs.designation,\n\t\t\t'Qualifications': Jobs.qualification,\n\t\t\t'Location': Jobs.location,\n\t\t\t'Last Date to Apply': Jobs.lastDate,\n\t\t\t'Link': Jobs.link\n\t\t}\n\t\tdataframe = pd.DataFrame.from_dict(jobs, orient='index')\n\t\tdataframe = dataframe.transpose()\n\t\tdataframe.to_csv(r'/home/ananth/WebScrape/Jobs.csv', index=True, header=True)\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "import re", "from bs4 import BeautifulSoup", "import pandas as pd", "import csv"]}], [{"term": "class", "name": "NYCrawler", "data": "class NYCrawler(object):\n\n\tdef __init__(self, starter_url):\n\t\tuser_agent = \"Amherst College SURF 2018, contact salfeld2018Amherst.edu with any questions.\"\n\t\topts = Options()\n\t\topts.add_argument(f\"user-agent={user_agent}\")\n\t\tself.driver = webdriver.Chrome(chrome_options=opts, executable_path='/Applications/chromedriver')\n\t\tself.base_url = starter_url\n\t\tself.driver.get(starter_url)\n\t\tself.links = []\n\t\twebscrape.time.sleep(5)\n\t\tself.log_in()\n\n\tdef log_in(self):\n\t\tbutton = self.driver.find_element_by_xpath('//*[@id=\"masthead-cap\"]/div[2]/div[3]/button[2]')\n\t\ttry:\n\t\t\tbutton.click()\n\t\texcept Exception:\n\t\t\tclose = self.driver.find_element_by_xpath('//*[@id=\"closeCross\"]')\n\t\t\tclose.click()\n\t\t\tbutton.click()\n\n\t\tusername = self.driver.find_element_by_xpath('//*[@id=\"username\"]')\n\t\tusername.send_keys(\"trydzews4@gmail.com\")\n\t\twebscrape.time.sleep(3)\n\t\tpassword = self.driver.find_element_by_xpath('//*[@id=\"password\"]')\n\t\tpassword.send_keys(\"headcount\")\n\t\twebscrape.time.sleep(3)\n\t\t# remember = self.driver.find_element_by_xpath('//*[@id=\"rememberMe\"]')\n\t\t# remember.click()\n\t\tsubmit = self.driver.find_element_by_xpath('//*[@id=\"submitButton\"]')\n\t\tsubmit.click()\n\t\twebscrape.time.sleep(100)\n\n\tdef get_robo_link(self, link):\n\t\tif \".com/\" in link:\n\t\t\trobo_link = link.split('com')[0] + \"com/robots.txt\"\n\t\telif \".org/\" in link:\n\t\t\trobo_link = link.split('org')[0] + \"org/robots.txt\"\n\t\telif \".edu/\" in link:\n\t\t\trobo_link = link.split('edu')[0] + \"edu/robots.txt\"\n\t\telse:\n\t\t\trobo_link = \"\"\n\t\treturn robo_link\n\n\tdef check_links(self, links):\n\t\tchecked_links = []\n\t\trp = robotparser.RobotFileParser()\n\n\t\tfor l in links:\n\t\t\trp.set_url(self.get_robo_link(l))\n\t\t\trp.read()\n\t\t\tif rp.can_fetch(\"*\", l):\n\t\t\t\tchecked_links.append(l)\n\t\tself.links = self.links + checked_links\n\t\treturn checked_links\n\n\tdef get_ny_links(self):\n\t\tlinks = []\n\n\t\t#section 1\n\t\ttry:\n\t\t\tsection1 = self.driver.find_element_by_class_name(\"rank\")\n\t\t\tones = section1.find_elements_by_tag_name(\"li\")\n\t\t\tfor o in ones:\n\t\t\t\td = o.find_element_by_tag_name(\"div\")\n\t\t\t\thref = str(d.find_element_by_tag_name(\"a\").get_attribute(\"href\"))\n\t\t\t\tif \"/photo/\" not in href and \"/video/\" not in href and \"/interactive\" not in href:\n\t\t\t\t\tif href not in links and href not in self.links:\n\t\t\t\t\t\tlinks.append(href)\n\t\texcept NoSuchElementException:\n\t\t\tprint(\"no section one\")\n\t\tprint(str(len(links)))\n\n\t\t#section2\n\t\ttry:\n\t\t\tsection2 = self.driver.find_element_by_css_selector(\".story-menu.theme-stream.initial-set\")\n\t\t\ttwos = section2.find_elements_by_tag_name(\"li\")\n\t\t\tfor t in twos:\n\t\t\t\td_tag = t.find_element_by_tag_name(\"div\")\n\t\t\t\thref = str(d_tag.find_element_by_tag_name(\"a\").get_attribute(\"href\"))\n\t\t\t\tif \"/photo/\" not in href and \"/video/\" not in href and \"/interactive\" not in href:\n\t\t\t\t\tif href not in links and href not in self.links:\n\t\t\t\t\t\tlinks.append(href)\n\t\texcept NoSuchElementException:\n\t\t\tprint(\"no section two\")\n\t\tprint(str(len(links)))\n\n\t\t#section3\n\t\ttry:\n\t\t\tsection3 = self.driver.find_element_by_css_selector(\".story-menu.theme-stream.additional-set\")\n\t\t\tthrees = section3.find_elements_by_tag_name(\"li\")\n\t\t\tfor t in threes:\n\t\t\t\ttry:\n\t\t\t\t\td_tag = t.find_element_by_tag_name(\"div\")\n\t\t\t\t\thref = str(d_tag.find_element_by_tag_name(\"a\").get_attribute(\"href\"))\n\t\t\t\t\tif \"/photo/\" not in href and \"/video/\" not in href:\n\t\t\t\t\t\tif href not in links and href not in self.links and \"/interactive\" not in href:\n\t\t\t\t\t\t\tlinks.append(href)\n\t\t\t\texcept NoSuchElementException:\n\t\t\t\t\tprint(\"paid ad\")\n\t\t\t\texcept StaleElementReferenceException:\n\t\t\t\t\tprint(\"stale\")\n\t\t\t\t\tprint(t)\n\t\texcept NoSuchElementException:\n\t\t\tprint(\"no section three\")\n\t\tprint(str(len(links)))\n\t\treturn links\n\n\tdef get_ny_article(self, url):\n\t\tto_return = [None] * 3\n\t\tto_return[0] = url\n\t\ttry:\n\t\t\tself.driver.get(url)\n\t\texcept TimeoutException:\n\t\t\treturn to_return\n\n\t\t#try to get title\n\t\ttry:\n\t\t\ttitle = self.driver.find_element_by_class_name(\"balancedHeadline\")\n\t\t\tto_return[1] = title.text\n\t\texcept NoSuchElementException:\n\t\t\ttry:\n\t\t\t\ttitle=self.driver.find_element_by_id(\"headline\")\n\t\t\t\tto_return[1] = title.text\n\t\t\texcept NoSuchElementException:\n\t\t\t\tprint(\"no title\")\n\t\t\t\tprint(url)\n\t\t\t\tto_return[1] = ''\n\n\t\t#try to get content\n\t\ttry:\n\t\t\ttext = ''\n\t\t\tsections = self.driver.find_elements_by_css_selector(\".css-18sbwfn.StoryBodyCompanionColumn\")\n\t\t\tfor sec in sections:\n\t\t\t\tparagraphs = sec.find_elements_by_tag_name(\"p\")\n\t\t\t\tfor p in paragraphs:\n\t\t\t\t\ttext = text + p.text\n\t\t\tto_return[2] = text\n\t\texcept NoSuchElementException:\n\t\t\tprint(\"no content\")\n\t\t\tto_return[2] = ''\n\t\treturn to_return\n\n\tdef get_ny_data(self, links):\n\t\tdata = []\n\t\tcount = 0\n\t\tfor link in links:\n\t\t\tprint(count)\n\t\t\tcontent = self.get_ny_article(link)\n\t\t\tif len(content[1]) > 0 and len(content[2]) > 0:\n\t\t\t\tdata.append(content)\n\t\t\twebscrape.time.sleep(10)\n\t\t\tcount +=1\n\t\treturn data\n\n\tdef collect_links(self):\n\t\tself.click_button()\n\t\tself.scroll_down()\n\t\tlinks = self.get_ny_links()\n\t\tprint(\"in collect links \" + str(len(links)))\n\t\treturn links\n\n\n\tdef click_button(self):\n\t\tbutton = self.driver.find_element_by_xpath('//*[@id=\"latest-panel\"]/div[1]/div/div/button')\n\t\tbutton.click()\n\n\tdef scroll_down(self):\n\t\tcount = 0\n\t\tSCROLL_PAUSE_TIME = 5\n\t\tlast_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n\n\t\twhile count < 50:\n\t\t\t# Scroll down to bottom\n\t\t\ttry:\n\t\t\t\tself.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n\n\t\t\t\t# Wait to load page\n\t\t\t\twebscrape.time.sleep(SCROLL_PAUSE_TIME)\n\n\t\t\t\t# Calculate new scroll height and compare with last scroll height\n\t\t\t\tnew_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n\t\t\t\tif new_height == last_height:\n\t\t\t\t\tprint(\"Reached bottom\")\n\t\t\t\t\tbreak\n\t\t\t\tlast_height = new_height\n\t\t\t\tprint(\"scrolling\")\n\t\t\t\tcount +=1\n\t\t\t\tprint(count)\n\t\t\texcept TimeoutException:\n\t\t\t\tbreak\n\n\tdef one_section(self, url):\n\t\tself.driver.get(url)\n\t\tlinks = self.collect_links()\n\t\tchecked_links = self.check_links(links)\n\t\tdata = self.get_ny_data(checked_links)\n\t\treturn data\n\n\tdef start(self):\n\t\tone = self.one_section(\"https://www.nytimes.com/section/us\")\n\t\ttwo = self.one_section(\"https://www.nytimes.com/section/world?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=World&pgtype=sectionfront\")\n\t\tthree = self.one_section(\"https://www.nytimes.com/section/politics?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=Politics&pgtype=sectionfront\")\n\t\tfour = self.one_section(\"https://www.nytimes.com/section/business?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=Business&pgtype=sectionfront\")\n\t\tfull_data = one + two + three + four\n\t\treturn full_data\n\n\t#FOR BIG RUN\n\n\tdef get_master_links(self, url):\n\t\tself.driver.get(url)\n\t\tlinks = self.collect_links()\n\t\tchecked_links = self.check_links(links)\n\t\treturn checked_links\n\n\tdef pickle_links(self):\n\t\tone = self.get_master_links(\"https://www.nytimes.com/section/us\")\n\t\tprint(\"length of one is \" + str(len(one)))\n\t\twebscrape.time.sleep(10)\n\t\ttwo = self.get_master_links(\"https://www.nytimes.com/section/world\")\n\t\tprint(\"length of two is \" + str(len(two)))\n\t\twebscrape.time.sleep(10)\n\t\tthree = self.get_master_links(\"https://www.nytimes.com/section/politics?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=Politics&pgtype=sectionfront\")\n\t\tprint(\"length of three is \" + str(len(three)))\n\t\twebscrape.time.sleep(10)\n\t\tfour = self.get_master_links(\"https://www.nytimes.com/section/business?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=Business&pgtype=sectionfront\")\n\t\tprint(\"length of four is \" + str(len(four)))\n\t\tfull_links = one + two + three + four\n\t\treturn full_links\n\n\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.common.exceptions import NoSuchElementException", "import pickle", "from selenium.common.exceptions import StaleElementReferenceException", "from selenium.common.exceptions import TimeoutException", "from urllib import robotparser", "import webscrape.time"]}], [], [], [], [], [{"term": "def", "name": "send_embed", "data": "def send_embed(embed_data):\n\tglobal webscrape_data\n\n\tdata = {\n\t\t\"embeds\": [embed_data]\n\t}\n\tsend_attempts = 0\n\twhile send_attempts < 3:\n\t\tembed_request = requests.post(webhook_url, json=data)\n\t\tif embed_request.status_code in [204, 200]:\n\t\t\tbreak\n\t\telse:\n\t\t\tprint(\"Sending Embed Failed [RETRYING]: {0} because {1}\\n\\nData: {2}\".format(embed_request.status_code, embed_request.reason, data))\n\t\t\tdata = {\n\t\t\t\t\"embeds\": [{\n\t\t\t\t\t\"title\": embed_data[\"title\"],\n\t\t\t\t\t\"description\": embed_data[\"description\"],\n\t\t\t\t\t\"url\": embed_data[\"url\"],\n\t\t\t\t\t\"color\": embed_data[\"color\"]\n\t\t\t\t}]\n\t\t\t}\n\t\t\tif send_attempts != 0:\n\t\t\t\ttime.sleep(30)\n\t\t\tsend_attempts += 1\n\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import random", "import json", "import time", "import os"]}, {"term": "def", "name": "get_nike_link_data", "data": "def get_nike_link_data(link):\n\tglobal webscrape_data\n\n\trequest = requests.get(link, headers=webscrape_data[\"headers\"][\"nike\"], proxies=webscrape_data[\"proxies\"])\n\tif request.status_code != 200:\n\t\tprint(\"Request Failed: {0} because {1}\\n\\nLink: {2}\".format(request.status_code, request.reason, link))\n\telse:\n\n\t\thtml_text = (request.text)\n\t\tsoup = BeautifulSoup(html_text, features=\"lxml\")\n\n\t\timage_element_list = soup.body.find(\"img\", {\"class\": \"css-10f9kvm u-full-width u-full-height css-1436l9y\"})\n\t\treturn ({\n\t\t\t\"item_name\": image_element_list[\"alt\"],\n\t\t\t\"image_url\": image_element_list[\"src\"]\n\t\t})\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import random", "import json", "import time", "import os"]}, {"term": "def", "name": "get_adidas_link_data", "data": "def get_adidas_link_data(link):\n\tglobal webscrape_data\n\n\trequest = requests.get(link, headers=webscrape_data[\"headers\"][\"adidas\"], proxies=webscrape_data[\"proxies\"])\n\tif request.status_code != 200:\n\t\tprint(\"Request Failed: {0} because {1}\\n\\nLink: {2}\".format(request.status_code, request.reason, link))\n\telse:\n\n\t\thtml_text = (request.text)\n\t\tsoup = BeautifulSoup(html_text, features=\"lxml\")\n\n\timage_element_list = soup.body.find(\"div\", {\"class\": \"images_container___3KxTB\"}).img\n\treturn ({\n\t\t\"item_name\": image_element_list[\"alt\"],\n\t\t\"image_url\": image_element_list[\"src\"]\n\t})\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import random", "import json", "import time", "import os"]}, {"term": "def", "name": "get_supreme_link_data", "data": "def get_supreme_link_data(link):\n\tglobal webscrape_data\n\n\trequest = requests.get(link, headers=webscrape_data[\"headers\"][\"supreme\"], proxies=webscrape_data[\"proxies\"])\n\tif request.status_code != 200:\n\t\tprint(\"Request Failed: {0} because {1}\\n\\nLink: {2}\".format(request.status_code, request.reason, link))\n\telse:\n\n\t\thtml_text = (request.text)\n\t\tsoup = BeautifulSoup(html_text, features=\"lxml\")\n\n\t\timage_element_list = soup.body.find(\"img\", {\"id\": \"img-main\"})\n\t\treturn ({\n\t\t\t\"item_name\": image_element_list[\"alt\"],\n\t\t\t\"image_url\": image_element_list[\"src\"]\n\t\t})\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import random", "import json", "import time", "import os"]}, {"term": "def", "name": "get_nike_link_stock", "data": "def get_nike_link_stock(link):\n\tglobal webscrape_data\n\tproxy = {\"http\": random.choice(webscrape_data[\"proxies\"])}\n\trequest = requests.get(link, headers=webscrape_data[\"headers\"][\"nike\"], proxies=proxy)\n\tif request.status_code != 200:\n\t\tprint(\"Getting Nike Stock Request Failed: {0} because {1}\\n\\nLink: {2}\".format(request.status_code, request.reason, link))\n\t\treturn {\n\t\t\t\"item_in_stock\": False,\n\t\t\t\"in_stock\": [],\n\t\t\t\"out_of_stock\": []\n\t\t}\n\telse:\n\t\thtml_text = (request.text)\n\t\tsoup = BeautifulSoup(html_text, features=\"lxml\")\n\n\t\tif soup.body.find(\"button\", {\"aria-label\": \"Add to Cart\"}) == None:\n\t\t\treturn {\n\t\t\t\t\"item_in_stock\": False,\n\t\t\t\t\"in_stock\": [],\n\t\t\t\t\"out_of_stock\": []\n\t\t\t}\n\n\t\tif soup.body.find(\"div\", {\"name\": \"skuAndSize\"}) == None:\n\t\t\treturn {\n\t\t\t\t\"item_in_stock\": True,\n\t\t\t\t\"in_stock\": [],\n\t\t\t\t\"out_of_stock\": []\n\t\t\t}\n\t\telement_list = soup.body.find(\"div\", {\"name\": \"skuAndSize\"}).find_all()\n\t\tnike_stock_attributes = {\n\t\t\t\"in_stock\": [\"data-css-1iiusdt\", \"data-css-lv2huc\", \"data-css-ikkzrh\"],\n\t\t\t\"out_of_stock\": [\"data-css-yyh50b\", \"data-css-137acxc\", \"data-css-y50moq\"]\n\t\t}\n\t\tstock_list = {\n\t\t\t\"item_in_stock\": True,\n\t\t\t\"in_stock\": [],\n\t\t\t\"out_of_stock\": []\n\t\t}\n\t\tfor element in element_list:\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import random", "import json", "import time", "import os"]}], [], [{"term": "def", "name": "clean_name", "data": "def clean_name(name):\n\tname_header = name.replace('__',' & ')\n\tname_header = name_header.replace('_',' ')\n\tname_header = name_header.replace(',',', ')\n\tname_header = name_header[0].upper() + name_header[1:]\n\treturn name_header\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "fresh_products", "data": "def fresh_products():\n\t#if request.method == 'POST':\n\t\t\n\treturn render_template(\"fresh_products.html\", user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "dairy", "data": "def dairy():\n\tname = 'zuivel'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "apero_products", "data": "def apero_products():\n\tname = 'aperitief'\n\tname_header = clean_name(name)\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Item {product_name} added to cart', category='success')\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "bread_pastry_products", "data": "def bread_pastry_products():\n\tname = 'brood__patisserie'\n\tname_header = clean_name(name)\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Item {product_name} added to cart', category='success')\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "meat_products", "data": "def meat_products():\n\tname = 'charcuterie'\n\tname_header = clean_name(name)\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Item {product_name} added to cart', category='success')\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "fruit_vegetables_products", "data": "def fruit_vegetables_products():\n\tname = 'groenten__fruit'\n\tname_header = clean_name(name)\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Item {product_name} added to cart', category='success')\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "butcher", "data": "def butcher():\n\tname = 'beenhouwerij'\n\tname_header = clean_name(name)\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Item {product_name} added to cart', category='success')\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "catering_prepared_meals_products", "data": "def catering_prepared_meals_products():\n\tname = 'traiteur__bereide_maaltijden'\n\tname_header = clean_name(name)\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Item {product_name} added to cart', category='success')\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "vegi_vegan_fresh", "data": "def vegi_vegan_fresh():\n\tname = 'vegetarisch_vegan'\n\tname_header = clean_name(name)\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Item {product_name} added to cart', category='success')\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "fish_sushi_products", "data": "def fish_sushi_products():\n\tname = 'vis__sushi'\n\tname_header = clean_name(name)\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Item {product_name} added to cart', category='success')\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "breakfast", "data": "def breakfast():\n\tname = 'ontbijt'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "world_kitchen", "data": "def world_kitchen():\n\tname = 'wereldkeuken'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "desserts_sugar_flour", "data": "def desserts_sugar_flour():\n\tname = 'desserts,suiker__bloem'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "candy", "data": "def candy():\n\tname = 'snoepgoed'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "chocolate", "data": "def chocolate():\n\tname = 'chocolade'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "chips_apero", "data": "def chips_apero():\n\tname = 'chips__aperitief'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "cookies_pies", "data": "def cookies_pies():\n\tname = 'koeken__taarten'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "rusk_crackers", "data": "def rusk_crackers():\n\tname = 'beschuit__crackers'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "canned", "data": "def canned():\n\tname = 'conserven'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "water", "data": "def water():\n\tname = 'water'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "fruit_vegetables_drinks", "data": "def fruit_vegetables_drinks():\n\tname = 'vruchten__groentensap'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "syrups", "data": "def syrups():\n\tname = 'siropen'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "milk_plant_based_drinks", "data": "def milk_plant_based_drinks():\n\tname = 'melk__plantaardige_dranken'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "hot_drinks", "data": "def hot_drinks():\n\tname = 'warme_dranken'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "soft_drinks", "data": "def soft_drinks():\n\tname = 'softdrinks'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "beer", "data": "def beer():\n\tname = 'bier'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "apero_strong_drink", "data": "def apero_strong_drink():\n\tname = 'aperitieven__sterke_drank'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "wines", "data": "def wines():\n\tname = 'wijnen'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "vegi_vegan_frozen", "data": "def vegi_vegan_frozen():\n\tname = 'vegetarisch__vegan'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "ice_desserts", "data": "def ice_desserts():\n\tname = 'ijs__desserten'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "pizza_quiches", "data": "def pizza_quiches():\n\tname = 'pizza__quiches'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "bread_cake", "data": "def bread_cake():\n\tname = 'brood__gebak'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "snacks_apero", "data": "def snacks_apero():\n\tname = 'snacks__aperitiefhapjes'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "starters", "data": "def starters():\n\tname = 'voorgerechten'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "prepared_meals", "data": "def prepared_meals():\n\tname = 'bereide_maaltijden'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "fish", "data": "def fish():\n\tname = 'vis'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "vegetables", "data": "def vegetables():\n\tname = 'groenten'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "milk_drinks", "data": "def milk_drinks():\n\tname = 'melk__dranken'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "baby_food", "data": "def baby_food():\n\tname = 'babyvoeding'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "baby_care", "data": "def baby_care():\n\tname = 'verzorging_baby'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "diapers", "data": "def diapers():\n\tname = 'luiers__luierbroekjes'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "detergent_baby", "data": "def detergent_baby():\n\tname = 'wasmiddelen_baby'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\t\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "mouth_hiegene", "data": "def mouth_hiegene():\n\tname = 'mondhygi\u00ebne'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "body", "data": "def body():\n\tname = 'lichaam'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "hair", "data": "def hair():\n\tname = 'haar'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "face", "data": "def face():\n\tname = 'gezicht'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "men", "data": "def men():\n\tname = 'mannen'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "hiegene", "data": "def hiegene():\n\tname = 'hygi\u00ebne'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "pharmacy", "data": "def pharmacy():\n\tname = 'apotheek'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "toiletpaper_papertowels_tissues", "data": "def toiletpaper_papertowels_tissues():\n\tname = 'toiletpapier,keukenpapier__zakdoeken'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "detergents", "data": "def detergents():\n\tname = 'wasmiddelen'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "laundry_care", "data": "def laundry_care():\n\tname = 'verzorging_van_de_was'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "dishwashing_products", "data": "def dishwashing_products():\n\tname = 'Vaatwasproducten'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "cleaning_products", "data": "def cleaning_products():\n\tname = 'schoonmaakproducten'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "air_freshener_refill", "data": "def air_freshener_refill():\n\tname = 'luchtverfrissers__navullingen'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "maintenance_accessoires", "data": "def maintenance_accessoires():\n\tname = 'onderhoudsaccessoires'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "cleaning_accessoires", "data": "def cleaning_accessoires():\n\tname = 'huishoudaccessoires'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "pesticides", "data": "def pesticides():\n\tname = 'insecticides'\n\tname_header = clean_name(name)\n\n\tstatic_directory = os.path.join(os.getcwd(),r'website/static')\n\tdf_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n\tproducts_info = df_products_info.to_dict('records')\n\n\tif request.method == 'POST':\n\t\tproduct_name = request.form.get('product_name')\n\t\tproduct_img = request.form.get('product_img')\n\t\tproduct_base_price = request.form.get('product_base_price')\n\t\tproduct_base_price_type = request.form.get('product_base_price_type')\n\t\tproduct_big_price = request.form.get('product_big_price')\n\t\tnew_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\t\tif product_name != '':\n\t\t\tflash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n\treturn render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}, {"term": "def", "name": "cart_items", "data": "def cart_items():\n\tif request.method == 'POST':\n\t\titemId = request.form.get('id')\n\t\tuserId = request.form.get('save_shoppinglist')\n\t\tif itemId:\n\t\t\titem = Selected_item.query.get(itemId)\n\t\t\tif item:\n\t\t\t\tif item.user_id == current_user.id:\n\t\t\t\t\tdb.session.delete(item)\n\t\t\t\t\tdb.session.commit()\n\t\t\t\t\tflash(f'{item.name} verwijderd uit mandje')\n\t\telif userId:\n\t\t\tuser = User.query.get(userId)\n\t\t\tif user:\n\t\t\t\tif user.id == current_user.id:\n\t\t\t\t\tif current_user.shoppinglist:\n\t\t\t\t\t\tnew_purchase = Purchases(user_id=current_user.id)\n\t\t\t\t\t\tdb.session.add(new_purchase)\n\t\t\t\t\t\tdb.session.commit()\n\t\t\t\t\t\tfor item in current_user.shoppinglist:\n\t\t\t\t\t\t\tnew_ordered_item = Ordered_item(name = item.name,img = item.img, base_price = item.base_price, base_price_type = item.base_price_type, big_price = item.big_price, purchase_id = new_purchase.id)\n\t\t\t\t\t\t\tdb.session.add(new_ordered_item)\n\t\t\t\t\t\t\tdb.session.delete(item)\n\t\t\t\t\t\t\tdb.session.commit()\n\t\t\t\t\t\tflash(f\"Winkelmandje succesvol doorgestuurd\", category='success')\n\t\t\t\t\t\treturn redirect(url_for('views.home'))\n\t\t\t\t\telse:\n\t\t\t\t\t\tflash(f\"Winkelmandje kan niet leeg worden opgeslaan\", category='error')\n\t\n\tsum_prices = 0\n\tfor item in current_user.shoppinglist:\n\t\tsum_prices += Decimal(item.base_price[:-1].replace(',','.'))\n\t#current_user.total_price = sum_prices  enkel nodig als ik echt in database wil opslaan\n\n\treturn render_template(\"shopping_list_display.html\",sum_prices=str(sum_prices), user=current_user)\n\n", "description": null, "category": "webscraping", "imports": ["from decimal import Decimal", "from flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app", "from flask_login import login_user, login_required, logout_user, current_user", "from .models import Note, Selected_item, User, Purchases, Ordered_item", "from . import db", "import json", "from werkzeug.security import generate_password_hash, check_password_hash", "import pandas as pd", "import os ", "from flask import send_from_directory"]}], [{"term": "class", "name": "classwebScrape:", "data": "class webScrape:\n\tdef __int__(self, url, text=None,lasttext = None):\n\t\tself.url = url\n\t\tself.text = text\n\t\tself.lasttext = lasttext\n\n\tdef scrape_to_text(self,page=None):\n\t\tif page is not None:\n\t\t\tres = requests.get((self.url).format(page))\n\t\telse:\n\t\t\tres = requests.get(self.url)\n\t\tsoup = bs4.BeautifulSoup(res.text,\"lxml\")\n\t\treturn soup\n\n\tdef get_list(self):\n\t\tsoup = webScrape.scrape_to_text()\n\t\tlists = soup.select(self.text)\n\t\ttemp_list = []\n\t\tfor temp in lists:\n\t\t\ttemp_list.append(temp.getText())\n\n\t\treturn temp_list\n\n\tdef scrape_all_pages(self):\n\t\twhile True:\n\t\t\tpage = 1\n\t\t\tsoup = webScrape.scrape_to_text(page)\n\t\t\ttemp = webScrape.get_list()\n\t\t\tif \"No quotes found!\" in soup.select(\".\"+self.lasttext)[1].getText():\n\t\t\t\tbreak\n\t\t\tpage = page + 1\n\n\t\t\treturn set(temp)\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "import bs4"]}], [{"term": "class", "name": "test_WebScrapeCharts", "data": "class test_WebScrapeCharts(unittest.TestCase):\n\n\n\tdef setup_webscrape(self):\n\t\tself.ws = ws.WebScrapeCharts()\n\t\tself.country_code = 'nl'\n\t\tself.recurrence = 'daily'\n\t\tself.filename = None\n\n\n\tdef check_file(self, filename):\n\t\ttry:\n\t\t\twith open(filename,'r') as rf:\n\t\t\t\tlines = rf.readlines()\n\t\t\t\tcounter = len(lines)\n\t\t\t\treturn counter == 200\n\t\texcept:\n\t\t\treturn False\n\n\tdef set_filename(self):\n\t\tself.filename = 'test_' + self.country_code + '_' + self.recurrence + '.txt'\n\n\n\t### TEST FUNCTIONS\n\n\tdef test_get_charts(self):\n\t\tself.setup_webscrape()\n\t\tself.assertTrue(self.ws.get_charts(self.country_code, self.recurrence))\n\t\tself.assertEqual(self.ws.url,'https://spotifycharts.com/regional/nl/daily/latest')\n\n\tdef test_get_charts_fail(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'jo'\n\t\tself.recurrence = 'daiy'\n\t\tself.assertFalse(self.ws.get_charts(self.country_code, self.recurrence))\n\n\t\n\tdef test_is_empty(self):\n\t\tself.setup_webscrape()\n\t\tself.assertTrue(self.ws.is_empty())\n\t\t# get the charts\n\t\tself.ws.get_charts(self.country_code, self.recurrence)\n\t\tself.assertFalse(self.ws.is_empty())\n\n\t### TEST SAVING -> done Manually as it creates files\n\tdef test_saving_for_testing(self):\n\t\tself.setup_webscrape()\n\t\t# USE DIFFERENT COUNTRY CODE AND RECURRENCE\n\t\tself.country_code = 'au'\n\t\tself.recurrence = 'weekly'\n\t\tself.set_filename()\n\n\t\t# Double check not that the file already exists and this test runs into an error\n\t\tif not self.check_file(self.filename):\n\t\t\tself.assertTrue(self.ws.save_for_testing(self.country_code, self.recurrence))\n\t\t\tself.assertTrue(self.check_file(self.filename))\n\n\tdef test_saving_for_testing_fail(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'ik'\n\t\tself.recurrence = 'day'\n\n\t\tself.assertFalse(self.ws.save_for_testing(self.country_code, self.recurrence))\n\n\n\n\tdef test_load_ids_from_file(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'de'\n\t\tself.recurrence = 'daily'\n\t\tself.set_filename()\n\n\t\tif self.check_file(self.filename):\n\t\t\t# file exists test the loading ids\n\t\t\t# test whether self.ws is empty\n\t\t\tself.assertTrue(self.ws.is_empty())\n\t\t\t# Test if ids are successfully loaded from file\n\t\t\tself.assertTrue(self.ws.load_ids_from_file(self.country_code, self.recurrence))\n\t\t\t# Test if self.ws is not empty\n\t\t\tself.assertFalse(self.ws.is_empty())\n\n\tdef test_load_ids_fail(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'jo'\n\t\tself.recurrence = 'dail'\n\t\tself.set_filename()\n\n\t\tself.assertFalse(self.ws.load_ids_from_file(self.country_code, self.recurrence))\n\n\tdef test_load_ids_is_full_fail(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'us'\n\t\tself.recurrence = 'daily'\n\t\tself.set_filename()\n\t\tself.ws.save_for_testing(self.country_code, self.recurrence)\n\n\t\tself.assertFalse(self.ws.load_ids_from_file(self.country_code, self.recurrence))\n\n\tdef test_set_default(self):\n\t\tself.setup_webscrape()\n\t\tself.assertFalse(self.ws.get_charts('jo', 'daily'))\n\t\tself.assertEqual(self.ws.url, 'https://spotifycharts.com/regional/jo/daily/latest')\n\n\t\tself.ws.set_default()\n\t\tself.assertEqual(self.ws.url, 'https://spotifycharts.com/regional/')\n\n", "description": null, "category": "webscraping", "imports": ["import Project.WebScrapeCharts as ws", "import unittest"]}], [{"term": "def", "name": "price_reduction", "data": "def price_reduction(investment, profit):\n\tratio = (investment/profit)*100\n\tif(ratio>=90):\n\t\treturn 7\n\telif(ratio<90 and ratio>=70):\n\t\treturn 5\n\telif(ratio<70 and ratio>=50):\n\t\treturn 4\n\telif(ratio<50 and ratio>=20):\n\t\treturn 2\n\telse:\n\t\treturn 1\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "calculate_price", "data": "def calculate_price(ratio, location, crop):\n\tif(location=='Bijapur'):\n\t\tbase = bij[crop]\n\t\tfinal = base - ((base*ratio)/100)\n\t\treturn final\n\telif(location=='Udupi'):\n\t\tbase = ud[crop]\n\t\tfinal = base - ((base*ratio)/100)\n\t\treturn final\n\telse:\n\t\tbase = bang[crop]\n\t\tfinal = base - ((base * ratio) / 100)\n\t\treturn final\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "bijapur", "data": "def bijapur():\n\tconn = sqlite3.connect('crop.db')\n\tc = conn.cursor()\n\tc.execute(\"SELECT id, crop, investment, profit FROM bijapur\")\n\tfor i in c.fetchall():\n\t\tindex = i[0]\n\t\tratio = price_reduction(i[2],i[3])\n\t\tfinal_price = calculate_price(ratio, 'Bijapur', i[1])\n\t\tc.execute(\"UPDATE bijapur SET price= ? WHERE id= ?\",(final_price,index))\n\tconn.commit()\n\tconn.close()\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "udupi", "data": "def udupi():\n\tconn = sqlite3.connect('crop.db')\n\tc = conn.cursor()\n\tc.execute(\"SELECT id, crop, investment, profit FROM udupi\")\n\tfor i in c.fetchall():\n\t\tindex = i[0]\n\t\tratio = price_reduction(i[2],i[3])\n\t\tfinal_price = calculate_price(ratio, 'Udupi', i[1])\n\t\tc.execute(\"UPDATE udupi SET price= ? WHERE id= ?\", (final_price, index))\n\tconn.commit()\n\tconn.close()\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "bangalore", "data": "def bangalore():\n\tconn = sqlite3.connect('crop.db')\n\tc = conn.cursor()\n\tc.execute(\"SELECT id, crop, investment, profit FROM bangalore\")\n\tfor i in c.fetchall():\n\t\tindex = i[0]\n\t\tratio = price_reduction(i[2],i[3])\n\t\tfinal_price = calculate_price(ratio, 'Bangalore rural', i[1])\n\t\tc.execute(\"UPDATE bangalore SET price= ? WHERE id= ?\", (final_price, index))\n\tconn.commit()\n\tconn.close()\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}], [{"term": "class", "name": "WebPageQuerySet", "data": "class WebPageQuerySet(QuerySet):\n\n\tdef webscrape_or_get(self, url):\n\t\tfrom ..models import WebPage\n\n\t\tresultSet = self.filter(url=url).order_by('-pk')[:1]\n\t\tanalysis = resultSet.first()\n\n\t\t# The given url has no cached data\n\t\tif(analysis == None):\n\t\t\t# perform new webscraping\n\t\t\tself.webscrape_and_save(url)\n\t\t\tresultSet = WebPage.objects.filter(url=url).order_by('-pk')[:1]\n\n\t\t# The given url has cached data. However it is outdated and has\n\t\t# not been automatically deleted yet by the regular jobs.\n\t\t# The replace() method is used to avoid problems caused by timezones\n\t\telif(datetime.now() > analysis.delete_on.replace(tzinfo=None)):\n\t\t\t# Therefore, delete it and perform new webscraping\n\t\t\tanalysis.delete()\n\t\t\t# perform new webscraping\n\t\t\tself.webscrape_and_save(url)\n\t\t\tresultSet = WebPage.filter(url=url).order_by('-pk')[:1]\n\n\n\t\treturn resultSet\n\n\tdef webscrape_and_save(self, url):\n\t\tfrom ..models import WebPage, Heading\n\t\tfrom .webscraping import analyze_page\n\n\t\t# scrapes the url and gets its data\n\t\tanalysis_data = analyze_page(url)\n\n\t\t# creates a new database entry for the url from the newly scraped data\n\t\tanalysis = WebPage()\n\t\tanalysis.url = url\n\t\tanalysis.html_version = analysis_data['html_version']\n\t\tanalysis.page_title = analysis_data['title']\n\n\t\tanalysis.internal_links = analysis_data['internal_links']\n\t\tanalysis.external_links = analysis_data['external_links']\n\t\tanalysis.inaccessible_links = analysis_data['inaccessible_links']\n\t\tanalysis.has_loginform = analysis_data['has_loginform']\n\t\tanalysis.save()\n\n\t\t# creates new database entries for the associated headings\n\t\tHEADING_TAGS = ['H1', 'H2', 'H3', 'H4','H5','H6']\n\n\t\tfor heading_count, tag in zip(analysis_data['headings'], HEADING_TAGS):\n\t\t\theading = Heading()\n\t\t\theading.type = tag\n\t\t\theading.count = heading_count\n\t\t\theading.webpage = analysis\n\t\t\theading.save()\n", "description": null, "category": "webscraping", "imports": ["from django.db.models.query import QuerySet", "from datetime import datetime", "\t\tfrom ..models import WebPage", "\t\tfrom ..models import WebPage, Heading", "\t\tfrom .webscraping import analyze_page"]}], [{"term": "def", "name": "resource_path", "data": "def resource_path(relative_path):\n\t\"\"\" Get absolute path to resource, works for dev and for PyInstaller \"\"\"\n\tbase_path = getattr(sys, '_MEIPASS', os.path.dirname(os.path.abspath(__file__)))\n\treturn os.path.join(base_path, relative_path)\n", "description": " Get absolute path to resource, works for dev and for PyInstaller ", "category": "webscraping", "imports": ["from PyQt5 import QtCore, QtGui, uic", "from PyQt5.QtWidgets import (", "import pandas as pd", "import random", "import os   ", "import sys", "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas", "from matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar", "from matplotlib import pyplot as plt, dates as mdates", "\t\t\timport json", "\t\t\t\tfrom pydrive.auth import GoogleAuth", "\t\t\t\tfrom pydrive.drive import GoogleDrive", "\t\t\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX", "\t\t\tfrom statsmodels.tsa.stattools import adfuller", "\t\t\timport numpy as np", "\timport ctypes"]}, {"term": "class", "name": "DYNARIMA", "data": "class DYNARIMA(QMainWindow):\n\tdef __init__(self):\n\t\tsuper(DYNARIMA, self).__init__()\n\t\tself.ui = uic.loadUi(resource_path('DESIGN.ui'), self) # load ui file\n\t\tself.prev_lags = self.new_lags = 1 # config_lags highlighter\n\t\tself.dataframe = self.trainset = self.testset = ''\n\t\t\n\t\t# ---- ASSOCIATE ELEMENTS TO VARIABLE ----\n\t\t# buttons\n\t\tself.btn_compile = self.findChild(QPushButton, 'btn_compile')\n\t\tself.btn_forecast = self.findChild(QPushButton, 'btn_forecast')\n\t\tself.btn_webscrape = self.findChild(QPushButton, 'btn_webscrape')\n\t\t\n\t\tself.btn_compile.clicked.connect(self.compile)\n\t\tself.btn_forecast.clicked.connect(self.forecast)\n\t\tself.btn_webscrape.clicked.connect(self.webscrape)\n\t\t\n\t\t# configs\n\t\tself.calendar = self.findChild(QCalendarWidget, 'calendar')\n\t\tself.calendar.selectionChanged.connect(self.calendar_)\n\t\tself.config_integrate = self.findChild(QSpinBox, 'config_integrate')\n\t\tself.config_lags = self.findChild(QSpinBox, 'config_lags')\n\t\tself.config_startdate = self.findChild(QDateEdit, 'config_startdate')\n\t\t\n\t\t# setup startdate\n\t\tself.selected_date = QtCore.QDate.currentDate()\n\t\tself.config_startdate.setDate(self.selected_date)\n\t\tself.calendar.setSelectedDate(self.selected_date)\n\t\tself.config_lags.valueChanged.connect(self.lags)\n\t\tself.config_startdate.dateChanged.connect(self.startdate)\n\t\tself.config_integrate.valueChanged.connect(self.integrate)\n\t\t\n\t\t# informations\n\t\tself.txt_accuracy = self.findChild(QLabel, 'info_txt_accuracy')\n\t\tself.txt_adf = self.findChild(QLabel, 'info_txt_adf')\n\t\tself.txt_aic = self.findChild(QLabel, 'info_txt_aic')\n\t\tself.txt_mae = self.findChild(QLabel, 'info_txt_mae')\n\t\tself.txt_model = self.findChild(QLabel, 'info_txt_model')\n\t\tself.txt_pvalue = self.findChild(QLabel, 'info_txt_pvalue')\n\t\t\n\t\t# matplot qwidgets\n\t\tself.matplot_container = self.findChild(QVBoxLayout, 'matplot_container')\n\t\t\t\t\n\t\t# progress bars\n\t\tself.progressbar = self.findChild(QProgressBar, 'progressbar')\n\t\tself.progressbar_text = self.findChild(QLabel, 'progressbar_text')\t   \n\t\tprint('-- LINKED UI AND LOGIC --')\n\t\t\n\t\t# ---- INITIALLY LOCK AND SET TEXT ----\n\t\t# index -1 initial (lock all except webscraper/update button)\n\t\t# index 0 webscrape \n\t\t# index 1 compile\n\t\t# index 2 forecast\n\t\tself.locker(-1)\n\t\tprint('-- LOCKED SEQUENCE --')\n\t\t\n\t\t\n\t\t# ---- CREATE PREREQUISITE FILES ----\n\t\t# if not found, create dataset and  folder with configurations \n\t\tself.toScrape = True\n\t\tif not os.path.exists(dir_dataset):\n\t\t\t\tos.makedirs(dir_dataset)\n\t\telif len(os.listdir(dir_dataset)) >= 4:\n\t\t\tself.btn_webscrape.setText('Compress')\n\t\t\tself.toScrape = False\n\t\tif not os.path.exists(dir_config):\n\t\t\tos.makedirs(dir_config)\n\t\tif len(os.listdir(dir_config)) < 2:\n\t\t\timport json\n\t\t\t# Google API OAuth2.0\n\t\t\tclient_secrets = json.dumps(\n\t\t\t\t{\"web\":{\n\t\t\t\t\t\"client_id\":\"626625711266-90bhqs8j4vj9cru2jre94cbqamn7e9j8.apps.googleusercontent.com\",\n\t\t\t\t\t\"project_id\":\"original-bot-295405\",\n\t\t\t\t\t\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\n\t\t\t\t\t\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"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\",\n\t\t\t\t\t\"redirect_uris\":[\"http://localhost:8080/\"],\n\t\t\t\t\t\"javascript_origins\":[\"http://localhost:8080\"]}\n\t\t\t\t})\n\t\t\t# Matplotlib Stylesheet\n\t\t\tmatplotstyle = \"\"\"\n\t\t\t\t# Seaborn common parameters\n\t\t\t\t# .15 = dark_gray\n\t\t\t\t# .8 = light_gray\n\t\t\t\tlegend.frameon: False\n\t\t\t\tlegend.numpoints: 1\n\t\t\t\tlegend.scatterpoints: 1\n\t\t\t\txtick.direction: out\n\t\t\t\tytick.direction: out\n\t\t\t\taxes.axisbelow: True\n\t\t\t\tfont.family: sans-serif\n\t\t\t\tgrid.linestyle: -\n\t\t\t\tlines.solid_capstyle: round\n\n\t\t\t\t# Seaborn darkgrid parameters\n\t\t\t\taxes.grid: True\n\t\t\t\taxes.edgecolor: white\n\t\t\t\taxes.linewidth: 0\n\t\t\t\txtick.major.size: 0\n\t\t\t\tytick.major.size: 0\n\t\t\t\txtick.minor.size: 0\n\t\t\t\tytick.minor.size: 0\n\n\t\t\t\t# from mplcyberpunk\n\t\t\t\ttext.color: 0.9\n\t\t\t\taxes.labelcolor: 0.9\n\t\t\t\txtick.color: 0.9\n\t\t\t\tytick.color: 0.9\n\t\t\t\tgrid.color: 2A3459\n\n\t\t\t\t# Custom\n\t\t\t\tfont.sans-serif: Overpass, Helvetica, Helvetica Neue, Arial, Liberation Sans, DejaVu Sans, Bitstream Vera Sans, sans-serif\n\t\t\t\taxes.prop_cycle: cycler('color', ['18c0c4', 'f62196', 'A267F5', 'f3907e', 'ffe46b', 'fefeff'])\n\t\t\t\timage.cmap: RdPu\n\t\t\t\tfigure.facecolor: 212946\n\t\t\t\taxes.facecolor: 212946\n\t\t\t\tsavefig.facecolor: 212946\n\t\t\t\t\"\"\"\n\t\t\t# write the files to the config\n\t\t\twith open(f\"{dir_config}client_secrets.json\", \"w\") as outfile:\n\t\t\t\toutfile.write(client_secrets)\n\t\t\twith open(f\"{dir_config}matplotlib-dark.mplstyle\", \"w\") as outfile:\n\t\t\t\toutfile.write(matplotstyle)\n\t\tplt.style.use(dir_config+'matplotlib-dark.mplstyle') # change the matplotlib theme\n\t\tprint('-- PREREQUISITES COMPLETE --')\t\n\t\t\n\tdef compile(self):\n\t\tif self.btn_compile.text() == 'Compile':\n\t\t\tself.progressbar_text.setText('Compiling...')\n\t\t\tself.progressbar.setValue(0)\n\t\t\tstart = self.config_startdate.date().toString(\"yyyy-MM-dd\") # starting date\n\t\t\tpredict = self.config_lags.value() # number of days to forecast\n\t\t\t\n\t\t\tprint('-- SPLITTING TRAIN AND TEST DATASETS --')\n\t\t\t# train date\n\t\t\ttrain_fr = pd.to_datetime('Jan 30, 2020').date() # DO NOT TOUCH\n\t\t\ttrain_to = (pd.to_datetime(start) + pd.DateOffset(days=-1)).date()\n\t\t\t# test date\n\t\t\ttest_fr  =  pd.to_datetime(start).date()\n\t\t\ttest_to  =  (test_fr + pd.DateOffset(days=predict-1)).date()\n\t\t\t# split dates of the dataset\n\t\t\tself.trainset = self.dataframe.loc[train_fr:train_to].rename(columns={'Cases':'Train'})\n\t\t\tself.testset  = self.dataframe.loc[test_fr:test_to].rename(columns={'Cases':'Test'})\n\t\t\tself.testset.Cases = ['Test']\n\t\t\t\n\t\t\tprint('--- TRAINING DATASET ---')\n\t\t\tprint(self.trainset)\n\t\t\tprint('--- TESTING DATASET ---')\n\t\t\tprint(self.testset)\n\t\t\t\n\t\t\ttitle = f'Train-Test Split [{self.trainset.index[0]} \u2014 {self.testset.index[-1]}]'\n\t\t\tself.clearLayout(self.matplot_container)\n\t\t\tdf_split = Plotter(title, self.trainset, self.testset)\n\t\t\tself.matplot_container.addWidget(df_split)\n\t\t\t\n\t\t\tself.progressbar_text.setText('Compilation Complete')\n\t\t\tself.progressbar.setValue(100)\n\t\t\tself.locker(1)\n\t\telif self.btn_compile.text() == 'Reset':\n\t\t\tself.progressbar.setValue(0)\n\t\t\tself.locker(0)\n\t\t\tself.progressbar_text.setText('Dataframe Reset')\n\t\n\tdef webscrape(self):\n\t\tself.locker() # lock eveything when this button is clicked\n\t\tif self.toScrape or self.btn_webscrape.text() == 'Update':\n\t\t\tself.thread = ThreadClass(parent=None, index=0, toScrape=True)\n\t\telse:\n\t\t\tself.thread = ThreadClass(parent=None, index=0, toScrape=False)\n\t\t# start and configure locker when this thread is done\n\t\tself.thread.progress_signal.connect(self.progress_worker)\n\t\tself.thread.output_signal.connect(self.output_worker)\n\t\tself.thread.start() \n\t\t\t \n\tdef forecast(self):\n\t\tself.locker()\n\t\tself.thread = ThreadClass(parent=None, index=2, dataframe=[self.trainset, self.testset, self.config_integrate.value()])\n\t\tself.thread.progress_signal.connect(self.progress_worker)\n\t\tself.thread.output_signal.connect(self.output_worker)\n\t\tself.thread.start() \n\t\t\n\tdef lags(self):\n\t\tformat, date, init = self.reset_highlight()\n\t\tself.new_lags = self.config_lags.value()\n\t\t# reset initial selected position\n\t\tself.calendar.setSelectedDate(date)\n\t\tself.update_highlight()\n\t\tself.prev_lags = self.new_lags\n\t\t\n\tdef startdate(self):\n\t\t# updates the calendar\n\t\tself.reset_highlight()\n\t\tself.calendar.setSelectedDate(self.config_startdate.date())\n\t\tself.selected_date = self.config_startdate.date()\n\t\tself.update_highlight()\n\t\t\n\tdef integrate(self):\n\t\tdiff = self.config_integrate.value()\n\t\tself.txt_model.setText(f\"Model: {(7, diff, 8)}\")\n\t\t\n\tdef calendar_(self):\n\t\t# updates the startdate config\n\t\tself.reset_highlight()\n\t\tself.config_startdate.setDate(self.calendar.selectedDate())\n\t\tself.selected_date = self.config_startdate.date()\n\t\tself.update_highlight()\n\t\n\tdef reset_highlight(self):\n\t\t# reset highlights\n\t\tformat = QtGui.QTextCharFormat()  \n\t\tdate = self.selected_date\n\t\tinit = self.calendar.palette().brush(QtGui.QPalette.Base)\n\t\tformat.setBackground(init)\n\t\tfor i in range(self.prev_lags):\n\t\t\tself.calendar.setDateTextFormat(date.addDays(i), format)\n\t\treturn format, date, init\n\t\n\tdef update_highlight(self):\n\t\t# change the temporary up-date to the number of lags  \n\t\tformat = QtGui.QTextCharFormat()  \n\t\tdate = self.selected_date\n\t\tactive = self.calendar.palette().brush(QtGui.QPalette.LinkVisited)\n\t\tformat.setBackground(active)\n\t\tfor i in range(self.new_lags):\n\t\t\tself.calendar.setDateTextFormat(date.addDays(i), format)\n\t\n\tdef locker(self, index=None):\n\t\t# lock everything\n\t\tself.btn_compile.setText('Compile')\n\t\tself.calendar.setDisabled(True)\n\t\tself.btn_compile.setDisabled(True)\n\t\tself.btn_forecast.setDisabled(True)\n\t\tself.btn_webscrape.setDisabled(True)\n\t\tself.config_integrate.setDisabled(True)\n\t\tself.config_lags.setDisabled(True)\n\t\tself.config_startdate.setDisabled(True)\n\t\t\n\t\tself.txt_adf.setText('ADF: 0.0')\n\t\tself.txt_pvalue.setText('P-Value: 0.0')\n\t\tself.txt_aic.setText('AIC: 0.0')\n\t\tself.txt_model.setText(f'Model: {(7, self.config_integrate.value(), 8)}' )\n\t\tself.txt_mae.setText('MAE: 0%')\n\t\tself.txt_accuracy.setText('Accuracy: 0%')\n\t\t\n\t\tif index==-1: # initial webscrape lock\n\t\t\tself.btn_webscrape.setText('Webscrape')\n\t\t\tself.btn_webscrape.setDisabled(False)\n\t\telif index==0 or index==2: # webscrape / forecast finished, what to unlock?\n\t\t\tself.btn_webscrape.setText('Update')\n\t\t\tself.calendar.setDisabled(False)\n\t\t\tself.btn_webscrape.setDisabled(False)\n\t\t\tself.btn_compile.setDisabled(False)\n\t\t\tself.config_lags.setDisabled(False)\n\t\t\tself.config_startdate.setDisabled(False)\n\t\telif index==1: # compile finished, what to unlock?\n\t\t\tself.btn_webscrape.setText('Update')\n\t\t\tself.btn_compile.setText('Reset')\n\t\t\tself.btn_webscrape.setDisabled(False)\n\t\t\tself.btn_compile.setDisabled(False)\n\t\t\tself.btn_forecast.setDisabled(False)\n\t\t\tself.config_integrate.setDisabled(False)\n\n\t# update the progressbar from thread emits\n\tdef progress_worker(self, counter, title, activate):\n\t\tindex = self.sender().index\n\t\tself.progressbar.setValue(counter)\n\t\tself.progressbar_text.setText(title)\n\t\tif activate: # activates locker based on index\n\t\t\tself.locker(index)\n\t\n\t# get the dataframe from the worker thread\n\tdef output_worker(self, data):\n\t\tindex = self.sender().index\n\t\tif index==0: # get output of webscrape\n\t\t\tself.dataframe = data[0]\n\t\t\tself.calendar.setMaximumDate(data[0].index[-1])\n\t\t\tself.config_startdate.setMaximumDate(data[0].index[-1])\n\t\t\t\n\t\t\ttitle = f'Webscraped Dataframe [{data[0].index[0]} \u2014 {data[0].index[-1]}]'\n\t\t\tdf_canvas = Plotter(title, data[0]) \n\t\t\tself.clearLayout(self.matplot_container)\n\t\t\tself.matplot_container.addWidget(df_canvas) # adding canvas to the layout\n\t\t\t\n\t\t\tprint('-- CANVAS PLOTTER CREATED --')\n\t\telif index==2: # get output of forecast\n\t\t\tadf, pvalue, aic, model, mae, accuracy, df_test, fit_model = data\t\n\t\t\tprint(adf, pvalue, aic, model, mae, accuracy)\n\n\t\t\tself.txt_adf.setText(f'ADF: {round(adf,8)}')\n\t\t\tself.txt_pvalue.setText(f'P-Value: {round(pvalue,8)}')\n\t\t\tself.txt_aic.setText(f'AIC: {round(aic,8)}')\n\t\t\tself.txt_model.setText(f'Model: {model}')\n\t\t\tself.txt_mae.setText(f'MAE: {round(mae,2)}%')\n\t\t\tself.txt_accuracy.setText(f'Accuracy: {round(accuracy,2)}%')\t\n\t\t\t\n\t\t\tplt.rc('font', size=6) # controls default text sizes\n\t\t\tplt.tight_layout()\n\n\t\t\tmodel_diagnostic = FigureCanvas(fit_model.plot_diagnostics(figsize=(30,10)))\n\t\t\tmodel_comparison = Plotter(f'Accuracy Analysis', df_test['Test'], df_test['Model'])\n\t\t\tself.clearLayout(self.matplot_container)\n\t\t\tself.matplot_container.addWidget(model_comparison) # adding canvas to the layout\n\t\t\tself.matplot_container.addWidget(model_diagnostic)   \n\t\t\t   \n\tdef clearLayout(self, layout):\n\t\tif layout is not None:\n\t\t\twhile layout.count():\n\t\t\t\tchild = layout.takeAt(0)\n\t\t\t\tif child.widget() is not None:\n\t\t\t\t\tchild.widget().deleteLater()\n\t\t\t\telif child.layout() is not None:\n\t\t\t\t\tself.clearLayout(child.layout())\n", "description": "\n\t\t\t\t# Seaborn common parameters\n\t\t\t\t# .15 = dark_gray\n\t\t\t\t# .8 = light_gray\n\t\t\t\tlegend.frameon: False\n\t\t\t\tlegend.numpoints: 1\n\t\t\t\tlegend.scatterpoints: 1\n\t\t\t\txtick.direction: out\n\t\t\t\tytick.direction: out\n\t\t\t\taxes.axisbelow: True\n\t\t\t\tfont.family: sans-serif\n\t\t\t\tgrid.linestyle: -\n\t\t\t\tlines.solid_capstyle: round\n\n\t\t\t\t# Seaborn darkgrid parameters\n\t\t\t\taxes.grid: True\n\t\t\t\taxes.edgecolor: white\n\t\t\t\taxes.linewidth: 0\n\t\t\t\txtick.major.size: 0\n\t\t\t\tytick.major.size: 0\n\t\t\t\txtick.minor.size: 0\n\t\t\t\tytick.minor.size: 0\n\n\t\t\t\t# from mplcyberpunk\n\t\t\t\ttext.color: 0.9\n\t\t\t\taxes.labelcolor: 0.9\n\t\t\t\txtick.color: 0.9\n\t\t\t\tytick.color: 0.9\n\t\t\t\tgrid.color: 2A3459\n\n\t\t\t\t# Custom\n\t\t\t\tfont.sans-serif: Overpass, Helvetica, Helvetica Neue, Arial, Liberation Sans, DejaVu Sans, Bitstream Vera Sans, sans-serif\n\t\t\t\taxes.prop_cycle: cycler('color', ['18c0c4', 'f62196', 'A267F5', 'f3907e', 'ffe46b', 'fefeff'])\n\t\t\t\timage.cmap: RdPu\n\t\t\t\tfigure.facecolor: 212946\n\t\t\t\taxes.facecolor: 212946\n\t\t\t\tsavefig.facecolor: 212946\n\t\t\t\t", "category": "webscraping", "imports": ["from PyQt5 import QtCore, QtGui, uic", "from PyQt5.QtWidgets import (", "import pandas as pd", "import random", "import os   ", "import sys", "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas", "from matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar", "from matplotlib import pyplot as plt, dates as mdates", "\t\t\timport json", "\t\t\t\tfrom pydrive.auth import GoogleAuth", "\t\t\t\tfrom pydrive.drive import GoogleDrive", "\t\t\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX", "\t\t\tfrom statsmodels.tsa.stattools import adfuller", "\t\t\timport numpy as np", "\timport ctypes"]}, {"term": "class", "name": "ThreadClass", "data": "class ThreadClass(QtCore.QThread): \n\tprogress_signal = QtCore.pyqtSignal(int, str, bool)\n\toutput_signal = QtCore.pyqtSignal(list)\n\tdef __init__(self, parent=None, index=0, toScrape=False, dataframe=[]):\n\t\tsuper(ThreadClass, self).__init__(parent)\n\t\tself.index = index\n\t\tself.is_running = True\n\t\tself.toScrape = toScrape\n\t\tself.dataframe = dataframe\n\t\t\n\tdef run(self):\n\t\tif self.index==0: # webscrape\n\t\t\tif self.toScrape: # if dataset is missing\n\t\t\t\t# --AUTHENTICATE--\n\t\t\t\tfrom pydrive.auth import GoogleAuth\n\t\t\t\tfrom pydrive.drive import GoogleDrive\n\t\t\t\t# reset progress bar\n\t\t\t\tself.progress_signal.emit(0,'Authenticate Google Drive', False)\n\t\t\t\t\n\t\t\t\tGoogleAuth.DEFAULT_SETTINGS['client_config_file'] = 'config/client_secrets.json'\n\t\t\t\tgauth = GoogleAuth()\n\t\t\t\t# Try to load saved client credentials\n\t\t\t\tgauth.LoadCredentialsFile(\"config/credentials.txt\")\n\t\t\t\tif gauth.credentials is None:\n\t\t\t\t\tgauth.LocalWebserverAuth() # Authenticate if they're not there\n\t\t\t\telif gauth.access_token_expired:\n\t\t\t\t\tgauth.Refresh() # Refresh them if expired\n\t\t\t\telse:\n\t\t\t\t\tgauth.Authorize() # Initialize the saved creds\n\t\t\t\t# Save the current credentials to a file\n\t\t\t\tgauth.SaveCredentialsFile(\"config/credentials.txt\")\n\t\t\t\t\n\t\t\t\t# -- WEBSCRAPE --\n\t\t\t\tdrive = GoogleDrive(gauth)\n\t\t\t\ttarget_folder = drive.ListFile( # Target DOH folder ID\n\t\t\t\t\t{'q': \"'1_PhyL7788CLgZ717TklQ_iuMxvvnrrNn' in parents and trashed=false\"}).GetList()\n\t\t\t\twebthreads = []\n\t\t\t\tprogress = 0\n\t\t\t\tfor idx, dataset in enumerate(target_folder):\n\t\t\t\t\t# find datasets and put list its information\n\t\t\t\t\tid = dataset['id'] # file ID \n\t\t\t\t\tquery = 'case information' # look for this\n\t\t\t\t\ttitle = dataset['title'].lower() # rename\n\t\t\t\t\tif (query in title):\n\t\t\t\t\t\tprint(f'FOUND.. {title} : {idx}')\n\t\t\t\t\t\twebthreads.append([title[52:], id, drive])\n\t\t\t\t\t# update progress bar\n\t\t\t\t\tprogress = progress + int((1/len(target_folder))*100)\n\t\t\t\t\tself.progress_signal.emit(progress, f'Retrieving Information... {title}', False)\t \n\t\t\t\t# -- RETRIEVE --\n\t\t\t\tprogress = 0\n\t\t\t\tfor thread in webthreads:\n\t\t\t\t\ttitle, id, drive = thread\n\t\t\t\t\tprint(f'dumping {thread} for {title}')\n\t\t\t\t\t# dump file in the console\n\t\t\t\t\tdownloaded = drive.CreateFile({'id': id}) \n\t\t\t\t\t# splice filename then download file\n\t\t\t\t\tdownloaded.GetContentFile(f'{dir_dataset+title}') \n\t\t\t\t\tprint(f'downloaded {title}')\n\t\t\t\t\t# update progress bar\n\t\t\t\t\tprogress = progress + int((1/len(webthreads))*100)\n\t\t\t\t\tself.progress_signal.emit(progress, f'Downloaded... {title}', False) \n\t\t\t\tself.progress_signal.emit(0, 'Download Compelete', False) \n\t\t\t# -- CONCATENATE --\n\t\t\t# emit a worker signal to the mainthread to reset progressbar\n\t\t\tself.progress_signal.emit(0,'Compiling...',False)\n\t\t\t# concatenate every scraped dataset to df\n\t\t\tfolder = os.listdir(dir_dataset) # create folder object \n\t\t\t# add store and read dataset information to list\n\t\t\tbatch = []\n\t\t\tprogress = 0\n\t\t\tfor csv in folder:\n\t\t\t\tif '.csv' in csv:\n\t\t\t\t\t# update progress bar\n\t\t\t\t\tprogress = progress + int((1/len(folder))*100)\n\t\t\t\t\tself.progress_signal.emit(progress, f'Reading {csv}', False)\n\t\t\t\t\t# append csv dump to batch list to concatenate\t\t\t\t\n\t\t\t\t\tbatch.append(pd.read_csv(dir_dataset+csv, usecols=['DateRepConf'], parse_dates=['DateRepConf']))\n\t\t\t# concatenate csv\n\t\t\tdf = pd.concat(batch, ignore_index=True)\n\t\t\t# check null values\n\t\t\tdf.isnull().values.any()\n\t\t\tprint(df,'\\n')\n\t\t\t# rename date-confirmed column\n\t\t\tdf.rename(columns={'DateRepConf':'Dates'}, inplace=True) \n\t\t\t# count distinct-repeating dates\n\t\t\tdf = df.groupby(df.Dates.dt.date).agg('count').rename(columns={'Dates':'Cases'})\n\t\t\tprint(df,'\\n')\n\t\t\tself.progress_signal.emit(100, f'Compression Successful  [{df.index[0]} \u2014 {df.index[-1]}]', True)\n\t\t\tself.output_signal.emit([df])\n\t\t\t\n\t\telif self.index == 2: # forecast\n\t\t\tself.progress_signal.emit(0,'Initializing Forecast', False)\n\t\t\t# Statistic Library: SARIMAX Model\n\t\t\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\t\t\t\n\t\t\tprint(self.dataframe)\n\t\t\tin_train = self.dataframe[0].reset_index()\n\t\t\tin_test = self.dataframe[1].reset_index()\n\t\t\tintegration = self.dataframe[2]\n\t\t\tpredict = len(in_test)\n\t\t\t\n\t\t\t# check for stationarity (adf and pvalue)\n\t\t\tprint('-- EVALUATING STATIONARITY --')\n\t\t\tprogress = 0\n\t\t\tcheck_stationary = 0\n\t\t\tfor i in range(integration):\n\t\t\t\t# update progress bar\n\t\t\t\tprogress = progress + int((1/integration)*100) \n\t\t\t\tcheck_stationary = in_train['Train'].diff().dropna()\n\t\t\t\tself.progress_signal.emit(progress, 'Evaluating Stationarity...', False)  \n\t\t\t# Statistic Library: Augmented Dickey Fuller Function\n\t\t\tfrom statsmodels.tsa.stattools import adfuller\n\t\t\tresult_stationary = adfuller(check_stationary)\n\t\t\tadf = result_stationary[0]\n\t\t\tpvalue = result_stationary[1]\n\t\t\tprint(f'ADF Statistic: {adf}')\n\t\t\tprint(f'p-value: {pvalue}')\n\n\t\t\t# get aic scores\n\t\t\tmin_params = (7,integration,8)\n\t\t\t# generate the sarimax instruction\n\t\t\tprint('-- BUILDING ARIMA OBJECT --')\n\t\t\tself.progress_signal.emit(25, f'Building Object ARIMA{min_params}...', False)\n\t\t\tarima = SARIMAX(in_train['Train'], order=min_params, simple_differencing=False)\n\t\t\t# fitting the model\n\t\t\tprint('-- FITTING THE MODEL --')\n\t\t\tself.progress_signal.emit(50, f'Fitting The Model... {arima}', False)\n\t\t\tmodel = arima.fit(disp=False)\n\t\t\taic = model.aic\n\t\t\tprint(model.summary())\n\t\t\t# forecast the model\n\t\t\tprint('-- GENERATING MODEL FORECAST --')\n\t\t\tself.progress_signal.emit(75, f'Forecasting the Model... {model}', False)\n\t\t\t# retrieve forecast values\n\t\t\tpredicted_values = model.get_prediction(end=model.nobs + predict).predicted_mean.round()\n\t\t\t# create new model column to existing dataframes\n\t\t\tout_train = in_train.assign(Model=predicted_values[:-predict-1])\n\t\t\tout_test = in_test.assign(Model=predicted_values[-predict-1:].reset_index(drop=True))\n\t\t\t# prints the error and accuracy in percent\t\t\t\n\t\t\tprint('-- SUMMARY --')\n\t\t\timport numpy as np\n\t\t\terr = np.subtract(out_test.Test, out_test.Model)\n\t\t\tabs_err = np.abs(err)\n\t\t\ttotal_cases = np.sum(out_test.Test)\n\t\t\ttotal_abs_err = np.sum(abs_err)\n\t\t\t\n\t\t\tmae = (total_abs_err/total_cases)*100\n\t\t\taccuracy = 100-(total_abs_err/total_cases)*100\n\t\t\tprint('MAE: {:0.2f}%'.format(mae))\n\t\t\tprint('Forecast Accuracy: {:0.2f}%'.format(accuracy))\n\t\t\tself.progress_signal.emit(100, f'Forecast Successful...', True)\n\t\t\tself.output_signal.emit([adf, pvalue, aic, min_params, mae, accuracy, out_test, model])\n\t\n\tdef stop(self):\n\t\tself.is_running = False\n\t\tprint('Stopping thread...', self.index)\n", "description": null, "category": "webscraping", "imports": ["from PyQt5 import QtCore, QtGui, uic", "from PyQt5.QtWidgets import (", "import pandas as pd", "import random", "import os   ", "import sys", "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas", "from matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar", "from matplotlib import pyplot as plt, dates as mdates", "\t\t\timport json", "\t\t\t\tfrom pydrive.auth import GoogleAuth", "\t\t\t\tfrom pydrive.drive import GoogleDrive", "\t\t\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX", "\t\t\tfrom statsmodels.tsa.stattools import adfuller", "\t\t\timport numpy as np", "\timport ctypes"]}, {"term": "class", "name": "Plotter", "data": "class Plotter(FigureCanvas):\n\t# # -- HOW THIS CLASS WORKS --\n\t# # a figure instance to plot on\n\t# self.figure = plt.figure()\n\t# # this is the Canvas Widget that\n\t# # displays the 'figure'it takes the\n\t# # 'figure' instance as a parameter to __init__\n\t# self.canvas = FigureCanvas(self.figure)\n\t# # adding canvas to the layout\n\t# self.matplot_container.addWidget(self.canvas)\n\t\n\tdef __init__(self, title, *dataframe, parent=None):\n\t\tsuper(Plotter, self).__init__(parent) \n\t\tself.figure = None\n\t\tplt.close(self.figure)\n\t\tself.title = title\n\t\t\n\t\t# Creating your plot\n\t\tfig, ax = plt.subplots(figsize=(30, 10))\n\t\t\n\t\t# Plot the train and test sets on the same axis ax\n\t\tfor obj in dataframe:\n\t\t\tobj.plot(ax=ax)\n\t\t\t\n\t\tfmt_month = mdates.MonthLocator() # Minor ticks every month.\n\t\tfmt_year = mdates.YearLocator() # Minor ticks every year.\n\t\tax.xaxis.set_minor_locator(fmt_month)\n\t\tax.xaxis.set_minor_formatter(mdates.DateFormatter('%b')) # '%b' to get the names of the month\n\t\tax.xaxis.set_major_locator(fmt_year)\n\t\tax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n\t\t# fontsize for month labels\n\t\tax.tick_params(labelsize=6, which='both')\n\t\t# create a second x-axis beneath the first x-axis to show the year in YYYY format\n\t\tsec_xaxis = ax.secondary_xaxis(-0.1)\n\t\tsec_xaxis.xaxis.set_major_locator(fmt_year)\n\t\tsec_xaxis.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n\t\t# Hide the second x-axis spines and ticks\n\t\tsec_xaxis.spines['bottom'].set_visible(False)\n\t\tsec_xaxis.tick_params(length=0, labelsize=6)\n\n\t\tplt.title(title, fontsize=7)\n\t\tplt.grid(which = 'both', linewidth=0.3)\n\t\tplt.xlabel('')\n\t\tplt.legend(loc='upper left') \n\t\t\n\t\tself.figure = fig\n\t\t\n\t# def mouseDoubleClickEvent(self, event):\n\t#\t print('matplot detached')\n\t#\t plt.show()\n\t\t\n", "description": null, "category": "webscraping", "imports": ["from PyQt5 import QtCore, QtGui, uic", "from PyQt5.QtWidgets import (", "import pandas as pd", "import random", "import os   ", "import sys", "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas", "from matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar", "from matplotlib import pyplot as plt, dates as mdates", "\t\t\timport json", "\t\t\t\tfrom pydrive.auth import GoogleAuth", "\t\t\t\tfrom pydrive.drive import GoogleDrive", "\t\t\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX", "\t\t\tfrom statsmodels.tsa.stattools import adfuller", "\t\t\timport numpy as np", "\timport ctypes"]}], [], [], [{"term": "def", "name": "check_file", "data": "def check_file(filename):\n\tprint(f'\\nchecking file:\\t{filename}')\n\n\ttry:\n\t\twith open(filename,'r') as rf:\n\t\t\tlines = rf.readlines()\n\t\t\tcounter = len(lines)\n\t\t\tprint(f'contains data:\\t{counter == 200}')\n\t\t\treturn counter == 200\n\texcept:\n\t\tprint(f'*** No such File exists -> Creating File {filename} ***')\n\t\treturn False\n", "description": null, "category": "webscraping", "imports": ["# import Project.UserInterface as ui", "# import Project.WebScrapeCharts as wsc", "# import Project.AudioFeatures as af", "# import Project.DistanceSongs as ds", "# import Project.MinSpanTree as mst", "# import Project.Visualize as vis", "# import Project.KMeans as km", "import UserInterface as ui", "import WebScrapeCharts as wsc", "import AudioFeatures as af", "import DistanceSongs as ds", "import MinSpanTree as mst", "import Visualize as vis", "import Project.KMeans as km", "import time"]}, {"term": "def", "name": "get_ids", "data": "def get_ids(cc,rc):\n\tfilename = f'test_{cc}_{rc}.txt'\n\n\tif check_file(filename):\n\t\t# Ids exist in file\n\t\tprint(f'load from file:\\t{filename}\\n')\n\t\twebscrape.load_ids_from_file(cc, rc)\n\telse:\n\t\t# ids don't exist yet, save them\n\t\tprint('no ids in file, need to webscrape for ids')\n\t\tprint(f'saving ids for future testing!\\n')\n\t\twebscrape.save_for_testing(cc, rc)\n\n\treturn webscrape.song_ids_array\n", "description": null, "category": "webscraping", "imports": ["# import Project.UserInterface as ui", "# import Project.WebScrapeCharts as wsc", "# import Project.AudioFeatures as af", "# import Project.DistanceSongs as ds", "# import Project.MinSpanTree as mst", "# import Project.Visualize as vis", "# import Project.KMeans as km", "import UserInterface as ui", "import WebScrapeCharts as wsc", "import AudioFeatures as af", "import DistanceSongs as ds", "import MinSpanTree as mst", "import Visualize as vis", "import Project.KMeans as km", "import time"]}, {"term": "def", "name": "get_features", "data": "def get_features(cc,rc, song_ids):\n\tfilename_feat = f'test_{cc}_{rc}_feat.txt'\n\t# check if features exist\n\tif check_file(filename_feat):\n\t\t# features exist, load them\n\t\tprint(f'load from file:\\t{filename_feat}\\n')\n\t\taudio.load_features(filename_feat)\n\telse:\n\t\t# Features doesnt exist, save them\n\t\tprint('no features in file, need to access spotify for features')\n\t\tprint(f'saving features for future testing!\\n')\n\t\taudio.save_features(filename_feat, song_ids)\n\treturn audio.features\n", "description": null, "category": "webscraping", "imports": ["# import Project.UserInterface as ui", "# import Project.WebScrapeCharts as wsc", "# import Project.AudioFeatures as af", "# import Project.DistanceSongs as ds", "# import Project.MinSpanTree as mst", "# import Project.Visualize as vis", "# import Project.KMeans as km", "import UserInterface as ui", "import WebScrapeCharts as wsc", "import AudioFeatures as af", "import DistanceSongs as ds", "import MinSpanTree as mst", "import Visualize as vis", "import Project.KMeans as km", "import time"]}], [{"term": "class", "name": "classStockTask:", "data": "class StockTask:\n\tdef __init__(self):\n\t\tself.stock_list = []\n\t\tself.get_stock_list()\n\t\tself.sentiment_class = SentimentClass()\n\n\tdef get_stock_list(self):\n\t\twith open(\"data/spy_list.csv\", \"r\") as csv_file:\n\t\t\tcsv_reader = csv.DictReader(csv_file)\n\t\t\tfor line in csv_reader:\n\t\t\t\tself.stock_list.append(line['Symbol'])\n\t\tcsv_file.close()\n\n\tdef start(self):\n\t\t\"\"\"\n\t\tStart the process of webscraping through all the stocks in the stock_list list. Then push the results through\n\t\tto machine learning, and write everything to CSV files.\n\t\t\"\"\"\n\t\tfor stock in self.stock_list:\n\t\t\t# Get the latest tweets\n\t\t\ttweets = webscrape(stock)\n\t\t\tfor i in range(len(tweets)):\n\t\t\t\ttweets[i] = clean_string(tweets[i])\n\t\t\tself.write_to_csv(stock, tweets, \"_latest\")\n\n\t\t\t# Get the top tweets\n\t\t\ttweets = webscrape(stock, top_results=True)\n\t\t\tfor i in range(len(tweets)):\n\t\t\t\ttweets[i] = clean_string(tweets[i])\n\t\t\tself.write_to_csv(stock, tweets, \"_top\")\n\t\t\tprint(f\"Wrote ${stock} to top & latest\")\n\t\t\ttime.sleep(15)\n\n\tdef write_to_csv(self, ticker: str, tweets, extra_title=\"\"):\n\t\twith open(f\"data/stock_name_csv/{ticker.upper()}{extra_title}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n\t\t\tcsv_writer = csv.writer(csv_file)\n\t\t\tfor tweet in tweets:\n\t\t\t\tsentiment = self.sentiment_class.get_sentiment(tweet)\n\t\t\t\tif len(sentiment.labels) > 0:  # If the Sentence model is empty/has no prediction, it will not be added\n\t\t\t\t\tcsv_writer.writerow([tweet, sentiment.labels[0].value, sentiment.labels[0].score])\n\t\tcsv_file.close()\n\n", "description": "\n\t\tStart the process of webscraping through all the stocks in the stock_list list. Then push the results through\n\t\tto machine learning, and write everything to CSV files.\n\t\t", "category": "webscraping", "imports": ["import csv", "from webscraper import webscrape", "from machine_learning import clean_string, SentimentClass", "import time"]}], [], [{"term": "class", "name": "Scepedata", "data": "class Scepedata():\n\tdef __init__(self,num):\n\t\tself.payscale = f\"https://www.payscale.com/college-salary-report/majors-that-pay-you-back/bachelors/page/{num}\"\n\t\tself.responde = requests.get(self.payscale, headers={\n\t\t\t\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"})\n\t\tself.respo_txt = self.responde.text\n\t\tself.webscrape = BeautifulSoup(self.respo_txt, \"html.parser\")\n\t\tself.major_name = self.webscrape.find_all(class_=\"csr-col--school-name\")\n\t\tself.total_page_num = self.webscrape.find_all(class_=\"pagination__btn--inner\")\n\t\tself.data_name = self.webscrape.find_all(class_='data-table__title')\n\t\tself.data_value = self.webscrape.find_all(class_=\"data-table__value\")\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import pandas as pd"]}], [{"term": "class", "name": "classwebscrape:", "data": "class webscrape:\n\tdef __init__(self,link):\n\t\tself.link=link\n\tdef word_filter(self):\n\t\tall_news=[]\n\t\tfor i in range(len(self.link)):\n\t\t\tsource=requests.get(news_href[i]).text\n\t\t\tsoup=BeautifulSoup(source,'lxml')\n\t\t\tall_link = soup.find_all('a')\n\t\t\tfor link in all_link:\n\t\t\t\tnew_text=re.sub(\"\\s+\" , \" \",link.text)\n\t\t\t\tif len(new_text) >= 50 and len(new_text) <= 150:\n\t\t\t\t\tif re.search('[a-zA-Z]',new_text) is not None:\n\n\t\t\t\t\t\tall_news.append(new_text)\n\t\treturn all_news \n\t\n\t@staticmethod\n\tdef text_filter(arr):\n\t\tstemmed_news=[]\t\n\t\tfor i in range(len(arr)):\n\t\t\tmod_text=re.sub('[^a-zA-Z]',' ',arr[i])\n\t\t\tmod_text=mod_text.lower()\n\t\t\tmod_text=mod_text.split(' ')\n\t\t\tmod_text=filter(None, mod_text)\n\t\t\tnews=[ps.lemmatize(sentence) for sentence in mod_text if sentence not in (stopwords.words('english'))]\n\t\t\tnews=','.join(news)\n\t\t\tnews=news.replace(\",\",\" \")\n\t\t\tstemmed_news.append(news)\n\t\treturn stemmed_news\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import pandas as pd", "import re", "from nltk.stem import PorterStemmer", "from nltk.corpus import stopwords", "import matplotlib.pyplot as plt", "from nltk.stem import WordNetLemmatizer", "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer", "from sklearn.naive_bayes import MultinomialNB"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape():\n\tglobal options\n\toptions = Options() \n\t \n\n\tdef webdriver_setup():\n\t\ttime.sleep(1)\n\t\n\t\toptions.add_argument('--no-sandbox')\n\t\toptions.add_argument('--disable-extensions')\n\t\toptions.add_argument('--disable-dev-shm-usage')\n\t\toptions.add_argument('--ignore-certificate-errors')\n\t\toptions.add_argument('--enable-logging')\n\t\toptions.add_experimental_option('detach', True)\n\t \n\tservice = Service(ChromeDriverManager().install())\n\tglobal driver\n\tdriver  = webdriver.Chrome(service=service, options=options)\n\turl = \"http://crashinformationky.org/AdvancedSearch\"   \n\t\n\tdriver.get(url)\n\tdriver.maximize_window()\n\ttime.sleep(1)\n", "description": null, "category": "webscraping", "imports": ["from zipfile import ZipFile", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support import expected_conditions as ec", "from selenium.webdriver.support.wait import WebDriverWait", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.keys import Keys", "import time", "from selenium.webdriver.common.action_chains import ActionChains", "import glob", "import pandas as pd", "import os", "import zipfile", "import webbrowser", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "fdate_input", "data": "\tdef date_input():\n\t\tWebDriverWait(driver, 30).until(ec.visibility_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[3]/a\"))).click()\n\t\ttime.sleep(1)\n\t\tWebDriverWait(driver, 30).until(ec.visibility_of_element_located((By.XPATH, \"/html/body/div[10]/div/div[3]\"))).click()\n\t\ttime.sleep(1)\n\t\tWebDriverWait(driver, 30).until(ec.visibility_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[3]/a\"))).click()\n\t\ttime.sleep(1)\n\t\tfield = driver.find_element(By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[3]/input\")\n\t\tfield.send_keys(Keys.CONTROL + \"a\")\n\t\tfield.send_keys(Keys.DELETE)\n\t\tfield.send_keys('01/01/2013')\n\t\tfield.send_keys(Keys.ENTER)\n\t\ttime.sleep(1)\n\t\trange = driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[2]/a')\n\t\trange.click()\n\t\tbetween = driver.find_element(By.XPATH, '/html/body/div[11]/div/div[9]')\n\t\tbetween.click()\n\t\ttime.sleep(1)\n\t\tWebDriverWait(driver, 30).until(ec.visibility_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[4]/a\"))).click()\n\t\ttime.sleep(1)\n\t\tfield = driver.find_element(By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[4]/input\")\n\t\tfield.send_keys(Keys.CONTROL + \"a\")\n\t\tfield.send_keys(Keys.DELETE)\n\t\tfield.send_keys('12/31/2021')\n\t\tfield.send_keys(Keys.ENTER)\n\t\ttime.sleep(1)\n", "description": null, "category": "webscraping", "imports": ["from zipfile import ZipFile", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support import expected_conditions as ec", "from selenium.webdriver.support.wait import WebDriverWait", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.keys import Keys", "import time", "from selenium.webdriver.common.action_chains import ActionChains", "import glob", "import pandas as pd", "import os", "import zipfile", "import webbrowser", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "fadd_county", "data": "\tdef add_county():\n\t\tadd_property= driver.find_element(By.XPATH, '//*[@id=\"QueryPanel\"]/div[3]/a')\n\t\tadd_property.click()\n\t\tcounty_name = driver.find_element(By.XPATH, '//*[@id=\"QueryPanel-EntitiesMenu\"]/div/div[4]')\n\t\tcounty_name.click()\n\t\ttime.sleep(1)\n\t\tcounty_name_select = driver.find_element(By.PARTIAL_LINK_TEXT, 'select')\n\t\tcounty_name_select.click()\n\t\ttime.sleep(1)\n\t\tcounty_input = driver.find_element(By.ID, 'searchBox')\n\t\tcounty_input.send_keys(county)\n\t\ttime.sleep(1)\n\t\tcounty_final = driver.find_element(By.XPATH, '//*[@id=\"QueryPanel-cond-2-EditorMenu\"]/div[2]/div')\n\t\ttime.sleep(1)\n\t\tcounty_final.click()\n", "description": null, "category": "webscraping", "imports": ["from zipfile import ZipFile", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support import expected_conditions as ec", "from selenium.webdriver.support.wait import WebDriverWait", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.keys import Keys", "import time", "from selenium.webdriver.common.action_chains import ActionChains", "import glob", "import pandas as pd", "import os", "import zipfile", "import webbrowser", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "froadway", "data": "\tdef roadway():\n\t\tadd_property= driver.find_element(By.XPATH, '//*[@id=\"QueryPanel\"]/div[3]/a')\n\t\tadd_property.click()\n\t\ttime.sleep(1)\n\t\troadway_property = driver.find_element(By.XPATH, '//*[@id=\"QueryPanel-EntitiesMenu\"]/div/div[10]')\n\t\troadway_property.click()\n\t\ttime.sleep(1)\n\t\tstarts_with = driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div[3]/div[2]/a')\n\t\tstarts_with.click()\n\t\ttime.sleep(1)\n\t\tcontains =  driver.find_element(By.XPATH, '/html/body/div[11]/div/div[2]')\n\t\tcontains.click()\n\t\ttime.sleep(1)\n\t\tenter_value = driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div[3]/div[3]/a')\n\t\tenter_value.click()\n\t\ttime.sleep(1)\n\t\tactions = ActionChains(driver)\n\t\tactions.send_keys(road)\n\t\tactions.send_keys(Keys.ENTER)\n\t\tactions.perform()\n\t\ttime.sleep(1)\n", "description": null, "category": "webscraping", "imports": ["from zipfile import ZipFile", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support import expected_conditions as ec", "from selenium.webdriver.support.wait import WebDriverWait", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.keys import Keys", "import time", "from selenium.webdriver.common.action_chains import ActionChains", "import glob", "import pandas as pd", "import os", "import zipfile", "import webbrowser", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "get_file", "data": "def get_file():\n\texecute= driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[1]/div[2]/div[2]')\n\texecute.click()\n\ttime.sleep(35)\n", "description": null, "category": "webscraping", "imports": ["from zipfile import ZipFile", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support import expected_conditions as ec", "from selenium.webdriver.support.wait import WebDriverWait", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.keys import Keys", "import time", "from selenium.webdriver.common.action_chains import ActionChains", "import glob", "import pandas as pd", "import os", "import zipfile", "import webbrowser", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "df_convert", "data": "def df_convert():\n\tglobal df\n\tdf= pd.read_csv('d:projects/Incident.txt')\n\tos.remove('d:projects/Incident.txt')\n", "description": null, "category": "webscraping", "imports": ["from zipfile import ZipFile", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support import expected_conditions as ec", "from selenium.webdriver.support.wait import WebDriverWait", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.keys import Keys", "import time", "from selenium.webdriver.common.action_chains import ActionChains", "import glob", "import pandas as pd", "import os", "import zipfile", "import webbrowser", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "display", "data": "def display():\n\twith open('str.html', 'w') as f:\n\t\tdf.to_html(f)\n \n\tfilename = 'str.html'\n\twebbrowser.open_new_tab(filename)\n\tplt.figure\n\tdf.plot.line()\n\tplt.show()\n", "description": null, "category": "webscraping", "imports": ["from zipfile import ZipFile", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support import expected_conditions as ec", "from selenium.webdriver.support.wait import WebDriverWait", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.keys import Keys", "import time", "from selenium.webdriver.common.action_chains import ActionChains", "import glob", "import pandas as pd", "import os", "import zipfile", "import webbrowser", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "main", "data": "def main():\n\ttime.sleep(1)\n\tprint('This program will show crash data for specific roadways in specific counties in Kentucky.\\n')\n\tglobal county \n\tcounty = input('Please select a county from Kentucky\\n')\n\tglobal road\n\troad = input('Please provide a valid road number in the county selected above. (ex: shelbyville road would be just 60)\\n')\n   \n\twebscrape = WebScrape\n\twebscrape.webdriver_setup()\n\twebscrape.date_input()\n\twebscrape.add_county()\n\twebscrape.roadway()\n\tget_file()\n\tdf_convert()\n\tdisplay()\n\t\n", "description": null, "category": "webscraping", "imports": ["from zipfile import ZipFile", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support import expected_conditions as ec", "from selenium.webdriver.support.wait import WebDriverWait", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.keys import Keys", "import time", "from selenium.webdriver.common.action_chains import ActionChains", "import glob", "import pandas as pd", "import os", "import zipfile", "import webbrowser", "import matplotlib.pyplot as plt"]}], [{"term": "def", "name": "sms", "data": "def sms():\n\ttime.sleep(300)\n\tsend_sms()\n", "description": null, "category": "webscraping", "imports": ["from flask_script import Manager", "from app import app", "from app import send_sms, webscrape", "import time"]}, {"term": "def", "name": "scrape", "data": "def scrape():\n\ttime.sleep(240)\n\twebscrape()\n", "description": null, "category": "webscraping", "imports": ["from flask_script import Manager", "from app import app", "from app import send_sms, webscrape", "import time"]}, {"term": "def", "name": "both", "data": "def both():\n\ttime.sleep(240)\n\twebscrape()\n\ttime.sleep(60)\n\tsend_sms()\n", "description": null, "category": "webscraping", "imports": ["from flask_script import Manager", "from app import app", "from app import send_sms, webscrape", "import time"]}], [], [], [{"term": "def", "name": "makeBar", "data": "def makeBar(position, data, x_labels, y_title, fill_color, line_bf):\n\tax = fig.add_subplot(position)\n\tax.bar(x_axis, data, width, color=fill_color, align='center')\n\tlabels = plt.xticks(x_axis, x_labels, rotation='vertical', ha='center')\n\tplt.grid(True)\n\tax.set_ylabel(y_title, fontweight='bold')\n\tplt.xlim([0,len(x_axis)+1])\n\n\n\tif line_bf == 1:\n\n\t\t# best fit of data\n\t\t(mu, sigma) = norm.fit(data)\n\n\t\t# the histogram of the data\n\t\t#bins = hist(datos, 60, normed=1, facecolor='green', alpha=0.75)\n\n\t\t# add a 'best fit' line\n\t\tdeg = round(max(x_axis)/3)\n\n\t\tif deg % 2 == 0:\n\t\t\tdeg += 1\n\n\t\tcoefficients = np.polyfit(x_axis, data, deg)\n\t\tpolynomial = np.poly1d(coefficients)\n\t\txs = np.linspace(min(data), max(data), 1000)\n\t\tys = polynomial(xs)\n\n\t\tplt.plot(xs,ys)\n\n\taxes = plt.gca()\n\taxes.set_ylim(min(data),max(data))\n", "description": null, "category": "webscraping", "imports": ["import numpy as np", "import csv", "import os", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "createFig", "data": "def createFig(table_name):\n\tglobal fig\n\tglobal x_axis\n\tglobal width\n\t#open csv\n\n\t# get list of csv files\n\t# iterate over list of files\n\n\t# read the csv\n\tres = csv.reader(open(table_path + table_name), delimiter = ',')\n\n\t# ang = anglers, ws = wild steelhead, h = hatchery steelhed, rel = released\n\tdate, num_ang, hrsPerWS, wsCaught, hrsPerHS, hsCaught, hrs_fished = [], [], [], [], [], [], []\n\n\t# populate lists from csv\n\tfor col in res:\n\t\tdate.append(col[1])\n\t\tnum_ang.append(int(col[2]))\n\t\thrsPerWS.append(float(col[3]))\n\t\twsCaught.append(int(col[4]))\n\t\thrsPerHS.append(float(col[5]))\n\t\thsCaught.append(int(col[6]))\n\t\thrs_fished.append(float(col[11]))\n\n\t\triver_name = col[0]\n\n\t#\n\t# bar plot creation\n\t#\n\n\twidth = 0.8\n\tx_axis = range(1,len(date)+1)\n\tbar_color = '#455A64'  # slate\n\tempty_list = ['']*len(date)\n\n\t# set size of the figure area\n\n\tfig = plt.figure(figsize=(15, 10))\n\ttitle = river_name + ' 2014/2015'\n\ttitle.upper()\n\tfig.suptitle(title.upper(), fontsize=16, fontweight='bold')\n\t# position, attributes, title\n\n\t# six plots\n\tnum_ang_ax = makeBar(322,num_ang, empty_list,'Number of Anglers',bar_color,0)\n\thrs_fished_ax = makeBar(321,hrs_fished,empty_list,'Hours Fished',bar_color,0)\n\thrsPerWS_ax = makeBar(323,hrsPerWS,empty_list,'Hours Per \\n Wild Steelhead','#FF5252',0)\n\tws_caught_ax = makeBar(324,wsCaught,empty_list,'Wild Steelhead Caught','#FF5252',0)\n\thrs_per_ws_ax = makeBar(325,hrsPerHS,date,'Hours Per \\n Hatchery Steelhead','#8bc34a',0)\n\ths_caught_ax = makeBar(326,hsCaught,date,'Hatchery Steelhead Caught','#8bc34a',0)\n\n\t# gives the x-axis labels enough room\n\tfig.tight_layout()\n\tplt.subplots_adjust(top=.92)\n\t#plt.savefig(fig_path + 'test.png', bbox_inches=\"tight\")\n\t#plt.show()\n\n\t# save the figure\n\tplt.savefig(fig_path + table_name[:3], bbox_inches='tight')\n", "description": null, "category": "webscraping", "imports": ["import numpy as np", "import csv", "import os", "import matplotlib.pyplot as plt"]}], [], [{"term": "class", "name": "classWeb:", "data": "class Web:\n\tdef WebScrape(self):\n\t\t''' This function prompts the user to analyse any website of their choice '''\n\n\t\twhile True:\n\t\t\task_to_scrape = input('Would you like to scrape a webiste (y/n) ')\n\t\t\tif ask_to_scrape == 'n':\n\t\t\t\tprint('Thanks for analyzing! come back again')\n\t\t\t\tbreak\n\t\t\t\t\n\t\t\telif ask_to_scrape == 'y':\n\t\t\t\turl= CheckUrl.ValidUrl()\n\t\t\t\tif url:\n\t\t\t\t\trequest_url = GetRequest().request(url)\n\t\t\t\t\tif request_url:\n\t\t\t\t\t\tpull = GetWords().pull_data(request_url)\n\t\t\t\t\t\tprint (f'The top word is: {pull[0][0]}')\n\t\t\t\t\t\tbar = BarChart().plot_chart(pull)\n\t\t\t\t\t\tprint(bar)\n\t\t\t\telse:\n\t\t\t\t\tprint('The website url is not valid ')\n\t\t\t\t\n\t\t\telse:\n\t\t\t\tprint (\" 'y' means yes, 'n' means no \")\n", "description": null, "category": "webscraping", "imports": ["from analyse import CheckUrl", "from get import GetRequest", "from top_words import GetWords", "from barchart import BarChart"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape():\n\n\tdef __init__(self):\n\t\tpass\n\n\tdef read_parsons():\n\n\t\t\"\"\"\n\t\tReads in Parson's code written by compute.py to use as variable\n\t\t\"\"\"\n\n\t\t#So that variable can be accessed elsewhere in the file\n\t\tglobal parsons_code \n\n\t\twith open(\"parson.txt\", \"r\") as parsons_file:\n\t\t\tparsons_code = parsons_file.read()\n\n\tdef scrape_musipedia():\n\n\t\t\"\"\"\n\t\t- Uses Selenium & ChromeDriver to extract information about top 3 most likely songs\n\t\t- Requires scraping html\n\t\t- Relies on Musipedia not being updated\n\t\t\"\"\"\n\n\t\t#Path details\n\t\tpath_of_driver = \"/Users/adityatatwawadi/Downloads/chromedriver\"\n\t\tdriver = webdriver.Chrome(executable_path = path_of_driver)\n\n\t\t#Access musipedia's website\n\t\twebsite_url = \"https://www.musipedia.org/melodic_contour.html\"\n\t\tdriver.get(website_url)\n\t\t#print(driver.title)\n\n\t\t#Enters in the Parson's code to search engine\n\t\tdriver.find_element(By.NAME, 'tx_mpsearch_pi1[pc]').send_keys(parsons_code)\n\n\t\t#Submits & accesses database\n\t\tclick_button = driver.find_element(By.NAME, 'tx_mpsearch_pi1[submit_button]')\n\t\tclick_button.click()\n\n\t\t#Scrolls down to top 3 identified songs\n\t\tdriver.execute_script(\"window.scrollBy(0,400)\")\n\n\t\t#Reframes & takes a screenshot of top3 recommendtaions\n\t\tS = lambda X: driver.execute_script('return document.body.parentNode.scroll' +X)\n\t\tdriver.set_window_size(S('Width'), S('Height'))\n\t\tdriver.find_element_by_tag_name('body').screenshot(\"musipedia_recommendations.png\")\n\n\t\t#Exits automated chrome\n\t\tdriver.quit()\n", "description": "\n\t\tReads in Parson's code written by compute.py to use as variable\n\t\t", "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.common.by import By"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape():\n\n\tdef __init__(self):\n\t\tpass\n\n\tdef read_parsons():\n\n\t\t\"\"\"\n\t\tReads in Parson's code written by compute.py to use as variable\n\t\t\"\"\"\n\n\t\t#So that variable can be accessed elsewhere in the file\n\t\tglobal parsons_code \n\n\t\twith open(\"parson.txt\", \"r\") as parsons_file:\n\t\t\tparsons_code = parsons_file.read()\n\n\tdef scrape_musipedia():\n\n\t\t\"\"\"\n\t\t- Uses Selenium & ChromeDriver to extract information about top 3 most likely songs\n\t\t- Requires scraping html\n\t\t- Relies on Musipedia not being updated\n\t\t\"\"\"\n\n\t\t#Path details\n\t\tpath_of_driver = \"/Users/adityatatwawadi/Downloads/chromedriver\"\n\t\tdriver = webdriver.Chrome(executable_path = path_of_driver)\n\n\t\t#Access musipedia's website\n\t\twebsite_url = \"https://www.musipedia.org/melodic_contour.html\"\n\t\tdriver.get(website_url)\n\t\t#print(driver.title)\n\n\t\t#Enters in the Parson's code to search engine\n\t\tdriver.find_element(By.NAME, 'tx_mpsearch_pi1[pc]').send_keys(parsons_code)\n\n\t\t#Submits & accesses database\n\t\tclick_button = driver.find_element(By.NAME, 'tx_mpsearch_pi1[submit_button]')\n\t\tclick_button.click()\n\n\t\t#Scrolls down to top 3 identified songs\n\t\tdriver.execute_script(\"window.scrollBy(0,400)\")\n\n\t\t#Reframes & takes a screenshot of top3 recommendtaions\n\t\tS = lambda X: driver.execute_script('return document.body.parentNode.scroll' +X)\n\t\tdriver.set_window_size(S('Width'), S('Height'))\n\t\tdriver.find_element_by_tag_name('body').screenshot(\"musipedia_recommendations.png\")\n\n\t\t#Exits automated chrome\n\t\tdriver.quit()\n", "description": "\n\t\tReads in Parson's code written by compute.py to use as variable\n\t\t", "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.common.by import By"]}], [{"term": "def", "name": "ncdefon_message", "data": "async def on_message(message):\n\t# we do not want the bot to reply to itself\n\tif message.author == client.user:\n\t\treturn\n\n\tif message.content.startswith('!hello'):\n\t\tmsg = 'Hello {0.author.mention}'.format(message)\n\t\t# await message.channel.send(msg)\n\t\ttestFile = File('options.csv')\n\t\tawait message.channel.send(file = testFile)\n\n\telif message.content.startswith('!recordanewmixtape'):\n\t\tmsg = 'No {0.author.mention}, you do it'.format(message)\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!tickers'):\n\t\tcmd = message.content\n\t\tcmd = cmd[9:]\n\t\tprint('tickers command ticker: ' + cmd)\n\t\ttry:\n\t\t\ttickers = WebScrape.printTickers(cmd)\n\t\t\tmsg = ''\n\t\t\tfor tick in tickers:\n\t\t\t\tmsg = msg + tick + '\\n'\n\t\texcept:\n\t\t\tmsg = 'Not a valid option\\nLook at !listOptions'\n\t\t\t\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!setOption'):\n\t\tcmd = message.content\n\t\tcmdParts = cmd.split()\n\t\tWebScrape.setOption(cmdParts[1], cmdParts[2])\n\t\tmsg = '{0.author.mention} new option added '.format(message) + cmdParts[2]\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!removeOption'):\n\t\tcmd = message.content\n\t\tcmdParts = cmd.split()\n\t\tWebScrape.removeOption(cmdParts[1])\n\t\tmsg = '{0.author.mention} option was removed '.format(message) + cmdParts[1]\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!listOptions'):\n\t\toptions = WebScrape.getOptions()\n\t\tmsg = ''\n\t\tif options == 'There are no current options setup use !setOptions to set a new option':\n\t\t\tmsg = options\n\t\telse:\n\t\t\tfor opt in options:\n\t\t\t\tmsg = msg + opt + '\\n'\n\t\tawait message.channel.send(msg)\n\n\telif message.content.startswith('!stock'):\n\t\tcmd = message.content\n\t\tcmdParts = cmd.split()\n\t\tGenerateGraph.makeGraph(cmdParts[1])\n\t\tgraphFile = File('stock_information.html')\n\t\tawait message.channel.send(file = graphFile)\n\n\n\telif message.content.startswith('!help'):\n\t\tmsg = 'Remember to seperate each part of the command with a space\\n'\n\t\tmsg = msg + '!setOption  \\n'\n\t\tmsg = msg + '!removeOption \\n'\n\t\tmsg = msg + '!listOptions\\n'\n\t\tmsg = msg + '!tickers \\n'\n\t\tmsg = msg + '!stock \\n'\n\t\tawait message.channel.send(msg)\n", "description": null, "category": "webscraping", "imports": ["import discord", "import json", "import WebScrape", "import GenerateGraph", "from discord import File"]}, {"term": "def", "name": "ncdefon_ready", "data": "async def on_ready():\n\tprint('Logged in as')\n\tprint(client.user.name)\n\tprint(client.user.id)\n\tprint('------')\n", "description": null, "category": "webscraping", "imports": ["import discord", "import json", "import WebScrape", "import GenerateGraph", "from discord import File"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01'\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n\t\t\tcolumn_names=df.columns.to_list(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n\t\t\tdestination_table=\"yankee_candle_reviews_agita\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01'\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n\t\t\tcolumn_names=df.columns.to_list(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n\t\t\tdestination_table=\"yankee_candle_reviews_agita\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01'# <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n\t\t\tcolumn_names=df.columns.tolist(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n\t\t\tdestination_table=\"yankee_candle_reviews_titi\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01'# <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n\t\t\tcolumn_names=df.columns.tolist(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n\t\t\tdestination_table=\"yankee_candle_reviews_titi\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01' # <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n\t\t\tcolumn_names=df.columns.to_list(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n\t\t\tdestination_table=\"yankee_candle_reviews_dimira2\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01' # <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n\t\t\tcolumn_names=df.columns.to_list(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n\t\t\tdestination_table=\"yankee_candle_reviews_dimira2\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01'\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\t# Essentially, we are removing the last list element from date_result (if date_result list is longer than rev_result).\n\t\t# This has some degree of error in it, but is chosen as an approach for simplicity and illustrative purposes.\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a cloud Trino database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values,\n\t\t\tcolumn_names=df.columns.to_list(),\n\t\t\tdestination_table=\"yankee_candle_reviews\"\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in table yankee_candle_reviews.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01'\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\t# Essentially, we are removing the last list element from date_result (if date_result list is longer than rev_result).\n\t\t# This has some degree of error in it, but is chosen as an approach for simplicity and illustrative purposes.\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a cloud Trino database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values,\n\t\t\tcolumn_names=df.columns.to_list(),\n\t\t\tdestination_table=\"yankee_candle_reviews\"\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in table yankee_candle_reviews.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01'\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\t# Essentially, we are removing the last list element from date_result (if date_result list is longer than rev_result).\n\t\t# This has some degree of error in it, but is chosen as an approach for simplicity and illustrative purposes.\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a cloud Trino database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values,\n\t\t\tcolumn_names=df.columns.to_list(),\n\t\t\tdestination_table=\"yankee_candle_reviews\"\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in table yankee_candle_reviews.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops['last_date_amazon'] = '2020-01-01'\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values,\n\t\t\tcolumn_names=df.columns.to_list(),\n\t\t\tdestination_table= \"yankee_candle_reviews_avramov\"\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops['last_date_amazon'] = '2020-01-01'\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values,\n\t\t\tcolumn_names=df.columns.to_list(),\n\t\t\tdestination_table= \"yankee_candle_reviews_avramov\"\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(url): #takes wiki url and returns set of all the links\n\tsource = requests.get(url).text\n\tsoup = BeautifulSoup(source,'html.parser')\n\t# print(soup.prettify())\n\n\tlinks = soup.find_all('a',href = True) #filter html for href links\n\t# print(links)\n\tfinal_links=[]\n\tfor link in links: #extract links with regex\n\t\tlinkregex = re.compile(r'href=\"(\\w|/)+\"')\n\t\tfinal_link = linkregex.search(str(link))\n\t\tif final_link is not None:\n\t\t\t# print(final_link.group())\n\t\t\tfinal_links.append(f'{WIKI_PREFIX}{final_link.group()[6:-1]}')\n\tfinal_links = list(set(final_links)) #rid duplicates\n\tfinal_links.remove(f'{WIKI_PREFIX}/wiki/Main_Page') #rid unnecessary link\n\t# print(final_links,len(final_links))\n\treturn final_links\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import re", "import json"]}], [{"term": "def", "name": "welcome", "data": "def welcome():\n\treturn render_template(\"index.html\")\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "hydro", "data": "def hydro():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"hydro.html\")\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "wind", "data": "def wind():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"wind.html\")\n\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "heatmap", "data": "def heatmap():\n\t\n\treturn render_template(\"heatmap.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "solar", "data": "def solar():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"solar.html\")\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "location", "data": "def location():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"location.html\")\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "webscrape_sunburst", "data": "def webscrape_sunburst():\n\tdata =  json.load(open(\"my_renewables.json\",\"r\")) \n\treturn render_template(\"webscrape.html\",r_last_refresh=data[\"last_scrape\"],renewable_title_0=data[\"articles \"][0],renewable_link_0=data[\"links\"][0],renewable_title_1=data[\"articles \"][1],renewable_link_1=data[\"links\"][2], renewable_title_2 = data[\"articles \"][2],renewable_link_2=data[\"links\"][4],renewable_title_3=data[\"articles \"][3],renewable_link_3=data[\"links\"][6])\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "scrape", "data": "def scrape():\n\trenewable_scrape.renewable_scrape()\n\treturn redirect(\"/webscrape\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "heatmapgeojson", "data": "def heatmapgeojson():\n\treturn jsonify(data = heatmapdata)\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "data", "data": "def data():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"data.html\")\n\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}], [{"term": "class", "name": "GeneralCommands", "data": "class GeneralCommands(commands.Cog):\r\n\tdef __init__(self,client):\r\n\t\tself.client = client\r\n\t\t\r\n\t@commands.command(name = 'conq', aliases = ['conqueror']) #add rune images + explanations later\r\n\tasync def displayConq(self,ctx):\r\n\t\tfile = discord.File('images/conq.png', filename = 'conq.png')\r\n\t\tawait ctx.send(file=file)\r\n\t\tawait ctx.send('\\'Standard\\' runes are unboxed. Runes with yellow boxes are alternative choices.')\r\n\t\t\r\n\t@commands.command(name = 'elec', aliases = ['electrocute'])\r\n\tasync def displayElec(self,ctx):\r\n\t\tfile = discord.File('images/elec.png', filename = 'elec.png')\r\n\t\tawait ctx.send(file=file)\r\n\t\tawait ctx.send('\\'Standard\\' runes are unboxed. Runes with yellow boxes are alternative choices.')\r\n\t\t\r\n\t@commands.command(name = 'lethality', aliases = ['leth','assassin'])\r\n\tasync def displayLethality(self,ctx):\r\n\t\tembed1 = discord.Embed(\r\n\t\t\ttitle = 'Lethality items',\r\n\t\t\tdescription = 'A pool of lethality items you should buy on lethality Talon. Note that items from this list and the bruiser list can both be in the same build and that there is no definitive order to build them.',\r\n\t\t\tcolor = discord.Color.blue()\r\n\t\t)\r\n\t\tembed1.add_field(name = '\u2605Prowler\u2019s Claw', \r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "efcheck", "data": "\t\tdef check(reaction, user):\r\n\t\t\treturn user == ctx.author and str(reaction.emoji) in {'1\ufe0f\u20e3', '2\ufe0f\u20e3', '3\ufe0f\u20e3', '\\U0001F5D1'} and reaction.message.id == message.id\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "yncdefdisplayBruiser", "data": "\tasync def displayBruiser(self,ctx):\r\n\t\tembed1 = discord.Embed(\r\n\t\t\ttitle = 'Bruiser items',\r\n\t\t\tdescription = 'A pool of bruiser mythic items you should buy on bruiser Talon. Note that items from this list and the bruiser list can both be in the same build and that there is no definitive order to build them.',\r\n\t\t\tcolor = discord.Color.blue()\r\n\t\t)\r\n\t\tembed1.add_field(name = '\u2605Goredrinker', \r\n\t\t\t\t\t\t value = 'The go-to mythic for bruiser Talon. It gives you CDR, damage, HP, and waveclear. ALWAYS build ironspike whip first for the waveclear. This gives all the essential stats for bruiser Talon.', \r\n\t\t\t\t\t\t inline = False)\r\n\t\tembed1.add_field(name = '\u2605Prowler\u2019s Claw', \r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "efcheck", "data": "\t\tdef check(reaction, user):\r\n\t\t\treturn user == ctx.author and str(reaction.emoji) in {'1\ufe0f\u20e3', '2\ufe0f\u20e3', '3\ufe0f\u20e3', '\\U0001F5D1'} and reaction.message.id == message.id\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "yncdefdisplayCombos", "data": "\tasync def displayCombos(self,ctx):\r\n\t\tawait ctx.send('')\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "yncdefdisplayAbility", "data": "\tasync def displayAbility(self,ctx):\r\n\t\tawait ctx.send('With Elec and Conq, you should always be going W max into Q max. 3 in W and then Q max is also viable for Conq.')\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "yncdefdisplayStart", "data": "\tasync def displayStart(self,ctx):\r\n\t\tembed = discord.Embed(\r\n\t\t\t\ttitle = 'Starting items',\r\n\t\t\t\tcolor = discord.Color.blue()\r\n\t\t)\r\n\t\tembed.add_field(name = 'Longsword + refillable',\r\n\t\t\t\t\t\tvalue = 'The gold standard. This works against every matchup.',\r\n\t\t\t\t\t\tinline = False)\r\n\t\tembed.add_field(name = 'Doran\\'s Blade + red pot',\r\n\t\t\t\t\t\tvalue = 'This is very good start for bruiser Talon. It is good against melee matchups as well as poke matchups. Less damage than LS but more sustain.',\r\n\t\t\t\t\t\tinline = False)\r\n\t\tembed.add_field(name = 'Doran\\'s Shield + red pot',\r\n\t\t\t\t\t\tvalue = 'Start this if you expect the poke to be very heavy. Examples being ADC matchups like Quinn or Lucian.',\r\n\t\t\t\t\t\tinline = False)\r\n\t\tembed.add_field(name = 'Corrupting pot',\r\n\t\t\t\t\t\tvalue = 'Viable, but less so than the above mentioned. Generally you would start this if you expect to use a lot of mana and to be poked a lot.',\r\n\t\t\t\t\t\tinline = False)\r\n\t\tawait ctx.send(embed = embed)\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "yncdefquote", "data": "\tasync def quote(self,ctx):\r\n\t\tquoteList = ['Live and die by the blade.','Pathetic!','There\\'s nowhere to hide.','Let\\'s finish this quickly.',\r\n\t\t\t\t\t 'Don\\'t cross me.','On the razor\\'s edge.','Your allegiances mean nothing to me.','I never compromise.',\r\n\t\t\t\t\t 'Only fools pledge life to honor.','They won\\'t survive.']\r\n\t\tawait ctx.send(random.choice(quoteList))\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "yncdefdisplayBoots", "data": "\tasync def displayBoots(self,ctx):\r\n\t\tembed = discord.Embed(\r\n\t\t\t\ttitle = 'Boots',\r\n\t\t\t\tcolor = discord.Color.blue()\r\n\t\t)\r\n\t\tembed.add_field(name = 'Plated Steelcaps',\r\n\t\t\t\t\t\tvalue = 'Provides armor. Build this if you\\'re against an AD comp or need more armor against AD champs.',\r\n\t\t\t\t\t\tinline = False)\r\n\t\tembed.add_field(name = 'Mercury\u2019s Treads',\r\n\t\t\t\t\t\tvalue = 'Provides tenacity and MR. Build this if you\\'re against burst mages and need the extra surviability or if you\\'re against CC comps.',\r\n\t\t\t\t\t\tinline = False)\r\n\t\tembed.add_field(name = 'Ionian Boots of Lucidity',\r\n\t\t\t\t\t\tvalue = 'Almost always built on lethality because they don\u2019t require resistances. You build Ionian Boots on bruiser if neither Steelcaps or Treads are applicable in the current match.',\r\n\t\t\t\t\t\tinline = False)\r\n\t\tawait ctx.send(embed = embed)\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "yncdefstatsv2", "data": "\tasync def statsv2(self,ctx,*,args = ''):\r\n\t\tif not args:\r\n\t\t\twebScrape = WebScraper('https://u.gg/lol/champions/talon/build')\r\n\t\t\tawait ctx.send(webScrape.parse())\r\n\t\t\treturn\r\n\t\targs = args.lower()\r\n\t\targsList = args.split()\r\n\t\t\r\n\t\trankSet = {'plat','platinum','dia','diamond'}\r\n\t\tregionSet = {'kr','na','eu','euw','br','eune','eun','jp','lan','las',\r\n\t\t\t\t\t\t\t\t   'oce','ru','tr'}\r\n\t\troleSet = {'top','jg','jungle','mid','middle','bot','adc','supp','support'}\r\n\t\t\r\n\t\t'''\r\n\t\tPermutations: 3 options to pick: Rank, region, role; Order matters\r\n\t\tIf 1 argument -> 3 pick 1 = 3 possibilities \r\n\t\tIf 2 arguments -> 3 pick 2 = 6 possibilities\r\n\t\tIf 3 arguments -> 3 pick 3 = 6 possibilities\r\n\t\t'''\r\n\t\t\r\n\t\tif len(argsList) == 1: #1 argument\r\n\t\t\tif argsList[0] in rankSet: #only argument is a rank, then we return the plat+ or diamond+ of the site\r\n\t\t\t\tif argsList[0] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper('https://u.gg/lol/champions/talon/build')\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper('https://u.gg/lol/champions/talon/build?rank=diamond_plus','Diamond+')\r\n\t\t\telif argsList[0] in regionSet: #only argument is a region, then we return that region for plat+\r\n\t\t\t\tregion = regionUrl(argsList[0])\r\n\t\t\t\twebScrape = WebScraper(region,'',argsList[0])\r\n\t\t\telif argsList[0] in roleSet: #only argument is a role, then we return that role for plat+\r\n\t\t\t\trole = roleUrl(argsList[0])\r\n\t\t\t\twebScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role,'','',argsList[0])\r\n\t\t\telse:\r\n\t\t\t\treturn\r\n\t\t\t\t\r\n\t\telif len(argsList) == 2: #2 arguments\r\n\t\t\t\r\n\t\t\tif argsList[0] in rankSet and argsList[1] in regionSet:\r\n\t\t\t\tregion = regionUrl(argsList[1])\r\n\t\t\t\tif argsList[0] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper(region,'',argsList[1])\r\n\t\t\t\telif argsList[0] in {'dia','diamond'}:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&rank=diamond_plus','Diamond+',argsList[1])\r\n\t\r\n\t\t\telif argsList[0] in rankSet and argsList[1] in roleSet: #@Talon Bot stats diamond top\r\n\t\t\t\trole = roleUrl(argsList[1])\r\n\t\t\t\tif argsList[0] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role,'','',argsList[1])\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role + '&rank=diamond_plus','Diamond+','',argsList[1])\r\n\t\t\t\t\t\r\n\t\t\telif argsList[0] in regionSet and argsList[1] in rankSet:\r\n\t\t\t\tregion = regionUrl(argsList[0])\r\n\t\t\t\tif argsList[1] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper(region,'',argsList[0])\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&rank=diamond_plus','Diamond+',argsList[0])\r\n\t\t\t\t\t\r\n\t\t\telif argsList[0] in regionSet and argsList[1] in roleSet: #@Talon Bot stats na top\r\n\t\t\t\tregion = regionUrl(argsList[0])\r\n\t\t\t\trole = roleUrl(argsList[1])\r\n\t\t\t\twebScrape = WebScraper(region + '&' + role,'',argsList[0],argsList[1])\r\n\t\t\t\t\r\n\t\t\telif argsList[0] in roleSet and argsList[1] in regionSet:\r\n\t\t\t\tregion = regionUrl(argsList[1])\r\n\t\t\t\trole = roleUrl(argsList[0])\r\n\t\t\t\twebScrape = WebScraper(region + '&' + role,'',argsList[1],argsList[0])\r\n\t\t\t\t\r\n\t\t\telif argsList[0] in roleSet and argsList[1] in rankSet:\r\n\t\t\t\trole = roleUrl(argsList[0])\r\n\t\t\t\tif argsList[1] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role,'','',argsList[0])\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role + '&rank=diamond_plus','Diamond+','',argsList[0])\r\n\t\t\t\r\n\t\t\telse:\r\n\t\t\t\treturn\r\n\t\t\t\t\t\r\n\t\telif len(argsList) == 3: #3 arguments\r\n\t\t\tif argsList[0] in rankSet and argsList[1] in regionSet and argsList[2] in roleSet:\r\n\t\t\t\trole = roleUrl(argsList[2])\r\n\t\t\t\tregion = regionUrl(argsList[1])\r\n\t\t\t\tif argsList[0] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role,'',argsList[1],argsList[2])\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[1],argsList[2])\r\n\t\t\t\t\t\r\n\t\t\telif argsList[0] in rankSet and argsList[1] in roleSet and argsList[2] in regionSet:\r\n\t\t\t\trole = roleUrl(argsList[1])\r\n\t\t\t\tregion = regionUrl(argsList[2])\r\n\t\t\t\tif argsList[0] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role,'',argsList[2],argsList[1])\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[2],argsList[1])\r\n\t\t\t\t\t\r\n\t\t\telif argsList[0] in regionSet and argsList[1] in rankSet and argsList[2] in roleSet:\r\n\t\t\t\trole = roleUrl(argsList[2])\r\n\t\t\t\tregion = regionUrl(argsList[0])\r\n\t\t\t\tif argsList[1] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role,'',argsList[0],argsList[2])\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[0],argsList[2])\r\n\t\t\t\t\t\r\n\t\t\telif argsList[0] in regionSet and argsList[1] in roleSet and argsList[2] in rankSet:\r\n\t\t\t\trole = roleUrl(argsList[1])\r\n\t\t\t\tregion = regionUrl(argsList[0])\r\n\t\t\t\tif argsList[2] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role,'',argsList[0],argsList[1])\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[0],argsList[1])\r\n\t\t\t\t\t\r\n\t\t\telif argsList[0] in roleSet and argsList[1] in rankSet and argsList[2] in regionSet:\r\n\t\t\t\trole = roleUrl(argsList[0])\r\n\t\t\t\tregion = regionUrl(argsList[2])\r\n\t\t\t\tif argsList[1] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role,'',argsList[2],argsList[0])\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[2],argsList[0])\r\n\t\t\t\t\t\r\n\t\t\telif argsList[0] in roleSet and argsList[1] in regionSet and argsList[2] in rankSet:\r\n\t\t\t\trole = roleUrl(argsList[0])\r\n\t\t\t\tregion = regionUrl(argsList[1])\r\n\t\t\t\tif argsList[2] in {'plat','platinum'}:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role,'',argsList[1],argsList[0])\r\n\t\t\t\telse:\r\n\t\t\t\t\twebScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[1],argsList[0])\r\n\t\t\t\r\n\t\t\telse: #argument not in a set\r\n\t\t\t\treturn\t\r\n\t\t\t\t\r\n\t\telse: #too many arguments\r\n\t\t\treturn\r\n\t\tawait ctx.send(webScrape.parse())\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "regionUrl", "data": "def regionUrl(args):\r\n\tif args == 'kr':\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=kr'\r\n\telif args == 'na':\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=na1'\r\n\telif args in {'eu','euw'}:\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=euw1'\r\n\telif args == 'br':\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=br1'\r\n\telif args in {'eune','eun'}:\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=eun1'\r\n\telif args == 'jp':\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=jp1'\r\n\telif args == 'lan':\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=la1'\r\n\telif args == 'las':\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=la2'\r\n\telif args == 'oce':\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=oc1'\r\n\telif args == 'ru':\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=ru'\r\n\telif args == 'tr':\r\n\t\treturn 'https://u.gg/lol/champions/talon/build?region=tr1'\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "roleUrl", "data": "def roleUrl(args):\r\n\tif args == 'top':\r\n\t\treturn 'role=top'\r\n\telif args in {'jg','jungle'}:\r\n\t\treturn 'role=jungle'\r\n\telif args in {'mid','middle'}:\r\n\t\treturn 'role=middle'\r\n\telif args in {'adc','bot'}:\r\n\t\treturn 'role=adc'\r\n\telif args in {'supp','support'}:\r\n\t\treturn 'role=support'\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}, {"term": "def", "name": "setup", "data": "def setup(client):\r\n", "description": null, "category": "webscraping", "imports": ["import discord\r", "from discord.ext import commands\r", "from discord.ext.commands.cooldowns import BucketType\r", "from .webscraper import WebScraper\r", "import nest_asyncio\r", "import asyncio\r", "import random\r"]}], [], [{"term": "class", "name": "DownloadWeb", "data": "class DownloadWeb(WebScrape.WebScrape):\n\t# def __init__(self, root = \"C:/xampp/htdocs/webscrape\", folder = \"\", srfolder = \"\" ):\n\t# self.root = root\n\t# self.folder = folder\n\t# self.srcFolder = srcFolder\n\n\tpass\n\n", "description": null, "category": "webscraping", "imports": ["from edtools import *", "from edtools.Scrape import WebScrape", "import sys"]}, {"term": "class", "name": "BrowseWeb", "data": "class BrowseWeb(WebScrape.browseWeb):\n\t# def __init__(self, root = \"C:/xampp/htdocs/webscrape\", folder = \"\", srfolder = \"\" ):\n\t# self.root = root\n\t# self.folder = folder\n\t# self.srcFolder = srcFolder\n\n\tpass\n\n", "description": null, "category": "webscraping", "imports": ["from edtools import *", "from edtools.Scrape import WebScrape", "import sys"]}, {"term": "def", "name": "scrape", "data": "def scrape( url, folder, rewriteIMG=1, rewriteFiles=1, downloadFiles = 1, createFolders = 1, createExt = 0  ):\n\n\n\tdef initScrape():\n\n\t\tapp = DownloadWeb( url = url, folder=folder, rewriteIMG=rewriteIMG, rewriteFiles=rewriteFiles, downloadFiles = downloadFiles, createFolders = createFolders, createExt = createExt )\n\n\t\t# -------------------------------------------------------\n\t\t# Scraping Procces\n\t\t# -------------------------------------------------------\n\n\n\t\t#app.getHtmlCode(url) # Get html code from web\n\t\tapp.loadCodeHtml(url)  # load html code from HDD\n\n\t\tapp.loadResourcesFromJson() # load list of resources from file\n\t\tapp.fixFoldersByReferers() # fix folders tree\n\t\tapp.saveResources() # download requests\n\n\t\tapp.makeCopyBackup() # create backups of files that need to be changed\n\t\tapp.beautyWeb() # beautifying files\n\t\tapp.saveHtml() # save index.html\n\n\t\tapp.fixResourcesChanges() # Fix resources errors and make conversions\n\t\tapp.convertWEbToLocal() # Convert web to local relative paths\n\n\n\t\t# -------------------------------------------------------\n\t\t# END Scraping Procces\n\t\t# -------------------------------------------------------\n\n\tinitScrape()\n\n", "description": null, "category": "webscraping", "imports": ["from edtools import *", "from edtools.Scrape import WebScrape", "import sys"]}, {"term": "def", "name": "browser", "data": "def browser( url, folder, rwi, rwf, dF, cf, ce ):\n\n\n\tappd = DownloadWeb( url = url, folder = folder, rewriteIMG = rwi, rewriteFiles = rwf, downloadFiles = dF, createFolders = cf, createExt = ce  )\n\n\tappb = BrowseWeb( folder = folder ) # Launching Selenium and BrowserMobProxy\n\tappb.setUrl( url ) # Just set url but not execute\n\n\tdef custom():\n\n\t\trequestsList = appb.getRequests()\n\n\t\tappd.setCode(appb.getCode(), url)\n\t\tappd.setResources(requestsList)\n\n\n\n\t\t# custom code from downloadweb( class )\n\n\t\t#appd.saveResources()\n\t\t# app.convertWEbToLocal()\n\t\t# appd.saveHtml()\n\n\n\twhile True:\n\n\t\tdata = input(\"--> \")\n\n\t\tif data in \" \\n\":\n\t\t\tpass\n\n\t\telif data == \"9\":\n\t\t\tcustom()\n\n\t\telif data in \"close exit out finish stop\" or data == \"0\":\n\t\t\tappb.exit()\n\t\t\tbreak\n\n\t\telif \"exec\" in data:\n\t\t\tdata = data.replace(\"exec \", \"\")\n\t\t\ttry: exec(data)\n\t\t\texcept: print(\"Unexpected error:\", sys.exc_info()[0])\n\n\t\telse:\n\t\t\tappb.command(data)\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from edtools import *", "from edtools.Scrape import WebScrape", "import sys"]}, {"term": "def", "name": "init", "data": "def init( url, folder, rwi, rwf, system, downloadFiles, createFolders, createExt  ):\n\n\n\tdef pInit():\n\t\tprint(\"\\n\" * 2 + \"=\" * 80 + \"\\n\" + \"\\n STARTED\\n\\n\" + \"=\" * 80 + \"\\n\")\n\n\tdef kInfo():\n\t\tprint( \"\\n 0 ) stop \\n\\n 1 ) start \\n 2 ) getrequests \\n 9 ) custom code \\n\\n\\n\" + \"=\" * 80 + \"\\n\")\n\n\tdef pEnd():\n\t\tprint(\"\\n\" * 5 + \"=\" * 80 + \"\\n\" + \"\\n ENDED\\n\\n\" + \"=\" * 80 + \"\\n\")\n\n\tdef start():\n\n\t\tif system == \"scrape\":\n\t\t\tscrape(url, folder, rwi, rwf, downloadFiles, createFolders, createExt )\n\n\t\tif system == \"browser\":\n\t\t\tbrowser( url, folder, rwi, rwf, downloadFiles, createFolders, createExt )\n\n\n\tpInit()\n\n\tkInfo()\n\n\tstart()\n\n\tpEnd()\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from edtools import *", "from edtools.Scrape import WebScrape", "import sys"]}], [{"term": "def", "name": "__init__", "data": "  def __init__(self, url: str, gpp_type: str, workout_dates: list = [], params: str = ''):\n\tself.url = url\n\tself.gpp_type = gpp_type\n\tself.workout_dates = workout_dates\n\tself.params = params\n\tself.wods_json = self.webscrape_data_to_json\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "from selenium import webdriver ", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import pandas as pd", "import json"]}, {"term": "def", "name": "replace_chars", "data": "  def replace_chars(s: str) -> str:\n\t  s = s.replace('\\n', '').replace('*', '').replace('\\t', '')\n\t  return s   \n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "from selenium import webdriver ", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import pandas as pd", "import json"]}, {"term": "def", "name": "webscrape_data_to_csv", "data": "  def webscrape_data_to_csv(self) -> bool:\n\tbool_csv_created = False\n\tdriver = webdriver.Chrome(options=self.get_chrome_options())\n\tdriver.get(self.url)\n\telem_to_find = \"proggy\"\n\ttry:\n\t  nlp_elements = driver.find_elements(By.CLASS_NAME, elem_to_find)\n\t  # df.append deprecated so using tmp list of dataframes to append then concat \n\t  tmp = []\t  \n\t  for nlp_phase in nlp_elements:\n\t\t  nlp_phase_innerHTML = self.replace_chars(nlp_phase.get_attribute('innerHTML'))\n\t\t  tmp.append(pd.read_html(\"\" + nlp_phase_innerHTML + \"\")[0])\n\t  nlp_df = pd.concat(tmp, ignore_index=True)\n\t  nlp_df.to_csv('sslp.csv', encoding='utf-8', index=False)\n\t  bool_csv_created = True\n\t  #print(calc_sslp_ph1())\n\tfinally:\n\t\tdriver.quit()   \n\t\treturn bool_csv_created\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "from selenium import webdriver ", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import pandas as pd", "import json"]}, {"term": "def", "name": "webscrape_data_to_json", "data": "  def webscrape_data_to_json(self) -> str:\n\tif self.gpp_type == 'DEUCE':\n\t  return self.webscrape_deuce_data_to_json()\n\telse: # PushJerk\n\t  return self.webscrape_pj_data_to_json()\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "from selenium import webdriver ", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import pandas as pd", "import json"]}, {"term": "def", "name": "webscrape_deuce_data_to_json", "data": "  def webscrape_deuce_data_to_json(self) -> str:\n\tjson_formatted_str = ''\n\tdriver = webdriver.Chrome(options=self.get_chrome_options())\n\t# TODO: get this working for singleton the loop it => for wod_date in workout_dates:\n\twod_date = self.workout_dates[0]\n\tdriver.get(self.url+wod_date)\n\tpopup_xpath = '/html/body/div[3]/div/img'\n\ttry:\n\t\tpopup = driver.find_element_by_xpath(popup_xpath)\n\t\tif popup.is_displayed:\n\t\t  popup.click() # Closes the popup\n\texcept Exception: #NoSuchElementException:\n\t  # no popup\n\t  pass\n\telse: \n\t  inner_html = '\\n\\t\\t\\t11/29/21 WOD\\n\\t\\t\\tDEUCE ATHLETICS GPP\\nComplete 4 rounds for quality of:\\n8 Barbell Strict Press (3x1x)\\n8 Single Kettlebell Lateral Lunge\\nThen, AMRAP 12\\n1,2,3,\u2026,\u221e\\nFront Squat (135/95)\\nDB Renegade Row (40/20)\\n**Every 2 min, 1 7th Street Corner Run\\nDEUCE GARAGE GPP\\n5-5-5-5-5\\nPendlay Row\\nThen, complete 3 rounds for quality of:\\n10 Single Arm Bent Over row (ea)\\n10-12 Parralette Push Ups\\n10 Hollow Body Lat Pulls\u00a0\\nThen, AMRAP8\\n6 Chest to Bar Pull Ups\\n8 HSPU\\n48 Double unders\\n\\t\\t'\n\t  inner_html = self.replace_chars(inner_html)\n\t  df_wod = pd.read_html('' + inner_html + '')\n\t  print(df_wod)\n\t  wod_link_xpath = '/html/body/div[1]/main/center/article/div/p/a'\n\t  wod_link = driver.find_element_by_xpath(wod_link_xpath)\n\t  ActionChains(driver).move_to_element(wod_link).click(wod_link).perform()\n\t  wod_element = WebDriverWait(driver, 10).until(\n\t\t  EC.presence_of_element_located((By.CLASS_NAME, \"wod_block\"))\n\t  )\n\t  wod_innerHTML = wod_element.get_attribute('innerHTML')\n\t  df_wod = pd.read_html(\"\" + wod_innerHTML + \"\")\n\t  wod_json_str = df_wod.to_json(orient='records')\n\t  obj_data = json.loads(wod_json_str)\n\t  json_formatted_str += json.dumps(obj_data, indent=4) \n\tfinally:\n\t  driver.quit()  \n\t  return json_formatted_str\t\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "from selenium import webdriver ", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import pandas as pd", "import json"]}, {"term": "def", "name": "webscrape_pj_data_to_json", "data": "  def webscrape_pj_data_to_json(self) -> str:\n\twod_list = []\n\ttry:\n\t\tdriver = webdriver.Chrome(chrome_options=self.get_chrome_options())\n\t\tdriver.get(self.url + self.gpp_type + self.params)\n\t\tsource_code = driver.page_source\n\t\tsoup = BeautifulSoup(source_code,'lxml')\n\t\tentry_block = soup.find_all('div', class_='entry-title')\n\t\tfor entries in entry_block:\n\t\t  pj_entry_url = entries.find('a')\n\t\t  wod_list.append(pj_entry_url)\n\t\t# for w in wods:\n\t\t#\t pubDate = w.find('pubDate').text\n\t\t#\t encoded_content = w.find('encoded')\n\t\t#\t for child in encoded_content.children:\n\t\t#\t\t content = str(child)\n\t\t#\t wod = {\n\t\t#\t\t 'pubDate': pubDate,\n\t\t#\t\t 'content': content,\n\t\t#\t }\n\t\t#\t wod_list.append(wod)\n\texcept Exception as e:\n\t\tprint('The scraping job failed. See exception: ')\n\t\tprint(e)\n\treturn json.dumps(wod_list)\t\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "from selenium import webdriver ", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import pandas as pd", "import json"]}, {"term": "def", "name": "get_chrome_options", "data": "  def get_chrome_options():\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--ignore-certificate-errors')\n\toptions.add_argument('--incognito')\n\toptions.add_argument('--headless')\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "from selenium import webdriver ", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import pandas as pd", "import json"]}], [], [], [{"term": "def", "name": "index", "data": "def index():\n\treturn render_template('index.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request", "import os", "from Scripts import eplWebscrape"]}, {"term": "def", "name": "copies", "data": "def copies():\n\tif request.method=='POST':\n\t\tos.popen('cp rankings/DefenderRank.csv rankings/DefenderRank\\ copy.csv')\n\t\tos.popen('cp rankings/ForwardRank.csv rankings/ForwardRank\\ copy.csv')\n\t\tos.popen('cp rankings/GoalieRank.csv rankings/GoalieRank\\ copy.csv')\n\t\tos.popen('cp rankings/MidfielderRank.csv rankings/MidfielderRank\\ copy.csv')\n\t\treturn render_template('copies.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request", "import os", "from Scripts import eplWebscrape"]}, {"term": "def", "name": "runScript", "data": "def runScript():\n\tif request.method=='POST':\n\t\teplWebscrape.webscrape()\n\t\treturn render_template('update.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request", "import os", "from Scripts import eplWebscrape"]}, {"term": "def", "name": "defenders", "data": "def defenders():\n\treturn render_template('defenders.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request", "import os", "from Scripts import eplWebscrape"]}, {"term": "def", "name": "goalies", "data": "def goalies():\n\treturn render_template('goalies.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request", "import os", "from Scripts import eplWebscrape"]}, {"term": "def", "name": "midfielders", "data": "def midfielders():\n\treturn render_template('midfielders.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request", "import os", "from Scripts import eplWebscrape"]}, {"term": "def", "name": "forwards", "data": "def forwards():\n\treturn render_template('forwards.html')\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request", "import os", "from Scripts import eplWebscrape"]}], [], [{"term": "def", "name": "webscrape_conncected", "data": "def webscrape_conncected(issueState, issueList, keywords, status, topic):\n\tfiles = []\n\tprint('Number of {} issues to be explored : {}'.format(issueState, len(issueList)))\n\n\tfor keyword in keywords:\n\t\tif status == 'test':\n\t\t\tfiles.append(open('issues/test/{}/{}/All-{}-Issues-Having-{}-in-{}.txt'.format(topic, issueState, issueState, keyword, framework), 'w+', encoding='utf-8'))\n\t\telse:\n\t\t\tfiles.append(open('issues/{}/{}/All-{}-Issues-Having-{}-in-{}.txt'.format(topic, issueState, issueState, keyword.replace('\\\\','').replace('/','-'), framework), 'w+', encoding='utf-8'))\n\n\tfor issue in issueList:\n\t\twith open(issue, 'r', encoding='utf-8') as f:\n\t\t\t#parsing the html file corresponding to the issue\n\t\t\tsoup = BeautifulSoup(f.read(), 'html5lib')\n\n\t\t\t#getting the first post which will correspond to the issue's description\n\t\t\t# issueDesc = soup.find('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n\n\t\t\t#extracting the issue's ID from the bug report\n\t\t\tissueIdMatch = re.search('\\\\d+', issue)\n\t\t\tissueId = issue[issueIdMatch.start() : issueIdMatch.end()]\n\n\t\t\tfor i, keyword in enumerate(keywords):\n\t\t\t\t#check if the html has the keyword\n\t\t\t\tissueTitle = soup.find('span', class_=\"js-issue-title markdown-title\")\n\t\t\t\tissueDescs = soup.findAll('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n\t\t\t\tissueDesc = '\\n'.join(issueDescs)\n\t\t\t\tissue_title_desc = issueTitle + '\\n' + issueDesc\n\t\t\t\tstringsContainingTheKeyword = issue_title_desc.findAll(text=re.compile(keyword, re.I))\n\t\t\t\tNumOfRepeat = len(stringsContainingTheKeyword)\n\t\t\t\tif (NumOfRepeat > 0):\n\t\t\t\t\tissueInfo = {\n\t\t\t\t\t'ID' : issueId,\n\t\t\t\t\t# 'Description' : issueDesc,\n\t\t\t\t\t'Num of Repeat': NumOfRepeat\n\t\t\t\t\t}\n\t\t\t\t\tfiles[i].write('ID: {}\\tNum of Repeat: {}\\n'.format(issueInfo['ID'], issueInfo['Num of Repeat']))\n\n", "description": null, "category": "webscraping", "imports": ["import sys", "import os", "import requests", "from bs4 import BeautifulSoup", "import subprocess", "import time", "import re"]}, {"term": "def", "name": "webscrape_separated", "data": "def webscrape_separated(issueState, issueList, keywords_set, status, topic):\n\tfiles = []\n\tprint('Number of {} issues to be explored : {}'.format(issueState, len(issueList)))\n\n\tfor keyword_set in keywords_set:\n\t\tkeyword = '-'.join(keyword_set)\n\t\tif status == 'test':\n\t\t\tfiles.append(open('issues/test/{}/{}/All-{}-Issues-Having-{}-in-{}.txt'.format(topic, issueState, issueState, keyword, framework), 'w+', encoding='utf-8'))\n\t\telse:\n\t\t\tfiles.append(open('issues/{}/{}/All-{}-Issues-Having-{}-in-{}.txt'.format(topic, issueState, issueState, keyword.replace('\\\\','').replace('/','-'), framework), 'w+', encoding='utf-8'))\n\n\tfor issue in issueList:\n\t\twith open(issue, 'r', encoding='utf-8') as f:\n\t\t\t#parsing the html file corresponding to the issue\n\t\t\tsoup = BeautifulSoup(f.read(), 'html5lib')\n\n\t\t\t#getting the first post which will correspond to the issue's description\n\t\t\t# issueDesc = soup.find('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n\n\t\t\t#extracting the issue's ID from the bug report\n\t\t\tissueIdMatch = re.search('\\\\d+', issue)\n\t\t\tissueId = issue[issueIdMatch.start() : issueIdMatch.end()]\n\n\t\t\tfor i, keyword_set in enumerate(keywords_set):\n\t\t\t\t#check if the html has the keyword\n\t\t\t\tNumOfRepeats = ''\n\t\t\t\tissueTitle = soup.find('span', class_=\"js-issue-title markdown-title\")\n\t\t\t\tissueDescs = soup.findAll('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n\t\t\t\tissueDesc = '\\n'.join(issueDescs)\n\t\t\t\tissue_title_desc = issueTitle + '\\n' + issueDesc\n\t\t\t\tfor keyword_index, keyword in enumerate(keyword_set):\n\t\t\t\t\tstringsContainingTheKeyword = issue_title_desc.findAll(text=re.compile(keyword, re.I))\n\t\t\t\t\tNumOfRepeat = len(stringsContainingTheKeyword)\n\t\t\t\t\tif (NumOfRepeat == 0):\n\t\t\t\t\t\tbreak\n\t\t\t\t\t\n\t\t\t\t\tif keyword_index == len(keyword_set) - 1:\n\t\t\t\t\t\tNumOfRepeats += str(NumOfRepeat)\n\t\t\t\t\t\tissueInfo = {\n\t\t\t\t\t\t\t'ID' : issueId,\n\t\t\t\t\t\t\t# 'Description' : issueDesc,\n\t\t\t\t\t\t\t'Num of Repeats': NumOfRepeats\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfiles[i].write('ID: {}\\tNum of Repeats: {}\\n'.format(issueInfo['ID'], issueInfo['Num of Repeats']))\n\n\t\t\t\t\telse:\n\t\t\t\t\t\tNumOfRepeats += str(NumOfRepeat) + ','\n\n", "description": null, "category": "webscraping", "imports": ["import sys", "import os", "import requests", "from bs4 import BeautifulSoup", "import subprocess", "import time", "import re"]}, {"term": "def", "name": "writeResults", "data": "def writeResults(issueState, issuesInfo, keyword):\n\ttry:\n\t\tf = open('All-{}-Issues-Having-{}-in-{}.txt'.format(issueState, keyword, framework), 'w+', encoding='utf-8')\n\t\tfor issueInfo in issuesInfo:\n\t\t\tf.write('ID: {}\\tNum of Repeat: {}\\tDescription: {}\\n'.format(issueInfo['ID'], issueInfo['Num of Repeat'], issueInfo['Description']))\n\texcept:\n\t\tprint('Something went wrong while writing the results')\n\tfinally:\n\t\tf.close()\n\n", "description": null, "category": "webscraping", "imports": ["import sys", "import os", "import requests", "from bs4 import BeautifulSoup", "import subprocess", "import time", "import re"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape():\n\tif request.method == 'POST':\n\t\tdriver = webdriver.Chrome(executable_path='./chromedriver.exe')\n\t\toptions = webdriver.ChromeOptions()\n\t\toptions.add_argument('--enable-javascript')\n\t\toptions.add_argument('headless')\n\t\t\n\t\turl = request.get_json()\n\t\tdriver.get(url)\n\n\t\ttitle = driver.find_element(By.CLASS_NAME, 'product-name').get_attribute('textContent')\n\t\tprice = driver.find_element(By.CLASS_NAME, 'regPrice').get_attribute('textContent')\n\t\tdesc = driver.find_element(By.CLASS_NAME, 'description-text').get_attribute('textContent')\n\t\t\n\t\treturn {'items': [url, title, price, desc]}\n\t\n\telse:\n\t\treturn \"Error\"\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request", "from selenium import webdriver", "from selenium.webdriver.common.by import By"]}], [], [], [], [{"term": "class", "name": "Feature", "data": "class Feature(Server):\n\t\n\tdef __init__(self, request):\n\t\tself.sn = StatusNews(request)\n\t\tself.d2h = Dist2Hospital(request)\n\t\tself.dgs = Diagnosis(request)\n\t\tself.wbs = Webscrape()\n\t\tsuper().__init__(request)\n\n\t\tself.intent = super().rcvIntent()\n\n\tdef main(self):\n\t\t# --------------------------#\n\t\t# INFECTION STATUS INTENT   #\n\t\t# --------------------------#\n\t\tif self.intent == \"infection-status-covid\":\n\t\t\treturn self.sn.infectionStatus()\n\n\t\t# --------------------------#\n\t\t# HEADLINE NEWS INTENT\t  #\n\t\t# --------------------------#\n\t\tif self.intent == \"latest-news-covid\":\n\t\t\treturn self.sn.headlineNews()\n\n\t\t# --------------------------#\n\t\t# Distance to Hospital\t  #\n\t\t# --------------------------#\n\t\tif self.intent == \"nearest-hospital-covid\" or self.intent == \"treatment-covid.yes.address\":\n\t\t\treturn self.d2h.dist2hospital()\n\n\t\t# --------------------------#\n\t\t# DIAGNOSIS INTENT\t\t  #\n\t\t# --------------------------#\n\t\tif self.intent == \"diagnosis-covid\":\n\t\t\treturn self.dgs.diagnosis()\n\n\t\t# --------------------------#\n\t\t# SYNC  INTENT\t\t\t  #\n\t\t# --------------------------#\n\t\tif self.intent == \"sync\":\n\t\t\ttry:\n\t\t\t\tself.wbs.statusScrapper()\n\t\t\t\tself.wbs.newsScrapper()\n\t\t\t\tself.dgs.updateResponses()\n\t\t\t\tself.text1 = \"Sync/update completed.\"\n\t\t\texcept:\n\t\t\t\tself.text1=\"Error occurred. Contact admin to debug.\"\n\t\t\tfinally:\n\t\t\t\treturn super().sendMsg()\n\n\t  \n\t\t\n", "description": null, "category": "webscraping", "imports": ["from chatbot_app.modules.status_news import StatusNews", "from chatbot_app.modules.dist2hospital import Dist2Hospital", "from chatbot_app.modules.diagnosis import Diagnosis", "from chatbot_app.modules.webscrape import Webscrape", "from chatbot_app.modules.dialogflow_msg import Server"]}], [{"term": "def", "name": "price_reduction", "data": "def price_reduction(investment, profit):\n\tratio = (investment/profit)*100\n\tif(ratio>=90):\n\t\treturn 7\n\telif(ratio<90 and ratio>=70):\n\t\treturn 5\n\telif(ratio<70 and ratio>=50):\n\t\treturn 4\n\telif(ratio<50 and ratio>=20):\n\t\treturn 2\n\telse:\n\t\treturn 1\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "calculate_price", "data": "def calculate_price(ratio, location, crop):\n\tif(location=='Bijapur'):\n\t\tbase = bij[crop]\n\t\tfinal = base - ((base*ratio)/100)\n\t\treturn final\n\telif(location=='Udupi'):\n\t\tbase = ud[crop]\n\t\tfinal = base - ((base*ratio)/100)\n\t\treturn final\n\telse:\n\t\tbase = bang[crop]\n\t\tfinal = base - ((base * ratio) / 100)\n\t\treturn final\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "bijapur", "data": "def bijapur():\n\tconn = sqlite3.connect('crop.db')\n\tc = conn.cursor()\n\tc.execute(\"SELECT id, crop, investment, profit FROM bijapur\")\n\tfor i in c.fetchall():\n\t\tindex = i[0]\n\t\tratio = price_reduction(i[2],i[3])\n\t\tfinal_price = calculate_price(ratio, 'Bijapur', i[1])\n\t\tc.execute(\"UPDATE bijapur SET price= ? WHERE id= ?\",(final_price,index))\n\tconn.commit()\n\tconn.close()\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "udupi", "data": "def udupi():\n\tconn = sqlite3.connect('crop.db')\n\tc = conn.cursor()\n\tc.execute(\"SELECT id, crop, investment, profit FROM udupi\")\n\tfor i in c.fetchall():\n\t\tindex = i[0]\n\t\tratio = price_reduction(i[2],i[3])\n\t\tfinal_price = calculate_price(ratio, 'Udupi', i[1])\n\t\tc.execute(\"UPDATE udupi SET price= ? WHERE id= ?\", (final_price, index))\n\tconn.commit()\n\tconn.close()\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "bangalore", "data": "def bangalore():\n\tconn = sqlite3.connect('crop.db')\n\tc = conn.cursor()\n\tc.execute(\"SELECT id, crop, investment, profit FROM bangalore\")\n\tfor i in c.fetchall():\n\t\tindex = i[0]\n\t\tratio = price_reduction(i[2],i[3])\n\t\tfinal_price = calculate_price(ratio, 'Bangalore rural', i[1])\n\t\tc.execute(\"UPDATE bangalore SET price= ? WHERE id= ?\", (final_price, index))\n\tconn.commit()\n\tconn.close()\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}], [{"term": "def", "name": "loadSwatches", "data": "def loadSwatches(data):\r\n\timages = []\r\n\tdirpath = os.path.dirname(os.path.realpath(__file__)).replace(\"\\\\\", \"/\")\r\n\tpathToFeature = dirpath + \"/Colors/%s\" %(data.currentFeature)\r\n\tfiles = os.listdir(pathToFeature)\r\n\tfor image in files:\r\n\t\tpath = pathToFeature + \"/\" + image\r\n\t\timages += [path]\r\n\tswatches = []\r\n\tfor pic in range(len(images)):\r\n\t\tphoto = cv2.imread(images[pic])\r\n\t\tphoto = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB)\r\n\t\tphoto = imutils.resize(photo, width = 36, inter = cv2.INTER_CUBIC)\r\n\t\tphoto2 = PIL.ImageTk.PhotoImage(image = PIL.Image.fromarray(photo))\r\n\t\tphotoData = [photo2, pic*36 + data.scrollX, photo]\r\n\t\tswatches += [photoData]\r\n\treturn swatches\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "loadTopProducts", "data": "def loadTopProducts(feature, data):\r\n\tdirpath = os.path.dirname(os.path.realpath(__file__)).replace(\"\\\\\", \"/\")\r\n\tpathToFeature = dirpath + \"/Top Products/%s\" %(feature)\r\n\tfiles = os.listdir(pathToFeature)\r\n\timages = []\r\n\tfor image in files:\t\t\t\t\t\t\t\t\r\n\t\tpath = pathToFeature + \"/\" + image\r\n\t\timages += [path]\r\n\tproducts = []\r\n\tfor pic in range(len(images)):\r\n\t\tphoto = cv2.imread(images[pic])\r\n\t\tphoto = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB)\r\n\t\tphoto = imutils.resize(photo, width = 100, inter = cv2.INTER_CUBIC)\r\n\t\tphoto2 = PIL.ImageTk.PhotoImage(image = PIL.Image.fromarray(photo))\r\n\t\tphotoData = [photo2, pic*100 + data.scrollY, photo]\r\n\t\tproducts += [photoData]\r\n\treturn products\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "convertPhoto", "data": "def convertPhoto(data):\r\n\timg = cv2.cvtColor(data.image, cv2.COLOR_BGR2RGB)\r\n\timg = imutils.resize(img, width = 500, inter = cv2.INTER_CUBIC)\r\n\tphoto = PIL.ImageTk.PhotoImage(image = PIL.Image.fromarray(img))\r\n\treturn photo \r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "makeover", "data": "def makeover(data):\r\n\timage = cv2.imread(data.imagePath)\r\n\tfor product in data.products:\r\n\t\tdata.objects[product].img = image\r\n\tfor product in data.products:\r\n\t\timage = data.objects[product].editPhoto()\r\n\treturn image\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "drawSwatches", "data": "def drawSwatches(canvas, data):\r\n\tfor i in range(len(data.swatches)):\r\n\t\tplacement = i * 36\r\n\t\tcanvas.create_image(placement + data.scrollX, data.height - 36, image = data.swatches[i][0], anchor = tkinter.NW)\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "drawTopProducts", "data": "def drawTopProducts(canvas, data):\r\n\tfor i in range(len(data.topProducts)):\r\n\t\tplacement = i*100\r\n\t\tcanvas.create_image(600, placement + data.scrollY, image = data.topProducts[i][0], anchor = tkinter.NW)\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "init", "data": "def init(data): \r\n\tprint(data.imagePath)\r\n\tdata.image = cv2.imread(data.imagePath)\r\n\tdata.photo = convertPhoto(data)\r\n\tdata.dimX, data.dimY, nc = data.img.shape\r\n\tdata.products = [\"lips\", \"shadow\", \"liner\", \"brows\", \"contour\", \"highlight\", \"blush\"] \r\n\tdata.currentFeature = data.products[0]\r\n\tdata.targets = {\"lips\" : webscrapeTopRated.getTopProducts(\"lips\"), \r\n\t\"shadow\" : webscrapeTopRated.getTopProducts(\"shadow\"), \r\n\t\"liner\" : webscrapeTopRated.getTopProducts(\"liner\"), \r\n\t\"brows\" : webscrapeTopRated.getTopProducts(\"brows\"), \r\n\t\"contour\" : webscrapeTopRated.getTopProducts(\"contour\"), \r\n\t\"highlight\" : webscrapeTopRated.getTopProducts(\"highlight\"), \r\n\t\"blush\" : webscrapeTopRated.getTopProducts(\"blush\")}\r\n\t# data.edited = dict()\r\n\tdata.scrollX = 0\r\n\tdata.scrollY = 0\r\n\tdata.swatches = loadSwatches(data)\r\n\tdata.topProducts = {\"lips\" : loadTopProducts(\"lips\", data), \r\n\t\"shadow\" : loadTopProducts(\"shadow\", data), \r\n\t\"liner\" : loadTopProducts(\"liner\", data), \r\n\t\"brows\" : loadTopProducts(\"brows\", data), \r\n\t\"contour\" : loadTopProducts(\"contour\", data), \r\n\t\"highlight\" : loadTopProducts(\"highlight\", data), \r\n\t\"blush\" : loadTopProducts(\"blush\", data)}\r\n\tdata.lips = makeup.Lips(data.image, \"lips\", 0.1, None)\r\n\tdata.shadow = makeup.Shadow(data.image, \"shadow\", 0.1, None)\r\n\tdata.liner = makeup.Liner(data.image, \"liner\", 0.6, None)\r\n\tdata.brows = makeup.Brows(data.image, \"brows\", 0.1, None)\r\n\tdata.contour = makeup.Contour(data.image, \"contour\", 0.1, None)\r\n\tdata.highlight = makeup.Highlight(data.image, \"highlight\", 0.1, None)\r\n\tdata.blush = makeup.Blush(data.image, \"blush\", 0.1, None)\r\n\tdata.objects = {\"lips\" : data.lips, \r\n\t\"shadow\" : data.shadow, \r\n\t\"liner\" : data.liner, \r\n\t\"brows\" : data.brows, \r\n\t\"contour\" : data.contour, \r\n\t\"highlight\" : data.highlight, \r\n\t\"blush\" : data.blush}\r\n\tdata.popup = False \r\n\tdata.input = \"\"\r\n\tprint(\"Done\")\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "mousePressed", "data": "def mousePressed(event, data):\r\n\tif 0 <= event.x <= 500 and data.height-36 <= event.y <= data.height:\r\n\t\tindex = (event.x - data.scrollX)//36\r\n\t\tcurrentImage = data.swatches[index][2]\r\n\t\tcurrentColor = currentImage[17, 17]\r\n\t\t(r, g, b) = (int(currentColor[0]), int(currentColor[1]), int(currentColor[2]))\r\n\t\tdata.objects[data.currentFeature].color = (b, g, r)\r\n\tfor j in range(0, data.height, data.height//len(data.products)):\r\n\t\tif 500 <= event.x <= 600:\r\n\t\t\tif j <= event.y <= j + data.height//len(data.products):\r\n\t\t\t\tdata.currentFeature = data.products[j//(data.height//len(data.products))]\r\n\t\t\t\tdata.scrollX = 0\r\n\t\t\t\tdata.scrollY = 0\r\n\t\t\t\tevent.x = 0\r\n\t\t\t\tevent.y = 0\r\n\t\t\t\tprint(data.currentFeature)\r\n\tif 15 <= event.x <= 105 and data.height - 165 <= event.y <= data.height - 120:\r\n\t\tif data.objects[data.currentFeature].alphaMax >= data.objects[data.currentFeature].alpha:\r\n\t\t\tdata.objects[data.currentFeature].alpha += 0.025\r\n\t\t\tprint(data.objects[data.currentFeature].alpha)\t\r\n\tif 15 <= event.x <= 105 and data.height - 105 <= event.y <= data.height - 60:\r\n\t\tif data.objects[data.currentFeature].alphaMin < data.objects[data.currentFeature].alpha:\r\n\t\t\tdata.objects[data.currentFeature].alpha -= 0.025\r\n\t\t\tprint(data.objects[data.currentFeature].alpha)\r\n\tif 600 < event.x <= data.width:\r\n\t\tindex = ((event.y - data.scrollY) - 50)//100\r\n\t\tlink = \"http://sephora.com\" + data.targets[data.currentFeature][index]\r\n\t\twebbrowser.open(link)\r\n\tif 380 <= event.x <= 470 and data.height - 105 <= event.y <= data.height - 60:\r\n\t\tdata.popup = True\r\n\tif 310 <= event.x <= 360 and 200 <= event.y <= 250:\r\n\t\tpath = os.path.dirname(data.imagePath) + \"finalImage.jpg\"\r\n\t\tcv2.imwrite(path, data.image)\r\n\t\ttry:\r\n\t\t\ttwilioSMS.getPhotoSendMessage(path, int(data.input))\r\n\t\t\tdata.input = \"\"\r\n\t\t\tdata.popup = False\r\n\t\texcept:\r\n\t\t\tdata.popup = False\r\n\tdata.image = makeover(data)\r\n\tdata.photo = convertPhoto(data)\r\n\tdata.swatches = loadSwatches(data)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "keyPressed", "data": "def keyPressed(event, data):\r\n\tif event.keysym == \"Left\":\r\n\t\tdata.scrollX += 10\r\n\t\tprint(data.scrollX)\r\n\tif event.keysym == \"Right\":\r\n\t\tdata.scrollX -= 10\r\n\t\tprint(data.scrollX)\r\n\tif event.keysym == \"Up\":\r\n\t\tdata.scrollY += 10\r\n\t\tprint(data.scrollY)\r\n\tif event.keysym == \"Down\":\r\n\t\tdata.scrollY -= 10\r\n\t\tprint(data.scrollY)\r\n\tif data.popup == True and str(event.char) in \"1234567890\":\r\n\t\tdata.input = str(data.input) \r\n\t\tdata.input += str(event.char)\r\n\t\tdata.input = int(data.input)\r\n\tif data.popup == True and event.keysym == \"BackSpace\":\r\n\t\tdata.input = str(data.input) \r\n\t\tif len(data.input) > 0:\r\n\t\t\tdata.input = data.input[0:-1]\r\n\t\t\tdata.input = int(data.input)\r\n\t\telse: \r\n\t\t\tdata.input = \"\"\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "redrawAll", "data": "def redrawAll(canvas, data):\r\n\tcanvas.create_image(0, 0, image = data.photo, anchor = tkinter.NW)\r\n\tfor i in range(len(data.swatches)):\r\n\t\tplacement = i * 36\r\n\t\tcanvas.create_image(placement + data.scrollX, data.height - 36, image = data.swatches[i][0], anchor = tkinter.NW)\r\n\tcanvas.create_rectangle(600, 0, data.width, data.height, fill = \"white\", width = 0)\r\n\tfor i in range(len(data.topProducts[data.currentFeature])):\r\n\t\tplacement = i*100 + 50\r\n\t\tcanvas.create_image(600, placement + data.scrollY, image = data.topProducts[data.currentFeature][i][0], anchor = tkinter.NW)\r\n\tfor j in range(0, data.height, round(data.height/len(data.products))):\r\n\t\tindex = j//(data.height//len(data.products))\r\n\t\tcanvas.create_rectangle(500, j, 600, j+100, fill = \"white\", width = 10) \r\n\t\tcanvas.create_text(550, j + 50, text = \"%s\" %(data.products[index]), fill = \"Black\", font = \"Arial 15\")\r\n\tcanvas.create_rectangle(605, 0, data.width, 50, fill = \"white\", width = 0)\r\n\tcanvas.create_text(612, 0, text = \"Bestsellers\", fill = \"Black\", font = \"Arial 15\", anchor = tkinter.NW)\r\n\tcanvas.create_text(610, 20, text = \"on Sephora\", fill = \"Black\", font = \"Arial 15\", anchor = tkinter.NW)\r\n\tcanvas.create_rectangle(15, data.height - 165, 105, data.height - 120, fill = \"white\", width = 5)\r\n\tcanvas.create_text(60, data.height - 143, text = \"%s\" %(\"sharpen\"), fill = \"Black\", font = \"Arial 15\")\r\n\tcanvas.create_rectangle(15, data.height - 105, 105, data.height - 60, fill = \"white\", width = 5)\r\n\tcanvas.create_text(60, data.height - 83, text = \"%s\" %(\"blend\"), fill = \"Black\", font = \"Arial 15\")\r\n\tcanvas.create_rectangle(380, data.height - 105, 470, data.height - 60, fill = \"white\", width = 5)\r\n\tcanvas.create_text(425, data.height - 83, text = \"%s\" %(\"text me!\"), fill = \"Black\", font = \"Arial 15\")\r\n\tif data.popup == True:\r\n\t\tcanvas.create_rectangle(200, 200, 300, 250, fill = \"white\", width = 5)\r\n\t\tcanvas.create_text(250, 225, text = \"%s\" %(str(data.input)), fill = \"Black\", font = \"Arial 10\")\r\n\t\tcanvas.create_rectangle(310, 200, 360, 250, fill = \"white\", width = 5)\r\n\t\tcanvas.create_text(335, 225, text = \"Done!\", fill = \"Black\")\r\n\r\n\t# for i in range(len(data.swatches)):\r\n\t#\t xPlacement = i*36 + 18\r\n\t#\t canvas.create_image(xPlacement, data.height - 18, \\\r\n\t#\t\t image = \"C:/Users/sanam/Documents/CMU/Year 1/Semester 2/15-112 Fundamentals of Programming/Term Project/Colors/%s\" \\\r\n\t#\t\t %(data.swatches[i]))\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}, {"term": "def", "name": "run", "data": "def run(width=300, height=300):\r\n\tdef redrawAllWrapper(canvas, data):\r\n\t\tcanvas.delete(ALL)\r\n\t\tcanvas.create_rectangle(0, 0, data.width, data.height,\r\n\t\t\t\t\t\t\t\tfill='white', width=0)\r\n\t\tredrawAll(canvas, data)\r\n\t\tcanvas.update()\t\r\n\r\n\tdef mousePressedWrapper(event, canvas, data):\r\n\t\tmousePressed(event, data)\r\n\t\tredrawAllWrapper(canvas, data)\r\n\r\n\tdef keyPressedWrapper(event, canvas, data):\r\n\t\tkeyPressed(event, data)\r\n\t\tredrawAllWrapper(canvas, data)\r\n\r\n\t# Set up data and call init\r\n\tclass Struct(object): pass\r\n\tdata = Struct()\r\n\tdata.width = width\r\n\tdata.height = height\r\n\troot = tkinter.Tk()\r\n\r\n\tdata.imagePath = askopenfilename()\r\n\tdata.img = cv2.imread(data.imagePath)\r\n\tdata.img = cv2.cvtColor(data.img, cv2.COLOR_BGR2RGB)\r\n\tdata.img = imutils.resize(data.img, width = 500, inter = cv2.INTER_CUBIC)\r\n\r\n\tinit(data)\r\n\t# create the root and the canvas\r\n\tcanvas = Canvas(root, width=data.width, height=data.height)\r\n\tcanvas.pack()\r\n\r\n\tdata.photo = PIL.ImageTk.PhotoImage(image = PIL.Image.fromarray(data.img))\r\n\tcanvas.create_image(0, 0, image = data.photo, anchor = tkinter.NW)\r\n\r\n\t# set up events\r\n\troot.bind(\"\", lambda event:\r\n\t\t\t\t\t\t\tmousePressedWrapper(event, canvas, data))\r\n\troot.bind(\"\", lambda event:\r\n\t\t\t\t\t\t\tkeyPressedWrapper(event, canvas, data))\r\n\tredrawAll(canvas, data)\r\n\tprint(\"Done\")\r\n\t# and launch the app\r\n\troot.mainloop()  # blocks until window is closed\r\n", "description": null, "category": "webscraping", "imports": ["import numpy as np\r", "import cv2\r", "import tkinter \r", "import PIL.Image\r", "import PIL.ImageTk\r", "import makeup\r", "import imutils\r", "from imutils import face_utils\r", "from tkinter import *\r", "import os\r", "import webscrapeTopRated\r", "import webbrowser\r", "import twilioSMS\r", "from tkinter.filedialog import askopenfilename\r"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape():\r\n\twith open(\"web_scrape.txt\", 'w')as wc:\r\n\t\tfor i in range(0,50):\r\n\t\t\ttry:\r\n\t\t\t\tresp = requests.get('http://3.95.249.159:8000/random_company').text\r\n\t\t\texcept:\r\n\t\t\t\tprint('cURL failed')\r\n\t\t\telse:\r\n\t\t\t\tsoup = BeautifulSoup(resp, 'html.parser')\r\n\t\t\t\tfor li in soup.find_all('li'):\r\n\t\t\t\t\tif(\"Name: \" in li.text):\r\n\t\t\t\t\t\twc.write(str(li.text))\r\n\t\t\t\t\t\twc.write('\\n')\r\n\t\t\t\t\tif(\"Purpose: \" in li.text):\r\n\t\t\t\t\t\twc.write(str(li.text))\r\n\t\t\t\t\t\twc.write('\\n\\n')\r\n\t\t\ttime.sleep(0.1)\r\n", "description": null, "category": "webscraping", "imports": ["import requests\r", "from bs4 import BeautifulSoup\r", "import time\r"]}], [{"term": "def", "name": "getMunicipalityList", "data": "def getMunicipalityList():\n\t# Import from existing CSV\n\tfilepath = os.getcwd() + \"\\\\backend\\\\webscrape\"\n\tdf = pd.read_csv(filepath + \"\\\\covidData.csv\")\n\tfmuni = df['Municipality'].tolist()\n   \n\tfullpath = os.getcwd() + \"\\\\geojson\"\n\tdir_list = os.listdir(fullpath)\n\t\n\tcount = 0\n\tfor file in dir_list:\n\t\tfile = (f\"{fullpath}\\\\{file}\")\n\t\tprint(file)\n\t\twith open(file, \"r\") as jsonFile:\n\t\t\tdata = json.load(jsonFile)\n\t\t\tfor i in range(0, len(data['features'])):\n\t\t\t\tpr = str()\n\t\t\t\tif \"ph18-\" in file or \"ph0\" in file:\n\t\t\t\t\tpr = data['features'][i]['properties']['name'].upper() \n\t\t\t\telse:\n\t\t\t\t\tpr = data['features'][i]['properties']['province'].upper()\n\t\t\t\t\n\t\t\t\tmatch = process.extractOne(pr, fmuni, scorer=fuzz.token_sort_ratio)\n\t\t\t\tcases = df.loc[df['Municipality'] == match[0], 'Active Cases'].iloc[0]\n\n\t\t\t\tdata['features'][i]['properties']['cases'] = cases \n\n\t\t\t\tprint(data['features'][i]['properties']['cases'])\n\t\t\t\t\n\t\twith open(file, \"w\") as jsonFile:\n\t\t\tjson.dump(data, jsonFile)\n\n", "description": null, "category": "webscraping", "imports": ["from tableauscraper import TableauScraper as TS", "from fuzzywuzzy import process, fuzz", "import pandas as pd", "import os", "import json"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(dbname):\n\tclick.echo('Using SQLite3 database: {}'.format(dbname))\n\tconn = sqlite3.connect(dbname)\n", "description": null, "category": "webscraping", "imports": ["import appg.webscrape as appg", "import click", "import sqlite3"]}], [], [], [], [{"term": "def", "name": "webscrape_azul", "data": "def webscrape_azul(list_airports_dict: list[dict]):\n\tazul.get_flights(list_airports_dict.copy(), 1)\n\tazul.get_flights(list_airports_dict.copy(), 15)\n\tazul.get_flights(list_airports_dict.copy(), 30)\n\n", "description": null, "category": "webscraping", "imports": ["from time import perf_counter", "from src import airport_dict", "from src.file_manager import input, output_excel, add_prices", "from src.scraper.azul import azul", "from src.scraper.gol import gol", "from src.scraper.latam import latam", "from src.util import util_get_logger, post_scraping"]}, {"term": "def", "name": "webscrape_gol", "data": "def webscrape_gol(list_airports):\n\tflight_list = gol.get_flights(list_airports, 1)\n\tadd_prices.insert_price(flight_list, 1)\n\tflight_list = gol.get_flights(list_airports, 15)\n\tadd_prices.insert_price(flight_list, 15)\n\tflight_list = gol.get_flights(list_airports, 30)\n\toutput_excel.write_file(flight_list)\n\n", "description": null, "category": "webscraping", "imports": ["from time import perf_counter", "from src import airport_dict", "from src.file_manager import input, output_excel, add_prices", "from src.scraper.azul import azul", "from src.scraper.gol import gol", "from src.scraper.latam import latam", "from src.util import util_get_logger, post_scraping"]}, {"term": "def", "name": "webscrape_latam", "data": "def webscrape_latam(list_airports):\n\tflight_list = latam.get_flights(list_airports, 1)\n\tadd_prices.insert_price(flight_list, 1)\n\tflight_list = latam.get_flights(list_airports, 15)\n\tadd_prices.insert_price(flight_list, 15)\n\tflight_list = latam.get_flights(list_airports, 30)\n\toutput_excel.write_file(flight_list)\n\n", "description": null, "category": "webscraping", "imports": ["from time import perf_counter", "from src import airport_dict", "from src.file_manager import input, output_excel, add_prices", "from src.scraper.azul import azul", "from src.scraper.gol import gol", "from src.scraper.latam import latam", "from src.util import util_get_logger, post_scraping"]}, {"term": "def", "name": "main", "data": "def main():\n\tlist_airports = input.get_airport_list()\n\tlist_airports_dict = airport_dict.get_airport_dict_list(list_airports)\n\twebscrape_azul(list_airports_dict)\n\twebscrape_gol(list_airports_dict)\n\twebscrape_latam(list_airports_dict)\n\tpost_scraping.create_csv_backup()\n\tpost_scraping.delete_old_backup()\n\tpost_scraping.delete_old_log()\n\n", "description": null, "category": "webscraping", "imports": ["from time import perf_counter", "from src import airport_dict", "from src.file_manager import input, output_excel, add_prices", "from src.scraper.azul import azul", "from src.scraper.gol import gol", "from src.scraper.latam import latam", "from src.util import util_get_logger, post_scraping"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(url):\n\tr= requests.get(url)\n\tsoup = BeautifulSoup(r.content, 'html.parser')\n\t\n\ttables = soup.find_all(\"table\", {\"class\": \"ticket-list\"})\n\tfor table in tables:\n\t\tlinks = table.tbody.a\n\t\treturn links.text\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import csv", "\turl = \"https://sourceforge.net/p/obo/plant-ontology-po-term-requests/search/?q=import_id%3A\" +SF"]}], [{"term": "class", "name": "WebscrapeConfig", "data": "class WebscrapeConfig(AppConfig):\n\tdefault_auto_field = 'django.db.models.BigAutoField'\n\tname = 'webscrape'\n", "description": null, "category": "webscraping", "imports": ["from django.apps import AppConfig"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\n\tdef get_gloss(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "from pyltp import SentenceSplitter"]}], [{"term": "class", "name": "HelloWorld", "data": "class HelloWorld(Resource):\n\tdef get(self):\n\t\twebscraper = WebScrape()\n\t\twebscraper.run()\n\t\twith open('../JSONWebServer/internships.json') as f:\n\t\t\tdata = json.load(f)\n\t\treturn data\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, redirect, url_for, request, redirect", "from flask_restful import Resource, Api", "from webScrape import WebScrape", "import json"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(pages):\n\tURL = f'https://www.capitoltrades.com/trades?page={pages}&pageSize=25'\n\t# \"https://app.capitoltrades.com/trades?page={pages}&pageSize=100\"\n\tr = requests.get(URL, timeout=(2, 20), headers=headers)\n\tsoup = BeautifulSoup(r.text, 'html.parser')\n\n\tprint()\n\tprint(f'Status Code:  {r.status_code}')\t\t\t\t\t\t\t\t\t\t # Check connection to webpage\n\tprint(f'Page Title: {soup.title.text}')\t\t\t\t\t\t\t\t\t\t # Check it is the right webpage by displaying title\n\t# trades = soup.find_all('tr', {'class': 'p-selectable-row ng-star-inserted'})\t\n\ttrades = soup.find_all('tr', {'class': 'q-tr'})\t\t\t\t\t\t\t\t # Find HTML Element by class name\n\tprint(len(trades))\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  # Print number of times ^that^ element is on page \n\n\tfor item in trades:\n\t\ttry:\n\t\t\tprint()\n\t\t\tfullname = item.find('span', {'class': 'firstName'}).text + \" \" + item.find('span', {'class': 'lastName'}).text\n\t\t\tprint(fullname)\n\t\t\tparty = item.find('span', {'class': 'party'}).text\n\t\t\tprint(party)\n\t\t\tstate = item.find('span', {'class': 'us-state-compact'}).text\n\t\t\tprint(state)\n\t\t\tdate = item.find('span', {'class': 'format--date-iso'}).find('time')['title']\n\t\t\tprint(date)\n\t\texcept:\n\t\t\tpass\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "import datetime"]}], [{"term": "class", "name": "huffCrawler", "data": "class huffCrawler(object):\n\n\tdef __init__(self, starter_url):\n\t\tuser_agent = \"Amherst College SURF 2018, contact salfeld2018Amherst.edu with any questions.\"\n\t\topts = Options()\n\t\topts.add_argument(f\"user-agent={user_agent}\")\n\t\tself.driver = webdriver.Chrome(chrome_options=opts, executable_path='/Applications/chromedriver')\n\t\tself.base_url = starter_url\n\t\tself.driver.get(starter_url)\n\t\tself.links = []\n\n\tdef get_robo_link(self, link):\n\t\tif \".com/\" in link:\n\t\t\trobo_link = link.split('com')[0] + \"com/robots.txt\"\n\t\telif \".org/\" in link:\n\t\t\trobo_link = link.split('org')[0] + \"org/robots.txt\"\n\t\telif \".edu/\" in link:\n\t\t\trobo_link = link.split('edu')[0] + \"edu/robots.txt\"\n\t\telse:\n\t\t\trobo_link = \"\"\n\t\treturn robo_link\n\n\tdef check_links(self, links):\n\t\tchecked_links = []\n\t\trp = robotparser.RobotFileParser()\n\n\t\tfor l in links:\n\t\t\trp.set_url(self.get_robo_link(l))\n\t\t\trp.read()\n\t\t\tif rp.can_fetch(\"*\", l):\n\t\t\t\tchecked_links.append(l)\n\t\tself.links = self.links + checked_links\n\t\treturn checked_links\n\n\n\tdef get_huff_links(self):\n\t\tlinks = []\n\t\tcards = self.driver.find_elements_by_class_name(\"card__details\")\n\t\tfor card in cards:\n\t\t\ttry:\n\t\t\t\tlabel = card.find_element_by_class_name(\"card__label\").text\n\t\t\t\tprint(label)\n\t\t\t\tif label != \"HUFFPOST PERSONAL\" and label != \"VIDEOS\" and label != \"COMEDY\" and label != \"ENTERTAINMENT\" and label != \"OPINION\" and label != \"STYLE & BEAUTY\":\n\t\t\t\t\tarticle = card.find_element_by_class_name(\"card__headline\")\n\t\t\t\t\ta_tag = article.find_element_by_tag_name(\"a\")\n\t\t\t\t\thref = str(a_tag.get_attribute(\"href\"))\n\t\t\t\t\tif \"huffingtonpost\" in href and href not in self.links:\n\t\t\t\t\t\tlinks.append(href)\n\t\t\texcept NoSuchElementException:\n\t\t\t\tarticle = card.find_element_by_class_name(\"card__headline\")\n\t\t\t\ta_tag = article.find_element_by_tag_name(\"a\")\n\t\t\t\thref = str(a_tag.get_attribute(\"href\"))\n\t\t\t\tif \"huffingtonpost\" in href and href not in self.links:\n\t\t\t\t\tlinks.append(href)\n\t\treturn links\n\n\tdef get_huff_article(self, link):\n\t\tto_return = [None] * 3\n\t\tto_return[0] = link\n\t\ttry:\n\t\t\tself.driver.get(link)\n\t\texcept TimeoutException:\n\t\t\treturn to_return\n\n\t\t#get title\n\t\ttry:\n\t\t\theader = self.driver.find_element_by_class_name(\"headline__title\")\n\t\t\tto_return[1] = header.text\n\t\t\tprint(\"header \" + header.text)\n\t\texcept NoSuchElementException:\n\t\t\tto_return[1] = ''\n\t\t\tprint(\"WHYYYYYY\")\n\n\t\t#get content\n\t\ttry:\n\t\t\ttext = ''\n\t\t\tbody = self.driver.find_element_by_id(\"entry-text\")\n\t\t\td_tags = body.find_elements_by_tag_name(\"div\")\n\t\t\tfor div in d_tags:\n\t\t\t\ttry:\n\t\t\t\t\tp = div.find_element_by_tag_name(\"p\")\n\t\t\t\t\tif len(p.text) > 0:\n\t\t\t\t\t\ttext = text + p.text\n\t\t\t\texcept NoSuchElementException:\n\t\t\t\t\t continue\n\t\t\tto_return[2] = text\n\t\texcept NoSuchElementException:\n\t\t\tprint(\"NOOOOO\")\n\t\t\tto_return[2] = ''\n\t\treturn to_return\n\n\tdef get_huff_data(self, links):\n\t\tdata = []\n\t\tcount = 0\n\t\tfor link in links:\n\t\t\tprint(count)\n\t\t\tcontent = self.get_huff_article(link)\n\t\t\tif len(content[1]) > 0 and len(content[2]) > 0:\n\t\t\t\tdata.append(content)\n\t\t\tcount += 1\n\t\t\twebscrape.time.sleep(7)\n\t\treturn data\n\n\tdef one_section(self, url):\n\t\tself.driver.get(url)\n\t\tlinks = self.collect_links(url)\n\t\tprint(len(links))\n\n\t\tchecked_links = self.check_links(links)\n\t\tprint(len(checked_links))\n\n\t\tdata = self.get_huff_data(checked_links)\n\t\tprint(len(data))\n\t\tprint(data[1])\n\t\treturn data\n\n\tdef collect_links(self,url):\n\t\tlinks = []\n\t\tone = self.get_huff_links()\n\t\tfor a in one:\n\t\t\tif a not in self.links:\n\t\t\t\tlinks.append(a)\n\n\t\tfor i in range(2, 20):\n\t\t\twebscrape.time.sleep(7)\n\t\t\tl = url + (f\"?page={i}\")\n\t\t\ttry:\n\t\t\t\tself.driver.get(l)\n\t\t\texcept Exception:\n\t\t\t\tbreak\n\t\t\tlist = self.get_huff_links()\n\t\t\tfor link in list:\n\t\t\t\tif link not in self.links:\n\t\t\t\t\tlinks.append(link)\n\t\treturn links\n\n\n\tdef start(self):\n\t\tone = self.one_section(\"https://www.huffingtonpost.com/section/us-news\")\n\t\ttwo = self.one_section(\"https://www.huffingtonpost.com/section/world-news\")\n\t\tthree = self.one_section(\"https://www.huffingtonpost.com/section/business\")\n\t\tfour = self.one_section(\"https://www.huffingtonpost.com/section/politics\")\n\t\tfull_data = one + two + three + four\n\t\tprint(len(full_data))\n\t\treturn full_data\n\n\tdef get_master_links(self, url):\n\t\tself.driver.get(url)\n\t\tlinks = self.collect_links(url)\n\t\tchecked_links = self.check_links(links)\n\t\treturn checked_links\n\n\tdef pickle_links(self):\n\t\tone = self.get_master_links(\"https://www.huffingtonpost.com/section/us-news\")\n\t\tprint(\"length of one is \" + str(len(one)))\n\t\tp_one = open(\"./data/huff_us_links.pkl\", \"wb\")\n\t\tdesc1 = \"huff us links\"\n\t\tpickle.dump((one, desc1), p_one)\n\t\tp_one.close()\n\n\t\twebscrape.time.sleep(10)\n\t\ttwo = self.get_master_links(\"https://www.huffingtonpost.com/section/world-news\")\n\t\tprint(\"length of two is \" + str(len(two)))\n\t\tp_two = open(\"./data/huff_world_links.pkl\", \"wb\")\n\t\tdesc2 = \"huff world links\"\n\t\tpickle.dump((two, desc2), p_two)\n\t\tp_two.close()\n\n\t\twebscrape.time.sleep(10)\n\t\tthree = self.get_master_links(\"https://www.huffingtonpost.com/section/business\")\n\t\tprint(\"length of three is \" + str(len(three)))\n\t\tp_three = open(\"./data/huff_buisness_links.pkl\", \"wb\")\n\t\tdesc3 = \"huff buisness links\"\n\t\tpickle.dump((three, desc3), p_three)\n\t\tp_three.close()\n\n\t\twebscrape.time.sleep(10)\n\t\tfour = self.get_master_links(\"https://www.huffingtonpost.com/section/politics\")\n\t\tprint(\"length of four is \" + str(len(four)))\n\t\tp_four = open(\"./data/huff_politics_links.pkl\", \"wb\")\n\t\tdesc4 = \"huff politics links\"\n\t\tpickle.dump((four, desc4), p_four)\n\t\tp_four.close()\n\n\t\tfull_links = one + two + three + four\n\t\treturn full_links\n\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.common.exceptions import NoSuchElementException", "from selenium.common.exceptions import TimeoutException", "from urllib import robotparser", "import pickle", "import webscrape.time"]}], [], [{"term": "def", "name": "record_price", "data": "def record_price():\r\n\r\n\theaders = {\r\n\t\t\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",\r\n\t\t\"Accept-Encoding\": \"gzip, deflate\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\r\n\t\t\"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"\r\n\t}\r\n\r\n\tpage = requests.get(URL, headers=headers)\r\n\tsoup1 = BeautifulSoup(page.content, \"html.parser\")\r\n\tsoup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\r\n\ttitle = soup2.find(id = 'productTitle').get_text().strip()\r\n\tprice = float(soup2.find(class_ = 'a-offscreen').get_text().strip()[1:])\r\n\tdate = datetime.date.today()\r\n\tdata = [title, price, date]\r\n\r\n\twith open('Amazon_webscrape.csv', 'a+', newline='', encoding='UTF8') as f:\r\n\t\twriter = csv.writer(f)\r\n\t\twriter.writerow(data)\r\n\r\n\t\tdf = pd.read_csv(r'C:\\Users\\zio_p\\PycharmProjects\\pythonProject\\AtA Web Scraping (Amazon)\\Amazon_webscrape.csv')\r\n\t\tprint(df)\r\n\t\treturn price\r\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd\r", "from bs4 import BeautifulSoup\r", "import requests\r", "import time\r", "import datetime\r", "import csv\r", "import smtplib\r", "import os\r"]}, {"term": "def", "name": "send_mail", "data": "def send_mail():\r\n\tserver = smtplib.SMTP_SSL('smtp.gmail.com',465)\r\n\tserver.ehlo()\r\n\t# server.starttls()\r\n\tserver.ehlo()\r\n\tserver.login('francesco.tintori93@gmail.com', os.environ(\"PASSCODE\"))\r\n\r\n\tsubject = \"Price Drop!\"\r\n\tbody = f\"The item you have been tracking at {URL} has dropped in price!\"\r\n\r\n\tmsg = f\"Subject: {subject}\\n\\n{body}\"\r\n\r\n\tserver.sendmail(\r\n\t\t'francesco.tintori93@gmail.com',\r\n\t\t'francesco.tintori93@gmail.com',\r\n\t\tmsg\r\n\t)\r\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd\r", "from bs4 import BeautifulSoup\r", "import requests\r", "import time\r", "import datetime\r", "import csv\r", "import smtplib\r", "import os\r"]}], [], [{"term": "def", "name": "webscrape_vienna", "data": "def webscrape_vienna(path_download_dir, path_chromedriver):\n\t\"\"\"\n\tFunction to download files via webscrapping, using selenium and google's\n\tchromedriver, which can be downloaded here: https://chromedriver.chromium.org/\n\n\tHow to webscrape:\n\t0. Download all required packages (incl. selenium and chromedriver)\n\t1. Open the url with the download links\n\t2. right cklick on the first download link and select 'inspect'\n\t3. right click again on the higlighted entry in firefox developer mode and choose 'copy path' -> 'xpath'\n\t4. do the same for the last download link\n\n\tArgs:\n\t- url_page (str): address of webpage where files should be downloaded\n\t- path_download_dir (str): path to dir in which downloads should be saved\n\t- path_driver (str): path to folder in which geckodriver is stored\n\t- path_xpath1(str): xpath of file where download should start with\n\t- path_xpath2 (str): xpath of file where download should stop (all download links in between will be downloaded)\n\t- sleep_time (int): on some pages it takes some time to load all download links; here, a timer can be set to wait\n\n\tReturns:\n\t-\n\n\tLast update: 21/06/21. By Felix.\n\n\t\"\"\"\n\tprint('Starting to download')\n\n\t# Set options for webdriver\n\toptions = ChromeOptions()\n\toptions.add_argument(\"--headless\")\n\tprefs = {\"profile.default_content_settings.popups\": 0,\n\t\t\t \"download.default_directory\": path_download_dir,\n\t\t\t \"directory_upgrade\": True}\n\toptions.add_experimental_option(\"prefs\", prefs)\n\n\t# Run webdriver from executable path of your choice\n\tdriver = webdriver.Chrome(executable_path=path_chromedriver, options=options)\n\n\t# -----------------------------------------------------------------------------\n\n\t# Get web page\n\t# time.sleep(1)\n\ti = 0\n\t# loop through columns\n\tfor col in range(78, 136 + 1):\n\t\tif col < 100:\n\t\t\tstr_col = '0' + str(col)\n\t\telse:\n\t\t\tstr_col = str(col)\n\t\tfor row in range(62, 107 + 1):\n\t\t\t# loop through rows\n\t\t\tif col < 100:\n\t\t\t\tstr_row = '0' + str(row)\n\t\t\telse:\n\t\t\t\tstr_row = str(row)\n\t\t\tpart_a = 'https://www.wien.gv.at/ma41datenviewer/downloads/ma41/geodaten/lod2_gml/'\n\t\t\tpart_b = '_lod2_gml.zip'\n\t\t\tpath = part_a + str_col + str_row + part_b\n\t\t\tdriver.get(path)\n\t\t\ttime.sleep(1)\n\t\ttime.sleep(20)\n\n\t\tprint('Downloaded col: ', col)\n\n\t# Sleep for 5s to ensure that everythings loaded properly\n\ttime.sleep(200)\n\tdriver.close()\n\n\t# -----------------------------------------------------------------------------\n\tprint('-------------')\n\tprint('Download sucessfull!')\n\tprint('Files downloaded to ' + path_download_dir)\n\n", "description": "\n\tFunction to download files via webscrapping, using selenium and google's\n\tchromedriver, which can be downloaded here: https://chromedriver.chromium.org/\n\n\tHow to webscrape:\n\t0. Download all required packages (incl. selenium and chromedriver)\n\t1. Open the url with the download links\n\t2. right cklick on the first download link and select 'inspect'\n\t3. right click again on the higlighted entry in firefox developer mode and choose 'copy path' -> 'xpath'\n\t4. do the same for the last download link\n\n\tArgs:\n\t- url_page (str): address of webpage where files should be downloaded\n\t- path_download_dir (str): path to dir in which downloads should be saved\n\t- path_driver (str): path to folder in which geckodriver is stored\n\t- path_xpath1(str): xpath of file where download should start with\n\t- path_xpath2 (str): xpath of file where download should stop (all download links in between will be downloaded)\n\t- sleep_time (int): on some pages it takes some time to load all download links; here, a timer can be set to wait\n\n\tReturns:\n\t-\n\n\tLast update: 21/06/21. By Felix.\n\n\t", "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options as ChromeOptions", "import time"]}, {"term": "def", "name": "main", "data": "def main():\n\n\t# Paths for webscrapper\n\tdriver_dir = \"/Users/Felix Wagner/Documents/0_program/0_webscraper/chromedriver\"\n\t# Path of output dir\n\tdownload_dir = \"/Drive/Downloads/Vienna\"\n\n\t# Start to webscrape\n\twebscrape_vienna(download_dir, driver_dir)\n\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options as ChromeOptions", "import time"]}], [], [], [{"term": "class", "name": "classHoliday:\r", "data": "class Holiday:\r\n\tdef __init__(self, name, date):\r\n\t\tself.name = name\r\n\t\tself.date = date\r\n\tdef __str__ (self):\r\n\t\treturn self.name + ' ('+ self.date +')'\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "class", "name": "HolidayEncoder", "data": "class HolidayEncoder(json.JSONEncoder):\r\n\tdef default(self, o):\r\n\t\treturn o.__dict__\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "__init__", "data": "   def __init__(self):\r\n\t   self.holidayList = []\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "getHTML", "data": "def getHTML(url):\r\n\tresponse = requests.get(url)\r\n\treturn response.text\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "initialJSON", "data": "def initialJSON():\t\r\n\t#opens json file \r\n\tf = open('holidays.json')\r\n\t# returns JSON object as \r\n\t# a dictionary\r\n\tdata = json.loads(f.read())\r\n  \r\n\t# Iterating through the json\r\n\t# list\r\n\tglobal holidayList\r\n\tholidayList = []\r\n\tfor i in data['holidays']:\r\n\t\tholidays = Holiday(i['name'], i['date'])\r\n\t\tholidayList.append(holidays)\r\n\r\n\t# Closing file\r\n\tf.close()\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "fromJSON", "data": "def fromJSON():\t\r\n\t#opens json file \r\n\tf = open('holidayFiles.json')\r\n\t# returns JSON object as \r\n\t# a dictionary\r\n\tdata = json.loads(f.read())\r\n  \r\n\t# Iterating through the json\r\n\t# list\r\n\tglobal holidayList\r\n\tholidayList = []\r\n\tfor i in data:\r\n\t\tholidays = Holiday(i['name'], i['date'])\r\n\t\tholidayList.append(holidays)\r\n\r\n\t# Closing file\r\n\tf.close()\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "writeJSON", "data": "def writeJSON():\r\n\t#open('holidayFiles.json', 'w').close()\r\n\twith open('holidayFiles.json', 'w', encoding='utf-8') as f:\r\n\t\tjson.dump(holidayList, f, ensure_ascii=False, indent=4, cls=HolidayEncoder)\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "webScrape", "data": "def webScrape(link, year):\r\n\r\n\turl = getHTML(link)\r\n\tsoup = BeautifulSoup(url, 'html.parser')\r\n\r\n\tout = [[td.text.strip() for td in tr.select('th, td')] for tr in soup.select('tr[data-mask]')]\r\n\r\n\twith open('file.csv', 'w', newline='') as f_out:\r\n\t\twriter=csv.writer(f_out)\r\n\t\twriter.writerows(out)\r\n\r\n\twith open('file.csv', 'r') as file:\r\n\t\treader = csv.reader(file)\r\n\t\tfor row in reader:\r\n\t\t\tcsvDate = year + ' ' + row[0]\r\n\t\t\tdate = str(datetime.strptime(csvDate , '%Y %b %d').date())\r\n\t\t\tholidayCSV = Holiday(row[2], date)\r\n\t\t\tholidayList.append(holidayCSV)\r\n\t\t\r\n   \r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "removeDuplicates", "data": "def removeDuplicates():\r\n\tglobal holidayList\r\n\tholidayList = [*set(holidayList)]\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "findHoliday", "data": "def findHoliday(HolidayName, Date):\r\n\tsameNameList = [x for x in holidayList if x.name == HolidayName]\r\n\tsameBothList = [y for y in sameNameList if y.date == Date]\r\n\ti = 0\r\n\twhile i != len(sameBothList):\r\n\t\treturn sameBothList[i]\r\n\t\t# Find Holiday in holidayList\r\n\t\t# Return Holiday\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "addHoliday", "data": "def addHoliday():\r\n\tprint(\"Add a Holiday\")\r\n\tholidayInput = input(\"Holiday:\")\r\n\tdateInput = input(\"Date (YYYY-MM-DD):\")\r\n\ttry:\r\n\t\tdateTest = datetime.strptime(dateInput, '%Y-%m-%d')\r\n\texcept ValueError:\r\n\t\tprint(\"Incorrect Date Format Try Again\")\r\n\t\treturn addHoliday()\r\n\tholidayUser = Holiday(holidayInput, dateInput)\r\n\tholidayList.append(holidayUser)\r\n\tprint(\"Added \" + holidayInput)\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "removeHoliday", "data": "def removeHoliday():\r\n\tprint(\"Remove a Holiday\")\r\n\tholidayInput = input(\"Holiday:\")\r\n\tdateInput = input(\"Date (YYYY-MM-DD):\")\r\n\ttry:\r\n\t\tdateTest = datetime.strptime(dateInput, '%Y-%m-%d')\r\n\texcept ValueError:\r\n\t\tprint(\"Incorrect Date Format Try Again\")\r\n\t\treturn removeHoliday()\r\n\ttry:\r\n\t\tholidayList.remove(findHoliday(holidayInput, dateInput))\r\n\texcept ValueError:\r\n\t\tprint(\"Incorrect Holiday Name\")\r\n\t\treturn removeHoliday()\r\n\tprint(holidayInput + \" Removed\")\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}, {"term": "def", "name": "viewHoliday", "data": "def viewHoliday(yearUser, weekNumUser):\r\n\tif weekNumUser == '':\r\n\t\ttodayDate = date.today()\r\n\t\tweekNumUser = str(todayDate.isocalendar().week)\r\n\t\tprint(weekNumUser)\r\n\t\r\n\tviewHolidayList = []\r\n\tfor x in holidayList:\r\n\t\tif yearUser == str(datetime.strptime(x.date, '%Y-%m-%d').date().isocalendar().year):\r\n\t\t\tif weekNumUser == str(datetime.strptime(x.date, '%Y-%m-%d').date().isocalendar().week):\r\n\t\t\t\tviewHolidayList.append(x.name + ' (' + x.date + ')')\r\n\tprint(*viewHolidayList, sep = \"\\n\")\r\n\treturn viewHolidayList\t  \r\n\r\n\r\n \r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["from ast import And\r", "import json\r", "import gc\r", "from config import websiteLink2020\r", "from config import websiteLink2021\r", "from config import websiteLink2022\r", "from config import websiteLink2023\r", "from config import websiteLink2024\r", "from config import apiLink\r", "from datetime import date, datetime\r", "from bs4 import BeautifulSoup\r", "import requests\r", "from dataclasses import dataclass\r", "import csv\r", "from flask import Flask, render_template, request\r"]}], [{"term": "class", "name": "\tThisclassprovidesuserwithrawdatasavedonalocalormounteddrive", "data": "\tThis class provides user with raw data saved on a local or mounted drive\n", "description": null, "category": "webscraping", "imports": ["import json", "import os", "import re", "import time", "import urllib.request", "#import string", "#from urllib.parse import urljoin", "import pdfplumber", "from selenium import webdriver", "from selenium.webdriver.chrome.service import Service"]}, {"term": "class", "name": "classScrape:", "data": "class Scrape:\n\t\"\"\"\n\tScrape functions for Nevada State Legislature website\n\t\"\"\"\n\n\tdef nv_scrape_pdf(webscrape_links, dir_chrome_webdriver, dir_save):\n\t\t\"\"\"\n\t\tWebscrape function for Nevada State Legislature Website.\n\n\t\tParameters\n\t\t----------\n\t\twebscrape_links : List\n\t\t\tList of direct link(s) to NV committee webpage.\n\t\t\tsee assets/weblinks/nv_weblinks.py for lists organized by chamber and committee\n\t\tdir_chrome_webdriver : String\n\t\t\tLocal directory that contains the appropriate Chrome Webdriver.\n\t\tdir_save : String\n\t\t\tLocal directory to save PDFs.\n\n\t\tReturns\n\t\t-------\n\t\tAll PDF files found on the webscrape_links, saved on local dir_save.\n\n\t\t\"\"\"\n\n\t\tif not isinstance(webscrape_links, list):\n\t\t\traise ValueError(\"webscrape_links must be a list\")\n\t\telse:\n\t\t\tpass\n\n\t\tif not os.path.exists(dir_chrome_webdriver):\n\t\t\traise ValueError(\"Chrome Webdriver not found\")\n\t\telse:\n\t\t\tpass\n\n\t\tif not os.path.exists(dir_save):\n\t\t\traise ValueError(\"Save directory not found\")\n\t\telse:\n\t\t\tpass\n\n\t\tfor link_index in range(len(webscrape_links)):\n\n\t\t\tservice = Service(dir_chrome_webdriver)\n\t\t\toptions = webdriver.ChromeOptions()\n\t\t\t# Chrome runs headless, \n\t\t\t# comment out \"options.add_argument('headless')\"\n\t\t\t# to see the action\n\t\t\toptions.add_argument('headless')\n\t\t\tdriver = webdriver.Chrome(service=service, options=options)\n\n\t\t\ttime.sleep(5)\n\t\t\tdriver.get(webscrape_links[link_index])\n\t\t\ttime.sleep(5)\n\n\t\t\tarrow01 = driver.find_element_by_id('divCommitteePageMeetings')\n\t\t\tarrow01.click()\n\t\t\ttime.sleep(5)\n\n\t\t\tarrow02 = driver.find_element_by_id('divMeetings')\n\t\t\tarrow02.click()\n\n\t\t\turl = driver.page_source\n\t\t\tregex_pattern = r'https.*Minutes.*\\.pdf'\n\t\t\tlines = url.split()\n\t\t\tmeeting_regex = re.compile(regex_pattern)\n\t\t\tall_files = []\n\n\t\t\tfor line in lines:\n\t\t\t\thit = meeting_regex.findall(line)\n\t\t\t\tif hit:\n\t\t\t\t\tall_files.extend(hit)\n\n\t\t\tfor filename in all_files:\n\t\t\t\tprint(filename)\n\n\t\t\tfolder_location = dir_save\n\n\t\t\tfor link in all_files:\n\t\t\t\tfilename = os.path.join(\n\t\t\t\t\tfolder_location, \"_\".join(link.split('/')[4:]))\n\t\t\t\turllib.request.urlretrieve(link, filename)\n\t\t\ttime.sleep(15)\n\n\t\t\tdriver.close()\n\n", "description": "\n\tScrape functions for Nevada State Legislature website\n\t", "category": "webscraping", "imports": ["import json", "import os", "import re", "import time", "import urllib.request", "#import string", "#from urllib.parse import urljoin", "import pdfplumber", "from selenium import webdriver", "from selenium.webdriver.chrome.service import Service"]}, {"term": "class", "name": "classProcess:", "data": "class Process:\n\t\"\"\"\n\tProcess functions for PDF transcripts scraped from Nevada State Legislature website\n\t\"\"\"\n\n\tdef nv_pdf_to_text(dir_load, nv_json_name):\n\t\t\"\"\"\n\t\tConvert all PDFs to a dictionary and then saved locally as a JSON file.\n\n\t\tParameters\n\t\t----------\n\t\tdir_load : String\n\t\t\tLocal location of the directory holding PDFs.\n\t\tnv_json_name : String\n\t\t\tJSON file name, include full local path.\n\n\t\tReturns\n\t\t-------\n\t\tA single JSON file, can be loaded as dictionary to work with.\n\n\t\t\"\"\"\n\t\tdirectory = dir_load\n\t\tn = 0\n\t\tcommittee = {}\n\n\t\tfile_list = sorted(os.listdir(directory))\n\t\tdel file_list[0]\n\n\t\tfor file in file_list:\n\t\t\tfilename = directory + file\n\t\t\tall_text = ''\n\t\t\twith pdfplumber.open(filename) as pdf:\n\t\t\t\tfor pdf_page in pdf.pages:\n\t\t\t\t\tsingle_page_text = pdf_page.extract_text()\n\t\t\t\t\tall_text = all_text + '\\n' + single_page_text\n\t\t\t\t\tcommittee[n] = all_text\n\t\t\tn = n + 1\n\n\t\twith open(nv_json_name, 'w') as file:\n\t\t\tjson.dump(committee, file, ensure_ascii=False)\n\n\tdef nv_text_clean(nv_json_path, trim=None):\n\t\t\"\"\"\n\t\tLoads JSON into environment as dictionary\n\t\tPreprocesses the raw PDF export from previously generated json\n\t\tOptional: Trims transcript to exclude list of those present\n\t\tand signature page/list of exhibits\n\n\t\tParameters\n\t\t----------\n\t\tnv_json_path : String\n\t\t\tLocal path of nv_json generated by nv_pdf_to_text.\n\t\ttrim: True/Default(None)\n\t\t\tProvides option to trim transcript to spoken section and transcriber notes\n\n\t\tReturns\n\t\t-------\n\t\tCleaned dictionary that excludes PDF formatting and (optional) front and back end\n\n\t\t\"\"\"\n\n\t\tfile_path = open(nv_json_path,)\n\t\tdata = json.load(file_path)\n\n\t\tif trim:\n\t\t\tfor key in data:\n\t\t\t\tif isinstance(data[key], str):\n\t\t\t\t\ttranscript = data[key]\n\t\t\t\t\tstart_location = re.search(\n\t\t\t\t\t\tr\"(CHAIR.*[A-z]\\:|Chair.*[A-z]\\:)\", transcript).start()\n\t\t\t\t\t# Starts transcript from when Chair first speaks\n\t\t\t\t\ttranscript = transcript[start_location:]\n\t\t\t\t\t# Removes signature page after submission (RESPECTFULLY\n\t\t\t\t\t# SUBMITTED)\n\t\t\t\t\tend_location = re.search(\n\t\t\t\t\t\tr\"(Respectfully\\sSUBMITTED\\:|RESPECTFULLY\\sSUBMITTED\\:|RESPECTFULLY\\sSUBMITTED)\",\n\t\t\t\t\t\ttranscript)\n\t\t\t\t\tif end_location:\n\t\t\t\t\t\tend_location_num = end_location.start()\n\t\t\t\t\t\ttranscript = transcript[:end_location_num]\n\t\t\t\t\ttranscript = re.sub(\n\t\t\t\t\t\tr\"Page\\s[0-9]{1,}\", \"\", transcript)  # Removes page number\n\t\t\t\t\ttranscript = re.sub(r\"\\n\", \"\", transcript)\n\t\t\t\t\ttranscript = transcript.strip()\n\t\t\t\t\ttranscript = \" \".join(transcript.split())\n\t\t\t\t\tdata[key] = transcript\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Incompatible File\")\n\n\t\t\treturn data\n\n\t\telse:\n\t\t\tfor key in data:\n\t\t\t\tif isinstance(data[key], str):\n\t\t\t\t\ttranscript = data[key]\n\t\t\t\t\ttranscript = re.sub(r\"Page\\s[0-9]{1,}\", \"\", transcript)\n\t\t\t\t\ttranscript = re.sub(r\"\\n\", \"\", transcript)\n\t\t\t\t\ttranscript = transcript.strip()\n\t\t\t\t\ttranscript = \" \".join(transcript.split())\n\t\t\t\t\tdata[key] = transcript\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Incompatible File\")\n\n\t\t\treturn data\n", "description": "\n\tProcess functions for PDF transcripts scraped from Nevada State Legislature website\n\t", "category": "webscraping", "imports": ["import json", "import os", "import re", "import time", "import urllib.request", "#import string", "#from urllib.parse import urljoin", "import pdfplumber", "from selenium import webdriver", "from selenium.webdriver.chrome.service import Service"]}], [{"term": "def", "name": "build_mapper", "data": "def build_mapper():\n\tmapper = {}\n\ttxtfiles, headers = webscrape(\"http://www.eecs70.org/\")\n\tfor i in range(len(txtfiles)):\n\t\twith open(txtfiles[i], encoding=\"utf8\") as f:\n\t\t\ttext = f.read().replace('\\n', '')\n\t\t\tkeywords = get_keywords(text)\n\t\t\tmapper[headers[i]] = keywords\n\treturn mapper\n", "description": null, "category": "webscraping", "imports": ["from data_extraction.webscrape import webscrape", "from topic_relationships.keywords import get_keywords"]}], [{"term": "def", "name": "welcome", "data": "def welcome():\n\n\treturn render_template(\"index.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "scrape", "data": "def scrape():\n\t# Run the scrape function\n\trenewable_data = renewable_scrape.renewable_scrape()\n\n\t# Update the Mongo database using update and upsert=True\n\tmongo.db.renewables.replace_one({}, renewable_data, upsert=True)\n\treturn redirect(\"/webscrape_sunburst\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "hydro", "data": "def hydro():\n\n\treturn render_template(\"hydro.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "wind", "data": "def wind():\n\n\treturn render_template(\"wind.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "heatmap", "data": "def heatmap():\n\n\treturn render_template(\"heatmap.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "solar", "data": "def solar():\n\n\treturn render_template(\"solar.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "location", "data": "def location():\n\n\treturn render_template(\"location.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "webscrape_sunburst", "data": "def webscrape_sunburst():\n\n\t#Take one instance from the Mongo DB\n\tdata = mongo.db.renewables.find_one()\n\n\treturn render_template(\"Webscrape_sunburst.html\",r_last_refresh=data[\"renewable_refresh\"],renewable_title_0=data[\"renewable_titles\"][0],renewable_link_0=data[\"renewable_links\"][0],renewable_title_1=data[\"renewable_titles\"][1],renewable_link_1=data[\"renewable_links\"][2], renewable_title_2 = data[\"renewable_titles\"][2],renewable_link_2=data[\"renewable_links\"][4],renewable_title_3=data[\"renewable_titles\"][3],renewable_link_3=data[\"renewable_links\"][6])\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "heatmapgeojson", "data": "def heatmapgeojson():\n\treturn jsonify(data = heatmapdata)\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "data", "data": "def data():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"data.html\")\n\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}], [{"term": "def", "name": "flip_scrap", "data": "def flip_scrap(url):\n\tuser_agent = {'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'}\n\tpage = requests.get(url,headers=user_agent)\n\tsoup=BeautifulSoup(page.content,\"lxml\")\n\tloc=soup.find_all('div',{'class':\"_3pLy-c row\"})\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "amazon_scrap", "data": "def amazon_scrap(url):\n\tuser_agent = {'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'}\n\tpage = requests.get(url,headers=user_agent)\n\tsoup=BeautifulSoup(page.content,\"lxml\")\n\tlocation=soup.find_all(\"div\", {\"class\":\"sg-col sg-col-4-of-12 sg-col-8-of-16 sg-col-12-of-20\"})\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "download", "data": "def download(mobile_name,df):\n\tresponse =make_response(df.to_csv())\n\tresponse.headers[\"Content-Disposition\"] = \"attachment; filename=\"+mobile_name+ '.csv'\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "home", "data": "def home():\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "price", "data": "def price():\n\treturn render_template('price.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "single_pred", "data": "def single_pred():\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "flipkart_webscrape", "data": "def flipkart_webscrape(mobile_name,scrap_pages):\n\tfliploc=[]\n\tfor j in range(scrap_pages):\n\t\tURL = \"https://www.flipkart.com/search?q=\"+mobile_name+\"&page=\"\n\t\tflip_location = flip_scrap(f'{URL}{str(j)}')\n\t\twhile(len(flip_location)==0):\n\t\t\tflip_location = flip_scrap(f'{URL}{str(j)}')\n\t\tfliploc.append(flip_location)\n\tflipkart=[]\n\tfor flip_product in fliploc:\n\t\tfor flip_product_src in flip_product:\n\t\t\tflip_specification=flip_product_src.find(\"li\",{\"class\":\"rgWa7D\"}).text\n\t\t\tflip_mobile = flip_product_src.find('div',{\"class\":\"_4rR01T\"}).text\n\t\t\tflip_mobilename=flip_mobile.split('(')[0]\n\t\t\tif 'Power Bank ' in  flip_mobilename:\n\t\t\t\tcontinue\n\t\t\ttry:  \n\t\t\t\tflip_mobcolor=flip_mobile.split('(')[1].split(',')[0]\n\t\t\t\tflip_ram=re.search(r'\\d+',(flip_specification.split('|')[0])).group()\n\t\t\t\tflip_storage=re.search(r'\\d+',(flip_specification.split('|')[1])).group()\n\t\t\t\n\t\t\texcept(IndexError,AttributeError):\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tflip_price=flip_product_src.find(\"div\", {\"class\":\"_3tbKJL\"}).text.split(\"\u20b9\")[1].strip()\n\t\t\texcept(IndexError,AttributeError):\n\t\t\t\tcontinue\n\t\t\tfill_flip=[]\n\t\t\tfill_flip.append(flip_mobilename.upper().strip())\n\t\t\tfill_flip.append(flip_mobcolor.upper().strip())\n\t\t\tfill_flip.append(flip_ram+'GB'.strip())\n\t\t\tfill_flip.append(flip_storage+'GB'.strip())\n\t\t\tfill_flip.append(int(flip_price.replace(',','').strip()))\n\t\t\tflipkart.append(fill_flip)\n\t\t\tdf_flipkart = pd.DataFrame(flipkart)\n\t\t\tdf_flipkart.columns = [\"Mobile\",\"Colour\",\"Ram\",\"Storage\",\"Flipkart_Price\"]\n\t\t\tdf_flipkart.drop_duplicates(inplace=True)\n\t\t\tflipkart_datframe=df_flipkart\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "scrap", "data": "def scrap():\n\tif request.method == \"POST\":\n\t\tmobile_name = request.form.get(\"mobile_name\")\n\t\tscrap_pages=request.form.get('scrap_pages')\n\t\tscrap_pages=int(scrap_pages)\n\t\t\n\tdf_flipkart=flipkart_webscrape(mobile_name,scrap_pages)\n\t#print(df_flipkart)\n\t#myData = list(flipkart_datframe.values)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "amazon_webscrape", "data": "def amazon_webscrape(amazon_mobile_name,amazon_scrap_pages):\n\tloc=[]\n\tfor j in range(amazon_scrap_pages):\n\t\tURL = \"https://www.amazon.in/s?k=\" + amazon_mobile_name.lower() + \"&page=\"\n\t\tlocation = amazon_scrap(f'{URL}{str(j)}')\n\t\twhile(len(location)==0):\n\t\t\tlocation = amazon_scrap(f'{URL}{str(j)}')\n\t\tloc.append(location)\n\tamazon=[]\n\tfor product_name in loc:\n\t\tfor product_src in product_name:\n\t\t\t\n\t\t\tspecification_amazon=product_src.find(\"span\", {\"class\":\"a-size-medium a-color-base a-text-normal\"}).text.split(\")\")[0]\n\t\t\tmobile_name=specification_amazon.split('(')[0].strip()\n\t\t\tif amazon_mobile_name.upper() not in mobile_name.upper() :\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tram_spec=re.search(r'\\d+',specification_amazon.split('(')[1].split(',')[1]).group()\n\t\t\t\tstorage_spec=re.search(r'\\d+',specification_amazon.split('(')[1].split(',')[2].replace('Storage','ROM').strip()).group()\n\t\t\t\tmob_color=specification_amazon.split('(')[1].split(',')[0]\n\t\t\texcept(IndexError,AttributeError):\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tamazon_price = product_src.find(\"span\", {\"class\":\"a-offscreen\"}).text.replace(\"\u20b9\",\"\")\n\t\t\texcept(AttributeError):\n\t\t\t\tcontinue\n\t\t\tamazon_mobile=[]\n\t\t\tamazon_mobile.append(mobile_name.upper().strip())\n\t\t\tamazon_mobile.append(mob_color.upper().strip())\n\t\t\tamazon_mobile.append(str(ram_spec)+'GB'.strip())\n\t\t\tamazon_mobile.append(str(storage_spec)+'GB'.strip())\n\t\t\tamazon_mobile.append(int(amazon_price.replace(',','').strip()))\n\t\t\tamazon.append(amazon_mobile)\n\t\t\tdf_amazon = pd.DataFrame(amazon)\n\t\t\tdf_amazon.columns = [\"Mobile\",\"Colour\",\"Ram\",\"Storage\",\"Amazon_Price\"]\n\t\t\tdf_amazon.drop_duplicates(inplace=True)\n\t\t\tamazon_dataframe=df_amazon\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "amazon", "data": "def amazon():\n\tif request.method == \"POST\":\n\t\tamazon_mobile_name = request.form.get(\"amazon_mobile\")\n\t\tamazon_scrap_pages=request.form.get('amazon_scrap_pages')\n\t\tamazon_scrap_pages=int(amazon_scrap_pages)\n\tdf_amazon=amazon_webscrape(amazon_mobile_name,amazon_scrap_pages)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "compare", "data": "def compare():\n\tif request.method == \"POST\":\n\t\tonline_mobile = request.form.get(\"mobile_name\")\n\t\tonline_scrap_pages=request.form.get('mobile_pages')\n\t\tdf_amazon=amazon_webscrape(online_mobile,int(online_scrap_pages))\n\t\tdf_flipkart=flipkart_webscrape(online_mobile,int(online_scrap_pages))\n\tresult = pd.merge(df_amazon, df_flipkart,how='inner')\n\tresult[\"Price_Difference\"] = abs(result[\"Flipkart_Price\"]-result[\"Amazon_Price\"])\n\t#print(result)\n\t#result.to_csv(\"iqoo.csv\", index=False)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "single", "data": "def single():\n\tif request.method == \"POST\":\n\t\tsingle_mobile = request.form.get(\"single_mobile_name\")\n\t\tsingle_color=request.form.get('single_mobile_color')\n\t\tsingle_ram=request.form.get('single_mobile_ram')+'GB'\n\t\tsingle_storage=request.form.get('single_mobile_storage')+'GB'\n\t\t# int_features = [str(x) for x in request.form.values()]\n\t\tdf = pd.read_csv(\"vivo.csv\")\n\t\t# final_features = [np.array(int_features)]\n\t\t# boolean_series = df['Mobile','Color','Ram','Storage'].isin(final_features)\n\t\t# filtered_df = df[boolean_series]\n\t\tdf2 = df[(df['Mobile'] == single_mobile.upper()) & (df['Colour'] == single_color.upper()) & (df['Ram'] == single_ram) & (df['Storage'] == single_storage)]\n\t\tres_pric_flipcart = df2['Flipkart_Price'].to_string(index=False)\n\t\tres_pric_amazon = df2['Amazon_Price'].to_string(index=False)\n\t\t#return render_template('single_pred.html',result={res_pric_amazon})\n\t\tif(int(res_pric_flipcart) == int(res_pric_amazon )):\n\t\t\t return render_template('single_pred.html', text_result='You can prefer both Amazon or Flipkart', result='Amazon Price : '+(res_pric_amazon),result1='Flipkart Price :'+(res_pric_flipcart))\n\t\tif (int(res_pric_flipcart) > int(res_pric_amazon )):\n\t\t\treturn render_template('single_pred.html', text_result='You can prefer  Amazon ', result='Amazon Price : '+(res_pric_amazon),result1='Flipkart Price :'+(res_pric_flipcart))\n\t\telse:\n\t\t\treturn render_template('single_pred.html', text_result='You can prefer Flipkart ', result='Amazon Price : '+(res_pric_amazon),result1='Flipkart Price :'+(res_pric_flipcart))\n\n\tprint(res_pric_flipcart)  \n\tprint(res_pric_amazon)  \n\n\t#print(final_features)\n\n\t\t\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, request, jsonify, render_template ,make_response", "import numpy as np", "import requests", "from datetime import datetime", "from datetime import date", "import csv", "import pandas as pd", "import flask_excel as excel", "import os", "import re", "from bs4 import BeautifulSoup"]}], [{"term": "def", "name": "request", "data": "def request(url):\n\tdriver.get(url)\n\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import selenium", "import urllib.parse", "import time", "import os", "from video_class import YtVideo"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(search: bool):\n\ttitles_array = []\n\turls_array = []\n\tcreators_array = []\n\n\t# get creators\n\tcreators = driver.find_elements_by_xpath(\n\t\t\"//a[@class='yt-simple-endpoint style-scope yt-formatted-string']\")\n\ti = 0\n\tfor creator in creators:\n\t\ti += 1\n\t\tif search == True:\n\t\t\t#print(\"we're searching\")\n\t\t\tif (i % 2) != 0:\n\t\t\t\t# print(creator.text)\n\t\t\t\tcreators_array.append(creator.text)\n\t\telse:\n\t\t\t# print(creator.text)\n\t\t\tcreators_array.append(creator.text)\n\n\turls = \"\"\n\n\t# get urls and titles\n\tif not search:\n\t\turls = driver.find_elements_by_id(\"video-title-link\")\n\telse:\n\t\turls = driver.find_elements_by_id(\"video-title\")\n\tfor url in urls:\n\t\turls_array.append(url.get_attribute(\"href\"))\n\t\ttitles_array.append(url.get_attribute(\"title\"))\n\n\t# for some reason, the first index of the array, should be the last, so i remove it and append it immediatly\n\ttitles_array.append(titles_array.pop(0))\n\tcreators_array.append(creators_array.pop(0))\n\turls_array.append(urls_array.pop(0))\n\n\t# Generate return value\n\treturn_value = []\n\tfor i in range(len(titles_array)):\n\t\tif i >= 20:\n\t\t\tbreak\n\t\ttry:\n\t\t\treturn_value.append(\n\t\t\t\tYtVideo(titles_array[i - 1], creators_array[i - 1], urls_array[i - 1]))\n\t\texcept:\n\t\t\tbreak\n\n\treturn return_value\n\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import selenium", "import urllib.parse", "import time", "import os", "from video_class import YtVideo"]}, {"term": "def", "name": "get_main_page", "data": "def get_main_page():\n\n\trequest('https://www.youtube.com/')\n\n\treturn webscrape(False)\n\t#driver.\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import selenium", "import urllib.parse", "import time", "import os", "from video_class import YtVideo"]}, {"term": "def", "name": "search", "data": "def search(text):\n\trequest(\"https://www.youtube.com/results?search_query=\" +\n\t\t\turllib.parse.quote(text))\n\n\tWebDriverWait(driver, 10).until(\n\t\tEC.element_to_be_clickable(\n\t\t\t(By.XPATH, \"//*[@id=\\\"logo-icon-container\\\"]\"))\n\t)\n\ttry:\n\t\tdriver.find_element_by_xpath(\n\t\t\t\"/html/body/ytd-app/div/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-background-promo-renderer\")\n\texcept:\n\t\ttime.sleep(0.3)\n\t\treturn webscrape(True)\n\telse:\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import selenium", "import urllib.parse", "import time", "import os", "from video_class import YtVideo"]}, {"term": "def", "name": "close_driver", "data": "def close_driver():\n\tdriver.quit()\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import selenium", "import urllib.parse", "import time", "import os", "from video_class import YtVideo"]}], [], [], [{"term": "def", "name": "webscrape", "data": "def webscrape(URL, string):\n\tpstring = processCD(string)\n\tpage = requests.get(URL)\n\tsoup = BeautifulSoup(page.content, 'html.parser')\n\tresults = soup.find(id=pstring[0])\n\tresults = results.find_all(pstring[1], class_=pstring[2])\n\treturn results\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup #install python-beautifulsoup"]}, {"term": "def", "name": "parseContainer", "data": "def parseContainer(containerList, string):\n\tpstring = processCED(string)\n\tlst = []\n\tfor elem in containerList:\n\t\tobj = {}\n\t\tfor i, cat in enumerate(pstring[1]):\n\t\t\titem = elem.find(pstring[0][i],class_=cat)\n\t\t\tif item == None: continue;\n\t\t\titem = (elem.find(pstring[0][i],class_=cat)).text\n\t\t\tobj[cat] = item.strip()\n\t\tif obj:\n\t\t\tlst.append(obj.copy())\n\treturn lst, {\"categories\": pstring[1]}\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup #install python-beautifulsoup"]}, {"term": "def", "name": "processCD", "data": "def processCD(string):\n\tpstring = string.split(\",\")\n\treturn pstring\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup #install python-beautifulsoup"]}, {"term": "def", "name": "processCED", "data": "def processCED(string):\n\tpstring = string.split(\"],[\")\n\tfor i, elem in enumerate(pstring):\n\t\tpstring[i] = elem.replace(\"[\",\"\").replace(\"]\",\"\")\n\t\tpstring[i] = pstring[i].split(\",\")\n\treturn pstring\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup #install python-beautifulsoup"]}, {"term": "def", "name": "main", "data": "def main():\n\t#url = \"https://www.monster.com/jobs/search/?q=Software-Developer&where=Seattle\"\n\t#cd = \"SearchResults,section,card-content\"\n\t#ced = \"[h2,div,div],[title,company,location]\"\n\turl = sys.argv[1]\n\tcd = sys.argv[2]\n\tced = sys.argv[3]\n\tresults = webscrape(url,cd)\n\tobjects, categories = parseContainer(results,ced)\n\tjsonCategories = json.dumps(categories)\n\tjsonObject = json.dumps(objects)\n\tprint(jsonCategories)\n\tprint(jsonObject)\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup #install python-beautifulsoup"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(URL, string):\n\tpstring = processCD(string)\n\tpage = requests.get(URL)\n\tsoup = BeautifulSoup(page.content, 'html.parser')\n\tresults = soup.find(id=pstring[0])\n\tresults = results.find_all(pstring[1], class_=pstring[2])\n\treturn results\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup", "import zmq"]}, {"term": "def", "name": "parseContainer", "data": "def parseContainer(containerList, string):\n\tpstring = processCED(string)\n\tlst = []\n\tfor elem in containerList:\n\t\tobj = {}\n\t\tfor i, cat in enumerate(pstring[1]):\n\t\t\titem = elem.find(pstring[0][i],class_=cat)\n\t\t\tif item == None: continue;\n\t\t\titem = (elem.find(pstring[0][i],class_=cat)).text\n\t\t\tobj[cat] = item.strip()\n\t\tif obj:\n\t\t\tlst.append(obj.copy())\n\treturn lst, {\"categories\": pstring[1]}\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup", "import zmq"]}, {"term": "def", "name": "processCD", "data": "def processCD(string):\n\tpstring = string.split(\",\")\n\treturn pstring\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup", "import zmq"]}, {"term": "def", "name": "processCED", "data": "def processCED(string):\n\tpstring = string.split(\"],[\")\n\tfor i, elem in enumerate(pstring):\n\t\tpstring[i] = elem.replace(\"[\",\"\").replace(\"]\",\"\")\n\t\tpstring[i] = pstring[i].split(\",\")\n\treturn pstring\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup", "import zmq"]}, {"term": "def", "name": "main", "data": "def main():\n\t#url = \"https://www.monster.com/jobs/search/?q=Software-Developer&where=Seattle\"\n\t#cd = \"SearchResults,section,card-content\"\n\t#ced = \"[h2,div,div],[title,company,location]\"\n\turl = sys.argv[1]\n\tcd = sys.argv[2]\n\tced = sys.argv[3]\n\tresults = webscrape(url,cd)\n\tobjects, categories = parseContainer(results,ced)\n\tjsonCategories = json.dumps(categories)\n\tjsonObject = json.dumps(objects)\n\tprint(jsonCategories)\n\tprint(jsonObject)\n", "description": null, "category": "webscraping", "imports": ["import sys ", "import requests", "import json", "from bs4 import BeautifulSoup", "import zmq"]}], [{"term": "def", "name": "price_reduction", "data": "def price_reduction(investment, profit):\r\n\tinvestment = int(investment)\r\n\tprofit = int(profit)\r\n\tratio = (investment/profit)*100\r\n\tif(ratio>=90):\r\n\t\treturn 7\r\n\telif(ratio<90 and ratio>=70):\r\n\t\treturn 5\r\n\telif(ratio<70 and ratio>=50):\r\n\t\treturn 4\r\n\telif(ratio<50 and ratio>=20):\r\n\t\treturn 2\r\n\telse:\r\n\t\treturn 1\r\n", "description": null, "category": "webscraping", "imports": ["import sqlite3\r", "import webscrape\r"]}, {"term": "def", "name": "calculate_price", "data": "def calculate_price(ratio, location, crop):\r\n\tif(location=='Bijapur'):\r\n\t\tbase = bij[crop]\r\n\t\tfinal = base - ((base*ratio)/100)\r\n\t\treturn final\r\n\telif(location=='Udupi'):\r\n\t\tbase = ud[crop]\r\n\t\tfinal = base - ((base*ratio)/100)\r\n\t\treturn final\r\n\telse:\r\n\t\tbase = bang[crop]\r\n\t\tfinal = base - ((base * ratio) / 100)\r\n\t\treturn final\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import sqlite3\r", "import webscrape\r"]}, {"term": "def", "name": "bijapur", "data": "def bijapur():\r\n\tconn = sqlite3.connect('crop.db')\r\n\tc = conn.cursor()\r\n\tc.execute(\"SELECT id, crop, investment, profit FROM bijapur\")\r\n\tfor i in c.fetchall():\r\n\t\tindex = i[0]\r\n\t\tratio = price_reduction(i[2],i[3])\r\n\t\tfinal_price = calculate_price(ratio, 'Bijapur', i[1])\r\n\t\tc.execute(\"UPDATE bijapur SET price= ? WHERE id= ?\",(final_price,index))\r\n\tconn.commit()\r\n\tconn.close()\r\n", "description": null, "category": "webscraping", "imports": ["import sqlite3\r", "import webscrape\r"]}, {"term": "def", "name": "udupi", "data": "def udupi():\r\n\tconn = sqlite3.connect('crop.db')\r\n\tc = conn.cursor()\r\n\tc.execute(\"SELECT id, crop, investment, profit FROM udupi\")\r\n\tfor i in c.fetchall():\r\n\t\tindex = i[0]\r\n\t\tratio = price_reduction(i[2],i[3])\r\n\t\tfinal_price = calculate_price(ratio, 'Udupi', i[1])\r\n\t\tc.execute(\"UPDATE udupi SET price= ? WHERE id= ?\", (final_price, index))\r\n\tconn.commit()\r\n\tconn.close()\r\n", "description": null, "category": "webscraping", "imports": ["import sqlite3\r", "import webscrape\r"]}, {"term": "def", "name": "bangalore", "data": "def bangalore():\r\n\tconn = sqlite3.connect('crop.db')\r\n\tc = conn.cursor()\r\n\tc.execute(\"SELECT id, crop, investment, profit FROM bangalore\")\r\n\tfor i in c.fetchall():\r\n\t\tindex = i[0]\r\n\t\tratio = price_reduction(i[2],i[3])\r\n\t\tfinal_price = calculate_price(ratio, 'Bangalore rural', i[1])\r\n\t\tc.execute(\"UPDATE bangalore SET price= ? WHERE id= ?\", (final_price, index))\r\n\tconn.commit()\r\n\tconn.close()\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import sqlite3\r", "import webscrape\r"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(url):\n\t# query website and return html\n\tdata = urllib2.urlopen(url)\n\n\t# parse the html using beautiful soup and store in variable \n\tsoup = str(BeautifulSoup(data, 'html.parser'))\n\tcampsite_json = json.loads(soup)\n\n\t# iterate through each campsite to check for availabilities\n\tavailable_campsites = []\n\tfor campsite in campsite_json['campsites']:\n\t\tdays = campsite_json['campsites'][campsite][\"availabilities\"]\n\t\tavailabilities = []\n\t\tfor key,value in days.items():\n\t\t\tif value != None:\n\t\t\t\tavailabilities.append(value)\n\t\tif not availabilities:\n\t\t\t#print(\"List is empty.\")\n\t\t\tpass\n\t\telse:\n\t\t\t#print(availabilities)\n\t\t\t#print(all(dawgie == 'Open' or 'Available' for dawgie in availabilities))\n\t\t\tif all(dawgie == 'Open' or 'Available' for dawgie in availabilities):\n\t\t\t\t#print(\"THERE IS AN AVAILABLE CAMPSITE!\")\n\t\t\t\tavailable_campsites.append(campsite)\n\t\t\telse:\n\t\t\t\t#print(\"NO CAMPSITES AVAILABLE.\")\n\t\t\t\tcontinue\n\t\t#print(\"------------------------------------------------------------------------\")\n\t#print(\"AVAILABLE CAMPSITES:\" + str(available_campsites))\n\treturn available_campsites\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "from textmagic.rest import TextmagicRestClient", "import requests, json, urllib2"]}], [{"term": "def", "name": "get_winner", "data": "def get_winner(tracker, team1, team2):\n\tif(tracker[team1] > tracker[team2]):\n\t\treturn team1\n\telse:\n\t\treturn team2\n", "description": null, "category": "webscraping", "imports": ["import flask", "from flask import request, jsonify", "from webscrape_turnover import get_teams_turnover", "from webscrape_point_diff import get_teams_point_diff", "from webscrape_OYG import get_teams_OYG", "from teams import teams"]}, {"term": "def", "name": "api_all", "data": "def api_all():\n\t# Get both parameters given\n\tteam1 = request.args['team1']\n\tteam2 = request.args['team2']\n\n\t# Convert the team name into the correct format above\n\tteam1 = teams[team1]\n\tteam2 = teams[team2]\n\n\t# Create dictionary of both teams to keep track of values\n\ttracker = {\n\t\tteam1: 0,\n\t\tteam2: 0\n\t}\n\n\t# Call the web scraper function and return stats for selected team\n\ttracker[get_teams_turnover(team1, team2)] += 1\n\ttracker[get_teams_point_diff(team1, team2)] += 1\n\ttracker[get_teams_OYG(team1, team2)] += 1\n\n\t# Check to see who is the winner with better stats\n\tresult = get_winner(tracker, team1, team2)\n\n\t# Convert into a json object after getting correct key again\n\tresult = list(teams.keys())[list(teams.values()).index(result)]\n\twinner = {'result': result}\n\n\t# Return as a json package\n\treturn jsonify(winner)\n", "description": null, "category": "webscraping", "imports": ["import flask", "from flask import request, jsonify", "from webscrape_turnover import get_teams_turnover", "from webscrape_point_diff import get_teams_point_diff", "from webscrape_OYG import get_teams_OYG", "from teams import teams"]}], [{"term": "def", "name": "index", "data": "def index():\n\twebscrape()\n\tbarchart()\n\treturn render_template(\"index.html\")\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template", "from web_scrape import webscrape", "from web_scrape import barchart"]}], [], [], [{"term": "class", "name": "TestRSCScraper", "data": "class TestRSCScraper(unittest.TestCase):\n\n\trsc_scraper = RSCWebScraper()\n\n\tdef test_get_doi_rsc(self):\n\t\t\"\"\"\n\t\tTest if the list of dois can be found with the RSCWebScraper\n\t\t\"\"\"\n\t\tdois = self.rsc_scraper.get_doi(query=\"battery materials\", page=3)\n\t\tlength = len(dois)\n\n\t\tself.assertEqual(length, 25)\n\n\tdef test_els_scraper(self):\n\t\t# As Elsevier web-scraper requires API key, we have no tests here.\n\t\t# See `examples/webscrape/elsevier.py` for more details.\n\t\tpass\n\n\tdef test_spr_scraper(self):\n\t\t# As Springer web-scraper requires API key, we have no tests here.\n\t\t# See `examples/webscrape/spr.py` for more details.\n\t\tpass\n\n", "description": "\n\t\tTest if the list of dois can be found with the RSCWebScraper\n\t\t", "category": "webscraping", "imports": ["import unittest", "from batterydataextractor.scrape import RSCWebScraper"]}], [{"term": "class", "name": "WebscrapeForm", "data": "class WebscrapeForm(ModelForm):\n\tclass Meta:\n\t\tmodel = WebScrape\n\t\tfields = ('category', 'url', 'date', 'price', 'imageUrl', 'profit')\n\n\t\twidgets = {\n\t\t\t'category': forms.Select(attrs={'class': 'form-control'}),\n\t\t\t'url': forms.TextInput(attrs={'class': 'form-control'}),\n\t\t\t'date': forms.TextInput(attrs={'class': 'form-control'}),\n\t\t\t'price': forms.TextInput(attrs={'class': 'form-control'}),\n\t\t\t'imageUrl': forms.TextInput(attrs={'class': 'form-control'}),\n\t\t\t'profit': forms.TextInput(attrs={'class': 'form-control'}),\n\t\t}\n\n", "description": null, "category": "webscraping", "imports": ["from django import forms", "from django.forms import ModelForm", "from .models import WebScrape, UserLogin"]}, {"term": "class", "name": "UserLoginForm", "data": "class UserLoginForm(ModelForm):\n\tclass Meta:\n\t\tmodel = UserLogin\n\t\tfields = ('first_name', 'last_name', 'email')\n\n\t\twidgets = {\n\t\t\t'first_name': forms.TextInput(attrs={'class': 'form-control'}),\n\t\t\t'last_name': forms.TextInput(attrs={'class': 'form-control'}),\n\t\t\t'email': forms.TextInput(attrs={'class': 'form-control'}),\n", "description": null, "category": "webscraping", "imports": ["from django import forms", "from django.forms import ModelForm", "from .models import WebScrape, UserLogin"]}], [], [{"term": "def", "name": "connect_to_SQL", "data": "def connect_to_SQL():\n\tload_dotenv()\n\tconn = mysql.connector.connect(user=os.getenv(\"USERNAME\"), password=os.getenv(\"PASSWORD\"),\n\t\t\t\t\t\t\t\t   host='127.0.0.1')\n\tcursor = conn.cursor()\n\treturn cursor, conn\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "createBaseballDB", "data": "def createBaseballDB(cursor, db_name):\n\tcursor.execute('DROP DATABASE IF EXISTS ' + db_name)\n\tcursor.execute('CREATE DATABASE ' + db_name)\n\tcursor.execute('USE ' + db_name)\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "createTable", "data": "def createTable(cursor, fields, table_name):\n\tcursor.execute('DROP TABLE IF EXISTS {}'.format(table_name))\n\tcolumns = ','.join(['{} {}'.format(key, fields[key]) for key in fields.keys()])\n\tcursor.execute('CREATE TABLE {} ({})'.format(table_name, columns))\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "loadPlayerNamesTable", "data": "def loadPlayerNamesTable(cursor, player_names_dict, table_name):\n\tfor player in player_names_dict:\n\t\tcursor.execute('INSERT INTO {} VALUES (\"{}\",\"{}\")'.format(table_name, player_names_dict[player], player))\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "loadHOFTable", "data": "def loadHOFTable(cursor, hall_of_fame_dict, table_name):\n\tfor player in hall_of_fame_dict:\n\t\tcursor.execute('INSERT INTO {} VALUES (\"{}\", \"{}\")'.format(table_name, player, hall_of_fame_dict[player]))\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "loadPlayerBiosTable", "data": "def loadPlayerBiosTable(cursor, bios_dict, table_name):\n\tfor player in bios_dict:\n\t\tdebut_date, final_game, bats, throws, career_length, excess_months, birth_state, birth_country = bios_dict[\n\t\t\tplayer]\n\t\tcursor.execute(\n\t\t\t'INSERT INTO {} VALUES (\"{}\", \"{}\", \"{}\", \"{}\",\"{}\", {},{}, \"{}\", \"{}\")'.format(table_name, player,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdebut_date, final_game,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbats, throws,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcareer_length,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\texcess_months, birth_state,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbirth_country))\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "loadCareerStatsTables", "data": "def loadCareerStatsTables(cursor, career_stats, table_name):\n\tfor player in career_stats:\n\t\tstats_string = ','.join(career_stats[player])\n\t\tsql = f'INSERT INTO {table_name} VALUES (\"{player}\",' + stats_string + ')'  # string slicing to remove extra comma and append a parenthesis to the sql command\n\t\tcursor.execute(sql)\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "createDBFields", "data": "def createDBFields():\n\tname_fields = {\n\t\t'PlayerID': 'VARCHAR(100)',\n\t\t'PlayerName': 'VARCHAR(100)'\n\t}\n\tplayer_bio_fields = {\n\t\t'PlayerID': 'VARCHAR(100)',\n\t\t'debutDate': 'DATE',\n\t\t'finalGameDate': 'DATE',\n\t\t'bats': 'CHAR(1)',\n\t\t'throws': 'CHAR(1)',\n\t\t'CareerLength_Years': 'INT',\n\t\t'MonthsExtra': 'INT',\n\t\t'birthState': 'VARCHAR(100)',\n\t\t'birthCountry': 'VARCHAR(100)'\n\n\t}\n\thall_of_fame_fields = {\n\t\t'PlayerID': 'VARCHAR(25)',\n\t\t'YearOfInduction': 'Year'\n\t}\n\n\tcareer_batting_stats = {\n\t\t# Career Batting Table Columns\n\t\t'PlayerID': 'VARCHAR(100)',\n\t\t'Games': 'INT',\n\t\t'AtBats': 'INT',\n\t\t'Runs': 'INT',\n\t\t'Hits': 'INT',\n\t\t'Doubles': 'INT',\n\t\t'Triples': 'INT',\n\t\t'Homeruns': 'INT',\n\t\t'RBI': 'INT',\n\t\t'Walks': 'INT',\n\t\t'IntentionalWalks': 'INT',\n\t\t'Strikeouts': 'INT',\n\t\t'HitByPitch': 'INT',\n\t\t'Sacrifice_Hits': 'INT',\n\t\t'Sacrifice_Flies': 'INT',\n\t\t'XI': 'INT',\n\t\t'ROE': 'INT',\n\t\t'GroundedIntoDoublePlays': 'INT',\n\t\t'StolenBases': 'INT',\n\t\t'CaughtStealing': 'INT',\n\t\t'BattingAVG': 'FLOAT',\n\t\t'On_BasePercent': 'FLOAT',\n\t\t'SluggingPercent': 'FLOAT',\n\t\t'BFW': 'FLOAT'\n\t}\n\n\tcareer_pitching_stats = {\n\t\t# Career Pitching Table Columns\n\t\t'PlayerID': 'VARCHAR(100)',\n\t\t'Games': 'INT',\n\t\t'GamesStarted': 'INT',\n\t\t'CompleteGames': 'INT',\n\t\t'Shutouts': 'INT',\n\t\t'GamesFinished': 'INT',\n\t\t'Saves': 'INT',\n\t\t'InningsPitched': 'INT',\n\t\t'Hits': 'INT',\n\t\t'BFP': 'INT',\n\t\t'Homeruns': 'INT',\n\t\t'Runs': 'INT',\n\t\t'EarnedRuns': 'INT',\n\t\t'Walks': 'INT',\n\t\t'IntentionalWalks': 'INT',\n\t\t'Strikeouts': 'INT',\n\t\t'SacrificeHits': 'INT',\n\t\t'SacrificeFlies': 'INT',\n\t\t'WildPitches': 'INT',\n\t\t'HitByPitch': 'INT',\n\t\t'Balks': 'INT',\n\t\t'Doubles': 'INT',\n\t\t'Triples': 'INT',\n\t\t'GroundedIntoDoublePlays': 'INT',\n\t\t'ROE': 'INT',\n\t\t'Wins': 'INT',\n\t\t'Losses': 'INT',\n\t\t'ERA': 'FLOAT',\n\t\t'RunSupport': 'FLOAT',\n\t\t'PW': 'FLOAT'\n\t}\n\tsingle_game_batting = {\n\t\t'playerID': 'VARCHAR(100)',\n\t\t'CaughtStealing': 'INT',\n\t\t'ConsecutiveGameHitStreaks': ''\n\t}\n\n\treturn name_fields, player_bio_fields, hall_of_fame_fields, career_batting_stats, career_pitching_stats\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "loadBaseballData", "data": "def loadBaseballData():\n\t\"\"\"\n\tThis function calls all of our webscraping python files which loads various sources from retrosheet into CSVs for database loading\n\t\"\"\"\n\twebscrapeHallOfFame.main()\n\twebscrapeTopIndividualPerf.main()\n\twebscrapeCareerLeaders.main()\n\n", "description": "\n\tThis function calls all of our webscraping python files which loads various sources from retrosheet into CSVs for database loading\n\t", "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "webscrapeCareerStatsForEachPlayer", "data": "def webscrapeCareerStatsForEachPlayer(playerNameDictionary):\n\t\"\"\"\n\tOpens batting/pitching files for adding career statistics\n\tThis function calls our separate webscrape file which goes to each individual players url and scrapes either their pitching record, fielding record or both\n\tAny errors in formatting such as players missing fields or players that did not have certain stats were skipped over in the webscraping process\n\tWe made sure not to abuse the webscraping of retrosheet by making sure only making calls out to the server 10 times per minute.\n\n\tI ran this particular function on my raspberry pi and it took roughly 35 hours total to webscrape all of the necessary data\n\n\t\"\"\"\n\t# TODO Better String Handling for Individual Player Career Stat Lines\n\tbatting_file = open('playerinformation/batting_stats.csv', 'a')\n\tpitching_file = open('playerinformation/pitching_stats.csv', 'a')\n\tplayer_dict_len = len(playerNameDictionary.keys())\n\tcurrent_index = 0\n\tplayer_list = list(playerNameDictionary.keys())\n\tfor player in player_list:\n\t\ttry:\n\t\t\twebscrapePlayerCareerStats.webscrapeCareerStats(player, playerNameDictionary, batting_file,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpitching_file)\n\t\t\tcurrent_index += 1\n\t\t\tprint('Current Completion Level: {}/{}'.format(current_index, player_dict_len))\n\t\texcept ValueError:\n\t\t\t# Players with invalid career line formats would be skipped, such as not having enough columns for data\n\t\t\tcontinue\n\n", "description": "\n\tOpens batting/pitching files for adding career statistics\n\tThis function calls our separate webscrape file which goes to each individual players url and scrapes either their pitching record, fielding record or both\n\tAny errors in formatting such as players missing fields or players that did not have certain stats were skipped over in the webscraping process\n\tWe made sure not to abuse the webscraping of retrosheet by making sure only making calls out to the server 10 times per minute.\n\n\tI ran this particular function on my raspberry pi and it took roughly 35 hours total to webscrape all of the necessary data\n\n\t", "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "getDataDirectories", "data": "def getDataDirectories(folder_name):\n\tdirectories = []\n\tfor root, dirs, files in os.walk(folder_name):\n\t\tfor file in files:\n\t\t\tif file.endswith('.csv'):\n\t\t\t\tdirectories.append(os.path.join(root, file))\n\treturn directories\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "convertDate", "data": "def convertDate(date):\n\tformat_string = \"%m/%d/%Y\"\n\n\ttry:\n\t\td = datetime.strptime(date, format_string)\n\texcept ValueError:\n\t\t# if there is no date we return 0000-01-01 to denote no date\n\t\treturn '0000-01-01', 0\n\n\treturn d, d.year\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "getPlayerNamesDictionary", "data": "def getPlayerNamesDictionary(filename):\n\tplayerNameDictionary = {}\n\twith open(filename) as file:\n\t\tfile.readline()\n\t\tplayer_info = csv.reader(file)\n\t\tfor line in player_info:\n\t\t\tline = [element.strip('\"') for element in line]\n\t\t\tplayerID, playerName = line[0], line[3] + ' ' + line[1]\n\t\t\tplayerNameDictionary[playerName] = playerID\n\n\treturn playerNameDictionary\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "getPlayerBiosDictionary", "data": "def getPlayerBiosDictionary(filename):\n\tplayerBioDictionary = {}\n\twith open(filename) as file:\n\t\theaders = [header.strip() for header in file.readline().split(',')]\n\t\tbats_index = headers.index('BATS')\n\t\tthrows_index = headers.index('THROWS')\n\t\tbirth_state_index = headers.index('BIRTH STATE')\n\t\tbirth_country_index = headers.index('BIRTH COUNTRY')\n\n\t\tdebut_date_index = headers.index('PLAY DEBUT')\n\t\tfinal_game_index = headers.index('PLAY LASTGAME')\n\t\tplayer_info = csv.reader(file)\n\t\tfor line in player_info:\n\t\t\tline = [element.strip('\"') for element in line]\n\t\t\tplayerID, debut_date, final_game, bats, throws = line[0], line[debut_date_index], line[final_game_index], \\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t line[bats_index], line[throws_index]\n\n\t\t\tbirth_state, birth_country = line[birth_state_index], line[birth_country_index]\n\n\t\t\t\"\"\"New Column for Career Length in Years\"\"\"\n\t\t\tdebut_date, debut_date_year = convertDate(debut_date)\n\t\t\tfinal_game, final_game_year = convertDate(final_game)\n\n\t\t\tcareer_length_in_years = final_game_year - debut_date_year\n\t\t\tif debut_date != '0000-01-01' and final_game != '0000-01-01':\n\t\t\t\tdate_diff = relativedelta.relativedelta(final_game, debut_date)\n\t\t\t\tyears, months, days = date_diff.years, date_diff.months, date_diff.days\n\t\t\t\t# if months is greater than or equal to one we want to include this as a year of playing since they at least started the season\n\t\t\t\tif months >= 1:\n\t\t\t\t\tcareer_length_in_years += 1\n\t\t\t\toutput_format_date = \"%Y-%m-%d\"\n\t\t\t\tdebut_date_str, final_game_str = datetime.strftime(debut_date, output_format_date), datetime.strftime(\n\t\t\t\t\tfinal_game, output_format_date)\n\n\t\t\tplayerBioDictionary[playerID] = [debut_date_str, final_game_str, bats, throws, career_length_in_years,\n\t\t\t\t\t\t\t\t\t\t\t months, birth_state, birth_country]\n\treturn playerBioDictionary\n\n", "description": "New Column for Career Length in Years", "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "getHallOfFamePlayersDictionary", "data": "def getHallOfFamePlayersDictionary(filename, playerNameDictionary, ):\n\thall_of_fame_dictionary = {}\n\twith open(filename) as file:\n\t\tfile.readline()\n\t\thall_of_fame_info = csv.reader(file)\n\t\tfor line in hall_of_fame_info:\n\t\t\tplayer_name, year_inducted = line[0], line[1]\n\t\t\tif player_name in playerNameDictionary:\n\t\t\t\tplayer_id = playerNameDictionary[player_name]\n\t\t\t\thall_of_fame_dictionary[player_id] = year_inducted\n\treturn hall_of_fame_dictionary\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "loadAllTimeLeaders", "data": "def loadAllTimeLeaders(filedirectories, playerDictionary, player_position, cursor):\n\tfor filename in filedirectories:\n\t\twith open(filename) as file:\n\t\t\theaders = file.readline()\n\t\t\theaders = headers.replace(\"\\n\", \"\")\n\t\t\tcategories = headers.split(\",\")\n\t\t\tcategories[0] = categories[0].replace(\" \", \"\")\n\t\t\tplayer_name_header = categories[0]\n\t\t\tcategory = categories[1]\n\t\t\tif category == \"G\":\n\t\t\t\tcategory_list = categories[1]\n\t\t\telse:\n\t\t\t\tcategory_list = categories[1:]\n\n\t\t\tfields = \"playerID VARCHAR(255), \" + player_name_header + \" VARCHAR(255)\"\n\t\t\tfor value in category_list:\n\t\t\t\tvalue = value.replace(\"/\", \"Per\")\n\t\t\t\tvalue = value.replace(\"%\", \"Percentage\")\n\t\t\t\tfields = fields + \", \" + value + \" FLOAT\"\n\t\t\ttable_name = player_position + category\n\t\t\tcursor.execute('DROP TABLE IF EXISTS {}'.format(table_name))\n\t\t\tcursor.execute('CREATE TABLE {} ({})'.format(table_name, fields))\n\t\t\tall_time_stats = csv.reader(file)\n\t\t\tfor line in all_time_stats:\n\t\t\t\tname = line[0]\n\t\t\t\tif name in playerDictionary:\n\t\t\t\t\tvalue_command = 'INSERT INTO {} VALUES (\"{}\", \"{}\"'.format(table_name, playerDictionary[name], name)\n\t\t\t\tfor values in line[1:]:\n\t\t\t\t\tif category == \"G\":\n\t\t\t\t\t\tvalue_command += ', \"{}\"'.format(values)\n\t\t\t\t\t\tbreak\n\t\t\t\t\tvalue_command += ', \"{}\"'.format(values)\n\t\t\t\tvalue_command += ')'\n\t\t\t\tcursor.execute(value_command)\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "getCareerStatsForPlayersDictionary", "data": "def getCareerStatsForPlayersDictionary(filename):\n\tcareer_stats_dict = {}\n\twith open(filename) as file:\n\t\tfile.readline()\n\t\tcareer_stats = csv.reader(file)\n\t\tfor line in career_stats:\n\t\t\tcareer_stats_dict[line[0]] = line[1:]\n\treturn career_stats_dict\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "addColumns", "data": "def addColumns(cursor, column, datatype, tbl_name, player_bio_dict, career_stats_dict):\n\tcursor.execute(f\"ALTER TABLE {tbl_name} ADD {column} {datatype}\")\n\tcursor.execute(f\"ALTER TABLE {tbl_name} ADD careerLength INT\")\n\n\tfor player in player_bio_dict:\n\t\ttry:\n\t\t\tgames = career_stats_dict[player][0]\n\t\t\tcareer_length = player_bio_dict[player][4]\n\t\t\tif float(career_length) == 0:\n\t\t\t\tcursor.execute(f\"UPDATE {tbl_name} SET {column} = 0 WHERE playerID ='{player}'\")\n\t\t\t\tcursor.execute(f\"UPDATE {tbl_name} SET careerLength = 0 WHERE playerID ='{player}'\")\n\t\t\t\tcontinue\n\t\t\tgames_per_year = float(games) / float(career_length)\n\t\t\tcursor.execute(f\"UPDATE {tbl_name} SET {column} = {round(games_per_year, 2)} WHERE PlayerID ='{player}'\")\n\t\t\tcursor.execute(f\"UPDATE {tbl_name} SET careerLength = {career_length} WHERE PlayerID ='{player}'\")\n\t\t\t# UPDATE table_name SET column1 = value1, column2 = value2, ...WHERE condition;\n\t\texcept KeyError:\n\t\t\tcontinue\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}, {"term": "def", "name": "main", "data": "def main():\n\t# loadBaseballData()  # function calls webscraping py files, will take a little while to complete\n\n\tcursor, conn = connect_to_SQL()\n\tcreateBaseballDB(cursor, \"baseballStats_db\")\n\tname_fields, player_bio_fields, hall_of_fame_fields, \\\n\tcareer_batting_stats_fields, career_pitching_stats_fields = createDBFields()\n\n\t# Player Names Table\n\tcreateTable(cursor, name_fields, 'PlayerNames')\n\tplayer_names_dict = getPlayerNamesDictionary('playerinformation/playerBios.csv')\n\tloadPlayerNamesTable(cursor, player_names_dict, 'PlayerNames')\n\tprint(\"PlayerNames Table Loaded...\")\n\n\t# Player Bios Table\n\tcreateTable(cursor, player_bio_fields, 'PlayerBios')\n\tplayer_bio_dict = getPlayerBiosDictionary('playerinformation/playerBios.csv')\n\tloadPlayerBiosTable(cursor, player_bio_dict, 'PlayerBios')\n\tprint(\"PlayerBios Table Loaded...\")\n\n\t# Hall Of Fame Table\n\tcreateTable(cursor, hall_of_fame_fields, 'HallOfFame')\n\thall_of_fame_dict = getHallOfFamePlayersDictionary('awards/The Hall of Fame.csv', player_names_dict)\n\tloadHOFTable(cursor, hall_of_fame_dict, 'HallOfFame')\n\tprint(\"HallOfFame Table Loaded...\")\n\n\t# All Time Batting Leaders Table\n\tall_time_batting_dirs = sorted(getDataDirectories('battingstats/careerleaders/'))\n\tbatting_string = \"Batting\"\n\tloadAllTimeLeaders(all_time_batting_dirs, player_names_dict, batting_string, cursor)\n\tprint(\"AllTimeBattingLeaders Tables Loaded...\")\n\n\t# All Time Pitching Leaders Table\n\tall_time_pitching_dirs = sorted(getDataDirectories('pitchingstats/careerleaders/'))\n\tpitching_string = \"Pitching\"\n\tloadAllTimeLeaders(all_time_pitching_dirs, player_names_dict, pitching_string, cursor)\n\tprint(\"AllTimePitchingLeaders Table Loaded...\")\n\n\t# Career Batting Stats for All Players Table\n\t# webscrapeCareerStatsForEachPlayer(player_names_dict)\n\tcreateTable(cursor, career_batting_stats_fields, 'CareerBattingStats')\n\tcareer_batting_stats = getCareerStatsForPlayersDictionary('playerinformation/batting_stats.csv')\n\tloadCareerStatsTables(cursor, career_batting_stats, 'CareerBattingStats')\n\tprint(\"CareerBattingStats Table Loaded...\")\n\n\t# Career Pitching Stats for All Players Table\n\tcreateTable(cursor, career_pitching_stats_fields, 'CareerPitchingStats')\n\tcareer_pitching_stats = getCareerStatsForPlayersDictionary('playerinformation/pitching_stats.csv')\n\tloadCareerStatsTables(cursor, career_pitching_stats, 'CareerPitchingStats')\n\tprint(\"CareerPitchingStats Table Loaded...\")\n\n\taddColumns(cursor, 'AVGGamesPerYear', 'FLOAT', 'CareerBattingStats', player_bio_dict, career_batting_stats)\n\taddColumns(cursor, 'AVGGamesPerYear', 'FLOAT', 'CareerPitchingStats', player_bio_dict, career_pitching_stats)\n\tconn.commit()\n\n", "description": null, "category": "webscraping", "imports": ["from dotenv import load_dotenv", "import mysql.connector, os, csv", "import webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats", "from datetime import datetime", "from dateutil import relativedelta"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(ticker: str, top_results=False):\n\tsession = HTMLSession()\n\turl = f\"https://twitter.com/search?q=%24{ticker}%20lang%3Aen&src=typed_query\"\n\tif not top_results:\n\t\turl += \"&f=live\"\n\n\tr = session.get(url)\n\tr.html.render(sleep=1, keep_page=True, scrolldown=1)\n\tsession.close()\n\n\tresult = []\n\ttweets = r.html.find(\"article\")  # A tweet object currently is the whole tweet textbox, including username/handle\n\n\t# Loop through each tweet and extract ONLY the text that is the actual tweet, not the handles or anything else\n\tfor tweet in tweets:\n\t\ttextbox = tweet.find(\"div\")\n\t\tfor div in textbox:\n\t\t\tif \"lang\" in div.attrs:  # The  with the \"lang\" class is the actual tweet, other  elements are\n\t\t\t\t# irrelevant\n\t\t\t\tresult.append(div.text)\n\t# Returns a list of  tweets\n\treturn result\n\n", "description": null, "category": "webscraping", "imports": ["from requests_html import HTMLSession", "import gc"]}], [{"term": "def", "name": "main", "data": "def main():\r\n\r\n\tbase_url = 'https://eoddata.com/stocklist/OTCBB/'\r\n\r\n\talphabet_string = string.ascii_uppercase\r\n\talphabet_list = list(alphabet_string)\r\n\tdata_set = {}\r\n\r\n\tfor i in alphabet_list:\r\n\t\turl = base_url + i + '.htm'\r\n\t\tpage = webscrape.LinkParser(url)\r\n\t\tdata_set[i] = page.gather_page_data()\r\n\r\n\treturn data_set\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import webscrape\r", "import string\r"]}], [{"term": "def", "name": "aws_credentials", "data": "def aws_credentials():\n\t\"\"\"Mocked AWS Credentials for moto.\"\"\"\n\tos.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\n\tos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\n\tos.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\n\tos.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\n\tos.environ[\"USER_EMAIL\"] = \"jyablonski9@gmail.com\"\n\n", "description": "Mocked AWS Credentials for moto.", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "setup_database", "data": "def setup_database():\n\t\"\"\" Fixture to set up an empty in-memory database \"\"\"\n\tconn = sqlite3.connect(\":memory:\")\n\tyield conn\n\n", "description": " Fixture to set up an empty in-memory database ", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "player_stats_data", "data": "def player_stats_data():\n\t\"\"\"\n\tFixture to load player stats data from a csv file for testing.\n\t\"\"\"\n\tfname = os.path.join(\n\t\tos.path.dirname(__file__), \"fixture_csvs/player_stats_data.csv\"\n\t)\n\tstats = pd.read_csv(fname)\n\tstats = get_player_stats_transformed(stats)\n\treturn stats\n\n", "description": "\n\tFixture to load player stats data from a csv file for testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "boxscores_data", "data": "def boxscores_data():\n\t\"\"\"\n\tFixture to load boxscores data from a csv file for testing.\n\t\"\"\"\n\tfname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/boxscores_data.csv\")\n\tdf = pd.read_csv(fname)\n\tday = (datetime.now() - timedelta(1)).day\n\tmonth = (datetime.now() - timedelta(1)).month\n\tyear = (datetime.now() - timedelta(1)).year\n\tseason_type = \"Regular Season\"\n\tdf = get_boxscores_transformed(df)\n\treturn df\n\n", "description": "\n\tFixture to load boxscores data from a csv file for testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "opp_stats_data", "data": "def opp_stats_data():\n\t\"\"\"\n\tFixture to load team opponent stats data from a csv file for testing.\n\t\"\"\"\n\tfname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/opp_stats_data.csv\")\n\tdf = pd.read_csv(fname)\n\tdf = get_opp_stats_transformed(df)\n\treturn df\n\n", "description": "\n\tFixture to load team opponent stats data from a csv file for testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "injuries_data", "data": "def injuries_data():\n\t\"\"\"\n\tFixture to load injuries data from a csv file for testing.\n\t\"\"\"\n\tfname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/injuries_data.csv\")\n\tdf = pd.read_csv(fname)\n\tdf = get_injuries_transformed(df)\n\treturn df\n\n", "description": "\n\tFixture to load injuries data from a csv file for testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "transactions_data", "data": "def transactions_data():\n\t\"\"\"\n\tFixture to load transactions data from a csv file for testing.\n\t\"\"\"\n\tfname = os.path.join(\n\t\tos.path.dirname(__file__), \"fixture_csvs/transactions_data.csv\"\n\t)\n\ttransactions = pd.read_csv(fname)\n\ttransactions = get_transactions_transformed(transactions)\n\treturn transactions\n\n", "description": "\n\tFixture to load transactions data from a csv file for testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "advanced_stats_data", "data": "def advanced_stats_data():\n\t\"\"\"\n\tFixture to load team advanced stats data from a csv file for testing.\n\t\"\"\"\n\tfname = os.path.join(\n\t\tos.path.dirname(__file__), \"fixture_csvs/advanced_stats_data.csv\"\n\t)\n\tdf = pd.read_csv(fname)\n\tdf = get_advanced_stats_transformed(df)\n\treturn df\n\n", "description": "\n\tFixture to load team advanced stats data from a csv file for testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "shooting_stats_data", "data": "def shooting_stats_data():\n\t\"\"\"\n\tFixture to load shooting stats data from a csv file for testing.\n\t\"\"\"\n\tfname = os.path.join(\n\t\tos.path.dirname(__file__), \"fixture_csvs/shooting_stats_data.csv\"\n\t)\n\tshooting_stats = pd.read_csv(fname)\n\tshooting_stats = get_shooting_stats_transformed(shooting_stats)\n\treturn shooting_stats\n\n", "description": "\n\tFixture to load shooting stats data from a csv file for testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "odds_data", "data": "def odds_data():\n\t\"\"\"\n\tFixture to load odds data from a csv file for testing.\n\t\"\"\"\n\tfname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/odds_data\")\n\twith open(fname, \"rb\") as fp:\n\t\tdf = pickle.load(fp)\n\tday = (datetime.now() - timedelta(1)).day\n\tmonth = (datetime.now() - timedelta(1)).month\n\tyear = (datetime.now() - timedelta(1)).year\n\tdf = get_odds_transformed(df)\n\treturn df\n\n", "description": "\n\tFixture to load odds data from a csv file for testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "pbp_transformed_data", "data": "def pbp_transformed_data():\n\t\"\"\"\n\tFixture to load boxscores data from a csv file for PBP Transform testing.\n\t\"\"\"\n\tfname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/pbp_data.csv\")\n\tboxscores = pd.read_csv(fname, parse_dates=[\"date\"])\n\tpbp_transformed = get_pbp_data_transformed(boxscores)\n\treturn pbp_transformed\n\n", "description": "\n\tFixture to load boxscores data from a csv file for PBP Transform testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "logs_data", "data": "def logs_data():\n\t\"\"\"\n\tFixture to load dummy error logs for testing\n\t\"\"\"\n\tdf = pd.DataFrame({\"errors\": \"Test... Failure\"})\n\treturn df\n\n", "description": "\n\tFixture to load dummy error logs for testing\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "schedule_data", "data": "def schedule_data(mocker):\n\t\"\"\"\n\tFixture to load schedule data from an html file for testing.\n\t*** THIS WORKS FOR ANY REQUESTS.GET MOCKING IN THE FUTURE ***\n\t\"\"\"\n\tfname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/schedule.html\")\n\twith open(fname, \"rb\") as fp:\n\t\tmock_content = fp.read()\n\n\t# IT WORKS\n\t# you have to first patch the requests.get response, and subsequently the return value of requests.get(url).content\n\tmocker.patch(\"nba_bbref_webscrape.schedule_functions.requests.get\").return_value.content = mock_content\n\n\tschedule = schedule_scraper(\"2022\", [\"february\", \"march\"])\n\treturn schedule\n\n", "description": "\n\tFixture to load schedule data from an html file for testing.\n\t*** THIS WORKS FOR ANY REQUESTS.GET MOCKING IN THE FUTURE ***\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "reddit_comments_data", "data": "def reddit_comments_data(mocker):\n\t\"\"\"\n\tFixture to load reddit_comments data from a csv file for testing.\n\t\"\"\"\n\tfname = os.path.join(\n\t\tos.path.dirname(__file__), \"fixture_csvs/reddit_comments_data.csv\"\n\t)\n\twith open(fname, \"rb\") as fp:\n\t\treddit_comments_fixture = pd.read_csv(\n\t\t\tfname, index_col=0\n\t\t)  # literally fuck indexes\n\n\t# mock a whole bunch of praw OOP gahbage\n\tmocker.patch(\"nba_bbref_webscrape.reddit_functions.praw.Reddit\").return_value = 1\n\tmocker.patch(\"nba_bbref_webscrape.reddit_functions.praw.Reddit\").return_value.submission = 1\n\tmocker.patch(\n\t\t\"nba_bbref_webscrape.reddit_functions.praw.Reddit\"\n\t).return_value.submission.comments.list().return_value = 1\n\tmocker.patch(\"nba_bbref_webscrape.reddit_functions.pd.DataFrame\").return_value = reddit_comments_fixture\n\n\treddit_comments_data = get_reddit_comments([\"fake\", \"test\"])\n\treturn reddit_comments_data\n\n", "description": "\n\tFixture to load reddit_comments data from a csv file for testing.\n\t", "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "twitter_stats_data", "data": "def twitter_stats_data(mocker):\n\tfname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/nba_tweets.csv\")\n\ttwitter_csv = pd.read_csv(fname)\n\n\tdf = mocker.patch(\n\t\t\"nba_bbref_webscrape.twitter_functions.pd.read_csv\"\n\t)  # mock the return value for the csv to use my fixture\n\tdf.return_value = twitter_csv\n\n\ttwint_mock = mocker.patch(\n\t\t\"nba_bbref_webscrape.twitter_functions.twint.run.Search\"\n\t)  # mock the twitter scrape so it doesnt run\n\ttwint_mock.return_value = 1\n\ttwitter_data = scrape_tweets(\"nba\")\n\treturn twitter_data\n\n", "description": null, "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}, {"term": "def", "name": "clean_player_names_data", "data": "def clean_player_names_data():\n\tdf = pd.DataFrame(\n\t\t{\n\t\t\t\"player\": [\n\t\t\t\t\"Marcus Morris Sr.\",\n\t\t\t\t\"Kelly Oubre Jr.\",\n\t\t\t\t\"Gary Payton II\",\n\t\t\t\t\"Robert Williams III\",\n\t\t\t\t\"Lonnie Walker IV\",\n\t\t\t]\n\t\t}\n\t)\n\tdf = clean_player_names(df)\n\treturn df\n\n", "description": null, "category": "webscraping", "imports": ["from datetime import datetime, timedelta", "import pickle", "import os", "import boto3", "import numpy as np", "import pandas as pd", "import pytest", "import pytest_mock", "import requests", "import sqlite3", "# import moto", "from nba_bbref_webscrape import *"]}], [{"term": "def", "name": "index", "data": "def index(request):\n\tcontext = {}\n\treturn render(request, 'core/index.html', context)\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import HttpResponse", "from .script import webScrape"]}, {"term": "def", "name": "about", "data": "def about(request):\n\tcontext = {}\n\treturn render(request, 'core/about.html', context)\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import HttpResponse", "from .script import webScrape"]}, {"term": "def", "name": "results", "data": "def results(request):\n\tif request.method == \"POST\":\n\t\tquery = \"\"\n\t\trequest.session['player_name'] = request.POST['player_name']\n\t\tquery += \" \" + request.POST['player_name']\n\n\t\tquery += \" \" + request.POST['card_keyword']\n\t\trequest.session['keyword'] = request.POST['card_keyword']\n\n\t\tquery += \" \" + request.POST['card_brand']\n\t\trequest.session['brand'] = request.POST['card_brand']\n\n\t\tquery += \" \" + request.POST['card_year']\n\t\trequest.session['year'] = request.POST['card_year']\n\t\t\n\t\trequest.session['exclude'] = request.POST['exclude']\n\t\tif request.POST['exclude']:\n\t\t\tfor term in request.POST['exclude'].split():\n\t\t\t\tquery += \" -\" + term\n\n\t\trequest.session['card_psa'] = request.POST.get('card_psa', '')\n\t\tpsa = request.POST.get('card_psa', '')\n\t\tif (psa != ''):\n\t\t\tquery += \" \" + psa\n\t\t\n\t\tquery = query.replace(\"  \", \" \")\n\t\tquery = query.replace(\" \", \"+\")\n\t\tdata = webScrape(query)\n\t\t#print(data)\n\t\tif data:\n\t\t\trequest.session['img_url'] = data[0]\n\t\t\trequest.session['avg_price'] = data[1]\n\t\t\trequest.session['ebay_listings'] = data[2]\n\t\telse:\n\t\t\trequest.session['img_url'] = ''\n\t\t\trequest.session['avg_price'] = ''\n\t\t\trequest.session['ebay_listings'] = ''\n\n\tcontext = {}\n\t#print(query)\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import HttpResponse", "from .script import webScrape"]}], [{"term": "def", "name": "chad", "data": "def chad(names, counts):\n\t#print(31231)\n\t#print(names)\n\t# print(len(names))\n\tfor i in range(len(names)):\n\t\talpaca.create_order(names[i], random.randint(2, 10))\n\t\t#print(\"hi\")\n\t# print(3213213)\n", "description": null, "category": "webscraping", "imports": ["import matplotlib.pyplot as plt", "import random", "import webscrape", "import alpaca", "import td_ameritrade"]}, {"term": "def", "name": "gambler", "data": "def gambler(names, platform_mode = \"alpaca\"):\n\trandom_idx = random.randint(0, len(names))\n\tmax_quantity = 0\n\tif platform_mode == \"alpaca\":\n\t\talpaca.create_order(names[random_idx], 1000)\n\tif platform_mode == \"TD\":\n\t\tREFRESH_TOEKN=input(\"REFRESH_TOEKN\")\n\t\tCONSUMER_KEY=input(\"CONSUMER_KEY\")\n\t\tACCOUNT_ID=input(\"ACCOUNT_ID\")\n\t\ttd_ameritrade.buyyyy(names)\n", "description": null, "category": "webscraping", "imports": ["import matplotlib.pyplot as plt", "import random", "import webscrape", "import alpaca", "import td_ameritrade"]}, {"term": "def", "name": "danger", "data": "def danger():\n\tgambler(\"TD\")\n\tpass\n\n\n", "description": null, "category": "webscraping", "imports": ["import matplotlib.pyplot as plt", "import random", "import webscrape", "import alpaca", "import td_ameritrade"]}], [], [{"term": "def", "name": "index", "data": "def index():\r\n\tif request.method == \"GET\":\r\n\t\treturn render_template('index.html')\r\n\tif request.method == \"POST\":\r\n\t\tif request.form['submit_button'] == \"webscrape\":\r\n\t\t\tlm.webscrape()\r\n\t\t\treturn render_template('index.html')\r\n\t\telif request.form['submit_button'] == \"generate_summary\":\r\n\t\t\tsummary_list = t5.generate_summary()\r\n\t\t\treturn render_template('index.html',contacts = summary_list)\r\n\t\telse:\r\n\t\t\tpass\r\n\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask\r", "from flask import render_template,request\r", "import t5_textsumm as t5\r", "import livemint as lm\r"]}], [{"term": "class", "name": "classMakeMountainDF:", "data": "class MakeMountainDF:\n\tdef __init__(self):\n\n\t\tself.CURRENT_DIRECTORY = os.getcwd()\n\n\t\tself.browser_options = webdriver.ChromeOptions()\n\t\tself.browser_options.add_argument(\"--no-sandbox\")\n\t\tself.browser_options.add_argument(\"--headless\")\n\t\tself.browser_options.add_argument(\"--disable-gpu\")\n\n\t\tself.browser = webdriver.Chrome(options=self.browser_options)\n\n\t\t# 2020 ticket prices, fetched manually\n\t\tself.dict_resort_prices = {\n\t\t\t\"Alpine Meadows\": 169,\n\t\t\t\"Arapahoe Basin\": 109,\n\t\t\t\"Aspen Snowmass\": 179,\n\t\t\t\"Bald Mountain\": 145,\n\t\t\t\"Beaver Creek\": 209,\n\t\t\t\"Copper\": 119,\n\t\t\t\"Crested Butte\": 129,\n\t\t\t\"Diamond Peak\": 104,\n\t\t\t\"Eldora\": 140,\n\t\t\t\"Jackson Hole\": 165,\n\t\t\t\"Loveland\": 89,\n\t\t\t\"Monarch\": 94,\n\t\t\t\"Steamboat\": 199,\n\t\t\t\"Taos\": 110,\n\t\t\t\"Telluride\": 149,\n\t\t\t\"Vail\": 209,\n\t\t\t\"Winter Park\": 139,\n\t\t\t\"Wolf Creek\": 76,\n\t\t}\n\n\tdef get_mountain_data(self, URL: str) -> pd.core.frame.DataFrame:\n\t\t\"\"\"\n\t\tInputs:\n\t\t\tURL from URLs (str)\n\t\tOutputs:\n\t\t\tPandas DataFrame of ski resort information\n\t\t\"\"\"\n\n\t\tself.browser.get(URL)\n\n\t\ttime.sleep(3)\n\n\t\tsoup = BeautifulSoup(self.browser.page_source, \"html.parser\")\n\n\t\t# JollyTurns parsing (runs breakdown)\n\t\tX_runs = soup.select(\n\t\t\t\"resort-glance div.row div.col-xs-12 div.row.text-left.statistics.ng-scope span.ng-binding\"\n\t\t)\n\t\tlst_runs = [run.text for run in X_runs]\n\t\tlst_runs = [run.replace(\" ski runs: \", \"\") for run in lst_runs]\n\t\tdf_ski_runs = pd.DataFrame({\"Runs\": lst_runs[0::2], \"total\": lst_runs[1::2]})\n\t\tdf_ski_runs = df_ski_runs.set_index(\"Runs\").T.reset_index(drop=True)\n\n\t\t# JollyTurns parsing (Chairlifts / total runs)\n\t\tX_lifts = soup.select(\"div.content-in-circle\")\n\t\tlst_lifts = [lift.text.lstrip() for lift in X_lifts]\n\t\tdf_lifts = pd.DataFrame({\"Lifts\": lst_lifts[0]}, index=[0])\n\n\t\t# JollyTurns parsing (Elevations)\n\t\tX_elevations = soup.select(\"resort-glance div.row div.col-xs-12 table tr td\")\n\t\tlst_elevations = [\n\t\t\televation.text for elevation in X_elevations if \"Lift\" not in elevation.text\n\t\t]\n\t\tlst_elevations = [\n\t\t\televation.replace(\" \\xa0\", \"\") for elevation in lst_elevations\n\t\t]\n\t\tlst_elevations = [elevation.replace(\" ft\", \"\") for elevation in lst_elevations]\n\t\tlst_elevations = [elevation.replace(\":\", \"\") for elevation in lst_elevations]\n\n\t\tdf_elevations = pd.DataFrame(\n\t\t\t{\"Elevation\": lst_elevations[0::2], \"Total\": lst_elevations[1::2]}\n\t\t)\n\t\tdf_elevations = df_elevations.set_index(\"Elevation\").T.reset_index(drop=True)\n\n\t\t# Combine total runs, total lifts, and elevation data\n\t\tdf_ski = pd.concat([df_ski_runs, df_lifts, df_elevations], axis=1)\n\n\t\tdf_ski[\"URL\"] = URL\n\n\t\treturn df_ski\n\n\tdef format_mountain_data_frame_values(\n\t\tself, df: pd.core.frame.DataFrame\n\t) -> pd.core.frame.DataFrame:\n\t\t\"\"\"\n\t\tPivot DataFrame, and format values\n\n\t\tInput\n\t\t\tdf: Pandas DataFrame\n\n\t\tOutput\n\t\t\tFormatted Pandas DataFrame\n\t\t\"\"\"\n\n\t\tlst_columns = [\n\t\t\t\"Top\",\n\t\t\t\"Base\",\n\t\t\t\"Lifts\",\n\t\t\t\"Vertical rise\",\n\t\t\t\"black\",\n\t\t\t\"blue\",\n\t\t\t\"double black\",\n\t\t\t\"green\",\n\t\t\t\"terrain park\",\n\t\t]\n\n\t\tdf[lst_columns] = df[lst_columns].fillna(0)\n\n\t\tdf[lst_columns] = df[lst_columns].astype(\"int\")\n\n\t\treturn df\n\n\tdef save_mountain_data(\n\t\tself, df: pd.core.frame.DataFrame\n\t) -> pd.core.frame.DataFrame:\n\t\t\"\"\"\n\t\tSave formatted mountain data to Parquet file\n\t\t\"\"\"\n\n\t\tcurrent_date = date.today().strftime(\"%Y%m%d\")\n\n\t\tdf.to_parquet(\n\t\t\tf\"{self.CURRENT_DIRECTORY}/data/mountain_data_{current_date}.parquet\",\n\t\t\tindex=False,\n\t\t)\n\n", "description": "\n\t\tInputs:\n\t\t\tURL from URLs (str)\n\t\tOutputs:\n\t\t\tPandas DataFrame of ski resort information\n\t\t", "category": "webscraping", "imports": ["import os", "import time", "import warnings", "from datetime import date", "import numpy as np", "import pandas as pd", "import requests", "from bs4 import BeautifulSoup", "from selenium import webdriver", "from tqdm import tqdm", "from src.webscrape_trails import WebscrapeTrails"]}], [{"term": "def", "name": "changeProviders", "data": "def changeProviders(currentProvider):\n\tif currentProvider in M.providerList:\n\t\tM.providerList.remove(currentProvider)\n\t\tif M.providerList:\n\t\t\tnextProvider = M.providerList[0]\n\t\t\tpageNumber = '0'\n\t\t\tWebScrape.loopThroughScrapePages(nextProvider, pageNumber)\n\t\telse:\n\t\t\treturn\n\n", "description": null, "category": "webscraping", "imports": ["import WebScrape", "import main as M"]}, {"term": "def", "name": "getProviderNames", "data": "def getProviderNames(service):\n\tif service == '':\n\t\tservice = 'Netflix'\n\n\telif service == 'disney-plus/':\n\t\tservice = 'Disney Plus'\n\n\telif service == 'hulu/':\n\t\tservice = 'Hulu'\n\n\telif service == 'hbo-max/':\n\t\tservice = 'HBO Max'\n\n\telif service == 'amazon-prime-video/':\n\t\tservice = 'Amazon Prime'\n\treturn service\n\n", "description": null, "category": "webscraping", "imports": ["import WebScrape", "import main as M"]}, {"term": "def", "name": "containsAny", "data": "def containsAny(sArg, charSet):\n\t# Check whether sequence str contains ANY of the items in set.\n\treturn 1 in [c in sArg for c in charSet]\n", "description": null, "category": "webscraping", "imports": ["import WebScrape", "import main as M"]}], [{"term": "class", "name": "Uniprot", "data": "class Uniprot():\n\t'''\n\tTODO: Add class and method documentation\n\t'''\n\tdef __init__(self,gene_name=None,gene_entry=None,organism=\"HUMAN\"):\n\t\tself.__gene_entry = gene_entry\n\t\tself.__gene_name = gene_name\n\t\tself.__organism = organism\n\n\n\tdef protein_entries(self):\n\t\tdata = urllib.parse.urlencode({\n\t\t\t'from': 'GENENAME',\n\t\t\t'to': 'ID',\n\t\t\t'format': 'list',\n\t\t\t'query': self.__gene_name,\n\t\t\t}).encode('utf-8')\n\n\t\treq = urllib.request.Request('https://www.uniprot.org/uploadlists/', data)\n\t\twith urllib.request.urlopen(req) as f:\n\t\t   response = f.read()\n\t\treturn [r for r in response.decode('utf-8').split() if self.__organism in r]\n\n\tdef protein_function(self):\n\t\t'''TODO: Override this webscrape with uniprot API support'''\n\t\titems = scrape('https://www.uniprot.org/uniprot/'+self.__gene_entry,'span')\n", "description": null, "category": "webscraping", "imports": ["from webscrape import scrape", "import urllib.parse", "import urllib.request"]}], [], [{"term": "def", "name": "get_NWS_web_data", "data": "def get_NWS_web_data(site):\n\t\"\"\"Retrieve data from National Weather Service website. Provided 'site' variable\n\tshould point to a page that provides tabular river data. This function returns\n\tthe contents of the site and a timestamp of the actual website scraping event.\n\t\"\"\"\n\tstart_time = UTC_NOW()\n\tclean_soup = retrieve_cleaned_html(site)\n\tfinish_time = UTC_NOW()\n\telapsed_time = finish_time - start_time\n\tguage_id = clean_soup.h1[\"id\"]\n\tguage_string = clean_soup.h1.string\n\tnws_class = clean_soup.find(class_=\"obs_fores\")\n\tnws_obsfores_contents = nws_class.contents\n\treturn (nws_obsfores_contents, guage_id, guage_string, start_time, elapsed_time)\n\n", "description": "Retrieve data from National Weather Service website. Provided 'site' variable\n\tshould point to a page that provides tabular river data. This function returns\n\tthe contents of the site and a timestamp of the actual website scraping event.\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}, {"term": "def", "name": "FixDate", "data": "def FixDate(s, currentyear, time_zone=pytzutc):\n\t\"\"\"Split date from time and add timezone label.\n\tUnfortunately, NWS chose not to include the year. \n\tThis will be problematic when forecast dates are into the next year.\n\tIf Observation dates are in December, Forecast dates must be checked \n\tand fixed for roll over into next year.\n\t\"\"\"\n\t# TODO check and fix end of year forecast dates\n\n\tp = parse(s) # parse is returning a timezone-naive datetime obj \n\t# additionally, parse is appending the current year into the object\n\taware = p.replace(tzinfo=time_zone)\n\n\treturn timefstring(aware)\n\n", "description": "Split date from time and add timezone label.\n\tUnfortunately, NWS chose not to include the year. \n\tThis will be problematic when forecast dates are into the next year.\n\tIf Observation dates are in December, Forecast dates must be checked \n\tand fixed for roll over into next year.\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}, {"term": "def", "name": "sort_and_label_data", "data": "def sort_and_label_data(web_data, guage_details, time):\n\t\"\"\"Returns a list of dicts containing relevant data from webscrape.\n\tNWS results are in UTC timezone for both observations and forecasts.\n\tResults are stored to CSV file with all timestamps in UTC.\n\t\"\"\"\n\n\treadings = []\n\tLCY = LOCAL_CURRENT_YEAR()\n\tguage_id, elev, milemarker, _ = guage_details\n\trelevant_labels = [TS_LABEL_STR, \"level\", \"flow\"]\n\tfor i, item in enumerate(web_data):\n\t\tif i >= 1:  # zeroth item is an empty list\n\t\t\t# locate the name of this section (observed / forecast)\n\t\t\tsection = item.find(class_=\"data_name\").contents[0]\n\t\t\tsect_name = section.split()[0]\n\t\t\trow_dict = {}\n\t\t\t# extract all readings from this section\n\t\t\tsection_data_list = item.find_all(class_=\"names_infos\")\n\t\t\t# organize the readings and add details\n\t\t\tfor i, data in enumerate(section_data_list):\n\t\t\t\telement = data.contents[0]\n\t\t\t\tpointer = i % 3  # each reading contains 3 unique data points\n\t\t\t\tlabel = relevant_labels[pointer]\n\t\t\t\tif pointer == 0:  # this is the element for date/time\n\t\t\t\t\telement = FixDate(element, LCY )\n\t\t\t\trow_dict[label] = element\n\t\t\t\tif pointer == 2:  # end of this reading\n\t\t\t\t\trow_dict[\"guage\"] = guage_id\n\t\t\t\t\trow_dict[\"scrape time\"] = time\n\t\t\t\t\trow_dict[\"elevation\"] = elev\n\t\t\t\t\trow_dict[\"milemarker\"] = milemarker\n\t\t\t\t\trow_dict[\"type\"] = sect_name\t\t\t \n\t\t\t\t\treadings.append(row_dict)  # add to the compilation\n\t\t\t\t\t# reset the dict for next reading\n\t\t\t\t\trow_dict={}\n\n\treturn readings\n\n", "description": "Returns a list of dicts containing relevant data from webscrape.\n\tNWS results are in UTC timezone for both observations and forecasts.\n\tResults are stored to CSV file with all timestamps in UTC.\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}, {"term": "def", "name": "generate_keys_based_on_timestamp", "data": "def generate_keys_based_on_timestamp(web_list):\n\t\"\"\"Take a list of dicts and return a key for each dict in list.\n\t\"\"\"\n\tkeys = []\n\n\tfor itm in web_list:\n\t\tcdt = itm[TS_LABEL_STR]\n\t\tkey = f\"{cdt}\"\n\t\tkeys.append(key)\n\treturn keys\n\n", "description": "Take a list of dicts and return a key for each dict in list.\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}, {"term": "def", "name": "decode_database_key", "data": "def decode_database_key(s):\n\t\"\"\"Extract Guage_id, Reading_type, Datestr form provided keystring.\n\t\"\"\"\n\tlst = s.split(\"-\")\n\tgid = lst[0]\n\trdng = lst[1]\n\tdstr = lst[2]\n\treturn (gid, rdng, dstr)\n\n", "description": "Extract Guage_id, Reading_type, Datestr form provided keystring.\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}, {"term": "def", "name": "Scrape_NWS_site", "data": "def Scrape_NWS_site(site):\n\t\"\"\"Return a dictionary of guage readings from supplied XML tabular text site.\n\t\"\"\"\n\tsite_url = site[\"guage_URL\"]\n\t# get the data for this guage\n\t(\n\t\traw_data,\n\t\tguage_id,\n\t\tfriendly_name,\n\t\tscrape_start_time,\n\t\tduration_of_scrape,\n\t) = get_NWS_web_data(site_url)\n\t\n\ttot_secs = duration_of_scrape.total_seconds()\n\tlogger.info(f\"Time to process website: {tot_secs} seconds.\")\n\tlogger.info(f\"Webscrape started at: {scrape_start_time}\")\n\t# TODO verify webscraping success\n\tguage_data = (guage_id, site[\"guage_elevation\"], site[\"milemarker\"], friendly_name)\n\tValuableData_listOfDicts = sort_and_label_data(raw_data, guage_data, timefstring(scrape_start_time))\n\t# TODO verify successful conversion of data\n\tdatabase_keys = generate_keys_based_on_timestamp(ValuableData_listOfDicts)\n\t# TODO compare length of keys_list to length of data_list for validity\n\tdatabase_dict = dict(zip(database_keys, ValuableData_listOfDicts))\n\t# TODO compare length of database to data_list to verify all items included\n\treturn (database_dict, ValuableData_listOfDicts)\n\n", "description": "Return a dictionary of guage readings from supplied XML tabular text site.\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}, {"term": "def", "name": "display_tabulardata", "data": "def display_tabulardata(datalist_of_dicts):\n\t\"\"\"create a new file based on the time of scrape\n\t\"\"\"\n\t#print(tabulate(datalist_of_dicts, headers=\"keys\"))\t\n\n", "description": "create a new file based on the time of scrape\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}, {"term": "def", "name": "save_results_to_storage", "data": "def save_results_to_storage(list_of_dicts):\n\t\"\"\"save unique readings and forecasts to longterm storage.\n\tlist_of_dicts is expected to contain one dict for each reading/forecast\n\t\"\"\"\n\tfname = list_of_dicts[0]['scrape time']\n\tlogger.info(f'Creating CSV filename: {fname}')\n\t# TODO organize storage as a tree of directories: YEAR/MONTH/DAY/xx:xx:xx\n\twrite_csv(list_of_dicts, filename=fname)\n\n", "description": "save unique readings and forecasts to longterm storage.\n\tlist_of_dicts is expected to contain one dict for each reading/forecast\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}, {"term": "def", "name": "update_web_scrape_results", "data": "def update_web_scrape_results():\n\t\"\"\"Update filesystem CSV records for latest website scrapes.\n\t\"\"\"\n\tfor guage in RIVER_GUAGES:\n\t\tdetails = RIVER_MONITORING_POINTS[guage]\n\t\tlogger.info(details)\n\t\tdbd, vdl = Scrape_NWS_site(details)\n\t\tdisplay_tabulardata(vdl)\n\t\tsave_results_to_storage(vdl)\n\t\tsleep(1) # guarantee at least 1 second difference in webscrapes timestamps\n\t\t# TODO verify webscraping success\n\t\tcount = len(dbd)\n\t\tfnme = details[GUAGE_NAME_KEY]\n\t\tlogger.info(f\"Total values retrieved: {count} from {fnme}\")\n\n\treturn True\n\n", "description": "Update filesystem CSV records for latest website scrapes.\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}, {"term": "def", "name": "Main", "data": "def Main():\n\t\"\"\"Update webscrape files.\n\t\"\"\"\n\tdefineLoggers(LOGGING_LEVEL, RUNTIME_NAME)\n\tlogger.info(\"Program Start.\")\n\ttoday = LOCAL_TODAY() \n\tlogger.info(f\"Today is: {today}\")\n\t\n\tupdate_web_scrape_results()\n\n\treturn True\n\n", "description": "Update webscrape files.\n\t", "category": "webscraping", "imports": ["from pathlib import Path", "from time import sleep", "# from pupdb.core import PupDB", "# from datetime import *", "from dateutil.parser import *", "from pytz import utc as pytzutc", "from time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY", "from time_strings import timefstring", "from loguru import logger", "# from tabulate import tabulate", "from core_logging_setup import defineLoggers", "from WebScrapeTools import retrieve_cleaned_html", "from RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY", "from data_2_csv import write_csv"]}], [], [{"term": "class", "name": "classWebscrape:", "data": "class Webscrape:\n\tkjorProg = True\n\n\t@staticmethod\n\tdef webScrape():\n\t\tnettSide = requests.get('https://www.akademika.no/computer-organization-and-architecture-global-edition/william-stallings/9781292096858')\n\t\ttree = html.fromstring(nettSide.content)\n\t\treturn tree.xpath('//*[@id=\"node-21225459\"]/div[2]/div[2]/div[1]/span/text()')\n\n\n", "description": null, "category": "webscraping", "imports": ["from lxml import html", "import requests"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(url):\n\tsession = HTMLSession()\n\n\t# Start the session, run the javascript and get the HTML\n\tr = session.get(url)\n\tr.html.render()\n\tdonor_info = r.html.find(\"#content\", first=True)\n\n\t# Split the data into a list. Now every even index (0, 2, 4, ...) should contain a label while every odd index,\n\t# (1, 3, 5, ...) should contain the actual data. This will make it easy to work with the list.\n\tdonor_info_list = donor_info.text.split(\"\\n\")\n\n\t# This turns the first item in the list from \"Donor: \" into two items, \"Donor:\" and \".\n\t# This is done to keep with the theme of every odd and every even index being a label or data.\n\tdonor_info_list[0] = donor_info_list[0].split(\" \")[1]\n\tdonor_info_list.insert(0, \"Donor\")\n\n\t# End the session in case the script is run again\n\tsession = None\n\n\t# Return the list\n\treturn donor_info_list[0:14]\n\n", "description": null, "category": "webscraping", "imports": ["from requests_html import HTMLSession", "\timport time"]}], [{"term": "def", "name": "print_hi", "data": "def print_hi(name):\n\t# Use a breakpoint in the code line below to debug your script.\n\tprint(f'Hi, {name}')  # Press \u2318F8 to toggle the breakpoint.\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import requests", "from bs4 import BeautifulSoup", "import matplotlib.pyplot as plt", "import threading", "from sklearn.linear_model import LinearRegression", "import pandas as pd", "from pylab import *", "from collections import namedtuple", "import SaveSQL", "import GraphCreator", "import WebScrape"]}], [], [], [{"term": "def", "name": "upload", "data": "def upload():\n\ts3 = boto3.resource('s3')\n\tbucket = s3.Bucket('de-car-times')\n\ts3.Object('de-car-times', f\"0:60-times/{''.join(car[i])}.csv\").put(Body=open(f\"/Users/kordellewalker/Documents/GitHub/DEProjects/Webscrape-DWH/Resultss/{''.join(car[i])}.csv\", 'rb'))\n", "description": null, "category": "webscraping", "imports": ["import boto3"]}], [], [], [{"term": "class", "name": "TechSpider", "data": "class TechSpider(scrapy.Spider):\n\tname = 'tech'\n\tallowed_domains = ['techcrunch.com']\n\tstart_urls = ['http://techcrunch.com/']\n\n\tdef parse(self, response):\n\t\tc=response.css('.content a::attr(href)').getall()\n\t\tfor i in c:\n\t\t\ti=response.urljoin(i)\n\t\t\t\n\t\t\tyield scrapy.Request(url=i,callback=self.parse_content)\n\n\n\n\tdef parse_content(self,response):\n\n\t\titem = WebscrapeItem()\n\t\tTitle=response.css('h1.article__title::text').extract()\n\t\tAuthor=response.css('div.article__byline a::text').extract()\n\t\tpost=response.css('div.article-content ::text').extract()\n\t\t\t\n\t\titem['title']= Title\n\t\titem['author']=Author\n\t\titem['post']=post\n\t\t\t\n\t\tyield item\n\t\t \n", "description": null, "category": "webscraping", "imports": ["import scrapy", "from items import WebscrapeItem"]}], [{"term": "def", "name": "welcome", "data": "def welcome():\n\treturn render_template(\"index.html\")\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "hydro", "data": "def hydro():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"hydro.html\")\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "wind", "data": "def wind():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"wind.html\")\n\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "heatmap", "data": "def heatmap():\n\t\n\treturn render_template(\"heatmap.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "solar", "data": "def solar():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"solar.html\")\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "location", "data": "def location():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"location.html\")\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\tdata =  json.load(open(\"my_renewables.json\",\"r\")) \n\treturn render_template(\"webscrape.html\",r_last_refresh=data[\"last_scrape\"],renewable_title_0=data[\"articles \"][0],renewable_link_0=data[\"links\"][0],renewable_title_1=data[\"articles \"][1],renewable_link_1=data[\"links\"][2], renewable_title_2 = data[\"articles \"][2],renewable_link_2=data[\"links\"][4],renewable_title_3=data[\"articles \"][3],renewable_link_3=data[\"links\"][6])\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "scrape", "data": "def scrape():\n\trenewable_scrape.renewable_scrape()\n\treturn redirect(\"/webscrape\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "heatmapgeojson", "data": "def heatmapgeojson():\n\treturn jsonify(data = heatmapdata)\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "data", "data": "def data():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"data.html\")\n\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}], [{"term": "def", "name": "hello_world", "data": "def hello_world():\n\treturn render_template('popup.html')\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, jsonify, request", "from flask_cors import CORS, cross_origin", "import json", "import os", "from crawler import *"]}, {"term": "def", "name": "testfunc", "data": "def testfunc():\n\n\tprint(\"testfunc\")\n\tmessage = {'flask': 'Hello World'}\n\tprint('sending hello world to extension')\n\treturn jsonify(message)\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, jsonify, request", "from flask_cors import CORS, cross_origin", "import json", "import os", "from crawler import *"]}, {"term": "def", "name": "saveTime", "data": "def saveTime():\n\n\tprint('saving time')\n\tdata = request.get_json()\n\tprint(data)\n\tif data == {}:\n\t\treturn jsonify({'flask': 'nothing received'})\n\n\tname = data['username']\n\ttime = data['time']\n\twebsite = data['website']\n\tprint(name, time, website)\n\tprint(d)\n\n\n\t#store stats of website into server\n\tif website not in websites.keys():\n\t\twebsites[website] = webscrape(website)\n\n\t#store user data into server\n\tif name in d.keys():\n\t\tif website in d[name].keys():\n\t\t\td[name][website].append(time)\n\t\telse:\n\t\t\td[name][website] = [time]\n\telse:\n\t\td[name] = {}\n\t\td[name][website] = [time]\n\n\n\t#save_file()\n\twith open(\"data.json\", \"w\") as f:\n\t\tjson.dump(d, f)\n\tmessage = {'flask': 'Time Saved!'}\n\tprint(d)\n\treturn jsonify(message)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, jsonify, request", "from flask_cors import CORS, cross_origin", "import json", "import os", "from crawler import *"]}, {"term": "def", "name": "deleteTime", "data": "def deleteTime():\n\tprint('saving time')\n\tdata = request.get_json()\n\tprint(data)\n\tif data == {}:\n\t\treturn jsonify({'flask': 'nothing received'})\n\n\tname = data['username']\n\twebsite = data['website']\n\tprint(name, website)\n\tprint(d)\n\n\t#store stats of website into server\n\tif website not in websites.keys():\n\t\twebsites[website] = webscrape(website)\n\n\tif name in d.keys():\n\t\tif website in d[name].keys():\n\t\t\tdel d[name][website]\n\n\t#save_file()\n\twith open(\"data.json\", \"w\") as f:\n\t\tjson.dump(d, f)\n\tmessage = {'flask': 'Time Deleted!'}\n\tprint(d)\n\treturn jsonify(message)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, jsonify, request", "from flask_cors import CORS, cross_origin", "import json", "import os", "from crawler import *"]}, {"term": "def", "name": "showTime", "data": "def showTime():\n\tprint(\"sending Times\")\n\tdata = request.get_json()\n\tprint(data)\n\tif data == {}:\n\t\treturn jsonify({'flask': 'nothing received'})\n\tname = data['username']\n\twebsite = data['website']\n\tprint(name, website)\n\tprint(d)\n\n\t#store stats of website into server\n\tif website not in websites.keys():\n\t\twebsites[website] = webscrape(website)\n\n\ttry:\n\t\ttemp = sorted(d[name][website])[:10]\n\texcept KeyError:\n\t\treturn jsonify({'flask': 'nothing received'})\n\n\treturn jsonify({\"time\": temp})\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, jsonify, request", "from flask_cors import CORS, cross_origin", "import json", "import os", "from crawler import *"]}, {"term": "def", "name": "showWords", "data": "def showWords():\n\tprint(\"showWords\")\n\tdata = request.get_json()\n\tprint(data)\n\tif data == {}:\n\t\treturn jsonify({'flask': 'nothing received'})\n\twebsite = data['website']\n\tprint(website)\n\tprint(d)\n\n\t#store stats of website into server\n\tif website not in websites.keys():\n\t\twebsites[website] = webscrape(website)\n\n\ttemp2 = websites[website]\n\ttemp2[0] = temp2[0][:10]\n\treturn jsonify({\"webstats\": temp2})\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, jsonify, request", "from flask_cors import CORS, cross_origin", "import json", "import os", "from crawler import *"]}], [{"term": "def", "name": "search_by_tag_view", "data": "def search_by_tag_view(request):\n\tif request.POST:\n\t\tdata = request.POST.get('search_hashtag')\n\t\tinstascrape.instascrape.delay(\n\t\t\t\"/home/emil/Desktop/instagram_influencers_application/webscrape_app/data/influenc_d.json\", 10, data)\n\treturn render(request, \"input_page.html\")\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from webscrape_app.webscrape_util import instascrape"]}], [{"term": "def", "name": "parse_results", "data": "def parse_results(\n", "description": null, "category": "webscraping", "imports": ["from contextlib import suppress", "from json import JSONDecodeError", "from requests import HTTPError, RequestException", "from tqdm import tqdm", "from scrape.company_result import CompanyResult", "from scrape.configs import JobScrapeConfig", "from scrape.web_scraper import webscrape_results"]}, {"term": "def", "name": "company_lookup", "data": "def company_lookup(company_alias: str) -> dict:\n\t\"\"\"Looks up the company JSON in BuiltInNYC. It passes this along to the superceding parse_results method,\n\twhich places it within the CompanyResult dataclass.\n\t\"\"\"\n\twith suppress(\n\t\tJSONDecodeError, RequestException, HTTPError, TypeError, AttributeError\n\t):\n\t\tcompany_page_url = f\"https://api.builtin.com/companies/alias/{company_alias}\"\n\t\tcomp_docs = webscrape_results(company_page_url, querystring={\"region_id\": \"5\"})\n\t\tindustries = [item.get(\"name\") for item in comp_docs[\"industries\"]]\n\t\tdata = {\n\t\t\t\"street_address\": comp_docs.get(\"street_address_1\"),\n\t\t\t\"suite\": comp_docs.get(\"street_address_2\"),\n\t\t\t\"city\": comp_docs.get(\"city\"),\n\t\t\t\"state\": comp_docs.get(\"state\"),\n\t\t\t\"zip\": comp_docs.get(\"zip\"),\n\t\t\t\"mission\": comp_docs.get(\"mission\"),\n\t\t\t\"url\": comp_docs.get(\"url\"),\n\t\t\t\"adjectives\": comp_docs.get(\"adjectives\"),\n\t\t\t\"industries\": industries,\n\t\t\t\"twitter\": comp_docs.get(\"twitter\"),\n\t\t\t\"email\": comp_docs.get(\"email\"),\n\t\t}\n\t\treturn data\n", "description": "Looks up the company JSON in BuiltInNYC. It passes this along to the superceding parse_results method,\n\twhich places it within the CompanyResult dataclass.\n\t", "category": "webscraping", "imports": ["from contextlib import suppress", "from json import JSONDecodeError", "from requests import HTTPError, RequestException", "from tqdm import tqdm", "from scrape.company_result import CompanyResult", "from scrape.configs import JobScrapeConfig", "from scrape.web_scraper import webscrape_results"]}], [{"term": "def", "name": "scrape", "data": "def scrape():\n\tsite = raw_input(\"Enter page: \")\n\n\t#open site. read so we can read in a string context\n\t#test for valid and complete URL\n\ttry:\n\t\tdata = urllib2.urlopen(site).read()\n\texcept ValueError:\n\t\tprint \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n\t\treturn\n\t\n\t#print data\n\n\t#try an open the pattern file.\n\ttry:\n\t\tpatternFile = open('config/webscrape.dat', 'r').read().splitlines()\n\texcept:\n\t\tprint \"There was an error opening the webscrape.dat file\"\n\t\traise\n\t#create counter for counting regex expressions from webscrape.dat\n\tcounter = 0\n\t#for each loop so we can process each specified regex\n\tfor pattern in patternFile:\n\t\tm = re.findall(pattern, data)\n\t\t#m will return as true/false. Just need an if m:\n\t\tif m:\n\t\t\tfor i in m:\n\t\t\t\t#open output/results file...append because we are cool\n\t\t\t\toutfile = open('scrape-RESULTS.txt', 'a')\n\t\t\t#print m\n\t\t\t\toutfile.write(str(i))\n\t\t\t\toutfile.write(\"\\n\")  # may be needed. can always be removed.\n\n\t\t\t#close the file..or else\n\t\t\t\toutfile.close()\n\t\t\t\tcounter+=1\n\t\t\t\tprint \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n\t\telse:  # only need an else because m is boolean\n\t\t\tcounter+=1\n\t\t\tprint \"No match for item \" + str(counter) + \". Continuing.\"\n\t\t\t# Continue the loop if not a match so it can go on to the next\n\t\t\t# sequence\n\t\t\t# NOTE: you don't *really* need an else here...\n\t\t\tcontinue\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "import re", "\timport readline  # nice when you need to use arrow keys and backspace", "import sys"]}], [{"term": "def", "name": "scrape", "data": "def scrape():\n\tsite = raw_input(\"Enter page: \")\n\n\t#open site. read so we can read in a string context\n\t#test for valid and complete URL\n\ttry:\n\t\tdata = urllib2.urlopen(site).read()\n\texcept ValueError:\n\t\tprint \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n\t\treturn\n\t\n\t#print data\n\n\t#try an open the pattern file.\n\ttry:\n\t\tpatternFile = open('config/webscrape.dat', 'r').read().splitlines()\n\texcept:\n\t\tprint \"There was an error opening the webscrape.dat file\"\n\t\traise\n\t#create counter for counting regex expressions from webscrape.dat\n\tcounter = 0\n\t#for each loop so we can process each specified regex\n\tfor pattern in patternFile:\n\t\tm = re.findall(pattern, data)\n\t\t#m will return as true/false. Just need an if m:\n\t\tif m:\n\t\t\tfor i in m:\n\t\t\t\t#open output/results file...append because we are cool\n\t\t\t\toutfile = open('scrape-RESULTS.txt', 'a')\n\t\t\t#print m\n\t\t\t\toutfile.write(str(i))\n\t\t\t\toutfile.write(\"\\n\")  # may be needed. can always be removed.\n\n\t\t\t#close the file..or else\n\t\t\t\toutfile.close()\n\t\t\t\tcounter+=1\n\t\t\t\tprint \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n\t\telse:  # only need an else because m is boolean\n\t\t\tcounter+=1\n\t\t\tprint \"No match for item \" + str(counter) + \". Continuing.\"\n\t\t\t# Continue the loop if not a match so it can go on to the next\n\t\t\t# sequence\n\t\t\t# NOTE: you don't *really* need an else here...\n\t\t\tcontinue\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "import re", "\timport readline  # nice when you need to use arrow keys and backspace", "import sys"]}], [{"term": "def", "name": "require_input", "data": "def require_input(recommend, question, requirement):\n\twhile True:\n\t\tprint(recommend)\n\t\tinp = input(question)\n\n\t\tif requirement(inp) is True:\n\t\t\tbreak\n\n\treturn inp\n", "description": null, "category": "webscraping", "imports": ["from utils.const import __CUR_DIR__", "from utils.Pipeline import *"]}, {"term": "def", "name": "yes_or_no", "data": "def yes_or_no(inp):\n\tif inp == \"y\":\n\t\treturn True\n\telif inp == \"n\":\n\t\treturn True\n\telse:\n\t\tprint(\"Input must be 'y' or 'n'\")\n\t\treturn False\n\n", "description": null, "category": "webscraping", "imports": ["from utils.const import __CUR_DIR__", "from utils.Pipeline import *"]}, {"term": "def", "name": "file_exists", "data": "def file_exists(inp):\n\tif not os.path.isfile(os.path.join(__CUR_DIR__, inp+\".xlsx\")):\n\t\tprint(\"File not found\")\n\t\treturn False\n\treturn True\n", "description": null, "category": "webscraping", "imports": ["from utils.const import __CUR_DIR__", "from utils.Pipeline import *"]}, {"term": "def", "name": "is_integer", "data": "def is_integer(inp):\n\ttry:\n\t\tint(inp)\n\t\treturn True\n\texcept:\n\t\tprint(\"Must be an integer\")\n\t\treturn False\n", "description": null, "category": "webscraping", "imports": ["from utils.const import __CUR_DIR__", "from utils.Pipeline import *"]}, {"term": "def", "name": "_def=require_input", "data": "use_def = require_input(\"Recommend -> Defaults? [y/n]:y\", \"Defaults? [y/n]:\", yes_or_no)\n", "description": null, "category": "webscraping", "imports": ["from utils.const import __CUR_DIR__", "from utils.Pipeline import *"]}, {"term": "def", "name": "ifuse_def==\"n\":", "data": "if use_def == \"n\":\n\n\tprint(\"1. Choose business directory name\")\n\tbus_dir_file = require_input(\"Do not put file extension, Recommend -> bus_dir:data\\\\london_bus_dir\",\n\t\t\t\t\t\t\t\t \"bus_dir:\", lambda inp: True)\n\n\tprint(\"\\n2. Set GoogleAPI request limit\")\n\tsession_limits = require_input(\"Recommend -> session_requests:4000\", \"session_requests:\", is_integer)\n\n\n\tprint(\"\\n3. Choose location input file\")\n\tarea_table_file = require_input(\n\t\t\"Location list, ###.xlsx must contain column called 'Location String' which it will use for the search \\nRecommend -> area_file:areas\\\\london\",\n\t\t\"area_file:\", file_exists)\n\n\tprint(\"\\n4. Choose places search keyword \\nUse different search keyword than 'Eyelashes'?\")\n\tchange_key = require_input(\"Recommend -> [y/n]:n\", \"Change keyword from 'Eyelashes'? [y/n]:\", yes_or_no)\n\n\tif change_key == \"n\":\n\t\tkey = \"Eyelashes\"\n\telse:\n\t\tkey = input(\"Enter keyword:\")\n\n", "description": null, "category": "webscraping", "imports": ["from utils.const import __CUR_DIR__", "from utils.Pipeline import *"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(URL):\n\tpage = requests.get(URL)\n\tsoup = bs(page.content,'html.parser')\n\t\n\ttable = soup.find_all('table')[0] \n\t\n\tdict = {\"Date\":[],\"Rate\":[]};\n\t\n\tfor row in table.find_all('tr'):\n\t\tdict[\"Date\"].append(row.text[0:11].strip())\n\t\tdict[\"Rate\"].append(row.text[25:35].strip())\n\t\t\n\tforex_df = pd.DataFrame(dict)\n\t\n\tforex_df['Date']= pd.to_datetime(forex_df['Date'])\n\t\n\tforex_df = forex_df[forex_df.Rate != 'ND']\n\t\n\tforex_df[\"Rate\"] = forex_df[\"Rate\"].astype(float).round(5)\n\tprint(forex_df.dtypes)\n\t\n\tforex_df.to_csv(r'forexscrape.csv')\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup as bs", "import requests", "import pandas as pd", "import pathlib"]}, {"term": "def", "name": "checkforwebscrape", "data": "def checkforwebscrape():\n\tfile = pathlib.Path('forexscrape.csv')\n\tif file.exists ():\n\t\t\"File already exists\"\n\telse:\n\t\tprint(\"Webscraping\")\n\t\twebscrape('https://www.federalreserve.gov/releases/h10/hist/dat00_eu.htm')\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup as bs", "import requests", "import pandas as pd", "import pathlib"]}], [{"term": "class", "name": "Node", "data": "class Node():\n\tdef __init__(self, topic, dist, children):\n\t\tself.topic = topic\n\t\tself.dist = dist\n\t\tself.children = children\n\n\n", "description": null, "category": "webscraping", "imports": ["# from \"../runner.py\" import mapper ", "from webscrape import webscrape"]}, {"term": "def", "name": "BFS", "data": "def BFS(Node n):\n\t\"\"\" Traverse n by 2 levels and add every node's topic traversed to a set\n\t1. access new subnodes through Wikimap\n\t2. traverse every node in the first level and add to set\n\t3. recurse on each node in the first level. Stop after.\n\n\tInput -> Node\n\tOutput -> Set of subnodes connected to Node w/in 2 lvls\n\t\"\"\"\n\tset = set([])\n\n\n", "description": " Traverse n by 2 levels and add every node's topic traversed to a set\n\t1. access new subnodes through Wikimap\n\t2. traverse every node in the first level and add to set\n\t3. recurse on each node in the first level. Stop after.\n\n\tInput -> Node\n\tOutput -> Set of subnodes connected to Node w/in 2 lvls\n\t", "category": "webscraping", "imports": ["# from \"../runner.py\" import mapper ", "from webscrape import webscrape"]}, {"term": "def", "name": "populate_map", "data": "def populate_map():\n\t\"\"\" creates hashmap between each topic node and the set of subnodes it is connected to within 2 levels\n\tkey -> topic node\n\tvalue -> set of subnodes\n\n\tInput -> none  (global var headers)\n\tOutput -> set of nodes and its corresponding subnodes\n\t\"\"\"\n\tmap = {}\n\tfor header in headers:\n\t\theader_node = Node(header, 0, None)\n\t\tsubnodes_set = BFS(header_node)\n\t\tmap.add(header_node, subnodes_set)\n\n", "description": " creates hashmap between each topic node and the set of subnodes it is connected to within 2 levels\n\tkey -> topic node\n\tvalue -> set of subnodes\n\n\tInput -> none  (global var headers)\n\tOutput -> set of nodes and its corresponding subnodes\n\t", "category": "webscraping", "imports": ["# from \"../runner.py\" import mapper ", "from webscrape import webscrape"]}, {"term": "def", "name": "calc_relations", "data": "def calc_relations(map):\n\t\"\"\" Given the hashmap, we look for duplicates in the values of every 2-combination of topics\n\tand create a mapping from every 2 combinations to a true or false, indicating whether there is\n\ta relation between the 2 topics or not.\n\t\"\"\"\n", "description": " Given the hashmap, we look for duplicates in the values of every 2-combination of topics\n\tand create a mapping from every 2 combinations to a true or false, indicating whether there is\n\ta relation between the 2 topics or not.\n\t", "category": "webscraping", "imports": ["# from \"../runner.py\" import mapper ", "from webscrape import webscrape"]}], [{"term": "def", "name": "error_free", "data": "def error_free(character):\n\tforbidden_characters_list = forbidden_characters\n\tfor forbidden_character in forbidden_characters_list:\n\t\tif forbidden_character in str(character):\n\t\t\treturn False\n\tif not verify_html(error_validation_url, character):\n\t\treturn False\n\treturn True\n\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4", "from app.settings import Settings", "# separated by commas to work correctly, so the formatting function is important"]}, {"term": "def", "name": "verify_html", "data": "def verify_html(url, character):\n\tverified = True\n\thtml = requests.get(f'{url}{character}')\n\ttry:\n\t\thtml.raise_for_status()\n\texcept Exception as exc:\n\t\tverified = False\n\treturn verified\n\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4", "from app.settings import Settings", "# separated by commas to work correctly, so the formatting function is important"]}, {"term": "def", "name": "request_html", "data": "def request_html(url, character):\n\thtml = requests.get(f'{url}{character}')\n\ttry:\n\t\thtml.raise_for_status()\n\texcept Exception as exc:\n\t\tprint(f'Add {character} to list of forbidden characters')\n\t\tpass\n\treturn html\n\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4", "from app.settings import Settings", "# separated by commas to work correctly, so the formatting function is important"]}, {"term": "def", "name": "parse_html", "data": "def parse_html(html):\n\tparsed_html = bs4.BeautifulSoup(html.text, 'lxml')\n\treturn parsed_html\n\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4", "from app.settings import Settings", "# separated by commas to work correctly, so the formatting function is important"]}, {"term": "def", "name": "produce_tags", "data": "def produce_tags(parsed_html, tag_type, tag_attr, tag_name):\n\ttag_list = []\n\tneeded_tags = parsed_html.findAll(lambda tag: tag.name == f'{tag_type}' and tag.get(f'{tag_attr}') == [f'{tag_name}'])\n\tfor needed_tag in needed_tags:\n\t\ttag_list.append(needed_tag.text)\n\treturn tag_list\n\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4", "from app.settings import Settings", "# separated by commas to work correctly, so the formatting function is important"]}, {"term": "def", "name": "format_translations", "data": "def format_translations(tag_list):\n\tformatted_tag_list = []\n\tfor tag in tag_list:\n\t\ttag_string = tag.replace('\\n', ' ')\n\t\ttag_string = tag_string.replace(',', ' ')\n\t\ttag_string = tag_string.replace('  ', ' ')\n\t\ttag_string = tag_string.replace(' ', ', ')\n\t\tformatted_tag_list.append(tag_string)\n\treturn ', '.join(formatted_tag_list)\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4", "from app.settings import Settings", "# separated by commas to work correctly, so the formatting function is important"]}, {"term": "def", "name": "limit_string", "data": "def limit_string(string, string_number):\n\tcommas_index = []\n\tfor pos, char in enumerate(string):\n\t\tif char == ',':\n\t\t\tcommas_index.append(pos)\n\n\tstring_produced = False\n\ti = string_number\n\n\twhile string_produced is False and i > 0:\n\t\ttry:\n\t\t\tstring = string[0: commas_index[i - 1]]\n\t\t\tstring_produced = True\n\t\texcept Exception as exc:\n\t\t\ti = i - 1\n\n\treturn string\n\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4", "from app.settings import Settings", "# separated by commas to work correctly, so the formatting function is important"]}, {"term": "def", "name": "webscrape_mule", "data": "def webscrape_mule(url, character, tag_type, tag_attr, tag_name, format_translations, string_number):\n\thtml = request_html(url, character)\n\tparsed_html = parse_html(html)\n\ttags = produce_tags(parsed_html, f'{tag_type}', f'{tag_attr}', f'{tag_name}')\n\tformatted_translations = format_translations(tags)\n\ttranslation = limit_string(formatted_translations, string_number)\n\treturn translation\n\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4", "from app.settings import Settings", "# separated by commas to work correctly, so the formatting function is important"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(character, string_number):\n\ttranslation = webscrape_mule(\n\t\t  url=Settings.url,\n\t\t  character=f'{character}',\n\t\t  tag_type=Settings.tag_type,\n\t\t  tag_attr=Settings.tag_attr,\n\t\t  tag_name=Settings.tag_name,\n\t\t  format_translations=Settings.format,\n\t\t  string_number=string_number,\n\t\t  )\n\treturn translation\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4", "from app.settings import Settings", "# separated by commas to work correctly, so the formatting function is important"]}], [{"term": "class", "name": "classWebScraper:", "data": "class WebScraper:\n\tdef __init__(self):\n\t\tself.s = Service(const.CHROME_DRIVER)\n\t\tself.options = Options()\n\t\tself.options.add_argument(const.CHROME_DETAILS)\n\t\tself.driver = webdriver.Chrome(service=self.s, options=self.options)\n\t\tself.project_names = []\n\t\tself.project_deadlines = []\n\t\tself.project_tam_sum = []\n\t\tself.project_tam_intenz = []\n\t\tself.project_tam_form = []\n\t\tself.project_bead_kezdet = []\n\t\tself.project_tam_min_sum = []\n\n\tdef webscrape_checker(self):\n\t\tprint(self.project_names)\n\t\tprint(self.project_deadlines)\n\t\tprint(self.project_tam_sum)\n\t\tprint(self.project_tam_intenz)\n\t\tprint(self.project_tam_form)\n\t\tprint(self.project_bead_kezdet)\n\t\tprint(self.project_tam_min_sum)\n\n\tdef webscrape(self):\n\t\tself.driver.get(const.TENDER_LINK)\n\t\tsleep(3)\n\t\ttender_on = True\n\t\ti = 1\n\t\twhile tender_on:\n\t\t\thtml_source = self.driver.page_source\n\t\t\twith open(f'html_files/file{i}.html', mode=\"w\", encoding=\"utf-8\") as fp:\n\t\t\t\tfp.write(html_source)\n\n\t\t\twith open(f'html_files/file{i}.html', mode=\"r\", encoding=\"utf-8\") as fp:\n\t\t\t\tcontent = fp.read()\n\n\t\t\tsoup = BeautifulSoup(content, 'html.parser')\n\n\t\t\tproject_titles = soup.find_all(name='p', class_=\"project-title\")\n\t\t\tproject_titles_ = [project_title.get_text() for project_title in project_titles]\n\t\t\tfor item in project_titles_:\n\t\t\t\tself.project_names.append(item)\n\n\t\t\tproject_descriptions = soup.find_all(name='div', class_=\"project-description\")\n\t\t\tproject_descriptions_ = [project_description.get_text() for project_description in project_descriptions]\n\t\t\tproject_descriptions_string = ''.join(project_descriptions_).replace(chr(160), ' ').replace('T\u00e1mogat\u00e1s maximum \u00f6sszege ', '').replace('T\u00e1mogat\u00e1si intenzit\u00e1s ', '').replace('T\u00e1mogat\u00e1s form\u00e1ja ','').replace('Bead\u00e1s kezdete ', '').replace('T\u00e1mogat\u00e1s minimum \u00f6sszege ', '').replace('P\u00e1ly\u00e1zati dokument\u00e1ci\u00f3Ter\u00fcleti szerepl\u0151k', '').replace('P\u00e1ly\u00e1zati dokument\u00e1ci\u00f3', '').replace('P\u00e1ly\u00e1zati kit\u00f6lt\u0151', '')\n\t\t\tproject_descriptions_fixed = project_descriptions_string.split('Bead\u00e1si hat\u00e1rid\u0151 :')\n\t\t\tproject_descriptions_fixed.remove('')\n\n\t\t\tfor item in project_descriptions_fixed:\n\t\t\t\tlist = item.split(':')\n\t\t\t\tself.project_deadlines.append(list[0])\n\t\t\t\tself.project_tam_sum.append(list[1])\n\t\t\t\tself.project_tam_intenz.append(list[2])\n\t\t\t\tself.project_tam_form.append(list[3])\n\t\t\t\tself.project_bead_kezdet.append(list[4])\n\t\t\t\tself.project_tam_min_sum.append(list[5])\n\n\t\t\ti = i + 1\n\t\t\tself.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n\t\t\tsleep(2)\n\t\t\tpage = soup.find(name=\"span\", class_=\"react-bootstrap-table-pagination-total\").get_text().replace('- ','').replace('/ ', '').split(' ')\n\t\t\tprint(page)\n\t\t\tif page[1] != page[2]:\n\t\t\t\tself.driver.find_element(By.XPATH,'//*[@id=\"root\"]/div/div[5]/div/div/div[3]/div[2]/div[2]/ul/li[13]/span/button').click()\n\t\t\t\tsleep(5)\n\t\t\telse:\n\t\t\t\ttender_on = False\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "from time import sleep", "from selenium import webdriver", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.common.by import By", "from selenium.webdriver.chrome.options import Options", "import const"]}], [{"term": "class", "name": "classstance_conversion:", "data": "class stance_conversion:\n\tdef Orthadox_stance_conversion(df):\n\t\tdf['Stance_Orth'] = df.apply(lambda x: x['Stance'] == 'Orthodox', axis = 1)\n\t\tdf['Stance_Orth'] = df['Stance_Orth'].replace([True, False], [1, 0])\n\t\treturn df\n\tdef Southpaw_stance_conversion(df):\n\t\tdf['Stance_South'] = df.apply(lambda x: x['Stance'] == 'Southpaw', axis=1)\n\t\tdf['Stance_South'] = df['Stance_South'].replace([True, False], [1, 0])\n\t\treturn df\n\tdef Switch_stance_conversion(df):\n\t\tdf['Stance_Switch'] = df.apply(lambda x: x['Stance'] == 'Switch', axis=1)\n\t\tdf['Stance_Switch'] = df['Stance_Switch'].replace([True, False], [1, 0])\n\t\tdf = df.drop(columns='Stance')\n", "description": null, "category": "webscraping", "imports": ["from Data_processing_and_webscrape.Web_scrape import all_fighter_df, dimension_conversion, most_recent_event", "from Utils import *"]}, {"term": "class", "name": "classNormalization:", "data": "class Normalization:\n\tdef normalization_minmax(df):\n\t\tdf = (df - min(df)) / (max(df) - min(df))\n\t\treturn df\n\tdef normalization_of_df(df):\n\t\tdf['Reach'] = Normalization.normalization_minmax(df['Reach'])\n\t\tdf['Ht.'] = Normalization.normalization_minmax(df['Ht.'])\n\t\tdf['L'] = Normalization.normalization_minmax(df['L'])\n", "description": null, "category": "webscraping", "imports": ["from Data_processing_and_webscrape.Web_scrape import all_fighter_df, dimension_conversion, most_recent_event", "from Utils import *"]}, {"term": "class", "name": "classht_reach_manipulation:", "data": "class ht_reach_manipulation:\n\tdef removing_str_from_int(df):\n\t\tdf['Ht.'] = df['Ht.'].replace(\"\\'\", '', regex=True).replace('\"', '', regex=True).replace(\n\t\t\t' ', '', regex=True).replace('--', '', regex=True)\n\t\tdf = df[df['Ht.'] != '']\n\t\tdf = df[df['Reach'] != '--']\n\t\treturn df\n\tdef ht_manipulation(df):\n\t\tdf['Ht.inch_0'] = df['Ht.'].str[0]\n\t\tdf['Ht.inch_1'] = df['Ht.'].str[1]\n\t\tdf['Ht.inch_2'] = df['Ht.'].str[2]\n\t\tdf = df.fillna('')\n\t\tdf['Ht.inch_0'] = df['Ht.inch_0']\n\t\tdf['Ht.inch_1'] = df['Ht.inch_1']\n\t\tdf['Ht.inch_2'] = df['Ht.inch_2']\n\t\tdf['Ht.inch_3'] = df['Ht.inch_1'] + df['Ht.inch_2']\n\t\tdf['Ht.inch_1'] = df['Ht.inch_3']\n\t\tdf['Ht.inch_0'] = dimension_conversion.ft_to_cm(dimension_conversion.str_to_int_convers(df['Ht.inch_0']))\n\t\tdf['Ht.inch_1'] = dimension_conversion.inch_to_cm(dimension_conversion.str_to_int_convers(df['Ht.inch_1']))\n\t\tdf['Ht.'] = df['Ht.inch_1'] + df['Ht.inch_0']\n\t\tdf = df.drop(columns=['Ht.inch_3', 'Ht.inch_2', 'Ht.inch_1', 'Ht.inch_0'])\n\t\treturn df\n\tdef reach_manipulation(df):\n\t\tdf['Reach'] = df['Reach'].replace('\"', '', regex=True)\n\t\tdf['Reach'] = dimension_conversion.inch_to_cm(dimension_conversion.str_to_int_convers(all_fighter_df['Reach']))\n", "description": null, "category": "webscraping", "imports": ["from Data_processing_and_webscrape.Web_scrape import all_fighter_df, dimension_conversion, most_recent_event", "from Utils import *"]}, {"term": "class", "name": "classevent_test_set:", "data": "class event_test_set:\n\tdef fighter_name_latest_event(df):\n\t\tdf = pd.DataFrame(df['Fighter'].str.split('  ', 1, expand=True).stack(). \\\n\t\t\t\t\t\t  reset_index(level=1, drop=True))\n\t\tdf[['First', 'Last']] = df[0].str.split(' ', 1, expand=True)\n\t\tdf = df[['First', 'Last']]\n\t\treturn df\n\tdef real_event_test_set(webscrape_df):\n\t\twebscrape_df = webscrape_df.fillna(null_number)\n\t\twebscrape_df = webscrape_df[webscrape_df.First != null_number].drop(columns=['Nickname', 'Wt.', 'Belt',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t 'W', 'D'])\n\t\treturn webscrape_df\n\tdef removing_null_fighter(df):\n\t\tNull_fighter_information = df[df.isna().any(axis=1)]\n\t\tNull_fighter_information = Null_fighter_information.drop(columns=['Ht.', 'Reach', 'L', 'Stance_Orth',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'Stance_South', 'Stance_Switch'])\n\t\treturn Null_fighter_information\n\t@staticmethod\n\tdef real_test_set_manipulation():\n\t\treal_test_set = even_or_odd_index(event_test_set.removing_null_fighter(merged_test_set), merged_test_set)\n\t\treal_test_set = real_test_set.drop(columns=['First', 'Last'])\n\t\treal_test_set = real_test_set[['L', 'Stance_Orth', 'Stance_South', 'Stance_Switch', 'Ht.', 'Reach']]\n\t\treal_test_set['Reach'] = Normalization.normalization_minmax(real_test_set['Reach'])\n\t\treal_test_set['Ht.'] = Normalization.normalization_minmax(real_test_set['Ht.'])\n\t\treal_test_set['L'] = Normalization.normalization_minmax(real_test_set['L'])\n", "description": null, "category": "webscraping", "imports": ["from Data_processing_and_webscrape.Web_scrape import all_fighter_df, dimension_conversion, most_recent_event", "from Utils import *"]}, {"term": "def", "name": "even_or_odd_index", "data": "def even_or_odd_index(null_df_fighter, df):\n\tfor i in null_df_fighter.index:\n\t\tdf = df.drop(index=i)\n\t\tif i % 2 != 0:\n\t\t\tdf = df.drop(index = i - 1)\n\t\telif i % 2 == 0:\n\t\t\tdf = df.drop(index = i + 1)\n", "description": null, "category": "webscraping", "imports": ["from Data_processing_and_webscrape.Web_scrape import all_fighter_df, dimension_conversion, most_recent_event", "from Utils import *"]}, {"term": "def", "name": "exporting_names_to_excel", "data": "def exporting_names_to_excel(df):\n\texport_dataframe = df[['First', 'Last']]\n\texport_dataframe.columns = ['First', 'Last']\n\treturn export_dataframe\n", "description": null, "category": "webscraping", "imports": ["from Data_processing_and_webscrape.Web_scrape import all_fighter_df, dimension_conversion, most_recent_event", "from Utils import *"]}], [{"term": "class", "name": "GraphValues", "data": "class GraphValues(WebAnalysis) :\n\tdef x_axis(self):\n\t\tx_axis_values = [item[0] for item in self.word_count()]\n\t\treturn x_axis_values\n\t\n\tdef y_axis(self):\n\t\ty_axis_values = [item[1] for item in self.word_count()]\n\t\treturn y_axis_values\n", "description": null, "category": "webscraping", "imports": ["from webanalysis import WebAnalysis", "from webscrape import *"]}], [{"term": "def", "name": "get_td_name", "data": "def get_td_name(td):\n\ta_tag = td.findAll('a')\n\tfor a in a_tag:\n\t\tif 'href' in a.attrs:\n\t\t\tname = a.attrs['href'].split('/')[-1]\n\t\t\t# td_name = name.replace('_', '/')\n\t\t\treturn name\n\t\telse:\n\t\t\tprint(a)   \n\t\n\treturn None\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "fget_ImageCollection", "data": "# def get_ImageCollection(name):\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "get_ImageCollection_tags", "data": "def get_ImageCollection_tags(td):\n\ttag_list = []\n\ta_tag = td.findAll('a')\n\tfor a in a_tag:\n\t\tif 'href' in a.attrs:\n\t\t\ttag = a.attrs['href'].split('/')[-1]\n\t\t\tif tag not in tag_list:\n\t\t\t\ttag_list.append(tag)\n\t\t\t\n\treturn tag_list\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "get_tbody_info", "data": "def get_tbody_info(tbody):\n\ttbody_info = {}\n\ttd_list = tbody.findAll('td')\n\n\tfor td in td_list:\n\t\tif td.attrs['class'] == ['ee-dataset']:\n\t\t\ttd_name = get_td_name(td)\n\t\t\t\n\t\telif td.attrs['class'] == ['ee-dataset-description-snippet']:\n\t\t\tquick_description = td.text\n\t\t\t\n\t\telif td.attrs['class'] == ['ee-tag-buttons', 'ee-small']:\n\t\t\ttag_list = get_ImageCollection_tags(td)\n\t\t\t\n\t\telse:\n\t\t\tprint(\"Undefined attributes in td object: \", td.attrs)\n\t\t\n\n\t# IC_id, image_collection = get_ImageCollection(td_name)\n\t\t\n\ttbody_info = {'dataset': td_name,\n\t\t\t\t   'tags': tag_list,\n\t\t\t\t   'description': quick_description\n\t\t\t\t  }\n\n\treturn tbody_info\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "parse_code_block", "data": "def parse_code_block(code_block):\n\t\"\"\" Break out the Code Embedded on Earth-Engine Dataset URL\n\n\tExample output of code_block:\n\n\t\tee.ImageCollection(\"\")\n\n\tdataset_type follows the ee.-> before first '('\n\tdataset_id inside \" \"\n\n\t\"\"\"\n\t# Extract Datatepy in code block\n\tdataset_type = code_block.text.split('.')[1]\n\tdataset_type = dataset_type.split('(')[0]\n\n\t# Extract ID from the code block 'ee.Image...(\"\")'\n\tdataset_id = code_block.text.split('(')[1]\n\tdataset_id = dataset_id.split(')')[0]\n\tdataset_id = dataset_id.replace('\"','')\n\n\treturn dataset_type, dataset_id\n\n\n", "description": " Break out the Code Embedded on Earth-Engine Dataset URL\n\n\tExample output of code_block:\n\n\t\tee.ImageCollection(\"\")\n\n\tdataset_type follows the ee.-> before first '('\n\tdataset_id inside \" \"\n\n\t", "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "validate_availability", "data": "def validate_availability(text):\n\t\"\"\" we will use this a test function to extract a start/end date\"\"\"\n\n\t# Test 0: ensure the text is actually a string before splitting\n\ttry:\n\t\tassert isinstance(text, str), \"not a string\"\n\texcept Exception as e:\n\t\t# raise Exception('Date Validate Failed: {}'.format(e))\n\t\tprint(e)\n\t\treturn None\n\n\t# Test 1: The text must be split evenly by ' - '\n\tsplit_text = text.split(' - ')\n\ttry:\n\t\tassert len(split_text) == 2, \"len()==2 Test\"\n\texcept Exception as e:\n\t\t# raise Exception('Date Validate Failed: {}'.format(e))\n\t\tprint(e)\n\t\treturn None\n\n\t# Test 2: DateTime Formatting\n\t# Test 2a: First split has DateTime format (Most Common ISO format)\n\ttry:\n\t\t# date_start = datetime.strptime(split_text[0], '%Y-%m-%')\n\t\tdate_start = datetime.datetime.fromisoformat(split_text[0])\n\n\t# Test 2b: Use custom strptime, non-ISO\n\texcept Exception as e:\n\t\tdate_start = datetime.datetime.strptime(split_text[0], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n\t\tassert isinstance(date_start, datetime.datetime), \"Not a Datetime object\"\n\n\texcept:\n\t\t# raise Exception('Date Validate Failed: {}'.format(e))\n\t\tprint(\"Not a datetime object: {}\".format(split_text[0]))\n\t\treturn None\n\n\tfinally:\n\t\t# Since we are packing this into a json file, the best way is to keep string\n\t\t# the date end will be determined later, but if we succeeded in date start\n\t\t# We have found our Date Availability Match!\n\t\tdate_range = {'dataset_start': split_text[0], 'dataset_end': split_text[1]}\n\t\treturn date_range\n\n", "description": " we will use this a test function to extract a start/end date", "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "seek_date_availability", "data": "def seek_date_availability(soup):\n\t\"\"\" This is difficult to be exact, the html is not clearly/uniquely marked\"\"\"\n\tdate_range = None\n\t# Method 1: the first dd on the page:\n\ttry:\n\t\tdd = soup.find(\"dd\")\n\t\tdate_range = validate_availability(dd.text)\n\t\tassert date_range is not None, \"Method 1\"\n\t\treturn date_range\n\texcept Exception as e:\n\t\t# raise Exception('Failed on {}, {}'.format(dd, e))\n\t\tprint(e)\n\n\t# Method 2: all dd's on page, find the one that satisfies a time format validation\n\tfor dd in soup.findAll(\"dd\"):\n\t\tdate_range = validate_availability(dd.text)\n\t\tif date_range is not None:\n\t\t\treturn date_range\n\n\tprint(\"METHOD 2 FAILED TO FIND DATE AVAILABILITY in each Method, SET DEFAULT\")\n\t# determine Earliest and Latest Date and set those as defaults\n\tdefault_start = None\n\tdefault_end = None\n\tdate_range = {'dataset_start': default_start, 'dataset_end': default_end}\n\treturn date_range\n\n\n\n", "description": " This is difficult to be exact, the html is not clearly/uniquely marked", "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "follow_dataset_link", "data": "def follow_dataset_link(tag_name, URL):\n\tdataset_url = URL + '/' + tag_name\n\tresponse = requests.get(dataset_url)\n\thtmlCode = response.text\n\tsoup = BeautifulSoup(response.text, 'html.parser')\n\tcode = soup.find(\"code\")\n\n\tdataset_type, dataset_id = parse_code_block(code)\n\tresult = {'dataset_id': dataset_id, 'dataset_type': dataset_type}\n\tresult.update(seek_date_availability(soup))\n\n\treturn result\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "save_catalog", "data": "def save_catalog(results, partial=False):\n\t\"\"\"Save this simple metadata to json \"\"\"\n\tPLUGIN_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\tMETADATA_DIR = os.path.join(PLUGIN_DIR, \"metadata\") \n\n\t# Base Filename\n\tfn = 'gee_catalog'\n\tftype = '.json'\n\t\n\t# if we have partial results, track the last result index\n\tif partial:\n\t\tfn = fn + '_(' + str(len(results)-1) + ')'\n\n\t# Add all\n\tfn = fn + ftype\n\t# now get full file path + name\n\tfilename = os.path.join(METADATA_DIR, fn)\n\n\twith open(filename, 'w') as outfile:\n\t\tjson.dump(results, outfile)\n\n\n", "description": "Save this simple metadata to json ", "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "get_catalog", "data": "def get_catalog():\n\t#=====================================\n\t# 0. Run the requests on the Google Earth Engine Datasets Catalog\n\t#=====================================\n\n\t# From the \"View all datasets\" Tab on developers.google.com/earth-engine\n\turl = \"https://developers.google.com/earth-engine/datasets/catalog\"\n\tresponse = requests.get(url) \n\thtmlCode = response.text \n\tsoup = BeautifulSoup(response.text, 'html.parser') \n\n\t# the 'tbody' element seems to be the best way to extract the useful metadata for each dataset\n\tall_tbodies = soup.findAll(\"tbody\")\n\n\t# total GEE datasets: 409 as of 2020-07-21\n\t# print(len(all_tbodies))\n\n\t#=====================================\n\t# 1. Iterate through each tbody to extract metadata\n\t#=====================================\n\n\t# this will result in a list of each dataset saved as a json metadata file\n\t# for allowing user to filter on tags/geography/time before \n\t# making requests to the gee server\n\twebscrape_results = []\n\n\tfor tbody in all_tbodies:\n\t\t# 1a. tbody will have 3 basic infos: ImageCollection Name, Tags, Description\n\t\t\n\t\ttry:\n\t\t\tthis_result = get_tbody_info(tbody)\n\t\t\tmore_result = follow_dataset_link(this_result['dataset'], url)\n\t\t\tthis_result.update(more_result)\n\t\t\twebscrape_results.append(this_result)\n\n\t\t\tupdate_msg = \"Webscrape Status: {} out of {}\".format(len(webscrape_results), len(all_tbodies))\n\t\t\tprint(update_msg, end=\"\\r\", flush=True)\n\n\t\texcept Exception as e:\n\t\t\tprint(e)\n\t\t\tprint(\"failed data extract on: \", tbody)\n\t\t\tsave_catalog(webscrape_results, partial=True)\n\t\t\treturn\n\n\n\t#=====================================\n\t# 2. Save All metadata to json, all tests passed\n\t#=====================================\n\tsave_catalog(webscrape_results, partial=False)\n\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "read_catalog", "data": "def read_catalog():\n\tcatalog_fn = os.path.join('metadata', 'gee_catalog.json')\n\tfpath = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\tfn = os.path.join(fpath, catalog_fn)\n\n\tif os.path.exists(fn):\n\t\twith open(fn, 'r') as in_file:\n\t\t\tdata = json.load(in_file)\n\t\t\treturn data\n\n\telse:\n\t\tprint(\"No Catalog Found...\\nnow running ee_catalog.py\")\n\t\tget_catalog()\n\t\treturn read_catalog()\n\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "test", "data": "def test():\n\tsave_catalog({})\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}], [{"term": "class", "name": "WebscrapeAdmin", "data": "class WebscrapeAdmin(admin.ModelAdmin):\n\n\tlist_display = ('name','price', 'hour', 'twenty_hours','seven_days','market_cap','volume','circulating_supply')\n\n", "description": null, "category": "webscraping", "imports": ["from django.contrib import admin", "from scrapes_data_app.models import * "]}], [], [{"term": "class", "name": "classWebScrape:\r", "data": "class WebScrape: \r\n\r\n\tdef enter_proxy_auth(self, proxy_username, proxy_password):\r\n\t\tpyautogui.typewrite(proxy_username)\r\n\t\tpyautogui.press('tab')\r\n\t\tpyautogui.typewrite(proxy_password)\r\n\t\tpyautogui.press('enter')\r\n\t\r\n\tdef open_a_page(self, driver, url):\r\n\t\tdriver.get(url)\r\n\t\t\r\n\tdef get_table(self, rows): \r\n\t\tself.rows = 1+rows\r\n\t\tself.cols = len(driver.find_elements_by_xpath(\r\n\t\t\t\"/html/body/table/tbody/tr/th\"))-1\r\n\t\ttable = []\r\n\t\tfor r in range(2, self.rows +1): # rows+1\r\n\t\t\tfor p in range(1, self.cols+1):\r\n\t\t\t\t# obtaining the text from each column of the table\r\n\t\t\t\tt = driver.find_element_by_xpath(\r\n\t\t\t\t\t\t\"/html/body/table/tbody/tr[\"+str(r)+\"]/td[\"+str(p)+\"]\").text\r\n\t\t\t\ttable.append(t)\r\n\t\ttable = np.array( table)\r\n\t\ttable = np.reshape( table, (self.rows -1, self.cols))\r\n\t\ttable[0,0] = table[0,0].split()[-1]\r\n\t\tdf= pd.DataFrame(data=table, index=table[:,0], columns = ['Date', '1 col',\t\r\n\t\t\t\t\t\t '2 col',\t'3 col',\t'4 col',\t'5 col',\t'6 col',\t\r\n\t\t\t\t\t\t '7 col',\t'8 col',\t'9 col',\t'10 col'])\r\n\t\treturn df.iloc[: , 1:]\r\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd\r", "import numpy as np\r", "import pyautogui\r", "from selenium import webdriver\r"]}], [{"term": "def", "name": "addDays", "data": "def addDays(num):\r\n\treturn ((num / 6) * 7)\r\n", "description": null, "category": "webscraping", "imports": ["import ast\r", "import requests\r", "import urllib.request\r", "import bs4 as bs\r", "import pandas as pd\r", "import datetime\r", "import os\r", "from datetime import timedelta\r"]}], [], [], [{"term": "def", "name": "index", "data": "def index():\n\tmars_data = mongo.db.mars_data.find()\n\treturn render_template(\"index.html\", mars_data=mars_data)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import mission_to_mars"]}, {"term": "def", "name": "scraper", "data": "def scraper():\n\tmars_data = mongo.db.mars_data\n\tmars_webscrape_re = mission_to_mars.scrape()\n\tmars_data.update({}, mars_webscrape_re, upsert=True)\n\treturn redirect(\"/\", code=302)\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import mission_to_mars"]}], [{"term": "def", "name": "main", "data": "def main(argv):\n\tpname = \"\"\n\twebsiteToSearch = \"all\"\n\ttry:\n\t\topts,args = getopt.getopt(argv,\"n:w:\",[\"name=\",\"website=\"])\n\texcept getopt.GetoptError:\n\t\texc_type, exc_obj, exc_tb = sys.exc_info()\n\t\tfname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n\t\tprint(exc_type, fname, exc_tb.tb_lineno)\n\t\tprint('start.py -n  [-w \"amazon\"|\"flipkart\"|\"all\"]')\n\t\tprint('start.py --name  [--website \"amazon\"|\"flipkart\"|\"all\"]')\n\t\tsys.exit(2)\n\t#opts = x for x in opts if x)\n\tprint(opts)\t\n\tfor opt in opts:\n\t\tif opt:\n\t\t\tif opt[0] in ('-n','--name'):\n\t\t\t\tpname = opt[1]\n\t\t\t\tprint(pname)\n\t\t\tif opt[0] in ('-w','--website'):\n\t\t\t\twebsiteToSearch = opt[1]\t \n\t\t#else:\n\t\t#\tprint('start.py -n  [-w \"amazon\"|\"flipkart\"|\"all\"]')\n\t\t#\tsys.exit()\n\tif(pname != \"\"):\t\t\t\n\t\tprint(\"Searching for product name:\" + pname)\n\t\tsearch(websiteToSearch,pname)\n\telse:\n\t\tprint('Product name to search cannot be empty')\n\t\tprint('start.py -n  [-a,-f]')\n\t\tprint('start.py --name  [-a,-f]')\n\t\tprint('-a = Search in amazon')\n\t\tprint('-f = Search in flipkart')\n\t\tsys.exit()\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "import switcher", "import amazon_india as amazon", "import flipkart_india as flipkart", "import models.product as productsmodel", "import sys,os", "import getopt"]}, {"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef start(self, website, productname):\n\t\t\tmethod_name = website+'WebScrape'\n\t\t\tmethod = getattr(self, method_name, lambda: 'Invalid')\n\t\t\treturn method(productname)\n\n\tdef amazonWebScrape(self, productname):\n\t\tprint(\"amazon:\"+productname)\n\t\tamazonproducts = amazon.search(productname)\n\t\treturn amazonproducts[:records_to_consider]\n\n\tdef flipkartWebScrape(self, productname):\n\t\tflipkartproducts = flipkart.search(productname)\n\t\treturn flipkartproducts[:records_to_consider]\n\n\tdef allSitesWebScrape(self, productname):\n\t\tprint(\"all:\"+productname)\n\t\tamazonproducts = amazon.search(productname)\n\t\tflipkartproducts = flipkart.search(productname)\n\t\treturn amazonproducts[:records_to_consider] + flipkartproducts[:records_to_consider]\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "import switcher", "import amazon_india as amazon", "import flipkart_india as flipkart", "import models.product as productsmodel", "import sys,os", "import getopt"]}, {"term": "def", "name": "search", "data": "def search(websiteToSearch,name):\n\tsearch = WebScrape()\n\tresult = search.start(websiteToSearch,name)\n\n\tprint(\"------ completed web scrapping ------\")\n\tfor product in result:\n\t\tprint(\"Name:\" + product.name + \" Current Price:\" + product.price + \" Original Price:\" + product.orginal_price + \" No of user rated:\" + product.no_of_users_rated + \" Rating:\" + product.rating + \" Website:\"+ product.website)\t  \n\t\t\t\t\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "import switcher", "import amazon_india as amazon", "import flipkart_india as flipkart", "import models.product as productsmodel", "import sys,os", "import getopt"]}], [{"term": "def", "name": "Play", "data": "def Play():\n\tmixer.init()\n\tmixer.music.load(\"sound.ogg\")\n\tmixer.music.play()\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment"]}, {"term": "def", "name": "pause", "data": "def pause():\n\tmixer.music.pause()\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment"]}, {"term": "def", "name": "unpause", "data": "def unpause():\n\tmixer.music.unpause()\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment"]}, {"term": "def", "name": "quitpy", "data": "def quitpy():\n\tsys.exit()\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment"]}, {"term": "def", "name": "volup", "data": "def volup():\n\tglobal vol\n\tmixer.music.set_volume(vol + 0.1)\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment"]}, {"term": "def", "name": "voldown", "data": "def voldown():\n\tglobal vol\n\tmixer.music.set_volume(vol - 0.1)\n\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment"]}, {"term": "def", "name": "check", "data": "def check():\n\tm = countryvar.get()\n\tm = str(m)\n\tprint(m)\n\treturn str(m)\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\tglobal x\n\tglobal b\n\tglobal thing\n\tglobal countryvar\n\tsrc = \"sound.mp3\"\n\tdst = \"sound.ogg\"\n\tmurl = str(url.get())\n\tresponse = requests.get(murl)\n\tresponse.raise_for_status()\n\tm = countryvar.get()\n\tm = str(m)\n\tprint(m)\n\tparse = bs4.BeautifulSoup(response.text, 'html.parser')\n\tx = str(parse.get_text())\n\tprint(x)\n\ttext = gTTS(x, lang=m)\n\ttext.save(\"sound.mp3\")\n\tAudioSegment.from_mp3(src).export(dst, format='ogg')\n\tb.state(['!disabled'])\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment"]}], [], [{"term": "def", "name": "relative_to_assets", "data": "def relative_to_assets(path: str) -> Path:\n\treturn ASSETS_PATH / Path(path)\n\n", "description": null, "category": "webscraping", "imports": ["from pathlib import Path", "from tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage", "import tkinter as tk", "import threading", "import pyautogui", "import time", "import cv2", "from pytesseract import *", "from PIL import Image", "from selenium import webdriver", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.common.by import By", "import mss", "import mss.tools"]}, {"term": "def", "name": "fwebscrape", "data": "\tdef webscrape(username, original):\n\t\tt = time.time()\n\n\t\toptions = webdriver.ChromeOptions()\n\t\toptions.headless = True\n\t\tdriver = webdriver.Chrome(options=options)\n\t\tdriver.get(f'https://cod.tracker.gg/warzone/profile/atvi/{username}/overview')\n\t\tpage_title = driver.find_elements(By.CLASS_NAME, 'lead')\n\n\t\tif not page_title or page_title[0] == \"WARZONE STATS NOT FOUND\":\n\t\t\tprint(\"WARZONE STATS NOT FOUND - Private profile\")\n\t\t\tusernameBox.delete(0, tk.END)\n\t\t\tusernameBox.insert(0, \"WARZONE STATS NOT FOUND - Private profile\")\n\n\t\telse:\n\t\t\tusernameBox.delete(0, tk.END)\n\t\t\tusernameBox.insert(0, original)\n\t\t\tsearch = driver.find_elements(By.CLASS_NAME, 'value')\n\n\t\t\tif len(search) > 4:\n\t\t\t\tprint(\"Wins:\", search[0].text)\n\t\t\t\twinsBox.delete(0, tk.END)\n\t\t\t\twinsBox.insert(0, search[0].text)\n\n\t\t\t\tprint(\"Win %:\", search[1].text)\n\t\t\t\twinPercentageBox.delete(0, tk.END)\n\t\t\t\twinPercentageBox.insert(0, search[1].text)\n\n\t\t\t\tprint(\"Kills:\", search[2].text)\n\t\t\t\tkillsBox.delete(0, tk.END)\n\t\t\t\tkillsBox.insert(0, search[2].text)\n\n\t\t\t\tprint(\"K/D:\", search[3].text)\n\t\t\t\tKDBox.delete(0, tk.END)\n\t\t\t\tKDBox.insert(0, search[3].text)\n\n\t\t\t\tprint(\"Score/min:\", search[4].text)\n\t\t\t\tscoreMinBox.delete(0, tk.END)\n\t\t\t\tscoreMinBox.insert(0, search[4].text)\n\n\t\t\telse:\n\t\t\t\tprint(\"Incorrect name or private profile\")\n\n\t\t\t\tusernameBox.delete(0, tk.END)\n\t\t\t\tusernameBox.insert(0, original)\n\n\t\t\t\twinsBox.delete(0, tk.END)\n\t\t\t\twinsBox.insert(0, \"-----\")\n\n\t\t\t\twinPercentageBox.delete(0, tk.END)\n\t\t\t\twinPercentageBox.insert(0, \"-----\")\n\n\t\t\t\tkillsBox.delete(0, tk.END)\n\t\t\t\tkillsBox.insert(0, \"-----\")\n\n\t\t\t\tKDBox.delete(0, tk.END)\n\t\t\t\tKDBox.insert(0, \"-----\")\n\n\t\t\t\tscoreMinBox.delete(0, tk.END)\n\t\t\t\tscoreMinBox.insert(0, \"-----\")\n\n\t\telapsed = time.time() - t\n\t\tprint(elapsed, \"Time to webscrape\")\n\t\twebscrapeBox.delete(0, tk.END)\n\t\twebscrapeBox.insert(0, str(round(elapsed, 2)) + \" seconds\")\n\n", "description": null, "category": "webscraping", "imports": ["from pathlib import Path", "from tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage", "import tkinter as tk", "import threading", "import pyautogui", "import time", "import cv2", "from pytesseract import *", "from PIL import Image", "from selenium import webdriver", "from webdriver_manager.chrome import ChromeDriverManager", "from selenium.webdriver.chrome.service import Service", "from selenium.webdriver.common.by import By", "import mss", "import mss.tools"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\t# <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n\t\t\tcolumn_names=, # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n\t\t\tdestination_table= # <- !!! ENTER HERE THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\tprops[\"last_date_amazon\"] = '2020-01-01'\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=df.values,\n\t\t\tcolumn_names=df.columns.to_list(),\n\t\t\tdestination_table=\"yankee_candle_reviews\"\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in table yankee_candle_reviews.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(url):\n\tr  = requests.get(url)\n\tdata = r.text\n\tsoup = BeautifulSoup(data,'html.parser')\n\tfor link in soup.find_all('a'):\n\t\tmylink = str(link.get('href'))\n\t\t#print(mylink)\t\t\n\t\tif (':' not in mylink and \"//\" not in mylink and mylink != \"\"):\n\t\t\tmylink = str(initialurl + \"/\" + mylink)\n\t\t\t#print (\"inside if: \" , mylink)\n\t\tif initialurl in mylink and mylink not in visited and mylink [0:4] == \"http\" and \"#\" not in mylink and \"None\" not in mylink:\n\t\t\tvisited.append(mylink)\n\t\t\tprint(mylink)\n\t\t\twith open(str(len(visited))+\".html\", \"w\", encoding=\"utf-8\") as filehandle:\n\t\t\t\tfilehandle.write(str(soup))\n\t\t\t\tfilehandle.close()\n\t\t\twebscrape(mylink)\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}], [{"term": "def", "name": "Play", "data": "def Play():\n\tmixer.init()\n\tmixer.music.load(\"sound.ogg\")\n\tmixer.music.play()\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "pause", "data": "def pause():\n\tmixer.music.pause()\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "unpause", "data": "def unpause():\n\tmixer.music.unpause()\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "quitpy", "data": "def quitpy():\n\tsys.exit()\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "volup", "data": "def volup():\n\tglobal vol\n\tmixer.music.set_volume(vol + 0.1)\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "voldown", "data": "def voldown():\n\tglobal vol\n\tmixer.music.set_volume(vol - 0.1)\n\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "check", "data": "def check():\n\tm = countryvar.get()\n\tm = str(m)\n\tprint(m)\n\treturn str(m)\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\tglobal x\n\tglobal b\n\tglobal thing\n\tglobal countryvar\n\tsrc = \"sound.mp3\"\n\tdst = \"sound.ogg\"\n\tmurl = str(url.get())\n\tresponse = requests.get(murl)\n\tresponse.raise_for_status()\n\tm = countryvar.get()\n\tm = str(m)\n\tprint(m)\n\tparse = bs4.BeautifulSoup(response.text, 'html.parser')\n\tx = str(parse.get_text())\n\tprint(x)\n\ttext = gTTS(x, lang=m)\n\ttext.save(\"sound.mp3\")\n\tAudioSegment.from_mp3(src).export(dst, format='ogg')\n\tb.state(['!disabled'])\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "stt", "data": "def stt():\n\tx = 1\n\twhile(1):\n\t\twith spr.Microphone() as source2:\n\n\t\t\tr.adjust_for_ambient_noise(source2, duration=0)\n\n\t\t\taudio2 = r.listen(source2)\n\n\t\t\tmyText = r.recognize_google(audio2)\n\t\t\tmyText = myText.lower()\n\n\t\t\tprint(myText)\n\t\t\tif (myText == 'quit'):\n\t\t\t\tbreak\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}], [{"term": "def", "name": "Play", "data": "def Play():\n\tmixer.init()\n\tmixer.music.load(\"sound.ogg\")\n\tmixer.music.play()\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "pause", "data": "def pause():\n\tmixer.music.pause()\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "unpause", "data": "def unpause():\n\tmixer.music.unpause()\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "quitpy", "data": "def quitpy():\n\tsys.exit()\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "volup", "data": "def volup():\n\tglobal vol\n\tmixer.music.set_volume(vol + 0.1)\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "voldown", "data": "def voldown():\n\tglobal vol\n\tmixer.music.set_volume(vol - 0.1)\n\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "check", "data": "def check():\n\tm = countryvar.get()\n\tm = str(m)\n\tprint(m)\n\treturn str(m)\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\tglobal x\n\tglobal b\n\tglobal thing\n\tglobal countryvar\n\tsrc = \"sound.mp3\"\n\tdst = \"sound.ogg\"\n\tmurl = str(url.get())\n\tresponse = requests.get(murl)\n\tresponse.raise_for_status()\n\tm = countryvar.get()\n\tm = str(m)\n\tprint(m)\n\tparse = bs4.BeautifulSoup(response.text, 'html.parser')\n\tx = str(parse.get_text())\n\tprint(x)\n\ttext = gTTS(x, lang=m)\n\ttext.save(\"sound.mp3\")\n\tAudioSegment.from_mp3(src).export(dst, format='ogg')\n\tb.state(['!disabled'])\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}, {"term": "def", "name": "stt", "data": "def stt():\n\tx = 1\n\twhile(1):\n\t\twith spr.Microphone() as source2:\n\n\t\t\tr.adjust_for_ambient_noise(source2, duration=0)\n\n\t\t\taudio2 = r.listen(source2)\n\n\t\t\tmyText = r.recognize_google(audio2)\n\t\t\tmyText = myText.lower()\n\n\t\t\tprint(myText)\n\t\t\tif (myText == 'quit'):\n\t\t\t\tbreak\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import pygame", "from gtts import gTTS", "import requests", "import bs4", "from pygame import mixer", "from tkinter import *", "from tkinter import ttk", "from tkinter import filedialog", "from tkinter import messagebox", "from os import path", "from pydub import AudioSegment", "import speech_recognition as spr", "import pyttsx3"]}], [{"term": "def", "name": "extract_html", "data": "def extract_html(url, headers=None):\n\tr = requests.get(url, headers = headers)\n\tresult = BeautifulSoup(r.text, 'html')\n\treturn result\n", "description": null, "category": "webscraping", "imports": ["# import packages", "import pandas as pd", "import numpy as np", "import os", "import requests", "import json", "from bs4 import BeautifulSoup", "from datetime import date, timedelta"]}, {"term": "def", "name": "conditional_bs4_results_key", "data": "def conditional_bs4_results_key(bs, tag, result_key, condition_key, condition_value):\n\t'''\n\tFunction to extract tag value from webscrape result if another key condition is met. Useful when a tag\n\tis used in multiple situations but is only relevant when other tag conditions are met\n\t\n\tInputs:\n\t- bs: BeatifulSoup object containing the webscrape result\n\t- tag - tag used in main search\n\t- result_key - key whose result is to be extracted\n\t- condition_key - key which is sometimes present, and identifies relevant results when certain condition is met\n\t- condition_result - value of key when condition is met\n\t\n\tOutputs:\n\t- list of results\n\t'''\n\toutput = []   \n\tfor res in bs.find_all(tag):\n\t\ttry:\n\t\t\tcond_actual = res[condition_key]\n\t\t\tif condition_value in list(cond_actual):\n\t\t\t\toutput.append(res[result_key])\n\t\texcept:\n\t\t\tpass\n\t\t\n\treturn list(set(output))\n\n", "description": null, "category": "webscraping", "imports": ["# import packages", "import pandas as pd", "import numpy as np", "import os", "import requests", "import json", "from bs4 import BeautifulSoup", "from datetime import date, timedelta"]}, {"term": "def", "name": "conditional_bs4_results_text", "data": "def conditional_bs4_results_text(bs, tag, condition_key, condition_value):\n\t'''\n\tFunction to extract text value from webscrape result if another key condition is met. Useful when a tag\n\tis used in multiple situations but is only relevant when other tag conditions are met\n\t\n\tInputs:\n\t- bs: BeatifulSoup object containing the webscrape result\n\t- tag - tag used in main search\n\t- condition_key - key which is sometimes present, and identifies relevant results when certain condition is met\n\t- condition_result - value of key when condition is met\n\t\n\tOutputs:\n\t- list of results\n\t'''\n\toutput = []   \n\tfor res in bs.find_all(tag):\n\t\ttry:\n\t\t\tcond_actual = [res[condition_key]]\n\t\t\tif condition_value in cond_actual:\n\t\t\t\toutput.append(res.text)\n\t\texcept:\n\t\t\tpass\n\t\t\n\treturn list(set(output))\n\n", "description": null, "category": "webscraping", "imports": ["# import packages", "import pandas as pd", "import numpy as np", "import os", "import requests", "import json", "from bs4 import BeautifulSoup", "from datetime import date, timedelta"]}, {"term": "def", "name": "get_table", "data": "def get_table(urls, headers=None):\n\t'''\n\t\n\n\tParameters\n\t----------\n\turls : list of individual posting urls\n\theaders : dictionary of headers - just user-agent string\n\n\tReturns\n\t-------\n\toutput : pandas dataframe\n\t\tdataframe including price, title, posted and updated time, and text description of each guitar.\n\n\t'''\n\t\n\t# set up lists to hold column values\n\ttitles = []\n\tprices = []\n\tposteds = []\n\tupdateds = []\n\tdescriptions = []\n\t\n\t# loop through each potential guitar listing page\n\tfor url in urls:\n\t\t\n\t\t# get searchable bs4 object\n\t\tresult = extract_html(url, headers=headers)\n\t\t\n\t\t# get title\n\t\ttitle = conditional_bs4_results_text(result, 'span', 'id', 'titletextonly')[0]\n\t\ttitles.append(title)\n\t\t\n\t\t# get price\n\t\tprice = result.find_all('span', 'price')[0].text\n\t\tprices.append(price)\n\t\t\n\t\t# get all dates - min will be posting date, max will be updating date\n\t\ttimes = [res.text.replace('\\n', '').strip() for res in result.find_all('time')]\n\t\tdates = pd.to_datetime(times).date\n\t\t\n\t\tposted = dates.min()\n\t\tupdated = dates.max()\n\t\t\n\t\tposteds.append(posted)\n\t\tupdateds.append(updated)\n\t\t\n\t\t# get body text description\n\t\tbody = conditional_bs4_results_text(result, 'section', 'id', 'postingbody')[0]\n\t\tbody = body.replace('QR Code Link to This Post', '').strip()\n\t\t\n\t\tdescriptions.append(body)\n\t\n\t# create table\n\ttable_dict = {'title' : titles,\n\t\t\t\t  'price' : prices, \n\t\t\t\t  'posted' : posteds,\n\t\t\t\t  'updated' : updateds,\n\t\t\t\t  'body' : descriptions,\n\t\t\t\t  'url' : urls}\n\t\n\t# convert to pd\n\toutput = pd.DataFrame(table_dict)\n\treturn output\n", "description": null, "category": "webscraping", "imports": ["# import packages", "import pandas as pd", "import numpy as np", "import os", "import requests", "import json", "from bs4 import BeautifulSoup", "from datetime import date, timedelta"]}], [{"term": "def", "name": "Webscrape_divID", "data": "def Webscrape_divID(URL, div_id):\r\n\t'''This function scrapes the website from the URL given to it.\\\r\n\tIt collects the entire website data and stores the data in the html format\\\r\n\t\tAlso it extracts the data segment based on the div_id'''\r\n\t\r\n\tHEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n\t\r\n\t# Making the HTTP Request\r\n\twebpage = requests.get(URL, headers=HEADERS)\r\n  \r\n\t# Creating the Soup Object containing all data\r\n\tsoup = BeautifulSoup(webpage.content, \"html.parser\")\r\n\r\n\tresults = soup.find(id=div_id)\r\n\t\r\n\tst.write(results.get_text())\r\n\t\r\n\t# model = [\"en_core_web_trf\"]\r\n\t# visualizers = [\"parser\"]\r\n\t# sts.visualize(model, results.get_text(), visualizers)\r\n\t\r\n\treturn results.get_text()\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Webscrape_Classname", "data": "def Webscrape_Classname(URL, classname):\r\n\t\r\n\t'''This function scrapes the website from the URL given to it.\\\r\n\tIt collects the entire website data and stores the data in the html format \\\r\n\tAlso it extracts the data segment based on the classname'''\r\n\t\r\n\tHEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n\t\r\n\t# Making the HTTP Request\r\n\twebpage = requests.get(URL, headers=HEADERS)\r\n  \r\n\t# Creating the Soup Object containing all data\r\n\tsoup = BeautifulSoup(webpage.content, \"html.parser\")\r\n   \r\n\tresults = soup.find(\"div\", class_= classname)\r\n\t\r\n\tst.write(results.get_text())\r\n\t\r\n\treturn results.get_text()\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Word_Frequency", "data": "def Word_Frequency(spacy_text):\r\n\t'''Visualize the Noun and Verb frequencies in the extracted text'''\r\n\t\r\n\t#Filtering for nouns and verbs only\r\n\tnouns_verbs = [token.text for token in spacy_text if token.pos_ in ('NOUN', 'VERB')]\r\n\t\r\n\tcv = CountVectorizer()\r\n\tX = cv.fit_transform(nouns_verbs)\r\n\tsum_words = X.sum(axis=0)\r\n\twords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\r\n\twords_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n\twf_df = pd.DataFrame(words_freq)\r\n\twf_df.columns = ['word', 'count']\r\n\t\r\n\t# sns.set(rc={'figure.figsize':(12,8)})\r\n\tsns.barplot(x = 'count', y = 'word', data = wf_df, palette=\"GnBu_r\")\r\n\tst.pyplot()\r\n\t\r\n\tst.write(\"**Word Count(Noun & Verb) of the Extracted Text:\\n**\")\r\n\tst.write(wf_df)\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "POS_Tag", "data": "def POS_Tag(data):\r\n\t'''Tag Parts of Speech to the Extracted data and visualize'''\r\n\t\r\n\tsts.visualize_parser(data)\r\n\tsts.visualize_ner(data, labels=nlp.get_pipe(\"ner\").labels)\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content1", "data": "def Replace_Content1(token):\r\n\t'''Find and replace selected tokens for Usecase 1'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'John' or token.text == 'John Dooley'):\r\n\t\treturn '[Your Name]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jennifer':\r\n\t\treturn \"[Your Manager's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn '[Date]'\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace1", "data": "def FindnReplace1(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content1, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content2", "data": "def Replace_Content2(token):\r\n\t'''Find and replace selected tokens for Usecase 2'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'Joe' or token.text == 'Joe Brown'):\r\n\t\treturn '[Your Name]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Steve':\r\n\t\treturn \"[Your Manager's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn '[Sickness Date]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'ORG':\r\n\t\treturn '[Hospital/Clinic Name]'\r\n\tif token.text == 'Joejoe.brown765@email.com555':\r\n\t\treturn '[Your Name]\\n [Your Email ID]'\r\n\tif token.text == '555':\r\n\t\treturn '\\n [Your Contact'\r\n\tif token.text == '5555':\r\n\t\treturn 'Number]'\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace2", "data": "def FindnReplace2(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content2, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content3", "data": "def Replace_Content3(token):\r\n\t'''Find and replace selected tokens for Usecase 3'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Smith':\r\n\t\treturn \"[Your Colleague's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonny\\n':\r\n\t\treturn \"[Your Name]\\n [Your Designation]\"\r\n\tif token.text == \"Formal\":\r\n\t\treturn 'Formal Birthday Wishes'\r\n\tif token.text == \"Mr.\":\r\n\t\treturn ''\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace3", "data": "def FindnReplace3(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content3, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content4", "data": "def Replace_Content4(token):\r\n\t'''Find and replace selected tokens for Usecase 4'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Paul JonesPhoneEmail':\r\n\t\treturn \"Regards,\\n [Your Name]\\n [Your Contact No.]\\n [Your Email ID]\\n\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn \"[Years of Experience]\"\r\n\tif token.text == \"Address\":\r\n\t\treturn \"\"\r\n\tif token.text == \"store\":\r\n\t\treturn \"\\b\"\r\n\tif token.text == \"retail\":\r\n\t\treturn \"\\b\"\r\n\t\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace4", "data": "def FindnReplace4(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content4, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content5", "data": "def Replace_Content5(token):\r\n\t'''Find and replace selected tokens for Usecase 5'''\r\n\t\r\n\tif token.text == \",\":\r\n\t\treturn \"\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Amy':\r\n\t\treturn \"[Your Colleague's Name],\\n\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonathan':\r\n\t\treturn \"\\n [Your Name]\\n [Your Contact number]\"\r\n\tif token.text == \"Sincerely\":\r\n\t\treturn \"\\n Sincerely,\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn \"[Timeline] and [Reason for Appreciation]\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace5", "data": "def FindnReplace5(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content5, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content6", "data": "def Replace_Content6(token):\r\n\t'''Find and replace selected tokens for Usecase 6'''\r\n\t\r\n\tif token.text == \",\":\r\n\t\treturn \"\"\r\n\tif token.text == \"Hello\":\r\n\t\treturn \"Dear [Sender's Name],\\n\"\r\n\tif token.text == \"COLLEAGUE\":\r\n\t\treturn \"[Your Colleague's Name]\"\r\n\tif token.text == \"Regards\":\r\n\t\treturn \"\\n Regards,\"\r\n\tif token.text == \"NAME\":\r\n\t\treturn \"\\n [Your Name]\"\r\n\tif token.text == \"do\":\r\n\t\treturn \"don't\"\r\n\tif token.text == \"n\u00e2\u20ac\u2122t\":\r\n\t\treturn \"\\b\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace6", "data": "def FindnReplace6(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content6, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content7", "data": "def Replace_Content7(token):\r\n\t'''Find and replace selected tokens for Usecase 7'''\r\n\t\r\n\tif token.text == \",\":\r\n\t\treturn \"\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Sam':\r\n\t\treturn \"[Your Partner's Name],\\n\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonathan':\r\n\t\treturn \"\\n[Your Name]\\n[Your Contact Number]\"\r\n\tif token.text == \"Please\":\r\n\t\treturn \"\\nPlease\"\r\n\tif token.text == \"Thank\":\r\n\t\treturn \"\\nThank\"\r\n\tif token.text == \"again!Sincerely\":\r\n\t\treturn \"\\b\\b, again!\\nSincerely,\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace7", "data": "def FindnReplace7(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content7, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}], [{"term": "class", "name": "Feature", "data": "class Feature(Server):\n\t\n\tdef __init__(self, request):\n\t\tsuper().__init__(request)\n\t\tself.sn = StatusNews(request)\n\t\tself.d2h = Dist2Hospital(request)\n\t\tself.dgs = Diagnosis(request)\n\t\tself.fb = Feedback(request)\n\t\tself.cr = CbotResponse(request)\n\t\tself.udb = UserDB(request)\n\t\tself.wbs = Webscrape()\n\t\tself.gg = Gen_graph()\n\t\tself.ntf = Notification()\n\n\t\tself.intent = super().rcvIntent()\n\t\tself.udb.storeId()\n\n\tdef main(self):\n\t\t# --------------------------#\n\t\t# INFECTION STATUS INTENT   #\n\t\t# --------------------------#\n\t\tif self.intent == \"infection-status-covid\":\n\t\t\treturn self.sn.infectionStatus()\n\n\t\t# --------------------------#\n\t\t# HEADLINE NEWS INTENT\t  #\n\t\t# --------------------------#\n\t\telif self.intent == \"latest-news-covid\":\n\t\t\treturn self.sn.headlineNews()\n\n\t\t# --------------------------#\n\t\t# Distance to Hospital\t  #\n\t\t# --------------------------#\n\t\telif self.intent == \"nearest-hospital-covid\" or self.intent == \"treatment-covid.yes.address\":\n\t\t\treturn self.d2h.dist2hospital()\n\n\t\t# --------------------------#\n\t\t# DIAGNOSIS INTENT\t\t  #\n\t\t# --------------------------#\n\t\telif self.intent == \"diagnosis-covid\":\n\t\t\treturn self.dgs.diagnosis()\n\n\t\t# --------------------------#\n\t\t# SYNC  INTENT\t\t\t  #\n\t\t# --------------------------#\n\t\telif self.intent == \"sync\":\n\t\t\t# try:\n\t\t\tself.wbs.statusScrapper()\n\t\t\tself.wbs.newsScrapper()\n\t\t\tself.gg.plot_it()\n\t\t\tself.dgs.updateResponses()\n\t\t\tself.main_text = \"Sync/update completed.\"\n\t\t\t# except:\n\t\t\t#\t self.main_text=\"Error occurred. Contact admin to debug.\"\n\t\t\t#\t print(\"There is an error!\")\n\t\t\t# finally:\n\t\t\treturn super().sendMsg(single=True)\n\n\t\t# --------------------------#\n\t\t# FEEDBACK GATHER\t\t   #\n\t\t# --------------------------#\n\t\telif self.intent == \"feedback-bad\" or self.intent == \"feedback-good\":\n\t\t\treturn self.fb.store_fb()\n\n\t\t# --------------------------#\n\t\t# FEEDBACK COMMENT\t\t  #\n\t\t# --------------------------#\n\t\telif self.intent == \"feedback\":\n\t\t\treturn self.fb.store_text_fb()\n\t\t\n\t\t# --------------------------#\n\t\t# GOODBYE\t\t\t\t   #\n\t\t# --------------------------#\n\t\telif self.intent == \"goodbye\":\n\t\t\treturn self.cr.goodbye()\n\t\t\n\t\t# --------------------------#\n\t\t# SUBSCRIPTION\t\t\t  #\n\t\t# --------------------------#\n\t\telif self.intent == \"subscribe\":\n\t\t\treturn self.udb.subscribe()\n\t\telif self.intent == \"unsubscribe\":\n\t\t\treturn self.udb.unsubscribe()\n\n\t\t# --------------------------#\n\t\t# CHECKIN AFTER ASSESSMENT  #\n\t\t# --------------------------#\n\t\telif self.intent == \"checkin-yes\" or self.intent == \"checkin-no\":\n\t\t\treturn self.udb.checkin()\n\n\t\t# --------------------------#\n\t\t# CHECKIN NOTIFICATION\t  #\n\t\t# --------------------------#\n\t\telif self.intent == \"checkin-notification\":\n\t\t\tself.ntf.send_checkin_days()\n\t\t\tself.main_text = \"Notification sent!\"\n\t\t\treturn super().sendMsg(single=True)\n\n", "description": null, "category": "webscraping", "imports": ["from chatbot_app.modules.dialogflow_msg import Server", "from chatbot_app.modules.status_news import StatusNews", "from chatbot_app.modules.dist2hospital import Dist2Hospital", "from chatbot_app.modules.diagnosis import Diagnosis", "from chatbot_app.modules.feedback import Feedback", "from chatbot_app.modules.cbot_response import CbotResponse", "from chatbot_app.modules.users_database import UserDB", "from chatbot_app.modules.webscrape import Webscrape", "from chatbot_app.modules.generate_graph import Gen_graph", "from chatbot_app.modules.notification import Notification"]}], [], [{"term": "class", "name": "NiftyGainers", "data": "class NiftyGainers(viewsets.ViewSet):\n\n\trenderer_classes = [TemplateHTMLRenderer]\n\ttemplate_name = 'application/listings.html'\n\n\tdef list(self, request, *args, **kwrgs):\n\n\t\ttry:\n\t\t\tresult = NiftyGainersService.retrieve()\n\n\t\t\tif result.get('success'):\n\t\t\t\tdata = result.get('data')\n\t\t\telse:\n\t\t\t\tdata = list()\n\n\t\t\treturn Response({'data': data})\n\n\t\texcept Exception as e:\n\t\t\traise e\n\n\tdef retrieve(self, request, *args, **kwrgs):\n\n\t\ttry:\n\t\t\tresult = NiftyGainersService.filter()\n\n\t\t\tif result.get('success'):\n\t\t\t\tdata = result.get('data')\n\t\t\telse:\n\t\t\t\tdata = list()\n\n\t\t\treturn Response({'data': data}, template_name='application/lists.html')\n\n\t\texcept Exception as e:\n\t\t\traise e\n", "description": null, "category": "webscraping", "imports": ["from django.urls import reverse", "from django.shortcuts import render, redirect", "from django.views.generic import View, TemplateView", "from django.utils.decorators import method_decorator", "from django.http import HttpResponse, JsonResponse, Http404, HttpResponseNotFound", "from rest_framework import generics", "from rest_framework import viewsets", "from rest_framework.views import APIView", "from rest_framework.response import Response", "from rest_framework.renderers import TemplateHTMLRenderer", "from webscrape.application.models import *", "from webscrape.application.responses import *", "from webscrape.application.services import *"]}], [{"term": "def", "name": "fdisplay_title", "data": "\tdef display_title(self):\n\t\tfor title in self.soup.find_all('title'):\n\t\t\tprint('\\033[1m' + \"Title of the website is : \",title.get_text() + '\\033[0m') # prints title in bold letters\n", "description": null, "category": "webscraping", "imports": ["#importing required libraries", "import requests", "from bs4 import BeautifulSoup", "import re"]}, {"term": "def", "name": "fdisplay_all_links", "data": "\tdef display_all_links(self):\n\t\tf = open(\"allLinks.txt\", \"w\")\t\t\t  # opening a file in write mode\n\t\t# traverse paragraphs from soup\n\t\tfor link in self.soup.find_all(\"a\"):\n\t\t\t\tdata = str(link.get('href'))\n\t\t\t\tf.write(data)\n\t\t\t\tf.write(\"\\n\")\n\t\tf.close()\n\t\tprint('\\033[1m' +\"All links output is stored in webscrape1.txt file \"+ '\\033[0m')\n", "description": null, "category": "webscraping", "imports": ["#importing required libraries", "import requests", "from bs4 import BeautifulSoup", "import re"]}, {"term": "def", "name": "fdisplay_href_links", "data": "\tdef display_href_links(self):\n\t\tf = open(\"hreflinks.txt\", \"w\")\n\t\t# traverse paragraphs from soup and has attribute href to find all links\n\t\tfor link in self.soup.find_all(\"a\",attrs={'href': re.compile(\"^http\")}):\n\t\t\tdata = str(link.get('href'))\n\t\t\tf.write(data)\n\t\t\tf.write(\"\\n\")\n\t\tf.flush()\n\t\tf.close()\n\t\tprint('\\033[1m' + \"All href output links are stored in webscrape2.txt file  \" + '\\033[0m')\n", "description": null, "category": "webscraping", "imports": ["#importing required libraries", "import requests", "from bs4 import BeautifulSoup", "import re"]}], [{"term": "def", "name": "welcome", "data": "def welcome():\n\n\treturn render_template(\"index.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "scrape", "data": "def scrape():\n\t# Run the scrape function\n\trenewable_data = renewable_scrape.renewable_scrape()\n\n\t# Update the Mongo database using update and upsert=True\n\tmongo.db.renewables.replace_one({}, renewable_data, upsert=True)\n\treturn redirect(\"/webscrape_sunburst\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "hydro", "data": "def hydro():\n\n\treturn render_template(\"hydro.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "wind", "data": "def wind():\n\n\treturn render_template(\"wind.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "heatmap", "data": "def heatmap():\n\n\treturn render_template(\"heatmap.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "solar", "data": "def solar():\n\n\treturn render_template(\"solar.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "location", "data": "def location():\n\n\treturn render_template(\"location.html\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "webscrape_sunburst", "data": "def webscrape_sunburst():\n\n\t#Take one instance from the Mongo DB\n\tdata = mongo.db.renewables.find_one()\n\n\treturn render_template(\"Webscrape_sunburst.html\",r_last_refresh=data[\"renewable_refresh\"],renewable_title_0=data[\"renewable_titles\"][0],renewable_link_0=data[\"renewable_links\"][0],renewable_title_1=data[\"renewable_titles\"][1],renewable_link_1=data[\"renewable_links\"][2], renewable_title_2 = data[\"renewable_titles\"][2],renewable_link_2=data[\"renewable_links\"][4],renewable_title_3=data[\"renewable_titles\"][3],renewable_link_3=data[\"renewable_links\"][6])\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "heatmapgeojson", "data": "def heatmapgeojson():\n\treturn jsonify(data = heatmapdata)\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}, {"term": "def", "name": "data", "data": "def data():\n\t\"\"\"Return dashboard.html.\"\"\"\n\treturn render_template(\"data.html\")\n\n", "description": "Return dashboard.html.", "category": "webscraping", "imports": ["import os", "import pandas as pd", "import numpy as np", "import sqlalchemy", "from sqlalchemy.ext.automap import automap_base", "from sqlalchemy.orm import Session", "from sqlalchemy import create_engine", "from flask import Flask, jsonify, render_template", "import sqlalchemy", "from flask import Flask, render_template, redirect", "from flask_pymongo import PyMongo", "import renewable_scrape", "import json"]}], [{"term": "class", "name": "WikiGraph", "data": "class WikiGraph(object):\n\tdef __init__(self):\n\t\tself.graph = dict() #graph of keys:values where values are list types\n\t\tself.visited = dict()\n\n\tdef wiki_bfs(self, s, destination): #do BFS given source node as link and destination link\n\t\tdef savedata(s, links, filename = FILENAME):\n\t\t\ttry:\n\t\t\t\twith open(filename) as savefile1:\n\t\t\t\t\tdata = json.load(savefile1)\n\t\t\t\t\tdata[s] = links\n\t\t\t\twith open(filename, 'w') as savefile2:\n\t\t\t\t\tjson.dump(data, savefile2, indent = 2)\n\n\t\t\texcept FileNotFoundError:\n\t\t\t\tedata = {s : links}\n\t\t\t\twith open(filename, 'w') as newsavefile:\n\t\t\t\t\t\tjson.dump(edata, newsavefile, indent = 2)\n\n\t\tqueue = []\n\t\tself.visited = {s:True} #visited dictionary to track which nodes are visited\n\t\tqueue.append(s)\n\t\tself.addEdges(s)  #neighbor links of source_node\n\t\tstart_time = time.time()\n\t\tlayer = 0\n\n\t\ttry:\n\t\t\twith open(FILENAME) as jsonfile:\n\t\t\t\tjsondata = json.load(jsonfile)\n\t\texcept FileNotFoundError:\n\t\t\tprint('Data file not found. Creating one...')\n\n\t\twhile queue:\n\t\t\tprint('#' * 100)\n\t\t\tprint(s)\n\t\t\tprint(f\"Moving from layer {layer} to {layer + 1}...\")\n\t\t\tprint('#' * 100)\n\t\t\tlayer += 1\n\n\t\t\tfor link in self.graph[s]: #each link in webscrape(s)   ##possible to refactor this into a map function?\n\n\t\t\t\tif link not in self.visited: #hasn't been visited and not recorded\n\t\t\t\t\tprint(link)\n\t\t\t\t\tself.visited[link] = True\n\t\t\t\t\tqueue.append(link)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif link in jsondata:\n\t\t\t\t\t\t\tself.graph[link] = jsondata[link]\n\t\t\t\t\t\t\tprint(f'Pulled from {FILENAME}')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.addEdges(link)\n\t\t\t\t\texcept:\n\t\t\t\t\t\tself.addEdges(link)\n\n\t\t\t\t\tsavedata(link, self.graph[link]) #save to json\n\n\t\t\t\t\t# check if destination is reached\n\n\t\t\t\t\tif destination in self.graph[link]:\n\t\t\t\t\t\tprint(\n\t\t\t\t\t\t\tf'Process finished with exit code 1: Breadth First Search Completed. Layers: {layer} Time: {round(time.time() - start_time, 3)}')\n\t\t\t\t\t\treturn\n\t\t\t\t\telif destination == link:\n\t\t\t\t\t\tprint(f'Process finished with exit code 2: Breadth First Search Completed. Layers: {layer} Time: {round(time.time() - start_time, 3)}')\n\t\t\t\t\t\treturn\n\t\t\ts = queue.pop(0)\n\n\tdef addEdges(self,p): #adds a neighborlink set from webscrape function v to page p\n\t\tself.graph[p] = self.webscrape(p)\n\n\tdef webscrape(self, url):  # takes wiki url and returns set of all the links\n\t\tsource = requests.get(url).text\n\t\tsoup = BeautifulSoup(source, 'html.parser')\n\t\t# print(soup.prettify())\n\n\t\tlinks = soup.find_all('a', href=True)  # filter html for href links\n\t\t# print(links)\n\t\tfinal_links = []\n\t\tfor link in links:  # extract links with regex\n\t\t\tlinkregex = re.compile(r'href=\"(\\w|/)+\"')\n\t\t\tfinal_link = linkregex.search(str(link))\n\t\t\tif final_link is not None and final_link not in self.visited:\n\t\t\t\t# print(final_link.group())\n\t\t\t\tfinal_links.append(f'{WIKI_PREFIX}{final_link.group()[6:-1]}')\n\t\tfinal_links = list(set(final_links))  # rid duplicates\n\t\tfinal_links.remove(f'{WIKI_PREFIX}/wiki/Main_Page')  # rid unnecessary link\n\t\t# print(final_links,len(final_links))\n\t\treturn final_links\n", "description": null, "category": "webscraping", "imports": ["from webscraper import WIKI_PREFIX", "import time", "import json", "import os", "from bs4 import BeautifulSoup", "import requests", "import re"]}], [{"term": "class", "name": "webscrape", "data": "class webscrape():\r\n\t\t\r\n\t\t\r\n\r\n\r\n\r\n\r\n\tdef performCalculation(int):\r\n\t\turl = \"https://www.basketball-reference.com/leagues/NBA_{}_totals.html\".format(year);\r\n\t\thtml = urlopen(url);\r\n\r\n\t\tsoup = BeautifulSoup(html);\r\n\t\tsoup.findAll('tr', limit =2);\r\n\t\theaders = [th.getText() for th in soup.findAll('tr', limit=2)[0].findAll('th')]\r\n\t\tprint(\"1\");\r\n\r\n\t\theaders = headers[1:];\r\n\t\trows = soup.findAll('tr')[1:]\r\n\t\tplayer_stats = [[td.getText() for td in rows[i].findAll('td')] for i in range(len(rows))]\r\n\r\n\t\tprint(\"2\");\r\n\r\n\t\tstats2 = pd.DataFrame(player_stats, columns = headers);\r\n\t\tstats2.head(10);\r\n\r\n\t\tstats = stats\r\n\r\n\r\n\t\tstats.to_csv(r\"C:\\Users\\Liam\\Documents\\NBA_{}.csv\".format(year), index = False)\r\n\t\tprint(\"3\");\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver\r", "from random import random\r", "from urllib.request import urlopen\r", "from bs4 import BeautifulSoup\r", "import pandas as pd\r", "import time\r", "import matplotlib.pyplot as plt\r", "import numpy as np\r", "import mplcursors\r"]}], [{"term": "def", "name": "mainfunction", "data": "def mainfunction():\n\tws = WebScrape()\n\tcovid_india = ws.covidIndia()\n\twrite_to_csv.write_csv(covid_india, \"india_report.csv\")\n\tstate_wise = ws.tableScrape()\n\theader = ws.header\n\tfor n, row in enumerate(state_wise[1:]):\n\t\trow.insert(0, n + 1)\n\tlogging.info(\"Sorting the States in alphabetical order\")\n\tsorted_list = marge_sort.mergesort(state_wise[1:])\n\tlogging.info(\"Finished sorting\")\n\tsorted_list.insert(0, header)\n\twrite_to_csv.write_csv(sorted_list, \"state_wise_table.csv\")\n\tws.close_webpage()\n\n", "description": null, "category": "webscraping", "imports": ["import logging", "from scrape import WebScrape", "import write_to_csv", "import marge_sort"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(stream,area):\n\tglobal page_count_acm\n\tglobal dictionary\n\tif area == \"\":\n\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'?page='+str(page_count_acm)\n\telif stream == \"\": \n\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+area+'?page='+str(page_count_acm)\n\telse: quote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'/'+area+'?page='+str(page_count_acm)\n\tprint quote_page_acm,'\\n'\n\tpage_acm = urllib2.urlopen(quote_page_acm)\n\tsoup_acm = BeautifulSoup(page_acm, 'html.parser')\n\tbox_acm = soup_acm.find_all('div',attrs={'class':'aiResultsMainDiv'})\n\ttemp_acm = soup_acm.find('span',attrs={'class':'aiPageTotalTop'}) \n\tif  temp_acm == None:\n\t\tprint \"No results found\"\n\t\tsys.exit(1)\n\tcount_total_acm = int(temp_acm.get_text())\n\tfor bx_acm in box_acm:\n\t\ttitle_acm = bx_acm.find('div',attrs={'class':'aiResultTitle'}).get_text().strip()\n\t\t#print 'Title:',bx.find('div',attrs={'class':'aiResultTitle'}).get_text().strip()\n\t\turl_acm = bx_acm.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href')\n\t\t#print 'URL:',bx.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href')\n\t\tdetails_acm = bx_acm.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')\n\t\tcompany_acm = details_acm[0].get_text().strip()\n\t\t#print 'Company:',details[0].get_text().strip()\n\t\tlocation_acm = details_acm[1].get_text().strip()\n\t\t#print 'Location:',details[1].get_text().strip()\n\t\tdate_acm = details_acm[2].get_text().strip()\n\t\t#print 'Date:',details[2].get_text().strip()+\"\\n\"\n\t\tif bx_acm.find('li',attrs={'id':'searchResultsCategoryDisplay'}) != None:\n\t\t\tcategory_acm = details_acm[3].get_text().strip()\n\t\telse :\n\t\t\tcategory_acm = 'None'\n\t\tdescription_acm = bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t#print 'Description:', bx.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t\n\t\t\n\n\t\tif (title_acm,location_acm) in dictionary:\n\t\t\tdictionary[title_acm,location_acm] = (dictionary[title_acm,location_acm],[company_acm,date_acm,category_acm,description_acm,'acm'])\n\t\n\t\telse:\n\t\t\tdictionary[title_acm,location_acm] = ([company_acm,date_acm,category_acm,description_acm,'acm'])\n  \n\n\n\n\t\t#dont use as of now\n\t\t#print 'Company:',bx.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find('li',attrs={'class':'aiResultsCompanyName'}).get_text().strip()\n\t\t#print 'Location:',bx.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')[1].get_text() + \"\\n\"\n\t\n\t\t#print bx.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t#print box.find('div',attrs={'class':'aiResultTitle'})\n\t\t#find('h3').get_text()\n\n\tif (page_count_acm < count_total_acm):\n\t\tpage_count_acm = page_count_acm + 1\n\t\twebscrape(stream,area)\n\t#for (x,y) in dictionary:\n\t#\tprint x\n\t#\tprint y\n\t#\tprint dictionary[x,y][0]\n\t#\tprint dictionary[x,y][1]\n\t#\tprint dictionary[x,y][2]\n\t#\tprint dictionary[x,y][3]\n\t#\tprint \"\\n\"\n\t#print dictionary  \n\n\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "from bs4 import BeautifulSoup", "import sys"]}, {"term": "def", "name": "webscrape1", "data": "def webscrape1(stream,area):\n\tglobal page_count_indeed\n\tglobal c_indeed\n\tglobal test_indeed\n\tglobal count_total_indeed \n\tglobal counter_indeed\n\tglobal cal_page_indeed\n\tglobal val_indeed\n\tglobal dictionary\n\t\n\tquote_page_indeed = 'https://www.indeed.com/jobs?q='+stream+'&l='+area+'&start='+str(page_count_indeed)\n\tprint quote_page_indeed,'\\n'\n\tpage_indeed = urllib2.urlopen(quote_page_indeed)\n\tsoup_indeed = BeautifulSoup(page_indeed, 'html.parser')\n\t\n\t#a = str(soup_indeed)\n\t#f = open('1.html','w')\n\t#f.write(a)\n\t#f.close()\n\t\n\t#print soup_indeed\n\n\t\n\tbox_indeed = soup_indeed.find_all('div',attrs={'class':'row'})\n\ttemmp_indeed = soup_indeed.find('div',attrs={'id':'searchCount'})\n\tif temmp_indeed == None:\n\t\tprint 'No results found'\n\t\tsys.exit(1)\n\tif test_indeed == 0:\n\t\tcount_total_indeed = int(temmp_indeed.get_text().split()[5].replace(',',''))\n\t\twhile val_indeed <= count_total_indeed:\n\t\t\tcal_page_indeed = cal_page_indeed + 1\n\t\t\tval_indeed = 25 * cal_page_indeed\n\t\ttest_indeed = 1\n\t\tcal_page_indeed = cal_page_indeed + 1\n\t\tif cal_page_indeed > 50:\n\t\t\tcal_page_indeed = 50\n\t\t#print page_indeed\n\t\t#print val_indeed\n\t#print count_total_indeed\n\t\n\tfor bx_indeed in box_indeed:\n\t\t#print c_indeed\n\t\t#print 'Title:',bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip()\n\t\ttitle_indeed = bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip()\n\t\t#print 'Company:', bx_indeed.find('span',attrs={'class':'company'}).get_text().strip()\n\t\tcompany_indeed = bx_indeed.find('span',attrs={'class':'company'}).get_text().strip()\n\t\t#print 'Location:', bx_indeed.find('span',attrs={'class':'location'}).get_text().strip()\n\t\tlocation_indeed = bx_indeed.find('span',attrs={'class':'location'}).get_text().strip()\n\t\t#print 'Description:', bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip(),'\\n'\n\t\tdescription_indeed = bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip()\n\t\t#c_indeed = c_indeed+1\n\t\tdate_indeed = 'None'\n\t\tcategory_indeed = 'None'\n\t\n\tif (title_indeed,location_indeed) in dictionary:\n\t\tdictionary[title_indeed,location_indeed] = (dictionary[title_indeed,location_indeed],[company_indeed,date_indeed,category_indeed,description_indeed,'indeed'])\n\telse:\n\t\tdictionary[title_indeed,location_indeed] = ([company_indeed,date_indeed,category_indeed,description_indeed,'indeed'])\t\n\n\t\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "from bs4 import BeautifulSoup", "import sys"]}], [], [], [{"term": "def", "name": "parse_results", "data": "def parse_results(\n", "description": null, "category": "webscraping", "imports": ["import re", "from contextlib import suppress", "from json import JSONDecodeError", "from requests import HTTPError, RequestException", "from tqdm import tqdm", "from scrape.configs import CompanyResult, JobScrapeConfig", "from scrape.web_funcs import webscrape_results"]}, {"term": "def", "name": "company_lookup", "data": "def company_lookup(company_alias: str):\n\t\"\"\"Looks up the company JSON in BuiltInNYC.\n\tIt passes this along to the superceding parse_results method,\n\twhich places it within the CompanyResult dataclass.\n\t\"\"\"\n\twith suppress(\n\t\tJSONDecodeError, RequestException, HTTPError, TypeError, AttributeError\n\t):\n\t\tcompany_page_url = f\"https://api.builtin.com/companies/alias/{company_alias}\"\n\t\tcomp_docs = webscrape_results(company_page_url, querystring={\"region_id\": \"5\"})  # type: ignore\n\t\tdata = {\n\t\t\t\"street_address\": comp_docs.get(\"street_address_1\"),\n\t\t\t\"suite\": comp_docs.get(\"street_address_2\"),\n\t\t\t\"city\": comp_docs.get(\"city\"),\n\t\t\t\"state\": comp_docs.get(\"state\"),\n\t\t\t\"zip_code\": comp_docs.get(\"zip\"),\n\t\t\t\"url\": comp_docs.get(\"url\"),\n\t\t}\n\t\treturn data\n", "description": "Looks up the company JSON in BuiltInNYC.\n\tIt passes this along to the superceding parse_results method,\n\twhich places it within the CompanyResult dataclass.\n\t", "category": "webscraping", "imports": ["import re", "from contextlib import suppress", "from json import JSONDecodeError", "from requests import HTTPError, RequestException", "from tqdm import tqdm", "from scrape.configs import CompanyResult, JobScrapeConfig", "from scrape.web_funcs import webscrape_results"]}], [{"term": "def", "name": "scrape_page", "data": "def scrape_page(URL):\n\tr = requests.get(URL)\n\thtml_content = r.text\n\tsoup = BeautifulSoup(html_content, 'html.parser')\n\n\ttry:\n\t\tinfo = [soup.find(*loc).text for loc in DATA_LOC]\n\t\ts_level = soup.find(*JOB_CRIT)\n\t\te_type = s_level.find_next('span')\n\t\tinfo.extend([s_level.text, e_type.text])\n\t\treturn info\n\texcept AttributeError:\n\t\treturn False\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import time", "import requests"]}, {"term": "def", "name": "scraper", "data": "def scraper(filename='jobslist.txt'):\n\toutput = []\n\tjob_dir = open('jobslist.txt', 'r')\n\tfor job in job_dir:\n\t\ts = scrape_page(job)\n\t\tif not s:\n\t\t\tcontinue\n\t\toutput.append(s)\n\treturn output\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import time", "import requests"]}, {"term": "def", "name": "main", "data": "def main():\n\tprint(\"Running webscrape on a single LinkedIn page: https://www.linkedin.com/jobs/view/2348402842/?refId=f1abe8b6-755d-4c06-a2db-f3bf1c70a591\")\n\tprint(\"Output:\", scrape_page(\"https://www.linkedin.com/jobs/view/2348402842/?refId=f1abe8b6-755d-4c06-a2db-f3bf1c70a591\"))\n\n\tprint(\"Running webscrape on a text file of LinkedIn pages: jobslist.txt\")\n\tprint(\"Output:\", scraper())\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import time", "import requests"]}], [{"term": "class", "name": "classwebScrape:", "data": "class webScrape:\n\n\tdef __init__(self):\n\t\t# driver = webdriver.Chrome(PATH, options=options)\n\t\tself.driver = webdriver.Chrome(PATH)\n\t\tself.driver.get(\"https://catalog.registrar.uiowa.edu/your-program/\")\n\n\tdef search_major(self, program):\n\t\t# search_bar = self.driver.find_element_by_id(\"quicksearch\")\n\t\t# search_bar.clear()\n\t\t# search_bar.send_keys(program)\n\t\t# search_bar.send_keys(Keys.RETURN)\n\t\t# program_name = self.driver.find_element_by_class_name(\"filterimage\")\n\t\t# span_name = program_name.find_element_by_tag_name(\"span\")\n\t\t# for x in span_name:\n\t\t\t# print(\"Hi\")\n\t\t# except:\n\t\t\t# print(\"This program does not exist, please enter another\")\n\t\tbuttons = WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable(\n\t\t\t(By.XPATH, \"//*[contains(text(), '{},')]\".format(program)))).click()\n\t\t# for btn in buttons:\n\t\t\t# btn.click()\n\t\t\t# break\n\t\tWebDriverWait(self.driver, 10).until(EC.element_to_be_clickable(\n\t\t\t(By.XPATH, '//a[text()=\"Requirements\"]'))).click()\n\t\trequirements_button = self.driver.find_element_by_id(\"requirementstexttab\")\n\t\trequirements_button.click()\n\t\tcourse_list = self.driver.find_element_by_class_name(\"sc_courselist\")\n\t\tcourse_odd = course_list.find_elements_by_class_name('odd')\n\t\tfor course in course_odd:\n\t\t\ttry:\n\t\t\t\tcourse_info = course.find_element_by_class_name(\"codecol\")\n\t\t\t\tcourse_name = course_info.find_element_by_tag_name(\"td\")\n\t\t\texcept:\n\t\t\t\tpass\n\t\tcourse_even = course_list.find_elements_by_class_name(\"even\")\n\t\tprint(self.driver.title)\n\t\tcheck_answer = False\n\t\twhile not check_answer:\n\t\t\tuser_input = input(\"Enter 0 to quit: \")\n\t\t\tif user_input == \"0\":\n\t\t\t\tcheck_answer = True\n\t\t\t\tself.driver.close()\n\t\t\t\tprint(\"Hello\")\n\n", "description": null, "category": "webscraping", "imports": ["import os", "from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "import logging"]}], [{"term": "class", "name": "classdimension_conversion:", "data": "class dimension_conversion:\n\tdef ft_to_cm(df_column):\n\t\tdf_column = df_column * 30.48\n\t\treturn df_column\n\tdef inch_to_cm(df_column):\n\t\tdf_column = df_column * 2.54\n\t\treturn df_column\n\tdef str_to_int_convers(df_column):\n\t\tdf_column = pd.to_numeric(df_column, errors = 'coerce')\n", "description": null, "category": "webscraping", "imports": ["from Utils import *"]}, {"term": "def", "name": "webscrape_function", "data": "def webscrape_function(array):\n\tdf = pd.DataFrame()\n\tfor i in array:\n\t\tdf = pd.concat([df, pd.read_html('http://ufcstats.com/statistics/fighters?char=' + i + '&page=all')[0]])\n\treturn df\n", "description": null, "category": "webscraping", "imports": ["from Utils import *"]}], [{"term": "def", "name": "main", "data": "def main():\n\tdata_config = json.load(open('config/data-params.json'))\n\tmodel_config = json.load(open('config/model-params.json'))\n\tweight_config = json.load(open('config/weight-params.json'))\n\twebscrape_config = json.load(open('config/webscrape-params.json'))\n\twebsite_config = json.load(open('config/website-params.json'))\n\treport_config = json.load(open('config/report-params.json'))\n\ttest_config = json.load(open('config/test-params.json'))\n\n\tos.system('git submodule update --init')\n\t\n\t# Getting the target\n\t# If no target is given, then run 'website'\n\tif len(sys.argv) == 1:\n\t\ttargets = 'website'\n\telse:\n\t\ttargets = sys.argv[1]\n\t\t\n\tif 'data' in targets:\n\t\tconvert_txt(**data_config)\n\tif 'autophrase' in targets:\n\t\tautophrase(data_config['outdir'], data_config['pdfname'], model_config['outdir'], model_config['filename'])\n\tif 'weight' in targets:\n\t\ttry:\n\t\t\tunique_key = '_' + sys.argv[2]\n\t\t\tchange_weight(unique_key, **weight_config)\n\t\texcept:\n\t\t\tchange_weight(unique_key='', **weight_config)\n\tif 'webscrape' in targets:\n\t\ttry:\n\t\t\tunique_key = '_' +  sys.argv[2]\n\t\t\twebscrape(unique_key, **webscrape_config)\n\t\texcept:\n\t\t\twebscrape(unique_key='', **webscrape_config)\n\tif 'report' in targets:\n\t\tconvert_report(report_config['experiment_in_path'], report_config['experiment_out_path'])\n\t\tconvert_report(report_config['analysis_in_path'], report_config['analysis_out_path'])\n\tif 'website' in targets:\n\t\tactivate_website(**website_config)\n\tif 'test' in targets:\n\t\tconvert_txt(test_config['indir'], data_config['outdir'], test_config['pdfname'],)\n\t\tautophrase(data_config['outdir'], test_config['pdfname'], model_config['outdir'], model_config['filename'])\n\t\tchange_weight(unique_key='', **weight_config)\n\t\twebscrape(unique_key='', **webscrape_config)\n\t\tconvert_report(report_config['experiment_in_path'], report_config['experiment_out_path'])\n\t\tconvert_report(report_config['analysis_in_path'], report_config['analysis_out_path'])\n\treturn\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import json", "from etl import convert_txt", "from model import autophrase", "from weight_phrases import change_weight", "from webscrape import webscrape", "from website import activate_website", "from utils import convert_report"]}], [], [], [{"term": "class", "name": "TestDataCollector", "data": "class TestDataCollector(TestCase):\n\n\n\n\tdef test_webscrape_user(self):\n\t\tpetition_html = codecs.open(\"test_petition.html\", 'r').read()\n\t\tuser_html = codecs.open(\"test_user.html\", 'r').read()\n\n\t\tnew_fields = webscrape(petition_html, user_html, \"user\")\n\n\t\tself.assertEqual(new_fields[\"creator_type\"], \"user\")\n\t\tself.assertFalse(new_fields[\"creator_has_website\"])\n\t\tself.assertEqual(new_fields[\"creator_city\"], \"dallas\")\n\t\tself.assertEqual(new_fields[\"creator_first_name\"], \"Clenesha\")\n\t\tself.assertEqual(new_fields[\"creator_last_name\"], \"Garland\")\n\t\tself.assertEqual(new_fields[\"creator_country\"], \"US\")\n\t\tself.assertEqual(new_fields[\"creator_description\"], None)\n\t\tself.assertEqual(new_fields[\"creator_display_name\"], \"Clenesha Garland\")\n\t\tself.assertEqual(new_fields[\"creator_locale\"], \"en-US\")\n\t\tself.assertEqual(new_fields[\"creator_photo\"], \"photos/2/im/tf/JDiMTfDinqfEyda-fullsize.jpg\")\n\t\tself.assertEqual(new_fields[\"creator_state\"], \"TX\")\n\t\tself.assertEqual(new_fields[\"creator_fb_permissions\"], 0)\n\t\tself.assertTrue(new_fields[\"creator_has_slug\"])\n\n\t\tself.assertEqual(new_fields[\"num_past_petitions\"], 1)\n\t\tself.assertEqual(new_fields[\"num_past_victories\"], 1)\n\t\tself.assertEqual(new_fields[\"num_past_verified_victories\"], 1)\n\t\tself.assertEqual(new_fields[\"last_past_victory_date\"], '2015-12-18')\n\t\tself.assertEqual(new_fields[\"last_past_verified_victory_date\"], '2015-12-18')\n\n\n\t\tself.assertEqual(new_fields[\"ask\"], 'President Barack Obama: Sharanda Jones does not deserve to die in prison')\n\t\tself.assertEqual(new_fields[\"calculated_goal\"], 300000)\n\t\tself.assertGreater(len(new_fields[\"description\"]), 10)\n\t\tself.assertTrue(new_fields[\"discoverable\"])\n\t\tself.assertEqual(new_fields[\"display_title\"], 'President Barack Obama: Sharanda Jones does not deserve to die in prison')\n\t\tself.assertEqual(new_fields[\"displayed_signature_count\"], 279890)\n\t\tself.assertFalse(new_fields[\"is_pledge\"], False)\n\t\tself.assertEqual(new_fields[\"is_victory\"], True)\n\t\tself.assertEqual(new_fields[\"is_verified_victory\"], True)\n\t\tself.assertEqual(new_fields[\"languages\"], ['en'])\n\t\tself.assertEqual(new_fields[\"original_locale\"], 'en-US')\n\t\tself.assertAlmostEqual(new_fields[\"progress\"], 93.2966666667)\n\t\tself.assertEqual(len(new_fields[\"tags\"]), 9)\n\t\tself.assertEqual(new_fields[\"victory_date\"], '2015-12-18')\n\t\tself.assertTrue(new_fields[\"has_video\"])\n\t\tself.assertEqual(new_fields[\"photo\"], \"photos/8/ut/wj/vnuTWJCdPnZLdes-fullsize.jpg\")\n\t\tself.assertEqual(len(new_fields[\"targets_detailed\"]), 1)\n\n\n\t\tself.assertEqual(len(new_fields), 35)\n\n\n\n\tdef test_webscrape_org(self):\n\t\tpetition_html = codecs.open(\"test_petition.html\", 'r').read()\n\t\tuser_html = codecs.open(\"test_org.html\", 'r').read()\n\n\t\tnew_fields = webscrape(petition_html, user_html, \"org\")\n\n\t\tself.assertEqual(new_fields[\"creator_type\"], \"org\")\n\t\tself.assertTrue(new_fields[\"creator_has_website\"])\n\t\tself.assertEqual(new_fields[\"creator_city\"], \"Washington\")\n\t\tself.assertEqual(new_fields[\"creator_photo\"], \"photos/5/rv/ss/EBrVSSjjVJpDkvK-fullsize.jpg\")\n\t\tself.assertEqual(new_fields[\"creator_country\"], \"US\")\n\t\tself.assertEqual(new_fields[\"creator_state\"], \"DC\")\n\t\tself.assertTrue(new_fields[\"creator_has_slug\"])\n\n\n\t\tself.assertTrue(new_fields[\"creator_has_address\"])\n\t\tself.assertFalse(new_fields[\"creator_has_contact_email\"])\n\t\tself.assertFalse(new_fields[\"creator_has_fb_page\"], None)\n\t\tself.assertTrue(new_fields[\"creator_mission\"])\n\t\tself.assertEqual(new_fields[\"creator_org_name\"], \"International Labor Rights Forum\")\n\t\tself.assertEqual(new_fields[\"creator_tax_country_code\"], None)\n\t\tself.assertEqual(new_fields[\"creator_tax_state_code\"], None)\n\t\tself.assertEqual(new_fields[\"creator_zipcode\"], \"20006\")\n\t\tself.assertEqual(new_fields[\"creator_postal_code\"], \"20006\")\n\n\t\tself.assertFalse(new_fields[\"creator_has_twitter\"])\n\t\tself.assertFalse(new_fields[\"creator_has_verified_req\"])\n\t\tself.assertFalse(new_fields[\"creator_has_verified_by\"])\n\t\tself.assertFalse(new_fields[\"creator_has_verified_at\"])\n\t\tself.assertFalse(new_fields[\"creator_has_video\"])\n\n\n\t\tself.assertEqual(new_fields[\"num_past_petitions\"], 5)\n\t\tself.assertEqual(new_fields[\"num_past_victories\"], 5)\n\t\tself.assertEqual(new_fields[\"num_past_verified_victories\"], 5)\n\t\tself.assertEqual(new_fields[\"last_past_victory_date\"], '2011-10-18')\n\t\tself.assertEqual(new_fields[\"last_past_verified_victory_date\"], '2011-10-18')\n\n\t\tself.assertEqual(len(new_fields), 43)\n\n\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from unittest import TestCase", "from data_collector import webscrape", "import codecs"]}], [], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\t# \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\t# \u722c\u53d6url\n\tdef get_url(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import os, re, json, traceback", "import requests", "from bs4 import BeautifulSoup", "from pyltp import SentenceSplitter"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\t# \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\t# \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n\tdef get_gloss(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import os, re, json, traceback", "import requests", "from bs4 import BeautifulSoup", "from pyltp import SentenceSplitter"]}], [{"term": "def", "name": "ncdefon_ready", "data": "async def on_ready():\n\tprint(f'We have logged in as {bot.user}')\n\n\t# Start task\n\ton_change.start()\n \n", "description": null, "category": "webscraping", "imports": ["import os, discord, webscrape", "from dotenv import load_dotenv", "from discord.ext import commands , tasks"]}, {"term": "def", "name": "ncdeftest", "data": "async def test(ctx, arg):\n\tawait ctx.send(arg)\n\n", "description": null, "category": "webscraping", "imports": ["import os, discord, webscrape", "from dotenv import load_dotenv", "from discord.ext import commands , tasks"]}, {"term": "def", "name": "ncdefget_last_msg_async", "data": "async def get_last_msg_async():\n\tchannel = bot.get_channel(int(CHANNELID))\n\tif channel is None:\n\t\treturn\n  \n\t\n\tmessage = await channel.fetch_message(channel.last_message_id)\n\tif message.embeds:   \n\t\treturn message.embeds[0]\n\n\treturn\n", "description": null, "category": "webscraping", "imports": ["import os, discord, webscrape", "from dotenv import load_dotenv", "from discord.ext import commands , tasks"]}, {"term": "def", "name": "ncdefon_change", "data": "async def on_change():\n\tprint('Getting news')\n\tlast_news = await get_last_msg_async()\n\tpost = webscrape.get_last_news()\n\tembed = discord.Embed(title=post[0], description=post[1], url=post[2])\n\n\tif last_news == embed:\n\t\treturn\n\telse:  \n\t\tchannel = bot.get_channel(CHANNELID)\n\t\tawait channel.send(embed=embed)\n", "description": null, "category": "webscraping", "imports": ["import os, discord, webscrape", "from dotenv import load_dotenv", "from discord.ext import commands , tasks"]}], [], [{"term": "def", "name": "scrape", "data": "def scrape():\n\tsite = raw_input(\"Enter page: \")\n\n\t#open site. read so we can read in a string context\n\t#test for valid and complete URL\n\ttry:\n\t\tdata = urllib2.urlopen(site).read()\n\texcept ValueError:\n\t\tprint \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n\t\treturn\n\t\n\t#print data\n\n\t#try an open the pattern file.\n\ttry:\n\t\tpatternFile = open('config/webscrape.dat', 'r').read().splitlines()\n\texcept:\n\t\tprint \"There was an error opening the webscrape.dat file\"\n\t\traise\n\t#create counter for counting regex expressions from webscrape.dat\n\tcounter = 0\n\t#for each loop so we can process each specified regex\n\tfor pattern in patternFile:\n\t\tm = re.findall(pattern, data)\n\t\t#m will return as true/false. Just need an if m:\n\t\tif m:\n\t\t\tfor i in m:\n\t\t\t\t#open output/results file...append because we are cool\n\t\t\t\toutfile = open('scrape-RESULTS.txt', 'a')\n\t\t\t#print m\n\t\t\t\toutfile.write(str(i))\n\t\t\t\toutfile.write(\"\\n\")  # may be needed. can always be removed.\n\n\t\t\t#close the file..or else\n\t\t\t\toutfile.close()\n\t\t\t\tcounter+=1\n\t\t\t\tprint \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n\t\telse:  # only need an else because m is boolean\n\t\t\tcounter+=1\n\t\t\tprint \"No match for item \" + str(counter) + \". Continuing.\"\n\t\t\t# Continue the loop if not a match so it can go on to the next\n\t\t\t# sequence\n\t\t\t# NOTE: you don't *really* need an else here...\n\t\t\tcontinue\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "import re", "\timport readline  # nice when you need to use arrow keys and backspace", "import sys"]}], [{"term": "class", "name": "classUserData:", "data": "class UserData:\n\n\tdef __init__(self, name):\n\t\tself.name = name\n\t\tself.freq_data = {}\n\t\tself.freq_data[1] = set()\n\t\tself.list_current = []\n\t\tself.price_getter = WebScrape()\n\n\tdef list_reset(self):\n\t\tself.list_current = []\n\n\tdef find_freqkey(self, product):\n\t\tkey = None\n\t\tfor k in self.freq_data.keys():\n\t\t\tif product in self.freq_data[k]:\n\t\t\t\tkey = k\n\t\t\t\tbreak\n\n\t\treturn key\n\n\tdef is_recommended_product(self, product):\n\t\treturn self.find_freqkey() > 0\n\n\tdef freq_upgrade(self, product, freq_prev):\n\t\tself.freq_data[freq_prev].remove(product)\n\n\t\tif freq_prev + 1 in self.freq_data.keys():\n\t\t\tself.freq_data[freq_prev + 1].add(product)\n\t\telse:\n\t\t\tself.freq_data[freq_prev + 1] = set()\n\t\t\tself.freq_data[freq_prev + 1].add(product)\n\n\tdef add_product(self, product):\n\t\tself.list_current.append(product)\n\t\tproduct_freqkey = self.find_freqkey(product)\n\t\tif not product_freqkey:\n\t\t\tself.freq_data[1].add(product)\n\t\telse:\n\t\t\tself.freq_upgrade(product, product_freqkey)\n\n\tdef get_ordered_recs_prices(self):\n\t\trecs = []\n\t\tfor freq in sorted(list(self.freq_data.keys()), reverse = True):\n\t\t\trecs.extend(self.freq_data[freq])\n\n\t\tprice = self.price_getter.selenium_script(recs)\n\t\treturn recs, price\n\n\tdef debug_print(self):\n\t\tprint(self.freq_data, self.list_current)\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from product_info_get import WebScrape"]}], [{"term": "def", "name": "main", "data": "def main():\n\n\t#Prompt for URL input\n\turl = input(\"Enter link to legislation on EUR-Lex: \")\n\n\t#TESTING DATA (use splice [500:800])\n\t#url = 'https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1536601415699&uri=CELEX:32018R0643'\n\n\n\t#--------------PHASE 1: Legislation Text Extraction---------------------------------------------------\n\t#Obtain legislation title and text by calling scrape_contents()\n\ttry:\n\t\tlegislation = scrape_contents(url)\n\n\texcept ValueError:\n\t\t#ValueError is thrown when input is not a supported EUR-Lex URL\n\t\tprint(\"Invalid URL\")\n\t\tsys.exit(-1)\n\n\t#Export and reload JSON data to 'data/legislation_webscrape.txt' for offline processing\n\twrite_json(legislation, 'data/legislation_webscrape.txt')\n\tlegislation = read_json('data/legislation_webscrape.txt')\n\n\t#text = legislation[\"text\"]\n\n\ttext = legislation[\"text\"][500:800] #Limit range for testing purposes\n\t# print(text) #Debugging purposes\n\n\t#TESTING DATA\n\t# text = 'Directive (EU) 2015/849 to facilitate supervision by competent authorities. As defined in Regulation (EC) No 1049/2001 of the European Parliament and of the Council'\n\t# text = '\u2018household customer\u2019 means household customer as defined in point 25 of Article 2 of Directive 2009/73/EC; 2.   Products listed in Annex I which are admitted free of import duties pursuant to Council Regulation (EC) No 1186/2009 (9) shall not be subject to the additional import duty. Article 8. Regulation (EC) No 673/2005 is repealed.'\n\n\n\n\t#--------------PHASE 2: Stage 1 Model---------------------------------------------------\n\t#Call getEntities_1 with text to obtain sections containing the Directives/Regulations referenced in it\n\tresponse_1_raw = getEntities_1(text)\n\t# print(json.dumps(response_1_raw, indent=3)) #Debugging purposes\n\n\t#Export and reload JSON data to 'data/stage1_data.txt' for offline processing\n\twrite_json(response_1_raw, 'data/stage1_data.txt')\n\tresponse_1_raw = read_json('data/stage1_data.txt')\n\n\n\n\t#--------------Stage 2 Preprocessing---------------------------------------------------\n\t#Extract text sections containing references to be analysed in Stage 2\n\tresponse_1_extract = [] #List to store dicts containing each reference text and type\n\n\tfor item in response_1_raw['entities']: #Iterate through detected entities\n\t\ttext = item['text'].replace('/', \" / \") #Extract text and add spaces between the forward slash\n\t\ttext = text.replace('(', \" (\")\n\t\ttype = item['type'] #Extract reference type\n\t\tresponse_1_extract.append({'text': text, 'type': type})\n\tresponse_1_extract = {'reference': response_1_extract}\n\n\t#Export and reload dictionary with references to 'data/stage1_extract.txt' for offline processing\n\twrite_json(response_1_extract, 'data/stage1_extract.txt')\n\tresponse_1_extract = read_json('data/stage1_extract.txt')\n\t# print(json.dumps(response_1_extract, indent=3)) #Debugging purposes\n\n\n\n\t#--------------PHASE 3: Stage 2 Model---------------------------------------------------\n\t#Call getEntities_2 with each reference to obtain breakdown of its components\n\tresponse_2_raw = []\n\tfor counter, item in enumerate(response_1_extract['reference']):\n\t\tresponse_2_raw.append(getEntities_2(item['text']))\n\n\t#Export and reload dictionary to 'data/stage2_data.txt'\n\twrite_json(response_2_raw, 'data/stage2_data.txt')\n\tresponse_2_raw = read_json('data/stage2_data.txt')\n\t#print(json.dumps(response_2_raw[1]['entities'], indent=3)) #Debugging purposes\n\n\n\t#--------------Results Processing---------------------------------------------------\n\tresponse_2_extract = [] #List to store dicts containing each component of reference\n\tfor item in response_2_raw:\n\t\tresponse_2_component_dict = {}\n\t\tfor component in item['entities']:\n\t\t\ttype = component['type']\n\t\t\ttext = component['text']\n\t\t\tresponse_2_component_dict[type] = text\n\t\tresponse_2_extract.append(response_2_component_dict)\n\tresponse_2_extract = {'result': response_2_extract}\n\n\t# Export and reload dictionary with results to 'data/stage2_extract.txt' for offline processing\n\twrite_json(response_2_extract, 'data/stage2_extract')\n\tresponse_2_extract = read_json('data/stage2_extract')\n\tprint(json.dumps(response_2_extract, indent=3))\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from EU_Legislation_NLP.web_scraping.bs4_EUR_Lex import scrape_contents", "from EU_Legislation_NLP.api_calls.stage_1 import getEntities_1", "from EU_Legislation_NLP.api_calls.stage_2 import getEntities_2", "from EU_Legislation_NLP.data.file_operations import write_json, read_json", "import json", "import sys", "\t# text = '\u2018household customer\u2019 means household customer as defined in point 25 of Article 2 of Directive 2009/73/EC; 2.   Products listed in Annex I which are admitted free of import duties pursuant to Council Regulation (EC) No 1186/2009 (9) shall not be subject to the additional import duty. Article 8. Regulation (EC) No 673/2005 is repealed.'"]}], [], [], [], [{"term": "class", "name": "WebscrapeConfig", "data": "class WebscrapeConfig(AppConfig):\n\tdefault_auto_field = 'django.db.models.BigAutoField'\n\tname = 'webScrape'\n", "description": null, "category": "webscraping", "imports": ["from django.apps import AppConfig"]}], [{"term": "def", "name": "callback_alarm", "data": "def callback_alarm(context: CallbackContext):\n\tsoup = webscrape.webscrape()\n\tfor index,i in enumerate(soup.select('tr[align=\"left\"]')):\n\t\tif index != 0:\n\t\t\tmylist=[td.text for td in i.find_all('td')]\n\t\t\tjobtitle = mylist[0]\n\t\t\tjobtype = mylist[1]\n\t\t\tjoblocation = mylist[2]\n\t\t\tjobarea = mylist[3]\n\t\t\tjoblink='https://careers.a-star.edu.sg/'+i.find('a')['href']\n\t\t\tfinaltext = f\"\"\"({index}).\\nJob title: {jobtitle}\\nJob type: {jobtype}\\nJob Location: {joblocation}\\nJob Field: {jobarea}\\nJob Link: {joblink}\\n\"\"\"\n\t\t\tcontext.bot.send_message(chat_id=context.job.context, text=finaltext)\n", "description": "({index}).\\nJob title: {jobtitle}\\nJob type: {jobtype}\\nJob Location: {joblocation}\\nJob Field: {jobarea}\\nJob Link: {joblink}\\n", "category": "webscraping", "imports": ["from telegram.ext import CommandHandler", "from telegram.ext import CallbackContext", "from telegram.ext import  Updater", "import constants", "import telegram", "import webscrape", "import datetime", "import pytz"]}, {"term": "def", "name": "start", "data": "def start(update: telegram.Update, context: CallbackContext):\n\tcontext.bot.send_message(chat_id=update.message.chat_id,\n\t\t\t\t\t\t\t text='Welcome. I will pull data at 9:00 AM , 12:00 PM and 5:30 PM.')\n\n\tcontext.job_queue.run_daily(callback_alarm,datetime.time(hour=9, minute=00, tzinfo=pytz.timezone('Asia/Singapore')),days=(0, 1, 2, 3, 4, 5, 6) , context=update.message.chat_id)\n\tcontext.job_queue.run_daily(callback_alarm,datetime.time(hour=12, minute=00, tzinfo=pytz.timezone('Asia/Singapore')),days=(0, 1, 2, 3, 4, 5, 6) , context=update.message.chat_id)\n\tcontext.job_queue.run_daily(callback_alarm,datetime.time(hour=17, minute=30, tzinfo=pytz.timezone('Asia/Singapore')),days=(0, 1, 2, 3, 4, 5, 6) , context=update.message.chat_id)\n", "description": null, "category": "webscraping", "imports": ["from telegram.ext import CommandHandler", "from telegram.ext import CallbackContext", "from telegram.ext import  Updater", "import constants", "import telegram", "import webscrape", "import datetime", "import pytz"]}, {"term": "def", "name": "main", "data": "def main():\n\tu = Updater(constants.TOKEN, use_context=True)\n\ttimer_handler = CommandHandler('start', start)\n\tu.dispatcher.add_handler(timer_handler)\n\tu.start_polling()\n", "description": null, "category": "webscraping", "imports": ["from telegram.ext import CommandHandler", "from telegram.ext import CallbackContext", "from telegram.ext import  Updater", "import constants", "import telegram", "import webscrape", "import datetime", "import pytz"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(username):\r\n\tt = time.time()\r\n\tpcent = username.index('%')\r\n\tusername = username[:pcent+1] + '23' + username[pcent+1:]\r\n\tprint(username)\r\n\t\r\n\toptions = webdriver.ChromeOptions()\r\n\toptions.headless = True\r\n\tdriver = webdriver.Chrome(options=options)\r\n\t#driver = webdriver.Chrome()\r\n\tdriver.get(f'https://cod.tracker.gg/warzone/profile/atvi/{username}/overview')\r\n\t#print(driver.content)\r\n\tpage_title = driver.find_elements(By.CLASS_NAME, 'lead')\r\n\t\r\n\tif not page_title or page_title[0] == \"WARZONE STATS NOT FOUND\":\r\n\t\tprint(\"WARZONE STATS NOT FOUND - Private profile\")\r\n\t\r\n\telse:\r\n\t\tsearch = driver.find_elements(By.CLASS_NAME, 'value')\r\n\t\tif len(search) > 4:\r\n\t\t\tprint(\"Wins:\", search[0].text)\r\n\t\t\tprint(\"Win %:\", search[1].text)\r\n\t\t\tprint(\"Kills:\", search[2].text)\r\n\t\t\tprint(\"K/D:\", search[3].text)\r\n\t\t\tprint(\"Score/min:\", search[4].text)\r\n\t\telse:\r\n\t\t\tprint(\"Incorrect name or private profile\")\r\n\telapsed = time.time() - t\r\n\tprint(elapsed, \"Time to webscrape\")\r\n\t\r\n\t#time.sleep(1)\r\n\tdriver.close() \r\n\trunCrappyCode()\r\n", "description": null, "category": "webscraping", "imports": ["import pyautogui\r", "import time\r", "import cv2\r", "from pytesseract import *\r", "from PIL import Image\r", "#import cloudscraper\r", "from selenium import webdriver\r", "#from webdriver_manager.chrome import ChromeDriverManager\r", "from selenium.webdriver.common.by import By\r", "import mss\r", "import mss.tools\r"]}, {"term": "def", "name": "runCrappyCode", "data": "def runCrappyCode():\r\n\t\t\r\n\tfound = False\r\n\ttime.sleep(1)\r\n\twhile not found:\r\n\t\t#Marcos, you might have to compare to different examples to get more accuracy and make this headless\r\n\t\tt = time.time()\r\n\t\tcoords = pyautogui.locateOnScreen('1ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n\t\tcoords1 = pyautogui.locateOnScreen('2ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n\t\tcoords2 = pyautogui.locateOnScreen('3ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n\t\tcoords3 = pyautogui.locateOnScreen('6ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n\r\n\t\t\r\n\t\telapsed = time.time() - t\r\n\t\t#print(elapsed)\r\n\t\tif coords or coords1 or coords2 or coords3:\r\n\t\t\tt = time.time()\r\n\t\t\twith mss.mss() as sct:\r\n\t\t\t\t# The screen part to capture\r\n\t\t\t\tregion = {'top': 938, 'left': 696, 'width': 339, 'height': 35}\r\n\t\t\t\r\n\t\t\t\t# Grab the data\r\n\t\t\t\timg = sct.grab(region)\r\n\t\t\t\r\n\t\t\t\t# Save to the picture file\r\n\t\t\t\tmss.tools.to_png(img.rgb, img.size, output='screenshot2.png')\r\n\t\t\t\r\n\t\t\t#im2 = pyautogui.screenshot(region = (700, 937, 263, 33))\r\n\t\t\telapsed = time.time() - t\r\n\t\t\t#print(elapsed)\r\n\t\t\tprint('found something')\r\n\t\t\t#im2.save(r\"screenshot2.png\")\r\n\t\t\t\r\n\t\t\timg = cv2.imread('screenshot2.png')\r\n\t\t\timg = cv2.resize(img, dsize=(526, 66), interpolation=cv2.INTER_CUBIC)\r\n\t\t\t'''\r\n\t\t\tdsize = (526, 66)\r\n\r\n\t\t\t# resize image\r\n\t\t\toutput = cv2.resize('screenshot2.png', dsize)\r\n\t\t\tcv2.imwrite('gray1.png',output)\r\n\t\t\t\r\n\t\t\t'''\r\n\t\t\tcv2.imwrite('screenshot2.png', img)\r\n\t\t\t#img = Image.open('screenshot2.png')\r\n\t\t\tresult = pytesseract.image_to_string(img)\r\n\t\t\tresult = result.rstrip()\r\n\t\t\t\r\n\t\t\tif result == '' or '#' not in result:\r\n\t\t\t\tprint(\"No string found\")\r\n\t\t\t\trunCrappyCode()\r\n\t\t\t\t\r\n\t\t\tif ']' in result:\r\n\t\t\t\tslicing = result.find(']')\r\n\t\t\t\tnewResult = result[slicing+1:]\r\n\t\t\t\tnewResult = newResult.replace('#', '%')\r\n\t\t\t\tprint(newResult)\r\n\t\t\t\t\r\n\t\t\t\twebscrape(newResult)\r\n\t\t\t\t\r\n\t\t\telif ']' not in result: \r\n\t\t\t\tnewResult = result.replace('#', '%')\r\n\t\t\t\tprint(newResult)\r\n\t\t\t\twebscrape(newResult)\r\n\t\t\t\t\r\n\t\telse:\r\n\t\t\t#print('ImageNotFound exception')\r\n\t\t\trunCrappyCode()\r\n\t\tfound = True\r\n", "description": null, "category": "webscraping", "imports": ["import pyautogui\r", "import time\r", "import cv2\r", "from pytesseract import *\r", "from PIL import Image\r", "#import cloudscraper\r", "from selenium import webdriver\r", "#from webdriver_manager.chrome import ChromeDriverManager\r", "from selenium.webdriver.common.by import By\r", "import mss\r", "import mss.tools\r"]}], [], [{"term": "def", "name": "webscrape", "data": "def webscrape():\n\t# grab the information from the website we'll be scrapping\n\trequest = requests.get('https://mtgadecks.net/')\n\tsoup = BeautifulSoup(request.text, 'html.parser')\n\n\t# get the links for all tier 1 decks\n\tlinks = soup.find(class_='col-md-3 float-left')\n\n\t# grab the first four decks, because that is all that will fit in tier 1\n\twebscrapedDecks = []\n\ttierDecks = soup.find_all(class_='tier')\n\tfor index, deck in enumerate(tierDecks):\n\t\tif (index < 4):\n\t\t\tnewDeck = deck.get_text().replace('\\n', '').replace(' ', '')\n\t\t\twebscrapedDecks.append(newDeck)\n\n\tlinkNames = []\n\tfor link in links.find_all('a', href=True):\n\t\tlinkNames.append(\"https://mtgadecks.net\" + link['href'])\n\n\t# connect to the database. If it doesn't exist, create it\n\tdatabaseRoutines.createDBIfNonexistent()\n\n\t# add to db any new decks, and send an email with the change(s)\n\tdatabaseRoutines.addDecksToDB(webscrapedDecks, linkNames)\n\n\t# update the database of any removals\n\tdatabaseRoutines.updateDB(webscrapedDecks)\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import databaseRoutines"]}], [], [{"term": "def", "name": "get_td_name", "data": "def get_td_name(td):\n\ta_tag = td.findAll('a')\n\tfor a in a_tag:\n\t\tif 'href' in a.attrs:\n\t\t\tname = a.attrs['href'].split('/')[-1]\n\t\t\t# td_name = name.replace('_', '/')\n\t\t\treturn name\n\t\telse:\n\t\t\tprint(a)   \n\t\n\treturn None\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "fget_ImageCollection", "data": "# def get_ImageCollection(name):\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "get_ImageCollection_tags", "data": "def get_ImageCollection_tags(td):\n\ttag_list = []\n\ta_tag = td.findAll('a')\n\tfor a in a_tag:\n\t\tif 'href' in a.attrs:\n\t\t\ttag = a.attrs['href'].split('/')[-1]\n\t\t\tif tag not in tag_list:\n\t\t\t\ttag_list.append(tag)\n\t\t\t\n\treturn tag_list\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "get_tbody_info", "data": "def get_tbody_info(tbody):\n\ttbody_info = {}\n\ttd_list = tbody.findAll('td')\n\n\tfor td in td_list:\n\t\tif td.attrs['class'] == ['ee-dataset']:\n\t\t\ttd_name = get_td_name(td)\n\t\t\t\n\t\telif td.attrs['class'] == ['ee-dataset-description-snippet']:\n\t\t\tquick_description = td.text\n\t\t\t\n\t\telif td.attrs['class'] == ['ee-tag-buttons', 'ee-small']:\n\t\t\ttag_list = get_ImageCollection_tags(td)\n\t\t\t\n\t\telse:\n\t\t\tprint(\"Undefined attributes in td object: \", td.attrs)\n\t\t\n\n\t# IC_id, image_collection = get_ImageCollection(td_name)\n\t\t\n\ttbody_info = {'dataset': td_name,\n\t\t\t\t   'tags': tag_list,\n\t\t\t\t   'description': quick_description\n\t\t\t\t  }\n\n\treturn tbody_info\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "parse_code_block", "data": "def parse_code_block(code_block):\n\t\"\"\" Break out the Code Embedded on Earth-Engine Dataset URL\n\n\tExample output of code_block:\n\n\t\tee.ImageCollection(\"\")\n\n\tdataset_type follows the ee.-> before first '('\n\tdataset_id inside \" \"\n\n\t\"\"\"\n\t# Extract Datatepy in code block\n\tdataset_type = code_block.text.split('.')[1]\n\tdataset_type = dataset_type.split('(')[0]\n\n\t# Extract ID from the code block 'ee.Image...(\"\")'\n\tdataset_id = code_block.text.split('(')[1]\n\tdataset_id = dataset_id.split(')')[0]\n\tdataset_id = dataset_id.replace('\"','')\n\n\treturn dataset_type, dataset_id\n\n\n", "description": " Break out the Code Embedded on Earth-Engine Dataset URL\n\n\tExample output of code_block:\n\n\t\tee.ImageCollection(\"\")\n\n\tdataset_type follows the ee.-> before first '('\n\tdataset_id inside \" \"\n\n\t", "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "validate_availability", "data": "def validate_availability(text):\n\t\"\"\" we will use this a test function to extract a start/end date\"\"\"\n\n\t# Test 0: ensure the text is actually a string before splitting\n\ttry:\n\t\tassert isinstance(text, str), \"not a string\"\n\texcept Exception as e:\n\t\t# raise Exception('Date Validate Failed: {}'.format(e))\n\t\tprint(e)\n\t\treturn None\n\n\t# Test 1: The text must be split evenly by ' - '\n\tsplit_text = text.split(' - ')\n\ttry:\n\t\tassert len(split_text) == 2, \"len()==2 Test\"\n\texcept Exception as e:\n\t\t# raise Exception('Date Validate Failed: {}'.format(e))\n\t\tprint(e)\n\t\treturn None\n\n\t# Test 2: DateTime Formatting\n\t# Test 2a: First split has DateTime format (Most Common ISO format)\n\ttry:\n\t\t# date_start = datetime.strptime(split_text[0], '%Y-%m-%')\n\t\tdate_start = datetime.datetime.fromisoformat(split_text[0])\n\n\t# Test 2b: Use custom strptime, non-ISO\n\texcept Exception as e:\n\t\tdate_start = datetime.datetime.strptime(split_text[0], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n\t\tassert isinstance(date_start, datetime.datetime), \"Not a Datetime object\"\n\n\texcept:\n\t\t# raise Exception('Date Validate Failed: {}'.format(e))\n\t\tprint(\"Not a datetime object: {}\".format(split_text[0]))\n\t\treturn None\n\n\tfinally:\n\t\t# Since we are packing this into a json file, the best way is to keep string\n\t\t# the date end will be determined later, but if we succeeded in date start\n\t\t# We have found our Date Availability Match!\n\t\tdate_range = {'dataset_start': split_text[0], 'dataset_end': split_text[1]}\n\t\treturn date_range\n\n", "description": " we will use this a test function to extract a start/end date", "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "seek_date_availability", "data": "def seek_date_availability(soup):\n\t\"\"\" This is difficult to be exact, the html is not clearly/uniquely marked\"\"\"\n\tdate_range = None\n\t# Method 1: the first dd on the page:\n\ttry:\n\t\tdd = soup.find(\"dd\")\n\t\tdate_range = validate_availability(dd.text)\n\t\tassert date_range is not None, \"Method 1\"\n\t\treturn date_range\n\texcept Exception as e:\n\t\t# raise Exception('Failed on {}, {}'.format(dd, e))\n\t\tprint(e)\n\n\t# Method 2: all dd's on page, find the one that satisfies a time format validation\n\tfor dd in soup.findAll(\"dd\"):\n\t\tdate_range = validate_availability(dd.text)\n\t\tif date_range is not None:\n\t\t\treturn date_range\n\n\tprint(\"METHOD 2 FAILED TO FIND DATE AVAILABILITY in each Method, SET DEFAULT\")\n\t# determine Earliest and Latest Date and set those as defaults\n\tdefault_start = None\n\tdefault_end = None\n\tdate_range = {'dataset_start': default_start, 'dataset_end': default_end}\n\treturn date_range\n\n\n\n", "description": " This is difficult to be exact, the html is not clearly/uniquely marked", "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "follow_dataset_link", "data": "def follow_dataset_link(tag_name, URL):\n\tdataset_url = URL + '/' + tag_name\n\tresponse = requests.get(dataset_url)\n\thtmlCode = response.text\n\tsoup = BeautifulSoup(response.text, 'html.parser')\n\tcode = soup.find(\"code\")\n\n\tdataset_type, dataset_id = parse_code_block(code)\n\tresult = {'dataset_id': dataset_id, 'dataset_type': dataset_type}\n\tresult.update(seek_date_availability(soup))\n\n\treturn result\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "save_catalog", "data": "def save_catalog(results, partial=False):\n\t\"\"\"Save this simple metadata to json \"\"\"\n\tPLUGIN_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\tMETADATA_DIR = os.path.join(PLUGIN_DIR, \"metadata\") \n\n\t# Base Filename\n\tfn = 'gee_catalog'\n\tftype = '.json'\n\t\n\t# if we have partial results, track the last result index\n\tif partial:\n\t\tfn = fn + '_(' + str(len(results)-1) + ')'\n\n\t# Add all\n\tfn = fn + ftype\n\t# now get full file path + name\n\tfilename = os.path.join(METADATA_DIR, fn)\n\n\twith open(filename, 'w') as outfile:\n\t\tjson.dump(results, outfile, indent=4)\n\n\n", "description": "Save this simple metadata to json ", "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "get_catalog", "data": "def get_catalog():\n\t#=====================================\n\t# 0. Run the requests on the Google Earth Engine Datasets Catalog\n\t#=====================================\n\n\t# From the \"View all datasets\" Tab on developers.google.com/earth-engine\n\turl = \"https://developers.google.com/earth-engine/datasets/catalog\"\n\tresponse = requests.get(url) \n\thtmlCode = response.text \n\tsoup = BeautifulSoup(response.text, 'html.parser') \n\n\t# the 'tbody' element seems to be the best way to extract the useful metadata for each dataset\n\tall_tbodies = soup.findAll(\"tbody\")\n\n\t# total GEE datasets: 409 as of 2020-07-21\n\t# print(len(all_tbodies))\n\n\t#=====================================\n\t# 1. Iterate through each tbody to extract metadata\n\t#=====================================\n\n\t# this will result in a list of each dataset saved as a json metadata file\n\t# for allowing user to filter on tags/geography/time before \n\t# making requests to the gee server\n\twebscrape_results = []\n\n\tfor tbody in all_tbodies:\n\t\t# 1a. tbody will have 3 basic infos: ImageCollection Name, Tags, Description\n\t\t\n\t\ttry:\n\t\t\tthis_result = get_tbody_info(tbody)\n\t\t\tmore_result = follow_dataset_link(this_result['dataset'], url)\n\t\t\tthis_result.update(more_result)\n\t\t\twebscrape_results.append(this_result)\n\n\t\t\tupdate_msg = \"Webscrape Status: {} out of {}\".format(len(webscrape_results), len(all_tbodies))\n\t\t\tprint(update_msg, end=\"\\r\", flush=True)\n\n\t\texcept Exception as e:\n\t\t\tprint(e)\n\t\t\tprint(\"failed data extract on: \", tbody)\n\t\t\tsave_catalog(webscrape_results, partial=True)\n\t\t\treturn\n\n\n\t#=====================================\n\t# 2. Save All metadata to json, all tests passed\n\t#=====================================\n\tsave_catalog(webscrape_results, partial=False)\n\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "working_catalog", "data": "def working_catalog():\n\t\"\"\" Manual filter for current working version\"\"\"\n\tall_catalog = read_catalog()\n\tworking_IC_set = [\n\t\t\t'WorldPop/GP', \n\t\t\t'S5P', \n\t\t\t'NCEP_RE', \n\t\t\t'NOAA'\n\t\t\t# 'NOAA/GOES', \n\t\t\t# 'NOAA/NWS',\n\t\t\t# 'NOAA/VIIRS'\n\t\t\t]\n\tnon_working_IC_set = [\n\t\t\t'NOAA/CFSR'\n\t\t\t'NOAA/DMSP-OLS/NIGHTTIME_LIGHTS'\n\t\t\t]\n\n\tnon_working_FC_set = [\n\t\t\t'BLM/AIM/v1/TerrADat/TerrestrialAIM',\n\t\t\t'FAO/GAUL/2015/level0',\n\t\t\t'NOAA/NHC/HURDAT2/atlantic',\n\t\t\t'NOAA/NHC/HURDAT2/pacific'\n\t\t\t]\n\n\tuse_catalog = []\n\n\tfor data in all_catalog:\n\n\t\tif data['dataset_type'] == 'FeatureCollection':\n\t\t\tif data['dataset_id'] not in non_working_FC_set:\n\t\t\t\tuse_catalog.append(data)\n\n\t\tif data['dataset_type'] == 'ImageCollection':\n\t\t\tif data['dataset_id'] in non_working_IC_set:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tfor working in working_IC_set:\n\t\t\t\t\tif working in data['dataset_id']:\n\t\t\t\t\t\tuse_catalog.append(data)\n\t\t\n\n\n\tsave_catalog(use_catalog)\n\treturn\n\n", "description": " Manual filter for current working version", "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "read_catalog", "data": "def read_catalog():\n\tcatalog_fn = os.path.join('metadata', 'gee_catalog.json')\n\tfpath = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\tfn = os.path.join(fpath, catalog_fn)\n\n\tif os.path.exists(fn):\n\t\twith open(fn, 'r') as in_file:\n\t\t\tdata = json.load(in_file)\n\t\t\treturn data\n\n\telse:\n\t\tprint(\"No Catalog Found...\\nnow running ee_catalog.py\")\n\t\tget_catalog()\n\t\treturn read_catalog()\n\n\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "test", "data": "def test():\n\tworking_catalog()\n", "description": null, "category": "webscraping", "imports": ["import os, sys", "import json", "import datetime", "import requests", "from bs4 import BeautifulSoup"]}], [], [{"term": "def", "name": "webscrape", "data": "def webscrape():\n\turl = \"https://ensaf.ac.ma/?controller=pages&action=info\"\n\treq = Request(url , headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'})\n\tuh=urlopen(req)\n\trawhtml=uh.read()\n\tscrape=soup(rawhtml,'lxml')\n\tinfo=scrape.find('div',class_='table-responsive')\n\ttest=info.tbody\n\tresults=test.find_all('tr')\n\tmodules=[]\n\tfor result in results:\n\t\t\tmodule=result.select_one(\"tr td:nth-of-type(2)\")\n\t\t\tif (module is not None):\n\t\t\t\tsousmodule=module.text\n\t\t\t\tsousmodule=sousmodule.replace('\\n', '')\n\t\t\t\tsousmodule=sousmodule.replace('\\t', '')\n\t\t\t\tsousmodule=sousmodule.replace('1.','')\n\t\t\t\tsousmodule=sousmodule.replace('\u00e9','e')\n\t\t\t\tsousmodule=sousmodule.replace('\u00e8','e')\n\t\t\t\tsousmodule=sousmodule.replace(\"\u2019\",\" \")\n\t\t\t\tsousmodule=sousmodule.replace(\"\u00f4\",'o')\n\t\t\t\tsousmodule=sousmodule.replace(\"'\",'')\n\t\t\t\tsousmodule=sousmodule.replace('\u00fb','u')\n\t\t\t\tsousmodule=sousmodule.replace('\u00e0','a')\n\t\t\t\tsousmodule=sousmodule.replace('\u0153','oe')\n\t\t\t\tmodules.append(sousmodule.strip())\n\tmodules2=[y for x in modules for y in x.split('2.')]\n\tmodules3=[y for x in modules2 for y in x.split('3.')]\n\twhile (\"\" in modules3):\n\t\tmodules3.remove(\"\")\n\tmodules3 = [y.strip() for y in modules3]\n\tmodules3=[x for x in modules3 if not x.startswith(\"Projet\") and not x.startswith(\"PFA\") and not x.startswith(\"TEC\") and not x.startswith(\"Fran\") and not x.startswith(\"Allemand\") and not x.startswith(\"Anglais\") and not x.startswith(\"Langue\") and not x.startswith(\"Espagnol\") and not x.startswith(\"Stage\")]\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import Request, urlopen", "from bs4 import BeautifulSoup as soup", "import sys"]}], [{"term": "class", "name": "AmazonSpiderSpider", "data": "class AmazonSpiderSpider(scrapy.Spider):\n\tname = 'amazon'\n\tstart_urls = ['https://www.amazon.com/s?k=masks+50pcs&ref=nb_sb_noss_1']\n\tpageNumber = 2\n\n\tdef parse(self, response):\n\t\titems = WebscrapeItem()\n\n\t\tproduct_name = response.css('.a-color-base.a-text-normal').css('::text').extract()\n\t\tproduct_price = response.css('.a-price-whole::text').extract()\n\n\t\titems['product_name'] = product_name\n\t\titems['product_price'] = product_price\n\n\t\tyield items\n\n\t\tnextPage = 'https://www.amazon.com/s?k=masks+50pcs&page=' + str(AmazonSpiderSpider.pageNumber)\n\n\t\tif AmazonSpiderSpider.pageNumber <= 100:\n\t\t\tAmazonSpiderSpider.pageNumber += 1\n", "description": null, "category": "webscraping", "imports": ["import scrapy", "from ..items import WebscrapeItem"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(url_page, path_download_dir, path_geckodriver, path_xpath1, path_xpath2, sleep_time=5):\n\t\"\"\"\n\tFunction to download files via webscrapping, using selenium and fireforx's\n\tgeckodriver, which can be downloaded here: https://github.com/mozilla/geckodriver/releases\n\n\tHow to webscrape:\n\t0. Download all required packages (incl. selenium and geckodriver)\n\t1. Open the url with the download links in Firefox\n\t2. right cklick on the first download link and select 'inspect'\n\t3. right click again on the higlighted entry in firefox developer mode and choose 'copy path' -> 'xpath'\n\t4. do the same for the last download link\n\n\tArgs:\n\t- url_page (str): address of webpage where files should be downloaded\n\t- path_download_dir (str): path to dir in which downloads should be saved\n\t- path_geckodriver (str): path to folder in which geckodriver is stored\n\t- path_xpath1(str): xpath of file where download should start with\n\t- path_xpath2 (str): xpath of file where download should stop (all download links in between will be downloaded)\n\t- sleep_time (int): on some pages it takes some time to load all download links; here, a timer can be set to wait\n\n\tReturns:\n\t-\n\n\tLast update: 21/06/21. By Felix.\n\n\t\"\"\"\n\t# intitialise download\n\tprint('Starting to download from ' + url_page)\n\n\t# Set options for webdriver\n\toptions = FirefoxOptions()\n\t# options.add_argument(\"--headless\")\n\t#options.headless = True\n\n\t# in this case we set the options to avoid pop up windows when downloading\n\tfp = webdriver.FirefoxProfile()\n\n\t# Set 0 = desktop, 1 = default download folder, 2 = specified donwload folder\n\tfp.set_preference(\"browser.download.folderList\", 2)\n\tfp.set_preference(\"browser.download.manager.showWhenStarting\", False)\n\n\t# Setting for specific download folder (downloadDir)\n\tfp.set_preference(\"browser.download.dir\", path_download_dir)\n\n\t# Setting to disable download pop up window and directly download file\n\tfp.set_preference(\n\t\t\"browser.helperApps.neverAsk.saveToDisk\",\n\t\t\"application/zip, application/octet-stream, multipart/x-zip, application/zip-compressed, application/x-zip-compressed\")\n\n\t# Update preferences\n\tfp.update_preferences()\n\n\t# options.profile(fp)\n\n\t# Run firefox webdriver from executable path of your choice\n\tdriver = webdriver.Firefox(firefox_profile=fp, executable_path=path_geckodriver, options=options)\n\t#driver = webdriver.Firefox(executable_path = path_geckodriver, options=options)\n\n\t# -----------------------------------------------------------------------------\n\n\t# Get web page\n\tdriver.get(url_page)\n\n\t# Sleep for 15s to load feed\n\tprint('Waiting {} secs to let all download links load...'.format(sleep_time))\n\ttime.sleep(sleep_time)\n\tprint('-------------')\n\n\t# Find elements by xpath\n\t#download_elems = driver.find_elements_by_xpath((path_xpath1))\n\t#print('Number of downloadable files: ', len(download_elems))\n\t# print('-------------')\n\n\t# Find where two xpath strings have different entries\n\tlist_pos_diff = [i for i in range(len(path_xpath1)) if path_xpath1[i] != path_xpath2[i]]\n\t# Get first position\n\tpos_diff = list_pos_diff[0]\n\n\t# Get pos of '['\n\tstr_front = path_xpath1[0:pos_diff]\n\t# here we use rindex to get last '['\n\tpos_start = str_front.rindex('[')\n\n\t# Get pos of ']'\n\tstr_end = path_xpath1[pos_start:]\n\tpos_end = str_end.index(']')\n\t# Build part 1 and part 2 of xpath\n\txpath_pt1 = path_xpath1[0:pos_start + 1]\n\txpath_pt2 = path_xpath1[pos_start + pos_end:]\n\n\t# Get start for range (starting value between [])\n\ta = int(path_xpath1[pos_start + 1:pos_start + pos_end])\n\t# Get end for range (end value between [])\n\tb = 1 + int(path_xpath2[pos_start + 1:(pos_start + pos_end + abs(len(path_xpath2) - len(path_xpath1)))])\n\n\t# the range is definded from the xpath path_xpath1 to xpath path_xpath2\n\tfor i in range(a, b):\n\t\t# build xpath for loop\n\t\tpath = xpath_pt1 + str(i) + xpath_pt2\n\t\tprint(path)\n\t\t# use selenium webdriver to click and download\n\t\tresults = driver.find_elements_by_xpath((path))[0]\n\t\tresults.click()\n\t\ttime.sleep(1)\n\n\t\tif i % 1000 == 0:\n\t\t\tprint('Number of Downloaded files: ', i)\n\n\t# -----------------------------------------------------------------------------\n\t# Sleep for 5s to ensure that everythings loaded properly\n\ttime.sleep(5)\n\tdriver.quit()\n\n\t# -----------------------------------------------------------------------------\n\tprint('-------------')\n\tprint('Download sucessfull!')\n\tprint('{} Files downloaded to '.format(b - a) + path_download_dir)\n\n", "description": "\n\tFunction to download files via webscrapping, using selenium and fireforx's\n\tgeckodriver, which can be downloaded here: https://github.com/mozilla/geckodriver/releases\n\n\tHow to webscrape:\n\t0. Download all required packages (incl. selenium and geckodriver)\n\t1. Open the url with the download links in Firefox\n\t2. right cklick on the first download link and select 'inspect'\n\t3. right click again on the higlighted entry in firefox developer mode and choose 'copy path' -> 'xpath'\n\t4. do the same for the last download link\n\n\tArgs:\n\t- url_page (str): address of webpage where files should be downloaded\n\t- path_download_dir (str): path to dir in which downloads should be saved\n\t- path_geckodriver (str): path to folder in which geckodriver is stored\n\t- path_xpath1(str): xpath of file where download should start with\n\t- path_xpath2 (str): xpath of file where download should stop (all download links in between will be downloaded)\n\t- sleep_time (int): on some pages it takes some time to load all download links; here, a timer can be set to wait\n\n\tReturns:\n\t-\n\n\tLast update: 21/06/21. By Felix.\n\n\t", "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.firefox.options import Options as FirefoxOptions", "import time"]}, {"term": "def", "name": "main", "data": "def main():\n\n\t# Paths for webscrapper\n\turlpage = 'https://services.cuzk.cz/gml/inspire/bu/epsg-4258/'\n\txpath1 = '/html/body/div[2]/div/div[7]/div[2]/ul/li[1]/div[6]/div[2]/ul/li[1]/a'\n\txpath2 = '/html/body/div[2]/div/div[7]/div[2]/ul/li[1]/div[6]/div[2]/ul/li[3]/a'\n\n\tgeckodriver_dir = '/Users/Felix/Documents/Studium/PhD/05_Projects/02_Estimate_Building_Heights/preprocessing/Webscraper_Thueringen/geckodriver'\n\n\t# Path of output dir\n\tdownload_dir = \"/Users/Felix/Desktop/test\"\n\n\t# Start to webscrape\n\twebscrape(urlpage, download_dir, geckodriver_dir, xpath1, xpath2, 15)\n\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.firefox.options import Options as FirefoxOptions", "import time"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(url_page, path_download_dir, path_geckodriver, path_xpath1, path_xpath2, sleep_time=5):\n\t\"\"\"\n\tFunction to download files via webscrapping, using selenium and fireforx's\n\tgeckodriver, which can be downloaded here: https://github.com/mozilla/geckodriver/releases\n\n\tHow to webscrape:\n\t0. Download all required packages (incl. selenium and geckodriver)\n\t1. Open the url with the download links in Firefox\n\t2. right cklick on the first download link and select 'inspect'\n\t3. right click again on the higlighted entry in firefox developer mode and choose 'copy path' -> 'xpath'\n\t4. do the same for the last download link\n\n\tArgs:\n\t- url_page (str): address of webpage where files should be downloaded\n\t- path_download_dir (str): path to dir in which downloads should be saved\n\t- path_geckodriver (str): path to folder in which geckodriver is stored\n\t- path_xpath1(str): xpath of file where download should start with\n\t- path_xpath2 (str): xpath of file where download should stop (all download links in between will be downloaded)\n\t- sleep_time (int): on some pages it takes some time to load all download links; here, a timer can be set to wait\n\n\tReturns:\n\t-\n\n\tLast update: 21/06/21. By Felix.\n\n\t\"\"\"\n\t# intitialise download\n\tprint('Starting to download from ' + url_page)\n\n\t# Set options for webdriver\n\toptions = FirefoxOptions()\n\t# options.add_argument(\"--headless\")\n\t#options.headless = True\n\n\t# in this case we set the options to avoid pop up windows when downloading\n\tfp = webdriver.FirefoxProfile()\n\n\t# Set 0 = desktop, 1 = default download folder, 2 = specified donwload folder\n\tfp.set_preference(\"browser.download.folderList\", 2)\n\tfp.set_preference(\"browser.download.manager.showWhenStarting\", False)\n\n\t# Setting for specific download folder (downloadDir)\n\tfp.set_preference(\"browser.download.dir\", path_download_dir)\n\n\t# Setting to disable download pop up window and directly download file\n\tfp.set_preference(\n\t\t\"browser.helperApps.neverAsk.saveToDisk\",\n\t\t\"application/zip, application/octet-stream, multipart/x-zip, application/zip-compressed, application/x-zip-compressed\")\n\n\t# Update preferences\n\tfp.update_preferences()\n\n\t# options.profile(fp)\n\n\t# Run firefox webdriver from executable path of your choice\n\tdriver = webdriver.Firefox(firefox_profile=fp, executable_path=path_geckodriver, options=options)\n\t#driver = webdriver.Firefox(executable_path = path_geckodriver, options=options)\n\n\t# -----------------------------------------------------------------------------\n\n\t# Get web page\n\tdriver.get(url_page)\n\n\t# Sleep for 15s to load feed\n\tprint('Waiting {} secs to let all download links load...'.format(sleep_time))\n\ttime.sleep(sleep_time)\n\tprint('-------------')\n\n\t# Find elements by xpath\n\t#download_elems = driver.find_elements_by_xpath((path_xpath1))\n\t#print('Number of downloadable files: ', len(download_elems))\n\t# print('-------------')\n\n\t# Find where two xpath strings have different entries\n\tlist_pos_diff = [i for i in range(len(path_xpath1)) if path_xpath1[i] != path_xpath2[i]]\n\t# Get first position\n\tpos_diff = list_pos_diff[0]\n\n\t# Get pos of '['\n\tstr_front = path_xpath1[0:pos_diff]\n\t# here we use rindex to get last '['\n\tpos_start = str_front.rindex('[')\n\n\t# Get pos of ']'\n\tstr_end = path_xpath1[pos_start:]\n\tpos_end = str_end.index(']')\n\t# Build part 1 and part 2 of xpath\n\txpath_pt1 = path_xpath1[0:pos_start + 1]\n\txpath_pt2 = path_xpath1[pos_start + pos_end:]\n\n\t# Get start for range (starting value between [])\n\ta = int(path_xpath1[pos_start + 1:pos_start + pos_end])\n\t# Get end for range (end value between [])\n\tb = 1 + int(path_xpath2[pos_start + 1:(pos_start + pos_end + abs(len(path_xpath2) - len(path_xpath1)))])\n\n\t# the range is definded from the xpath path_xpath1 to xpath path_xpath2\n\tfor i in range(a, b):\n\t\t# build xpath for loop\n\t\tpath = xpath_pt1 + str(i) + xpath_pt2\n\t\tprint(path)\n\t\t# use selenium webdriver to click and download\n\t\tresults = driver.find_elements_by_xpath((path))[0]\n\t\tresults.click()\n\t\ttime.sleep(1)\n\n\t\tif i % 1000 == 0:\n\t\t\tprint('Number of Downloaded files: ', i)\n\n\t# -----------------------------------------------------------------------------\n\t# Sleep for 5s to ensure that everythings loaded properly\n\ttime.sleep(5)\n\tdriver.quit()\n\n\t# -----------------------------------------------------------------------------\n\tprint('-------------')\n\tprint('Download sucessfull!')\n\tprint('{} Files downloaded to '.format(b - a) + path_download_dir)\n\n", "description": "\n\tFunction to download files via webscrapping, using selenium and fireforx's\n\tgeckodriver, which can be downloaded here: https://github.com/mozilla/geckodriver/releases\n\n\tHow to webscrape:\n\t0. Download all required packages (incl. selenium and geckodriver)\n\t1. Open the url with the download links in Firefox\n\t2. right cklick on the first download link and select 'inspect'\n\t3. right click again on the higlighted entry in firefox developer mode and choose 'copy path' -> 'xpath'\n\t4. do the same for the last download link\n\n\tArgs:\n\t- url_page (str): address of webpage where files should be downloaded\n\t- path_download_dir (str): path to dir in which downloads should be saved\n\t- path_geckodriver (str): path to folder in which geckodriver is stored\n\t- path_xpath1(str): xpath of file where download should start with\n\t- path_xpath2 (str): xpath of file where download should stop (all download links in between will be downloaded)\n\t- sleep_time (int): on some pages it takes some time to load all download links; here, a timer can be set to wait\n\n\tReturns:\n\t-\n\n\tLast update: 21/06/21. By Felix.\n\n\t", "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.firefox.options import Options as FirefoxOptions", "import time"]}, {"term": "def", "name": "main", "data": "def main():\n\n\t# Paths for webscrapper\n\turlpage = 'https://geoportal.geoportal-th.de/gaialight-th/_apps/atomfeedexplorer/?#feed=http%3A//geoportal.geoportal-th.de/dienste/atom_th_gebaeude%3Ftype%3Ddataset%26id%3D97d152b8-9e00-49f3-9ae4-8bbb30873562'\n\txpath1 = '/html/body/div[2]/div/div[7]/div[2]/ul/li[1]/div[6]/div[2]/ul/li[1]/a'\n\txpath2 = '/html/body/div[2]/div/div[7]/div[2]/ul/li[1]/div[6]/div[2]/ul/li[3]/a'\n\n\tgeckodriver_dir = '/Users/Felix/Documents/Studium/PhD/05_Projects/02_Estimate_Building_Heights/preprocessing/Webscraper_Thueringen/geckodriver'\n\n\t# Path of output dir\n\tdownload_dir = \"/Users/Felix/Desktop/test\"\n\n\t# Start to webscrape\n\twebscrape(urlpage, download_dir, geckodriver_dir, xpath1, xpath2, 15)\n\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.firefox.options import Options as FirefoxOptions", "import time"]}], [{"term": "class", "name": "Webscrape", "data": "class Webscrape():\n\t'''classes are cool, no other real reason to use this - probably going to only have one function'''\n\tdef __init__(self):\n\t\tself.webpath = \"https://api.tracker.gg/api/v2/rocket-league/standard/profile/\"\n\t\tself.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n\n\tdef retrieveDataRLTrackerFromURL(self,url):\n\t\theaders = self.headers\n\t\tplayerdata = {} # define the playerdata dict\n\t\tpage = requests.get(url, headers=headers)\n\t\tif page.status_code == 200:\n\t\t\tdata = json.loads(page.text)\n\t\t\tsegs = data[\"data\"][\"segments\"]\n\t\t\tfor segment in segs:\n\t\t\t\t\t\tif \"playlist\" in segment['type']:\n\t\t\t\t\t\t\tplayerdata[segment['metadata']['name']] = {'rank': segment['stats']['tier']['metadata']['name'], 'iconUrl': segment['stats']['tier']['metadata']['iconUrl'], 'division': segment['stats']['division']['metadata']['name'], 'mmr': segment['stats']['rating']['value']}\n\n\t\treturn playerdata\n\n\tdef retrieveDataRLTracker(self,gamertag=\"reasel\",platform=\"steam\"):\n\t\twebpath = self.webpath\n\t\theaders = self.headers\n\t\tplayerdata = {} # define the playerdata dict\n\t\tplayerdata[gamertag] = {} # define the gamertag dict\n\t\tpage = requests.get(\"%(webpath)s%(platform)s/%(gamertag)s\" % locals(), headers=headers)\n\t\tif page.status_code == 200:\n\t\t\tdata = json.loads(page.text)\n\t\t\tsegs = data[\"data\"][\"segments\"]\n\t\t\tfor segment in segs:\n\t\t\t\t\t\tif \"playlist\" in segment['type']:\n\t\t\t\t\t\t\tplayerdata[segment['metadata']['name']] = {'rank': segment['stats']['tier']['metadata']['name'], 'iconUrl': segment['stats']['tier']['metadata']['iconUrl'], 'division': segment['stats']['division']['metadata']['name'], 'mmr': segment['stats']['rating']['value']}\n\n\t\treturn playerdata\n\n", "description": null, "category": "webscraping", "imports": ["import argparse", "import requests", "import json", "import csv", "\tfrom pprint import pprint # pprint is cool"]}, {"term": "def", "name": "singleRun", "data": "def singleRun(gamertag,platform):\n\t'''Single run of Webscrape.retrieveDataRLTracker'''\n\tscrape = Webscrape()\n\tdata = scrape.retrieveDataRLTracker(gamertag=gamertag,platform=platform)\n\tif data is not None:\n\t\tpprint(data)\n", "description": null, "category": "webscraping", "imports": ["import argparse", "import requests", "import json", "import csv", "\tfrom pprint import pprint # pprint is cool"]}, {"term": "def", "name": "manyRun", "data": "def manyRun(playerCsv):\n\tscrape = Webscrape()\n\twith open(playerCsv, newline='', mode='r+') as csvfile:\n\t\treader = csv.DictReader(csvfile)\n\t\twith open('output.csv',  mode='r+', newline='') as outputfile:\n\t\t\twriter = csv.DictWriter(outputfile, fieldnames=reader.fieldnames)\n\t\t\twriter.writeheader()\n\t\t\tfor row in reader:\n\t\t\t\tdata = scrape.retrieveDataRLTrackerFromURL(url=row['Link to Stats'])\n\t\t\t\tif 'Ranked Duel 1v1' in data:\n\t\t\t\t\trow['1\\'s Icon']\t\t   = data['Ranked Duel 1v1']['iconUrl']\n\t\t\t\t\trow['1\\'s Division']\t   = data['Ranked Duel 1v1']['division']\n\t\t\t\t\trow['1\\'s MMR']\t\t\t= data['Ranked Duel 1v1']['mmr']\n\t\t\t\t\trow['1\\'s Rank']\t\t   = data['Ranked Duel 1v1']['rank']\n\n\t\t\t\tif 'Ranked Doubles 2v2' in data:\n\t\t\t\t\trow['2\\'s Icon']\t\t   = data['Ranked Doubles 2v2']['iconUrl']\n\t\t\t\t\trow['2\\'s Division']\t   = data['Ranked Doubles 2v2']['division']\n\t\t\t\t\trow['2\\'s MMR']\t\t\t= data['Ranked Doubles 2v2']['mmr']\n\t\t\t\t\trow['2\\'s Rank']\t\t   = data['Ranked Doubles 2v2']['rank']\n\n\t\t\t\tif 'Ranked Standard 3v3' in data:\n\t\t\t\t\trow['3\\'s Icon']\t\t   = data['Ranked Standard 3v3']['iconUrl']\n\t\t\t\t\trow['3\\'s Division']\t   = data['Ranked Standard 3v3']['division']\n\t\t\t\t\trow['3\\'s MMR']\t\t\t= data['Ranked Standard 3v3']['mmr']\n\t\t\t\t\trow['3\\'s Rank']\t\t   = data['Ranked Standard 3v3']['rank']\n\n\t\t\t\tif 'Un-Ranked' in data:\n\t\t\t\t\trow['Casual Icon']\t\t = data['Un-Ranked']['iconUrl']\n\t\t\t\t\trow['Casual Division']\t = data['Un-Ranked']['division']\n\t\t\t\t\trow['Casual MMR']\t\t  = data['Un-Ranked']['mmr']\n\t\t\t\t\trow['Casual Rank']\t\t = data['Un-Ranked']['rank']\n\n\t\t\t\tif 'Tournament Matches' in data:\n\t\t\t\t\trow['Tournament Icon']\t = data['Tournament Matches']['iconUrl']\n\t\t\t\t\trow['Tournament Division'] = data['Tournament Matches']['division']\n\t\t\t\t\trow['Tournament MMR']\t  = data['Tournament Matches']['mmr']\n\t\t\t\t\trow['Tournament Rank']\t = data['Tournament Matches']['rank']\n\n\t\t\t\tif 'Hoops' in data:\n\t\t\t\t\trow['Hoops Icon']\t\t  = data['Hoops']['iconUrl']\n\t\t\t\t\trow['Hoops Division']\t  = data['Hoops']['division']\n\t\t\t\t\trow['Hoops MMR']\t\t   = data['Hoops']['mmr']\n\t\t\t\t\trow['Hoops Rank']\t\t  = data['Hoops']['rank']\n\n\t\t\t\tif 'Snowday' in data:\n\t\t\t\t\trow['Snowday Icon']\t\t= data['Snowday']['iconUrl']\n\t\t\t\t\trow['Snowday Division']\t= data['Snowday']['division']\n\t\t\t\t\trow['Snowday MMR']\t\t = data['Snowday']['mmr']\n\t\t\t\t\trow['Snowday Rank']\t\t= data['Snowday']['rank']\n\n\t\t\t\tif 'Dropshot' in data:\n\t\t\t\t\trow['Dropshot Icon']\t   = data['Dropshot']['iconUrl']\n\t\t\t\t\trow['Dropshot Division']   = data['Dropshot']['division']\n\t\t\t\t\trow['Dropshot MMR']\t\t= data['Dropshot']['mmr']\n\t\t\t\t\trow['Dropshot Rank']\t   = data['Dropshot']['rank']\n\n\t\t\t\tif 'Rumble' in data:\n\t\t\t\t\trow['Rumble Icon']\t\t = data['Rumble']['iconUrl']\n\t\t\t\t\trow['Rumble Division']\t = data['Rumble']['division']\n\t\t\t\t\trow['Rumble MMR']\t\t  = data['Rumble']['mmr']\n\t\t\t\t\trow['Rumble Rank']\t\t = data['Rumble']['rank']\n\t\t\t\twriter.writerow(row)\n\t\t\n\t\t\n\t\t\n\n", "description": null, "category": "webscraping", "imports": ["import argparse", "import requests", "import json", "import csv", "\tfrom pprint import pprint # pprint is cool"]}], [{"term": "def", "name": "get_movie", "data": "def get_movie(name, movie_dict):\r\n\tfor i in movie_dict:\r\n\t\tif(re.search(name, i) != None):\r\n\t\t\treturn movie_dict[i]\r\n\treturn 0\r\n\r\n", "description": null, "category": "webscraping", "imports": ["# importing the required modules\r", "from bs4 import BeautifulSoup\r", "import webbrowser\r", "import requests\r", "import re\r"]}], [], [{"term": "class", "name": "Config", "data": "class Config():\n\tAPP_NAME = 'FarmBuild'\n\n\t# Enable debug mode\n\tDEBUG = True\n\tALLOWED_HEADERS = ['Origin', 'Accept', 'Content-Type', 'X-Requested-With', 'X-CSRF-Token']\n\tALLOWED_ORIGINS = '*'\n\tALLOWED_METHODS = ['GET', 'HEAD', 'POST', 'OPTIONS', 'PUT', 'PATCH', 'DELETE']\n\n\t# TODO\n\t# This is where frontend should go, create a route for all UI files\n\t# Setup template folder for webpages\n\tTEMPLATE_FOLDER = \"templates\"\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask", "from flask import render_template, send_from_directory", "import webscrape"]}, {"term": "def", "name": "index_page", "data": "def index_page():\n\treturn render_template('index.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask", "from flask import render_template, send_from_directory", "import webscrape"]}, {"term": "def", "name": "scrape_page", "data": "def scrape_page():\n\treturn render_template('earthquake.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask", "from flask import render_template, send_from_directory", "import webscrape"]}, {"term": "def", "name": "run_script", "data": "def run_script():\n\tfile = open(r'real_time/webscrape.py', 'r').read()\n\treturn exec(file)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask", "from flask import render_template, send_from_directory", "import webscrape"]}, {"term": "def", "name": "send_js", "data": "def send_js(path):\n\tprint(path)\n\treturn send_from_directory('templates/js', path)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask", "from flask import render_template, send_from_directory", "import webscrape"]}, {"term": "def", "name": "send_css", "data": "def send_css(path):\n\treturn send_from_directory('templates/css', path)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask", "from flask import render_template, send_from_directory", "import webscrape"]}, {"term": "def", "name": "send_img", "data": "def send_img(path):\n\treturn send_from_directory('templates/img', path)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask", "from flask import render_template, send_from_directory", "import webscrape"]}, {"term": "def", "name": "send_json", "data": "def send_json(path):\n\treturn send_from_directory('templates/json', path)\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask", "from flask import render_template, send_from_directory", "import webscrape"]}, {"term": "def", "name": "assets", "data": "def assets(path):\n\treturn send_from_directory('templates/assets', path)\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask", "from flask import render_template, send_from_directory", "import webscrape"]}], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape :\n\n\tdef __init__(self) :\n\t\tpass\n\tpass\n\n\tdef doScrape(self) : \n\t\timport xlsxwriter\n\t\tworkbook = xlsxwriter.Workbook( \"\uc624\ub298\uc758 \uc99d\uad8c\uc2dc\uc138.xlsx\" )\n\t\tworksheet = workbook.add_worksheet()\n\t\trow = 0\n\t\tcol = 0 \n\n\t\tline = \"\\n\" + \"#\"*80 + \"\\n\"\n\n\t\t# import libraries\n\t\tfrom bs4 import BeautifulSoup\n\n\t\t# specify the url\n\t\thtml_url = \"http://vip.mk.co.kr/newSt/rate/item_all.php\"\n\n\t\t# query the website and return the html to the variable \u2018page\u2019\n\n\t\tlogging.info( \"Getting a html data from %s\" % html_url )\n\n\t\tfrom urllib.request import urlopen\n\t\thtml_page = urlopen( html_url )\n\t\thtml_src = html_page.read()\n\n\t\tlogging.info( \"Done. Getting a html data from %s\" % html_url )\n\n\t\t# parse the html using beautiful soup and store in variable `soup`\n\t\tsoup = BeautifulSoup( html_src, \"html.parser\" , from_encoding=\"euc-kr\" )\n\n\t\tprint( \"title = %s\" % soup.title ) \n\t\tprint( \"title.string = %s\" % soup.title.string )\n\t\ttable = soup.table\t\t\n\t\ttable_subs = table.find_all( \"table\" )\n\t\tprint( line )\t\t \n\t\tjisu = table_subs[ 1 ]\n\t\ttrs = jisu.find_all( \"tr\" )\n\t\tfor tr in trs :\n\t\t\ttds = tr.find_all( \"td\" )\n\t\t\tcol = 0 \n\t\t\tfor td in tds : \n\t\t\t\tprint( \"td = %s\" % td.string )\n\t\t\t\tworksheet.write(row, col, td.string )\n\t\t\t\tcol += 1\n\t\t\tpass\n\t\t\trow += 1\n\t\tpass \n\n\t\tjisu = table_subs[ 4 ]\n\t\ttrs = jisu.find_all( \"tr\" )\n\t\tfor tr in trs :\n\t\t\ttds = tr.find_all( \"td\" )\n\t\t\tcol = 0 \n\t\t\tfor td in tds : \n\t\t\t\tprint( \"td = %s\" % td.string )\n\t\t\t\tworksheet.write(row, col, td.string )\n\t\t\t\tcol += 1\n\t\t\tpass\n\t\t\trow += 1\n\t\tpass \n\t\tprint( line )\n\t\tworkbook.close() \n\tpass\n", "description": null, "category": "webscraping", "imports": ["import logging", "\t\timport xlsxwriter", "\t\t# import libraries", "\t\tfrom bs4 import BeautifulSoup", "\t\tfrom urllib.request import urlopen"]}], [{"term": "def", "name": "webscrape_presale", "data": "def webscrape_presale(parsepage):\n\t'''This function will take a page and scrape its data for sale lot information\n\tIt will also download sale shapefile and move it to directory of this script file\n\t'''\n\n\t#webscrape sale page - gathering lot serial numbers from html \n\tserialnums = parsepage.find_all(\"span\", \"lot-name\")\n\t#ist comprehension - appending text into serialnums\n\tserialnums = [i.text for i in serialnums]\n\t\n\t#storing all data from tag 'td's with clas name \"lot-legal\n\t#this html container/tag has 3 pieces of information\n\tlegalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n\t\n\t#initializing empty arrays\n\tacres = []\n\tdesc = []\n\tcounty = []\n\t\n\tfor item in legalinfo:\n\t\tcounty.append(item.contents[0].text)\n\t\tdesc.append(item.contents[1].text)\n\t\t#getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n\t\tacres.append(float(re.split(\":\\W\",item.contents[2])[1].replace(',','')))\n\n\t##getting shapefile from webpage  \n\t#clicking link of where shapefile is stored on sale page\t\n\tdriver.find_element_by_link_text(\"GIS Data WGS84\").click()\n\tdriver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n\t\n\t#getting list of filenames in downloads\n\ttry:\n\t\tdownloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir)\n\texcept:\n\t\tdownloaddir = \"C:/Users/mishaun/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir) \n\t\t\n\t#pattern will find downloaded file name of shapefile\n\tpattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n\t\n\t#searching through filenames in downlaods folder\n\tfinds = []\n\tfor file in downloads:\n\t\tif re.findall(pattern, file):\n\t\t\tfinds.append(file)\n\t\t\tbreak\n\t\t\n\t#moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n\tshutil.copy(downloaddir + finds[0], filepath)\n\t\n\ttry:\n\t\tos.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n\texcept:\n\t   pass\n   \n\treturn acres, desc, county, serialnums\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "fillexcel", "data": "def fillexcel():\n\t'''\n\tThis function will take scraped (global) values for lots and insert into sale spreadsheet \n\t'''\t\n\t\n\t#opening template sale notebook for modifications\n\t#preserving vba to keep formatting of workbook preserved - also keeping formulas \n\twb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n\tsheet = wb.active\n\t\n\t#updating sheet title to sale title\n\tsheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n\t\n\t#inserting values from webscrape into spreadsheet -8th row is where data rows begin\n\tfor i in range(0,len(serials)):\n\t\tsheet.cell(row = 8+i, column = 2, value = serials[i])\n\t\tsheet.cell(row = 8+i, column = 6, value = acres[i])\n\t\tsheet.cell(row = 8+i, column = 7, value = counties[i])\n\t\tsheet.cell(row = 8+i, column = 8, value = descriptions[i])\n\t\n\t#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n\tif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n\t\tprint(\"File already exists - Preventing overwrite of changes in excel file\")\n\telse:\n\t\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\t\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "fillwinnings", "data": "def fillwinnings():\n\t'''This function will take ourwinnings dictionary and add values to created spreadsheet\n\t'''\n\t\n\t#### insert our winnings into sale spreadsheet\n\twb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n\tsheet = wb.active\n\t\n\tfor i in range(0,len(ourwinnings)):\n\t\t#row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n\t\n\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "write_fillable_pdf", "data": "def write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n\t'''\n\tThis function will fill in pdf's forms based on a form pdf\n\t'''\n\t\n\ttemplate_pdf = pdfrw.PdfReader(input_pdf_path)\n\tannotations = template_pdf.pages[0][ANNOT_KEY]\n\tfor annotation in annotations:\n\t\tif annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n\t\t\tif annotation[ANNOT_FIELD_KEY]:\n\t\t\t\tkey = annotation[ANNOT_FIELD_KEY][1:-1]\n\t\t\t\tif key in data_dict.keys():\n\t\t\t\t\tannotation.update(\n\t\t\t\t\t\tpdfrw.PdfDict(V='{}'.format(data_dict[key]))\n\t\t\t\t\t)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "wonlotsDF", "data": "def wonlotsDF():\n\t'''\n\tThis function will create a dataframe of the won lots by reading information\n\tfrom completed sale note spreadsheet\n\tThe dataframe will then be used to parse pdf's \n\t'''\n\t#using openpyxl in order to read formulated values from spreadsheet\n\t# NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n\tdata_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n\tdataSheet = data_onlyWB.active\n\t\n\t#covnerting spreadsheet into dataframe\n\tdf = pd.DataFrame(dataSheet.values)\n\t\n\t#slicing the dataframe to get only relevant data\n\tdf = df.iloc[6:,1:25]\n\t#setting columns to first row of dataframe\n\tdf.columns = df.iloc[0]\n\t#dropping the repeated row with column names\n\tdf = df.drop(index =[6])\n\t\n\t#filtering data frame with values only won by magnum\n\twonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n\treturn wonlotsdf\n\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "createBidSheets", "data": "def createBidSheets():\n\t'''\n\tThis function will take a template pdf and generate pdf's based on wonlots dataframe\n\t'''\n\t#calling funciton wonlotsDF in ordre for bid sheets to be created \n\twonlotsdf = wonlotsDF()\n\n\ttemplatePDF = 'bidsheet template.pdf'\n\t\n\tfor i in range(0,len(wonlotsdf.index)):\n\t\n\t\tOutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n\t\t\n\t\tfields = {\n\t\t\t\t\"State\": stinitials,\n\t\t\t\t\"Date of Sale\": date,\n\t\t\t\t'Check Box for Oil and Gas' : \"x\",\n\t\t\t\t\"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n\t\t\t\t\"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n\t\t\t\t\"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n\t\t\t\t\"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n\t\t\t\t\"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n\t\t\t\t\"City\" : \"Corpus Christi\",\n\t\t\t\t\"State_2\": \"TX\",\n\t\t\t\t\"Zip Code\" : \"78401\"\n\t\t\t\t}\n\t\t\n\t\twrite_fillable_pdf(templatePDF, OutputPath, fields)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "openDI", "data": "def openDI():\n\t'''This function will open up DrillingInfo and log user in\n\t'''\n\t\n\tdriver = webdriver.Chrome(filepath + \"/chromedriver.exe\")\n\twait = WebDriverWait(driver, 20)\n\t\t\n\tdriver.get(\"https://app.drillinginfo.com/gallery/\")\n\tuserfield = driver.find_element_by_name(\"username\")\n\tpassfield = driver.find_element_by_name(\"password\")\n\t\n\tuserfield.click()\n\tuserfield.send_keys(\"mbhaktamgm\")\n\tpassfield.click()\n\tpassfield.send_keys(\"itheCwe\")\n\tpassfield.send_keys(Keys.RETURN)\n\t\n\tmyworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n\tmyworkspaces.click()\n\t\n\tdefault_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n\tdefault_workspace.click()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}], [{"term": "def", "name": "webscrape_presale", "data": "def webscrape_presale(parsepage):\n\t'''This function will take a page and scrape its data for sale lot information\n\tIt will also download sale shapefile and move it to directory of this script file\n\t'''\n\n\t#webscrape sale page - gathering lot serial numbers from html \n\tserialnums = parsepage.find_all(\"span\", \"lot-name\")\n\t#ist comprehension - appending text into serialnums\n\tserialnums = [i.text for i in serialnums]\n\t\n\t#storing all data from tag 'td's with clas name \"lot-legal\n\t#this html container/tag has 3 pieces of information\n\tlegalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n\t\n\t#initializing empty arrays\n\tacres = []\n\tdesc = []\n\tcounty = []\n\t\n\tfor item in legalinfo:\n\t\tcounty.append(item.contents[0].text)\n\t\tdesc.append(item.contents[1].text)\n\t\t#getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n\t\tacres.append(float(re.split(\":\\W\",item.contents[2].text)[1].replace(',','')))\n\n\t##getting shapefile from webpage  \n\t#clicking link of where shapefile is stored on sale page\t\n\tdriver.find_element_by_link_text(\"GIS Data WGS84\").click()\n\tdriver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n\t\n\t#getting list of filenames in downloads\n\ttry:\n\t\tdownloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir)\n\texcept:\n\t\tdownloaddir = \"C:/Users/mishaun/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir) \n\t\t\n\t#pattern will find downloaded file name of shapefile\n\tpattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n\t\n\t#searching through filenames in downlaods folder\n\tfinds = []\n\tfor file in downloads:\n\t\tif re.findall(pattern, file):\n\t\t\tfinds.append(file)\n\t\t\tbreak\n\t\t\n\t#moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n\tshutil.copy(downloaddir + finds[0], filepath)\n\t\n\ttry:\n\t\tos.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n\texcept:\n\t   pass\n   \n\treturn acres, desc, county, serialnums\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw", "import pandas as pd"]}, {"term": "def", "name": "fillexcel", "data": "def fillexcel():\n\t'''\n\tThis function will take scraped (global) values for lots and insert into sale spreadsheet \n\t'''\t\n\t\n\t#opening template sale notebook for modifications\n\t#preserving vba to keep formatting of workbook preserved - also keeping formulas \n\twb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n\tsheet = wb.active\n\t\n\t#updating sheet title to sale title\n\tsheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n\t\n\t#inserting values from webscrape into spreadsheet -8th row is where data rows begin\n\tfor i in range(0,len(serials)):\n\t\tsheet.cell(row = 8+i, column = 2, value = serials[i])\n\t\tsheet.cell(row = 8+i, column = 6, value = acres[i])\n\t\tsheet.cell(row = 8+i, column = 7, value = counties[i])\n\t\tsheet.cell(row = 8+i, column = 8, value = descriptions[i])\n\t\n\t#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n\tif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n\t\tprint(\"File already exists - Preventing overwrite of changes in excel file\")\n\telse:\n\t\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\t\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw", "import pandas as pd"]}, {"term": "def", "name": "fillwinnings", "data": "def fillwinnings():\n\t'''This function will take ourwinnings dictionary and add values to created spreadsheet\n\t'''\n\t\n\t#### insert our winnings into sale spreadsheet\n\twb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n\tsheet = wb.active\n\t\n\tfor i in range(0,len(ourwinnings)):\n\t\t#row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n\t\n\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw", "import pandas as pd"]}, {"term": "def", "name": "write_fillable_pdf", "data": "def write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n\t'''\n\tThis function will fill in pdf's forms based on a form pdf\n\t'''\n\t\n\ttemplate_pdf = pdfrw.PdfReader(input_pdf_path)\n\tannotations = template_pdf.pages[0][ANNOT_KEY]\n\tfor annotation in annotations:\n\t\tif annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n\t\t\tif annotation[ANNOT_FIELD_KEY]:\n\t\t\t\tkey = annotation[ANNOT_FIELD_KEY][1:-1]\n\t\t\t\tif key in data_dict.keys():\n\t\t\t\t\tannotation.update(\n\t\t\t\t\t\tpdfrw.PdfDict(V='{}'.format(data_dict[key]))\n\t\t\t\t\t)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw", "import pandas as pd"]}, {"term": "def", "name": "wonlotsDF", "data": "def wonlotsDF():\n\t'''\n\tThis function will create a dataframe of the won lots by reading information\n\tfrom completed sale note spreadsheet\n\tThe dataframe will then be used to parse pdf's \n\t'''\n\t#using openpyxl in order to read formulated values from spreadsheet\n\t# NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n\tdata_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n\tdataSheet = data_onlyWB.active\n\t\n\t#covnerting spreadsheet into dataframe\n\tdf = pd.DataFrame(dataSheet.values)\n\t\n\t#slicing the dataframe to get only relevant data\n\tdf = df.iloc[6:,1:25]\n\t#setting columns to first row of dataframe\n\tdf.columns = df.iloc[0]\n\t#dropping the repeated row with column names\n\tdf = df.drop(index =[6])\n\t\n\t#filtering data frame with values only won by magnum\n\twonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n\treturn wonlotsdf\n\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw", "import pandas as pd"]}, {"term": "def", "name": "createBidSheets", "data": "def createBidSheets():\n\t'''\n\tThis function will take a template pdf and generate pdf's based on wonlots dataframe\n\t'''\n\t#calling funciton wonlotsDF in ordre for bid sheets to be created \n\twonlotsdf = wonlotsDF()\n\n\ttemplatePDF = 'bidsheet template.pdf'\n\t\n\tfor i in range(0,len(wonlotsdf.index)):\n\t\n\t\tOutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n\t\t\n\t\tfields = {\n\t\t\t\t\"State\": stinitials,\n\t\t\t\t\"Date of Sale\": date,\n\t\t\t\t'Check Box for Oil and Gas' : \"x\",\n\t\t\t\t\"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n\t\t\t\t\"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n\t\t\t\t\"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n\t\t\t\t\"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n\t\t\t\t\"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n\t\t\t\t\"City\" : \"Corpus Christi\",\n\t\t\t\t\"State_2\": \"TX\",\n\t\t\t\t\"Zip Code\" : \"78401\"\n\t\t\t\t}\n\t\t\n\t\twrite_fillable_pdf(templatePDF, OutputPath, fields)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw", "import pandas as pd"]}, {"term": "def", "name": "openDI", "data": "def openDI():\n\t'''This function will open up DrillingInfo and log user in\n\t'''\n\t\n\tdriver = webdriver.Chrome(filepath + \"/chromedriver.exe\")\n\twait = WebDriverWait(driver, 20)\n\t\t\n\tdriver.get(\"https://app.drillinginfo.com/gallery/\")\n\tuserfield = driver.find_element_by_name(\"username\")\n\tpassfield = driver.find_element_by_name(\"password\")\n\t\n\tuserfield.click()\n\tuserfield.send_keys(\"mbhaktamgm\")\n\tpassfield.click()\n\tpassfield.send_keys(\"itheCwe\")\n\tpassfield.send_keys(Keys.RETURN)\n\t\n\tmyworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n\tmyworkspaces.click()\n\t\n\tdefault_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n\tdefault_workspace.click()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw", "import pandas as pd"]}], [{"term": "def", "name": "run", "data": "def run():\n\t#Load the YAML params first\n\n\tyaml_params = get_input_yaml(sys.argv[1])\n\n\n\tjobs = get_jobs_from_yaml_params(yaml_params)\n\toutput_dir, _ = os.path.split(os.path.abspath(sys.argv[1]))\n\n\tfor job in jobs:\n\t\tif job['job_type'] == TRAIN_SCRAPE_JOB:\n\t\t\tprint(f'Running train scrape job with name {job[\"job_name\"]}')\t  \n\t\t\tjob['output-working-dir'] = output_dir\n\t\t\tjob['input-dir'] = os.path.abspath(os.path.join(output_dir, job['input-dir']))\n\t\t\trun_training_object_detection_webscrape_loop(**job)\n\t\t\tgenerate_json_file(job)\n\t\t\n\t\telif job['job_type'] == ANNOTATE_JOB:\n\t\t\tjob['output-working-dir'] = output_dir\n\t\t\tjob['input-dir'] = os.path.abspath(os.path.join(output_dir, job['input-dir']))\n\t\t\tprint(f'Running annotate job with name {job[\"job_name\"]}')\n\t\t\trun_object_detection_annotation_loop(**job)\n\n\t\telif job['job_type'] == DOWNLOAD_JOB:\n\t\t\tif job['api-name'] in SUPPORTED_DOWNLOAD_APIS:\n\t\t\t\tjob['output-working-dir'] = output_dir\n\n\t\t\tprint(f'Running download job with name {job[\"job_name\"]} and api: {job[\"api-name\"]}')\t  \n\t\t\tdownload_dataset(job['api-name'], job)\n\n\t\telif job['job_type'] == EXPORT_JOB:\n\t\t\tjob['output-working-dir'] = output_dir\n\t\t\t\n\t\t\tprint(f'Exporting dataset now to ...')\n\t\t\texport_dataset(job)\n\t\telse:\n\t\t\tprint(f'Unrecognized job type - {job[\"job-type\"]} - for job {job[\"job_name\"]}')\n\t\t\n\n", "description": null, "category": "webscraping", "imports": ["import yaml", "import os", "import sys", "\tfrom config import BASE_DIR", "\tfrom flockfysh.config import BASE_DIR", "from utilities.pipelines.training_webscrape_loop import run_training_object_detection_webscrape_loop", "from utilities.pipelines.annotation_loop import run_object_detection_annotation_loop", "from utilities.parse_config.input_validator import get_jobs_from_yaml_params, get_input_yaml ", "from utilities.parse_config.job_config import EXPORT_JOB, TRAIN_SCRAPE_JOB, ANNOTATE_JOB, DOWNLOAD_JOB, SUPPORTED_DOWNLOAD_APIS", "from utilities.output_generation.misc_output import generate_json_file", "from utilities.dataset_setup.download_dataset import download_dataset", "from utilities.dataset_setup.download_dataset import export_dataset"]}], [{"term": "class", "name": "classModel:", "data": "class Model:\n\t'''\n\tThis class initializes the Spacy pipline for NER.  You can initialize it by calling\n\tmodel = model() then to pass a text you call model.ner(text).  To get a dataframe of\n\tN number of articles then you call model.get_ner_for_all(articles).\n\t'''\n\tdef __init__(self):\n\t\tself.model = spacy.load(\"en_core_web_sm\")\n\n\tdef ner(self, content):\n\t\treturn self.model(content)\n\n\tdef get_ner_for_all(self, article):\n\t\t''''\n\t\tThis function is used to obtain NER results for each content in the article\n\t\tand is place in a new dataframe.\n\t\t'''\n\t\tfinal_out = article.copy()\n\t\tfor index, row in final_out.iterrows():\n\t\t\tspacy_results = self.model(row['article content'])\n\t\t\tarticle_ner = get_unique_results(spacy_results)\n\t\t\tfinal_out.iloc[[index], [1]] = [article_ner]\n\t\treturn final_out\n", "description": null, "category": "webscraping", "imports": ["import spacy", "import json", "import argparse", "from ws_nbc import WebScrape"]}, {"term": "def", "name": "get_unique_results", "data": "def get_unique_results(model_output):\n\t'''\n\tThis function prepares a dictionary:\n\n\tarticle = {\n\t\t'NAME' : [...],\n\t\t'ORGANIZATION' : [...],\n\t\t'LOCATION' : [...]\n\t}\n\n\tIt then appends the desired entities from the NER model output into the dictionary.  The entities are,\n\tORG (company, organizations, etc.), PERSON (Name or person), and GPE (Location).\n\t'''\n\n\tarticle = {'NAME':[], 'ORGANIZATION':[], 'LOCATION':[]}\n\n\tfor word in model_output.ents:\n\t\tif word.label_ == 'PERSON' and (word.text not in article[\"NAME\"]):\n\t\t\tarticle[\"NAME\"].append(word.text)\n\t\telif word.label_ == 'ORG' and (word.text not in article[\"ORGANIZATION\"]):\n\t\t\tarticle[\"ORGANIZATION\"].append(word.text)\n\t\telif word.label_ == 'GPE' and (word.text not in article[\"LOCATION\"]):\n\t\t\tarticle[\"LOCATION\"].append(word.text)\n\treturn article\n", "description": null, "category": "webscraping", "imports": ["import spacy", "import json", "import argparse", "from ws_nbc import WebScrape"]}, {"term": "def", "name": "save_to_json", "data": "def save_to_json(results, path):\n\toutputDict = results.set_index('article link').to_dict()['article content']\n\n\twith open(path+'output.json', 'w') as fp:\n\t\tjson.dump(outputDict, fp,  indent=4)\n", "description": null, "category": "webscraping", "imports": ["import spacy", "import json", "import argparse", "from ws_nbc import WebScrape"]}, {"term": "def", "name": "save_to_csv", "data": "def save_to_csv(results, path):\n\tresults.set_index('article link').to_csv(path+'output.csv')\n", "description": null, "category": "webscraping", "imports": ["import spacy", "import json", "import argparse", "from ws_nbc import WebScrape"]}], [], [], [{"term": "def", "name": "main", "data": "def main():\n\tif len(sys.argv) != 2:\n\t\tprint('usage: python realestate.py zipcodes')\n\t\tsys.exit(1)\n\tzips = csvread(sys.argv[1])\n\tprint(zips)\n", "description": null, "category": "webscraping", "imports": ["import sys, os", "# import zipcodes", "from csvreader import csvread", "from scrapeweb import webscrape", "#import numpy as np"]}], [{"term": "def", "name": "main", "data": "def main():\n\tif len(sys.argv) != 2:\n\t\tprint('usage: python realestate.py zipcodes')\n\t\tsys.exit(1)\n\tzips = csvread(sys.argv[1])\n\tprint(zips)\n", "description": null, "category": "webscraping", "imports": ["import sys, os", "# import zipcodes", "from csvreader import csvread", "from webscraper2 import webscrape", "#import numpy as np"]}], [{"term": "def", "name": "index", "data": "def index():\r\n\tdata = mongo.db.webscrapes.find_one()\r\n\t# print(data)\r\n\treturn render_template('index.html',data = data)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, redirect\r", "import scrape_mars\r", "from flask_pymongo import PyMongo\r"]}, {"term": "def", "name": "scrape", "data": "def scrape():\r\n\twebscrapes_handle = mongo.db.webscrapes\r\n\twebscrape = scrape_mars.scrape()\r\n\twebscrapes_handle.update({}, webscrape, upsert=True)\r\n\treturn redirect(\"/\",code=302)\r\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, redirect\r", "import scrape_mars\r", "from flask_pymongo import PyMongo\r"]}], [{"term": "class", "name": "TestWebScrape", "data": "class TestWebScrape(DataSourceTest):\n\t'''\n\ttest getting rdfA based triples from Webpages\n\t'''\n\n\tdef testCrawlType(self):\n\t\t'''\n\t\ttest CrawlType isValid\n\t\t'''\n\t\tself.assertTrue(CrawlType.isValid(\"Event\"))\n\t\tself.assertFalse(CrawlType.isValid(\"Homepage\"))\n\n\tdef testWebScrape(self):\n\t\t'''\n\t\ttest getting rdfA encoded info from a webpage\n\t\t'''\n\t\tdebug=self.debug\n\t\turl=\"http://ceur-ws.org/Vol-2635/\"\n\t\tscrape=WebScrape(timeout=20 if self.inCI() else 3)\n\t\tscrapeDescr=[\n\t\t\t{'key':'acronym', 'tag':'span','attribute':'class', 'value':'CEURVOLACRONYM'},\n\t\t\t{'key':'title',   'tag':'span','attribute':'class', 'value':'CEURFULLTITLE'},\n\t\t\t{'key':'loctime', 'tag':'span','attribute':'class', 'value':'CEURLOCTIME'}\n\t\t]\n\t\tscrapedDict=scrape.parseWithScrapeDescription(url,scrapeDescr)\n\t\tif scrape.err:\n\t\t\tprint(scrape.err)\n\t\t\tprint(\"We might not be able to do anything about it\")\n\t\t\treturn\n\t\tif debug:\n\t\t\tprint(scrapedDict)\n\t\tself.assertEqual('DL4KG2020',scrapedDict[\"acronym\"])\n\t\tself.assertEqual('Heraklion, Greece, June 02, 2020',scrapedDict[\"loctime\"])\n\t\tself.assertEqual('Proceedings of the Workshop on Deep Learning for Knowledge Graphs (DL4KG2020)',scrapedDict[\"title\"])\n\t\tpass\n\n", "description": null, "category": "webscraping", "imports": ["import unittest", "from corpus.datasources.webscrape import WebScrape", "from corpus.datasources.wikicfpscrape import CrawlType", "from tests.datasourcetoolbox import DataSourceTest", "\t#import sys;sys.argv = ['', 'Test.testName']"]}], [{"term": "def", "name": "write_content", "data": "def write_content(examples):\n\t'''\n\tMethod: given a list of examples, annotates and writes examples into\n\t\ta file in dataturks formatting\n\tInput: list of examples - list of strings\n\t'''\n\n\tsearch = word\n\t#search = ac\n\n\tfilename = \"../twitter_data/acronyms.json\"\n\n\tfor e in examples:\n\t\t#Clean example \n\t\te = e.lower()\n\t\tstarts = [m.start() for m in re.finditer(search, e)]\n\n\t\t#Check to see starting points of word exists\n\t\tif not starts:\n\t\t\tcontinue\n\n\t\t#Set list of ends\n\t\tends = [i+len(search)-1 for i in starts]\n\t\t\n\n\t\t#Annotate examples into dictionaries\n\t\tlabels = [\"Slang\" for _ in starts]\n\t\tpoints = [{\"start\":s, \"end\":e, \"text\":ac} for s,e in zip(starts,ends)]\n\t\t\n\t\tannotation = []\n\t\tfor i in range(len(starts)):\n\t\t\tannotation.append({\n\t\t\t\t\"label\": [labels[i]],\n\t\t\t\t\"points\": [points[i]], \n\t\t\t})\n\n\t\td = {\n\t\t\t\"content\": e,\n\t\t\t\"annotation\":annotation,\n\t\t}\n\n\t\t#Output dictionaries as a json\n\t\twith open(filename, 'a') as outfile:\n\t\t\tjson_data = json.dumps(d)\n\t\t\toutfile.write(json_data)\n\t\t\toutfile.write('\\n')\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pickle", "import json", "from bs4 import BeautifulSoup", "import re"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\t'''\n\tMethod: webscrape all words, and examples from dictionary.com/e/slang\n\t'''\n\tglobal word\n\tnew_url = dict_url + \"ad-hoc\"\n\n\twhile (True):\n\t\tprint(word)\n\t\ttry:\n\t\t\t#request html from specified url \n\t\t\thtml = requests.get(new_url).text\n\t\t\tparsed_html = BeautifulSoup(html, 'html.parser')\n\n\t\t\t#parse out list of examples and clean punctuation\n\t\t\texamples = parsed_html.findAll('div', class_=\"examples__item__content\")\n\t\t\texamples = list(filter(None, [BeautifulSoup(str(e), 'html.parser').text.strip() for e in examples]))\n\t\t\texamples = [e.replace(\"\u201c\", \"\\\"\").replace(\"\u2019\", \"\\'\").replace(\"\u201d\", \"\\\"\").replace('\u2026', \"\") for e in examples]\n\n\t\t\t#write examples into a json file \n\t\t\twrite_content(examples)\n\n\t\t\t#find next link and set next url \n\t\t\tlinks = parsed_html.findAll('a', class_=\"next\")\n\t\t\tif len(links) == 0:\n\t\t\t\tbreak\n\t\t\tlink = str(links[0])\n\t\t\ts = link.find('href=\"') + 6\n\t\t\tlink = link[s:]\n\t\t\te = link.find('\">')\n\t\t\tlink = link[:e]\n\t\t\tnew_url = link\n\n\t\t\t#find next word and set word \n\t\t\tword = new_url.split('/')[-1].replace('-', ' ')\n\t\texcept:\n\t\t\tcontinue\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pickle", "import json", "from bs4 import BeautifulSoup", "import re"]}, {"term": "def", "name": "get_defs", "data": "def get_defs():\n\t'''\n\tMethod: webscrape all words and definitions from dictionary.com/e/slang\n\t'''\n\n\tglobal word\n\tdata = {\n\t\t'definition': [],\n\t}\n\tnew_url = dict_url + \"ad-hoc\"\n\n\twhile (True):\n\t\ttry:\n\t\t\t#request html from specified url \n\t\t\thtml = requests.get(new_url).text\n\t\t\tparsed_html = BeautifulSoup(html, 'html.parser')\n\t\t\tdefinition = parsed_html.findAll('div', class_=\"article-word__header__content__holder\")\n\n\t\t\t#find next link and set next url \n\t\t\tlinks = parsed_html.findAll('a', class_=\"next\")\n\t\t\tif len(links) == 0:\n\t\t\t\tbreak\n\t\t\tlink = str(links[0])\n\t\t\ts = link.find('href=\"') + 6\n\t\t\tlink = link[s:]\n\t\t\te = link.find('\">')\n\t\t\tlink = link[:e]\n\t\t\tnew_url = link\n\n\t\t\t#parse out definition of slang\n\t\t\tif len(definition) < 1:\n\t\t\t\tbreak\n\n\t\t\t#clean definition from html and save definition to dict\n\t\t\tresult = re.search('(.*)', str(definition[0]))\n\t\t\tfinal_def = result.group(1)\n\t\t\tfinal_def = re.sub(\"<[^>]*>\", \"\", final_def);\n\t\t\td = {\n\t\t\t\t'word': word,\n\t\t\t\t'def': final_def,\n\t\t\t}\n\t\t\tdata['definition'].append(d)\n\n\t\t\t#set next word\n\t\t\tword = new_url.split('/')[-1].replace('-', ' ')\n\t\t\t\n\t\texcept:\n\t\t\tcontinue\n\n\t#write definitions of slang to file \n\twith open(\"../twitter_data/definitions3.json\", 'a') as outfile:\n\t\t\tjson_data = json.dumps(data)\n\t\t\toutfile.write(json_data)\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pickle", "import json", "from bs4 import BeautifulSoup", "import re"]}, {"term": "def", "name": "webscrape2", "data": "def webscrape2():\n\t'''\n\tMethod: webscrape all acronyms, definitions, and examples from dictionary.com/e/acronyms\n\t'''\n\n\tglobal ac\n\tdata = {\n\t\t'definition': [],\n\t}\n\tnew_url = ac_url + ac\n\n\twhile (True):\n\t\tprint(ac)\n\t\ttry:\n\t\t\t#request html from specified url \n\t\t\thtml = requests.get(new_url).text\n\t\t\tparsed_html = BeautifulSoup(html, 'html.parser')\n\n\t\t\t#parse out list of examples and clean punctuation\n\t\t\texamples = parsed_html.findAll('div', class_=\"examples__item__content text\")\n\t\t\texamples = list(filter(None, [BeautifulSoup(str(e), 'html.parser').text.strip() for e in examples]))\n\t\t\texamples = [e.replace(\"\u201c\", \"\\\"\").replace(\"\u2019\", \"\\'\").replace(\"\u201d\", \"\\\"\").replace('\u2026', \"\") for e in examples]\n\t\t\t\n\t\t\t#write examples into a json file \n\t\t\twrite_content(examples)\n\n\t\t\t#parse out definition of acronym\n\t\t\tdefinition = parsed_html.findAll('div', class_=\"article-word__header__content__holder\")\n\t\t\t\n\t\t\t#find next link and set next url \n\t\t\tlinks = parsed_html.findAll('a', class_=\"next\")\n\t\t\tif len(links) == 0:\n\t\t\t\tbreak\n\t\t\tlink = str(links[0])\n\t\t\ts = link.find('href=\"') + 6\n\t\t\tlink = link[s:]\n\t\t\te = link.find('\">')\n\t\t\tlink = link[:e]\n\t\t\tnew_url = link\n\n\t\t\t#clean definition from html and save definition to dict\n\t\t\tif len(definition) < 1:\n\t\t\t\tbreak\n\t\t\tresult = re.search('(.*)', str(definition[0]))\n\t\t\tfinal_def = result.group(1)\n\t\t\tfinal_def = re.sub(\"<[^>]*>\", \"\", final_def);\n\t\t\td = {\n\t\t\t\t'word': ac,\n\t\t\t\t'def': final_def,\n\t\t\t}\n\t\t\tdata['definition'].append(d)\n\n\t\t\t#set next word\n\t\t\tac = new_url.split('/')[-1].replace('-', ' ')\n\n\t\texcept:\n\t\t\t#continue\n\t\t\tpass\n\n\t#write definitions of acronyms to file \n\twith open(\"../twitter_data/definitions3.json\", 'a') as outfile:\n\t\t\tjson_data = json.dumps(data)\n\t\t\toutfile.write(json_data)\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pickle", "import json", "from bs4 import BeautifulSoup", "import re"]}], [{"term": "class", "name": "classAutoSD:", "data": "class AutoSD:\n\n\tdef __init__(self):\n\t\tprint('initiating the bot...')\n\t\tglobal USER, PASSWORD, COOKIES_PATH, COOKIES, chrome, session_ID, URL, LOGS, LOGS_PATH\n\t\tUSER = 'antonoium'\n\t\tPASSWORD = 'madutzu93classiC'\n\t\tCOOKIES = \"C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\cookies.txt\"\n\t\tCHROMEDRIVER_PATH = \"C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\chromedriver.exe\"\n\t\tCOOKIES_PATH = Path(COOKIES)\n\t\tLOGS = \"C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\logs.txt\"\n\t\tLOGS_PATH = Path(LOGS)\n\n\t\t# Headless ( no GUI)\n\t\toptions = Options()\n\t\toptions.add_argument('--headless')\n\t\toptions.add_argument('--disable-gpu')\n\t\t#driver = webdriver.Chrome(options=options)\n\t\tchrome = webdriver.Chrome(executable_path=CHROMEDRIVER_PATH, options=options)  # This will open the Chrome window\n\t\tchrome.minimize_window()  # for the browser to start minimized\n\n\n\t\tsession_ID = chrome.session_id\n\t\tURL = chrome.command_executor._url\n\t\t#print(session_ID, URL)\n\n\t@staticmethod\n\tdef save_cookies(driver, location):\n\t\tpickle.dump(driver.get_cookies(), open(location, \"wb\"))\n\n\t@staticmethod\n\tdef load_cookies(driver, location, url='https://servicedesk.csiltd.co.uk/'):\n\t\tcookies = pickle.load(open(location, \"rb\"))\n\t\tdriver.delete_all_cookies()\n\t\turl = \"https://google.com\" if url is None else url\n\t\tdriver.get(url)\n\t\tfor cookie in cookies:\n\t\t\tdriver.add_cookie(cookie)\n\n\tdef obtain_sd_cookies(self):\n\n\t\tchrome.get('https://servicedesk.csiltd.co.uk/')  # Group Unassigned\n\n\t\tuser_field = \"//input[@id='username']\"\n\t\tpassword_field = \"//input[@id='password']\"\n\t\tlogin_button = \"//button[@id='loginSDPage']\"\n\n\t\tuser_element = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(user_field))\n\t\tuser_element.send_keys(USER)\n\n\t\tpassword_element = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(password_field))\n\t\tpassword_element.send_keys(PASSWORD)\n\n\t\tlogin_element = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(login_button))\n\t\tlogin_element.click()\n\n\t\tpprint.pprint(chrome.get_cookies())  # to visualise them in console\n\t\tself.save_cookies(chrome, COOKIES)\n\t\tchrome.quit()\n\n\t@staticmethod\n\tdef inject_javascript():\n\t\t# get script content from separate file\n\t\tmy_file = open(\"C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\highlight_script.js\", \"rt\")\n\t\tcontents = my_file.read()\n\t\tmy_file.close()\n\t\tchrome.execute_script(contents) # replace with selenium doing things\n\n\n\n\t# Only for logs.txt\n\t@staticmethod\n\tdef get_logs(ticket_id, agent, current_technician, subject, priority):\n\t\tx = datetime.datetime.now()\n\t\tformatted = \"%a, %d %b %Y, at %H:%M\"\n\t\tdate_time = x.strftime(formatted)\n\t\tprint(date_time)\n\n\t\ttext = date_time + \" - \" + \" [\" + priority + \"] \" + ticket_id + \" : \" + agent + \" , \" + current_technician + \" , \" + \"[\" + subject + \"]\"\n\n\t\tif LOGS_PATH.is_file():\n\n\t\t\t# Fix for : EOFError: Ran out of input\n\t\t\ttry:\n\t\t\t\tlogs = pickle.load(open(LOGS, \"rb\"))  # read from logs already\n\t\t\t\tsaved_text = logs + \"\\n\" + text\n\t\t\t\treturn saved_text\n\t\t\texcept EOFError:\n\t\t\t\tsaved_text = text\n\t\t\t\treturn saved_text\n\n\t\telse:\n\t\t\t# In case logs.txt doesn't exist in the location\n\t\t\tsaved_text = text\n\t\t\treturn saved_text\n\n\t@staticmethod\n\tdef save_logs(text, location):\n\t\tpickle.dump(text, open(location, \"wb\"))\n\n\n\t# Main functions\n\t@staticmethod\n\tdef open_assigned_to(nr): # This is recreation of my highlight_script.js as Selenium\n\t\trow = nr # starts with 0 being first, 4 means 5th row\n\t\tpopup_path = \"//table[@class='DialogBox']\"\n\t\tticket_id_path = \"//td[@id='RequestsView_r_\"+row+\"_5']//div\"\n\t\tsubject_path = \"//td[@id='RequestsView_r_\"+row+\"_6']//div/a\"\n\t\tassig_cell = \"//td[@id='RequestsView_r_\"+row+\"_7']//div/div\"\n\t\tpriority_path = \"// td[ @ id = 'RequestsView_r_\"+row+\"_8']//div/table/tbody/tr/td[last()]\"\n\t\tall_rows = \"//*[@id='RequestsView_TABLE']/tbody/tr[position() > 2]\"\n\n\n\t\tprint(assig_cell)\n\n\t\t# Cookies might have expired if you get an error here\n\t\tassigned_cell = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(assig_cell))\n\t\tagent = assigned_cell.text.strip()\n\n\n\t\t# Counting total tickets number\n\t\tall_rows_len = WebDriverWait(chrome, 10).until(lambda chrome:chrome.find_elements_by_xpath(all_rows))\n\t\tcount = len(all_rows_len)\n\t\tprint('---- Group Unassigned : ' + str(count) + ' tickets -----')\n\n\t\t#repeating_tickets and their count\n\n\n\t\tticket_id = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(ticket_id_path)).text.strip()\n\t\tsubject = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(subject_path)).text.strip()\n\t\tpriority = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(priority_path)).text.strip()\n\n\t\tif agent == 'Unassigned':\n\t\t\tassigned_cell.click()\n\t\t\tpopup = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(popup_path))\n\t\t\treturn popup, agent, ticket_id, subject, priority, row, 'Step1: Popup clicked:'\n\n\t\traise Exception(\"Fail - Not Unassigned\")\n\n\t@staticmethod\n\tdef get_technician(msg2):\n\t\ttechnician = \"//div[@id='s2id_selectTechnician']\"\n\t\ttech_name_xpath = \"//div[@id='s2id_selectTechnician']//a[@class='select2-choice']//span\"\n\n\t\tif msg2 is not None:\n\t\t\tcurrent_tech = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(tech_name_xpath)).text.strip() # these 2 lines represent the first span text of technician\n\t\t\t# technician_elem.click()\n\n\t\t\treturn current_tech, 'Step2: Current technician: '\n\n\t\traise Exception(\"Technician is NULL 394934834\")\n\n\t@staticmethod\n\tdef open_group(current_technician):\n\t\tif current_technician == \"NONE\":\n\t\t\tgroup_xpath = \"//div[@id='s2id_assignGroup']//a[@class='select2-choice']\"\n\t\t\topen_group = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(group_xpath)).click()\n\t\t\treturn open_group, 'Step3: Group list opened'\n\n\t\traise Exception(\"Fail - Current technician is not NONE\")\n\n\t@staticmethod\n\tdef select_service_desk(msg):\n\t\tif msg == \"Step3: Group list opened\":\n\t\t\tgroups_ul_xpath = \"//div[@id='select2-drop']//ul//li\"\n\t\t\tgroups = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_elements_by_xpath(groups_ul_xpath))\n\t\t\t#return groups[5].text 'Csi Facilities'\n\n\t\t\tfor group in groups:\n\t\t\t\tif group.text == \"Service Desk\":\n\t\t\t\t\tgroup.click()\n\t\t\t\t\treturn 'Step4: Chosen Service Desk'\n\n\t\traise Exception(\"Fail - groups list was not opened\")\n\n\n\t#def GROUP_UNASSIGNED(self):\n\n\tdef main(self, nr):\n\t\ttry:\n\t\t\tif not COOKIES_PATH.is_file():\n\t\t\t\tprint('Cookies.txt not found, obtaining cookies.')\n\t\t\t\tself.obtain_sd_cookies()  # If cookies.txt not to be found, log in and create em\n\n\t\t\t\t# Otherwise skip logging and use cookies\n\t\t\tself.load_cookies(chrome, COOKIES)\n\t\t\tchrome.get(\n\t\t\t\t'https://servicedesk.csiltd.co.uk/WOListView.do?requestViewChanged=true&viewName=38020_MyView&globalViewName=All_Requests')\n\n\t\t\t# Main execution chain\n\t\t\tpopup, agent, ticket_id, subject, priority, row, msg1 = self.open_assigned_to(nr)\n\t\t\tprint(msg1 + \" \" + priority + \" \" + ticket_id + \" - \" + agent + \" , \" + \"[\" + subject + \"]\")  # \"Opened AssignedTo popup for:\"\n\n\t\t\tcurrent_technician, msg2 = self.get_technician(msg1)\n\t\t\tprint(msg2 + \" \" + current_technician)  # 'Current technician: '\n\n\t\t\tgroup, msg3 = self.open_group(current_technician)\n\t\t\tprint(msg3)\n\n\t\t\tmsg4 = self.select_service_desk(msg3)\n\t\t\tprint(msg4)\n\n\t\t\ttxt = self.get_logs(ticket_id, agent, current_technician, subject, priority)\n\t\t\tself.save_logs(txt, LOGS)\n\n\t\t\t#self.inject_javascript()\n\n\t\t\treturn \"[\" + row + \"]\" + priority + \" \" + ticket_id + \" - \" + agent + \" , \" + \"[\" + subject + \"]\"\n\n\t\texcept Exception as e:\n\t\t\tlogging.error(traceback.format_exc())\n\t\t\t# Logs the error appropriately.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\t# Get Js Code from other file as string for selenium browser execute\n\t\t# myfile = open(\"lorem.txt\", \"rt\") # open lorem.txt for reading text\n\t\t# contents = myfile.read()\t\t # read the entire file into a string\n\t\t# myfile.close()\t\t\t\t   # close the file\n\t\t# print(contents)\t\t\t\t  # print contents\n\n\t#def repeated(self):\n", "description": null, "category": "webscraping", "imports": ["import pickle", "import pprint", "from selenium import webdriver", "from selenium.webdriver.support.wait import WebDriverWait", "from pathlib import Path", "import traceback", "import logging", "from selenium.webdriver.chrome.options import Options # headless", "import datetime"]}], [], [{"term": "def", "name": "render_docs", "data": "def render_docs():\n\treturn render_template_string(docs_html())\n\n", "description": null, "category": "webscraping", "imports": ["# Import Libraries (heroku requires a . before local imports)", "from flask import Flask, jsonify, render_template_string", "from .scrape import webscrape_single_section, webscrape_all_sections", "from .render import docs_html", "from .db import *"]}, {"term": "def", "name": "single_course", "data": "def single_course(term, course, section):\n\t# Formatting\n\tterm = term.lower()\n\tcourse = course.lower()\n\turl = f\"{course}.{section}.{term}\"\n\n\t# Scrape coursebook\n\tclass_info = webscrape_single_section(url)\n\n\t# Send response\n\treturn jsonify({'data': class_info})\n", "description": null, "category": "webscraping", "imports": ["# Import Libraries (heroku requires a . before local imports)", "from flask import Flask, jsonify, render_template_string", "from .scrape import webscrape_single_section, webscrape_all_sections", "from .render import docs_html", "from .db import *"]}, {"term": "def", "name": "all_courses", "data": "def all_courses(course):\n\tcourse = course.lower()\n\tcourse_list = webscrape_all_sections(course)\n\treturn jsonify({\"data\": course_list})\n\n", "description": null, "category": "webscraping", "imports": ["# Import Libraries (heroku requires a . before local imports)", "from flask import Flask, jsonify, render_template_string", "from .scrape import webscrape_single_section, webscrape_all_sections", "from .render import docs_html", "from .db import *"]}, {"term": "def", "name": "single_course_grade", "data": "def single_course_grade(term, course, section):\n\tgrade_data = get_single_course_grade(term, course, section)\n\treturn jsonify({\"data\": grade_data})\n\n", "description": null, "category": "webscraping", "imports": ["# Import Libraries (heroku requires a . before local imports)", "from flask import Flask, jsonify, render_template_string", "from .scrape import webscrape_single_section, webscrape_all_sections", "from .render import docs_html", "from .db import *"]}, {"term": "def", "name": "all_course_grades", "data": "def all_course_grades(term, course):\n\tgrade_data = get_all_course_grades(term, course)\n\treturn jsonify({\"data\": grade_data})\n\n", "description": null, "category": "webscraping", "imports": ["# Import Libraries (heroku requires a . before local imports)", "from flask import Flask, jsonify, render_template_string", "from .scrape import webscrape_single_section, webscrape_all_sections", "from .render import docs_html", "from .db import *"]}, {"term": "def", "name": "get_prof_data", "data": "def get_prof_data(name):\n\tprof_data = fetch_prof(name)\n\treturn jsonify({\"data\": prof_data})\n\n", "description": null, "category": "webscraping", "imports": ["# Import Libraries (heroku requires a . before local imports)", "from flask import Flask, jsonify, render_template_string", "from .scrape import webscrape_single_section, webscrape_all_sections", "from .render import docs_html", "from .db import *"]}], [{"term": "def", "name": "generateTermsFromFiles", "data": "def generateTermsFromFiles(basedir):\n\tbasedir = basedir + \"/static\"\n\tmultFiles = []\n\tuniqueTerms = dict()  # Declare Empty Dict untuk nyimpen semua unique terms\n\tfullMatrix = []  # array of dict\n\tfileNames = [\"skip this\"]\n\tfor filenames in os.listdir(basedir):\n\t\tif filenames.endswith(\".html\"):\n\t\t\tfileNames.append(filenames)\n\t\t\tjoinpath = os.path.join(basedir, filenames)\n\t\t\tfile = open(joinpath).read()\n\t\t\tdocs = convert(Cleaningkata(file))\n\t\t\tdocs = docs.split()\n\t\t\tdocs = Countwords(docs)\n\t\t\ttermDocs = docs.copy()\n\t\t\ttermDocs = {x: 0 for x in termDocs}  # Dictionary comprehension\n\t\t\tuniqueTerms.update(termDocs)\n\t\t\tmultFiles.append(docs)\n\t\telse:\n\t\t\tcontinue\n\treturn generateMatrixFromTerms(fullMatrix, uniqueTerms, multFiles, fileNames)\n\n", "description": null, "category": "webscraping", "imports": ["import os", "from filtering import Cleaningkata, Cleaningquery, convert, Countwords", "from webscrape import webscrape"]}, {"term": "def", "name": "generateMatrixFromTerms", "data": "def generateMatrixFromTerms(fullMatrix, uniqueTerms, multFiles, fileNames):\n\telem = dict()\n\tfor keys in uniqueTerms:\n\t\telem[keys] = 0\n\tfullMatrix.append(elem)\n\tfor docs in multFiles:\n\t\telem = dict()\n\t\tfor keys in uniqueTerms:\n\t\t\tif keys in docs:\n\t\t\t\telem[keys] = docs[keys]\n\t\t\telse:  # keys not in docs\n\t\t\t\telem[keys] = 0\n\t\tfullMatrix.append(elem)\n\n\treturn [uniqueTerms, fullMatrix, fileNames]\n\n", "description": null, "category": "webscraping", "imports": ["import os", "from filtering import Cleaningkata, Cleaningquery, convert, Countwords", "from webscrape import webscrape"]}, {"term": "def", "name": "generateTermsFromWebscrap", "data": "def generateTermsFromWebscrap():\n\tfullMatrix = []\n\tsemiFullMatrix = []\n\tuniqueTerms = dict()\n\twebs = ['https://www.worldoftales.com/fairy_tales/Hans_Christian_Andersen/Andersen_fairy_tale_47.html#gsc.tab=0',\n\t\t\t'https://www.worldoftales.com/fairy_tales/Brothers_Grimm/THE%20GOOSE-GIRL.html#gsc.tab=0', 'https://www.worldoftales.com/European_folktales/English_folktale_116.html#gsc.tab=0']\n\tfor web in webs:\n\t\tdocs = webscrape(web)\n\t\ttermDocs = docs.copy()\n\t\ttermDocs = {x: 0 for x in termDocs}\n\t\tuniqueTerms.update(termDocs)\n\t\tsemiFullMatrix.append(docs)\n\treturn generateMatrixFromWebTerms(uniqueTerms, fullMatrix, semiFullMatrix, webs)\n", "description": null, "category": "webscraping", "imports": ["import os", "from filtering import Cleaningkata, Cleaningquery, convert, Countwords", "from webscrape import webscrape"]}, {"term": "def", "name": "generateMatrixFromWebTerms", "data": "def generateMatrixFromWebTerms(uniqueTerms, fullMatrix, semiFullMatrix, webs):\n\telem = dict()\n\tfor keys in uniqueTerms:\n\t\telem[keys] = 0\n\tfullMatrix.append(elem)\n\tfor docs in semiFullMatrix:\n\t\telem = dict()\n\t\tfor keys in uniqueTerms:\n\t\t\tif keys in docs:\n\t\t\t\telem[keys] = docs[keys]\n\t\t\telse:\n\t\t\t\telem[keys] = 0\n\t\tfullMatrix.append(elem)\n\treturn [uniqueTerms, fullMatrix, webs]\n\n", "description": null, "category": "webscraping", "imports": ["import os", "from filtering import Cleaningkata, Cleaningquery, convert, Countwords", "from webscrape import webscrape"]}, {"term": "def", "name": "generateQueryVector", "data": "def generateQueryVector(query):\n\tdocs = convert(Cleaningquery(query))\n\tdocs = docs.split()\n\treturn Countwords(docs)\n", "description": null, "category": "webscraping", "imports": ["import os", "from filtering import Cleaningkata, Cleaningquery, convert, Countwords", "from webscrape import webscrape"]}, {"term": "def", "name": "updateTerms", "data": "def updateTerms(fullMatrix, queryVector):\n\tnewFullMatrix = []\n\tfor docsDict in fullMatrix:\n\t\ttempDocsDict = docsDict.copy()\n\t\tdocsDict = {x: 0 for x in queryVector}\n\t\tdocsDict.update(tempDocsDict)\n\t\tnewFullMatrix.append(docsDict)\n\treturn newFullMatrix  # terms newFullMatrix sudah lengkap bersama query\n", "description": null, "category": "webscraping", "imports": ["import os", "from filtering import Cleaningkata, Cleaningquery, convert, Countwords", "from webscrape import webscrape"]}], [{"term": "def", "name": "home", "data": "def home(request):\n\tcadastros = Cadastro.objects.all()\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render, redirect", "from rest_framework import viewsets", "from .models import Cadastro", "from .serializer import CadastroSerializer", "from bs4 import BeautifulSoup", "import requests", "from .webscrapping import webscrape"]}, {"term": "class", "name": "CadastroViewSet", "data": "class CadastroViewSet(viewsets.ModelViewSet):\n\tqueryset = Cadastro.objects.all()\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render, redirect", "from rest_framework import viewsets", "from .models import Cadastro", "from .serializer import CadastroSerializer", "from bs4 import BeautifulSoup", "import requests", "from .webscrapping import webscrape"]}, {"term": "class", "name": "slista_itens", "data": "#class lista_itens():\n\t#cadastro = Cadastro.objects,all()\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render, redirect", "from rest_framework import viewsets", "from .models import Cadastro", "from .serializer import CadastroSerializer", "from bs4 import BeautifulSoup", "import requests", "from .webscrapping import webscrape"]}, {"term": "def", "name": "add", "data": "def add(request):\n\tpage = requests.get('https://nerdstore.com.br/categoria/especiais/game-of-thrones/')\n\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t# Se odownloadfor realizado \u00e9 retornado com status 200\n\t# page.status_code\n\t# exibir conteudo da pagina\n\t# page.content\n\t# definicao da div para busca de itens, div que contem os itens desejados.\n\titens = soup.find_all('li', class_='product')\n\t# teste para verificar se est\u00e1 trazendo os itens\n\t# print(len(itens))\n\t# for para percorrer a div que cont\u00e9m os itens e capturar os itensdesejados\n\tfor i in itens:\n\t\t# buscar pela class\n\t\tnome = i.find('h2', class_='woocommerce-loop-product__title').text\n\t\tlink = i.find('a', class_='woocommerce-LoopProduct-link')['href']\n\t\tpreco = i.find('span', class_='woocommerce-Price-amount amount').text\n\t\tcad = Cadastro()\n\t\tcad.nome = nome\n\t\tcad.link = link\n\t\tcad.preco = preco\n\t\tcad.save()\n\t#adiciona = CadastroSerializer(request.webscrape() or None)\n\t#adiciona.save()\n\tcadastros = Cadastro.objects.all()\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render, redirect", "from rest_framework import viewsets", "from .models import Cadastro", "from .serializer import CadastroSerializer", "from bs4 import BeautifulSoup", "import requests", "from .webscrapping import webscrape"]}], [{"term": "def", "name": "connect", "data": "def connect():\n\tmindb = mysql.connector.connect(\n\t\thost=sql_host,\n\t\tuser=sql_user,\n\t\tpassword=sql_pass\n\t)\n\n\tmincursor = mindb.cursor()\n\n\tsql = f\"USE {db_name}\"\n\n\tmincursor.execute(sql)\n\tmindb.commit()\n\tmincursor.close()\n\t#print(\"Connected til database...\")\n\treturn mindb\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "makeCursor", "data": "def makeCursor(mindb):\n\tmincursor = mindb.cursor()\n\treturn mincursor\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "resetTable", "data": "def resetTable():\n\tmindb = connect()\n\tmincursor = makeCursor(mindb)\n\tsql = f\"DROP TABLE {db_table}\"\n\tmincursor.execute(sql)\n\n\tsql = f\"\"\"CREATE TABLE `{db_name}`.`{db_table}` (\n\t  `idlektier` INT NOT NULL AUTO_INCREMENT,\n\t  `titel` VARCHAR(45) NOT NULL,\n\t  `frist` DATETIME NOT NULL,\n\t  `elevtid` VARCHAR(45),\n\t  `slettet` BOOL,\n\t  PRIMARY KEY (`idlektier`));\"\"\"\n\tmincursor.execute(sql)\n\n\tprint(\"Tabel resat!\")\n\tmincursor.close()\n\tmindb.close()\n", "description": "CREATE TABLE `{db_name}`.`{db_table}` (\n\t  `idlektier` INT NOT NULL AUTO_INCREMENT,\n\t  `titel` VARCHAR(45) NOT NULL,\n\t  `frist` DATETIME NOT NULL,\n\t  `elevtid` VARCHAR(45),\n\t  `slettet` BOOL,\n\t  PRIMARY KEY (`idlektier`));", "category": "webscraping", "imports": ["import mysql.connector", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "uploadToTable", "data": "def uploadToTable():\n\tnames, frister, elevtid = indsaml(webScrape(lectio_base + \"login.aspx\", lectio_base + lectio_opgaver, lectio_user, lectio_pass))\n\tif len(os.listdir(\"html/\")) >= 4:\n\t\tos.remove(\"html/\" + sorted(os.listdir(\"html/\"))[0])\n\n\tmindb = connect()\n\tmincursor = makeCursor(mindb)\n\tfor i in range(len(names)):\n\t\tsql = f\"INSERT INTO {db_table} (titel, frist, elevtid, slettet) VALUES (%s, %s, %s, False)\"\n\t\tval = (names[i], frister[i], elevtid[i])\n\t\n\t\t#print(val)\n\t\tmincursor.execute(sql, val)\n\t\t\n\t\tmindb.commit()\n\tprint(\"Data uploadet!\")\n\tmincursor.close()\n\tmindb.close()\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "showNext", "data": "def showNext(limitval):\n\tmindb = connect()\n\tmincursor = makeCursor(mindb)\n\tsql = f\"SELECT titel, frist, elevtid FROM {db_table} WHERE frist > curtime() AND slettet = 0 ORDER BY frist LIMIT {limitval}\"\n\tmincursor.execute(sql)\n\tresultat = mincursor.fetchall()\n\n\n\tresultater = []\n\n\tfor x in resultat:\n\t\tresultater.append(x)\n\t\n\t#print(\"Lektier optalt...\")\n\tmincursor.close()\n\tmindb.close()\n\t#print(list(resultater[0]))\n\t#print(resultater)\n\treturn resultater\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "deleteOld", "data": "def deleteOld():\n\tmindb = connect()\n\tmincursor = makeCursor(mindb)\n\t\n\t# Fjerner gamle lektier\n\tsql = f\"UPDATE {db_table} SET slettet = True WHERE frist < curtime() AND slettet = False\"\n\t#sql = f\"DELETE FROM {db_table} WHERE frist < curtime()\"\n\t\n\tmincursor.execute(sql)\n\tmindb.commit()\n\n\t# Fjerner duplicates -- Beholder den lektier med lavest ID\n\tsql = f\"DELETE t1 FROM {db_table} t1 INNER JOIN {db_table} t2 WHERE t1.idlektier > t2.idlektier AND t1.titel = t2.titel;\"\n\tmincursor.execute(sql)\n\tmindb.commit()\n\n\tprint(\"Gamle lektier fjernet!\")\n\tmincursor.close()\n\tmindb.close()\n\n\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}], [{"term": "def", "name": "connect", "data": "def connect():\n\tmindb = mariadb.connect(\n\t\thost=sql_host,\n\t\tuser=sql_user,\n\t\tpassword=sql_pass,\n\t\tdatabase=db_name\n\t)\n\n\tmincursor = mindb.cursor()\n\tmincursor.close()\n\t#print(\"Connected til database...\")\n\treturn mindb\n", "description": null, "category": "webscraping", "imports": ["import mariadb", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "makeCursor", "data": "def makeCursor(mindb):\n\tmincursor = mindb.cursor()\n\treturn mincursor\n", "description": null, "category": "webscraping", "imports": ["import mariadb", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "resetTable", "data": "def resetTable():\n\tmindb = connect()\n\tmincursor = makeCursor(mindb)\n\tsql = f\"DROP TABLE {db_table}\"\n\tmincursor.execute(sql)\n\n\tsql = f\"\"\"CREATE TABLE `{db_name}`.`{db_table}` (\n\t  `idlektier` INT NOT NULL AUTO_INCREMENT,\n\t  `titel` VARCHAR(45) NOT NULL,\n\t  `frist` DATETIME NOT NULL,\n\t  `elevtid` VARCHAR(45),\n\t  `slettet` BOOL,\n\t  PRIMARY KEY (`idlektier`));\"\"\"\n\tmincursor.execute(sql)\n\n\tprint(\"Tabel resat!\")\n\tmincursor.close()\n\tmindb.close()\n", "description": "CREATE TABLE `{db_name}`.`{db_table}` (\n\t  `idlektier` INT NOT NULL AUTO_INCREMENT,\n\t  `titel` VARCHAR(45) NOT NULL,\n\t  `frist` DATETIME NOT NULL,\n\t  `elevtid` VARCHAR(45),\n\t  `slettet` BOOL,\n\t  PRIMARY KEY (`idlektier`));", "category": "webscraping", "imports": ["import mariadb", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "uploadToTable", "data": "def uploadToTable():\n\tnames, frister, elevtid = indsaml(webScrape(lectio_base + \"login.aspx\", lectio_base + lectio_opgaver, lectio_user, lectio_pass))\n\tif len(os.listdir(\"html/\")) >= 4:\n\t\tos.remove(\"html/\" + sorted(os.listdir(\"html/\"))[0])\n\n\tmindb = connect()\n\tmincursor = makeCursor(mindb)\n\tfor i in range(len(names)):\n\t\tsql = f\"INSERT INTO {db_table} (titel, frist, elevtid, slettet) VALUES (%s, %s, %s, False)\"\n\t\tval = (names[i], frister[i], elevtid[i])\n\t\n\t\t#print(val)\n\t\tmincursor.execute(sql, val)\n\t\t\n\t\tmindb.commit()\n\tprint(\"Data uploadet!\")\n\tmincursor.close()\n\tmindb.close()\n", "description": null, "category": "webscraping", "imports": ["import mariadb", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "showNext", "data": "def showNext(limitval):\n\tmindb = connect()\n\tmincursor = makeCursor(mindb)\n\tsql = f\"SELECT titel, frist, elevtid FROM {db_table} WHERE frist > curtime() AND slettet = 0 ORDER BY frist LIMIT {limitval}\"\n\tmincursor.execute(sql)\n\tresultat = mincursor.fetchall()\n\n\n\tresultater = []\n\n\tfor x in resultat:\n\t\tresultater.append(x)\n\t\n\t#print(\"Lektier optalt...\")\n\tmincursor.close()\n\tmindb.close()\n\t#print(list(resultater[0]))\n\t#print(resultater)\n\treturn resultater\n", "description": null, "category": "webscraping", "imports": ["import mariadb", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}, {"term": "def", "name": "deleteOld", "data": "def deleteOld():\n\tmindb = connect()\n\tmincursor = makeCursor(mindb)\n\t\n\t# Fjerner gamle lektier\n\tsql = f\"UPDATE {db_table} SET slettet = True WHERE frist < curtime() AND slettet = False\"\n\t#sql = f\"DELETE FROM {db_table} WHERE frist < curtime()\"\n\t\n\tmincursor.execute(sql)\n\tmindb.commit()\n\n\t# Fjerner duplicates -- Beholder den lektier med lavest ID\n\tsql = f\"DELETE t1 FROM {db_table} t1 INNER JOIN {db_table} t2 WHERE t1.idlektier > t2.idlektier AND t1.titel = t2.titel;\"\n\tmincursor.execute(sql)\n\tmindb.commit()\n\n\tprint(\"Gamle lektier fjernet!\")\n\tmincursor.close()\n\tmindb.close()\n\n\n", "description": null, "category": "webscraping", "imports": ["import mariadb", "from data_indsamler import indsaml, webScrape", "from dotenv import load_dotenv", "import os"]}], [{"term": "def", "name": "make_celery", "data": "def make_celery(app):\n\tcelery = Celery(\n\t\tapp.import_name,\n\t\tbackend=app.config['CELERY_RESULT_BACKEND'],\n\t\tbroker=app.config['CELERY_BROKER_URL']\n\t)\n\tcelery.conf.update(app.config)\n\n\tclass ContextTask(celery.Task):\n\t\tdef __call__(self, *args, **kwargs):\n\t\t\twith app.app_context():\n\t\t\t\treturn self.run(*args, **kwargs)\n\n\tcelery.Task = ContextTask\n\n\treturn celery\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "create_app", "data": "def create_app():\n\tapp = Flask(__name__)\n\tapp.secret_key = 'need to set os env variable for value'\n\twith app.app_context():\n\t\tapp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite://///Users/jiaweitchea/desktop/fyp/webscrap/loreal_db.sqlite3'\n\t\t#app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite://///mnt/c/users/ryan/work_ryan/y4s1/fyp/webscrap-website/loreal_db.sqlite3'\n\t\tapp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\t\tapp.secret_key = os.urandom(24)\n\n\t\t#Celery configuration\n\t\tapp.config['CELERY_RESULT_BACKEND'] = 'redis://localhost:6379/0'\n\t\tapp.config['CELERY_BROKER_URL'] = 'redis://localhost:6379/0'\n\t\tapp.config['CELERY_CREATE_MISSING_QUEUES'] = True\n\n\t\tdb.init_app(app)\n\t\tmigrate.init_app(app,db)\n\t\tlogin_manager.init_app(app)\n\n\t\tapp.register_blueprint(auth_blueprint)\n\t\tapp.register_blueprint(model_blueprint)\n\treturn app\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "__init__", "data": "def __init__(self,name,price,last_scraped):\n\tself.name = name\n\tself.price = price\n\tself.last_scraped = last_scraped\n\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "configure_setting", "data": "def configure_setting(app):\n\twith app.app_context():\n\t\tfrom model import Setting\n\t\tfrom model import Directory\n\n\t\tset_obj = db.session.query(Setting).order_by(Setting.id.desc()).first()\n\t\tdir_obj = db.session.query(Directory).order_by(Directory.id.desc()).first()\n\n\t\trotate_proxy = set_obj.rotate_proxy\n\t\tfetch_proxies = set_obj.fetch_proxies\n\t\trotating_proxy_page_retry = set_obj.rotating_proxy_page_retry\n\t\tno_of_concurrent_request = set_obj.no_of_concurrent_request\n\t\tdownload_delay = set_obj.download_delay\n\t\tdownload_timeout = set_obj.download_timeout\n\t\tno_of_retry = set_obj.no_of_retry\n\t\ttracker_output = dir_obj.tracker_filepath\n\n\t\tconfigure_setting = {\n\t\t'RETRY_TIMES': no_of_retry,\n\t\t'CONCURRENT_REQUESTS': no_of_concurrent_request,\n\t\t'DOWNLOAD_DELAY': download_delay,\n\t\t'DOWNLOAD_TIMEOUT': download_timeout,\n\t\t'NUMBER_OF_PROXIES_TO_FETCH': fetch_proxies ,\n\t\t'ROTATING_PROXY_PAGE_RETRY_TIMES': rotating_proxy_page_retry,\n\t\t'ROTATED_PROXY_ENABLED': rotate_proxy,\n\t\t'tracker_output' : tracker_output\n\t\t}\n\n\treturn configure_setting\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "load_user", "data": "def load_user(user_id):\n\tfrom model import User\n\t# since the user_id is just the primary key of our user table, use it in the query for the user\n\treturn User.query.get(int(user_id))\n\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "check_non_empty_space_in_val", "data": "def check_non_empty_space_in_val(input):\n\tif input and not input.isspace():\n\t\treturn True\n\treturn False\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "query_reviews", "data": "def query_reviews():\n\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\tfrom PIL import Image\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\n\tproject = 'crafty-chiller-276910'\n\tcwd = os.getcwd()\n\t#change secret key path based on where you store it\n\tsecret_key_path = os.path.join(cwd,'credential_file.json')\n\n\t#initialize bq client\n\tclient = bigquery.Client.from_service_account_json(secret_key_path)\n\n\tquery = '''\n\t\t\tSELECT cleaned_text\n\t\t\tFROM `crafty-chiller-276910.cleaned_items.reviews`\n\t\t\t'''\n\n\tstart = time.time()\n\tquery_job = client.query(query)\n\tprint(\"Time taken to query:\",time.time() - start)\n\n\t#Convert query to list type\n\tresult_query = list(query_job.result())\n\n\treviews = \" \"\n\n\tstart = time.time()\n\n\t#Concatenate all reviews to string format\n\tfor i in range(len(result_query)):\n\t\treview = result_query[i][0]\n\t\tif review is not None:\n\t\t\treviews += review\n\n\t#Remove stop words to refine review text\n\tstopwords = ['en' ,'tous' ,'ca' ,'le' ,'also', 'im', 'like', 'wa', 'ha', 'tey','est','go','doe','give']\n\tpreprocessed_text = reviews.split()\n\tresultwords  = [word for word in preprocessed_text if word.lower() not  in stopwords and word.isnumeric() == False]\n\tresult = ' '.join(resultwords)\n\n\t#Generate Review wordcloud\n\talice_mask = np.array(Image.open(cwd +'/static/img/alice3.jpg'))\n\n\twordcloud = WordCloud(background_color='white',\n\t\t\t\t\t  mask=alice_mask,contour_width=1.5, contour_color='steelblue').generate(result)\n\n\timage_output_path = cwd +'/static/img/alice.jpg'\n\twordcloud.to_file(image_output_path)\n\tprint(\"Time taken to generate wordcloud:\",time.time() - start)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "query_reviews_contributors", "data": "def query_reviews_contributors():\n\tcwd = os.getcwd()\n\t#change secret key path based on where you store it\n\tsecret_key_path = os.path.join(cwd,'credential_file.json')\n\n\t#initialize bq client\n\tclient = bigquery.Client.from_service_account_json(secret_key_path)\n\n\tquery = '''\n\t\tSELECT\u00a0profile_name,count(*)\u00a0as\u00a0number_of_reviews\u00a0FROM\u00a0`crafty-chiller-276910.cleaned_items.reviews`\n\t\twhere\u00a0profile_name\u00a0!= \"Amazon Customer\" AND profile_name\u00a0!= \"Kindle Customer\"\n\t\tgroup\u00a0by\u00a0profile_name\n\t\torder\u00a0by\u00a0number_of_reviews\u00a0desc\n\t\tLimit 10\n\t\t\t'''\n\t# get df with query result\n\tdf = client.query(query).to_dataframe()\n\n\t#Write dataframe into JSON file\n\tdf.to_json(\"dashboard_data/top_review_contributors.json\", orient='records')\n\tprint(\"Successfully written 'top_review_contributors.json' file\")\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "query_review_numbers", "data": "def query_review_numbers():\n\tcwd = os.getcwd()\n\t#change secret key path based on where you store it\n\tsecret_key_path = os.path.join(cwd,'credential_file.json')\n\n\t#initialize bq client\n\tclient = bigquery.Client.from_service_account_json(secret_key_path)\n\t#open product mapping file\n\tfile_path = os.path.join(cwd,'dashboard_data/product_mapping.json')\n\n\tquery = '''\n\t\tSELECT\u00a0ASIN,count(*)\u00a0as\u00a0number_of_reviews\u00a0FROM\u00a0`crafty-chiller-276910.cleaned_items.reviews`\n\t\tgroup\u00a0by\u00a0ASIN\n\t\torder\u00a0by\u00a0number_of_reviews\u00a0desc\n\t\tLIMIT 10\n\t\t\t'''\n\n\t# get df with query result\n\tdf = client.query(query).to_dataframe()\n\n\t'''\n\t# replace ASIN with product names (for top few items)\n\twith open(file_path, \"r\") as file:\n\t\tmapping = json.load(file)\n\t\tfor value in df['ASIN']:\n\t\t\tif value in mapping:\n\t\t\t\tdf['ASIN'] = df['ASIN'].replace([value],mapping[value])\n\t'''\n\n\t#Write dataframe into JSON file\n\tdf.to_json(\"dashboard_data/products_with_most_reviews.json\", orient='records')\n\tprint(\"Successfully written 'products_with_most_reviews.json' file\")\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "retrieve_data_from_json", "data": "def retrieve_data_from_json(filepath):\n\tf = open(filepath, 'r+')\n\tdata = json.load(f)\n\tdata = json.dumps(data) #stringify json value\n\treturn data\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "index", "data": "def index():\n\n\twebscrape_data = retrieve_data_from_json('dashboard_data/webscrape_counter.json')\n\treview_contributors_data = retrieve_data_from_json('dashboard_data/top_review_contributors.json')\n\tproduct_reviews_data = retrieve_data_from_json('dashboard_data/products_with_most_reviews.json')\n\toutstanding_data = retrieve_data_from_json('amazonreviews/output/logs/outstanding_items.json')\n\n\t#pass product_mapping json value without stringify\n\tf = open('dashboard_data/product_mapping.json', 'r+')\n\tproduct_mapping = json.load(f)\n\n\t#remove product descriptiona and store product names\n\ti = 0\n\tnew_product_mapping = {}\n\tfor key,value in product_mapping.items():\n\t\tnew_product_mapping[key] = value.split(\",\")[0]\n\t\tif i == 4:\n\t\t\tbreak\n\t\ti+=1\n\n\treturn render_template(\"dashboard.html\",name=current_user.name,url ='/static/img/alice.jpg',\n\t\t\t\t\t\t   webscrape_data=webscrape_data,\n\t\t\t\t\t\t   review_contributors_data=review_contributors_data,\n\t\t\t\t\t\t   product_reviews_data=product_reviews_data,\n\t\t\t\t\t\t   outstanding_data=outstanding_data,\n\t\t\t\t\t\t   product_mapping = new_product_mapping)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "dashboard_update", "data": "def dashboard_update():\n\tupdate_status = request.form['update_status']\n\n\t#Run celery task when update btn is triggered\n\tif request.method == \"POST\":\n\t\tquery_reviews.apply_async(queue='queue3')\n\t\tquery_reviews_contributors.apply_async(queue='queue4')\n\t\tquery_review_numbers.apply_async(queue='queue5')\n\n\twebscrape_data = retrieve_data_from_json('dashboard_data/webscrape_counter.json')\n\treview_contributors_data = retrieve_data_from_json('dashboard_data/top_review_contributors.json')\n\tproduct_reviews_data =  retrieve_data_from_json('dashboard_data/products_with_most_reviews.json')\n\toutstanding_data = retrieve_data_from_json('amazonreviews/output/logs/outstanding_items.json')\n\n\treturn render_template(\"dashboard.html\",name=current_user.name,url ='/static/img/alice.jpg',\n\t\t\t\t\t\t   webscrape_data=webscrape_data,\n\t\t\t\t\t\t   review_contributors_data=review_contributors_data,\n\t\t\t\t\t\t   product_reviews_data=product_reviews_data,\n\t\t\t\t\t\t   outstanding_data=outstanding_data)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\tfrom model import Product\n\tproducts = Product.query.all()\n\n\treturn render_template(\"webscrape.html\",name=current_user.name,products=products)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "clear_file", "data": "def clear_file(filepath,filename):\n\topen(filepath+filename,'w').close()\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "update_counter_file", "data": "def update_counter_file():\n\t# get current date\n\tcurrent_date = datetime.today().strftime('%Y-%m-%d')\n\n\t# load json counter file to read/write to\n\twith open('dashboard_data/webscrape_counter.json', 'r+') as infile:\n\t\t# load current file unless it is empty which causes the jsondecode error\n\t\ttry:\n\t\t\tcurrent_counts = json.load(infile)\n\t\t\tprint(\"JSON WEBSCRAPE COUNTER FILE LOADED SUCCESSFULLY\", current_counts)\n\t\t\tif current_date in current_counts:\n\t\t\t\tcurrent_counts[f'{current_date}'] += 1\n\t\t\telse:\n\t\t\t\tcurrent_counts[f'{current_date}'] = 1\n\t\texcept ValueError: #catches json decode error\n\t\t\t# initialize the first time when the json counter file is empty\n\t\t\tprint(\"JSON WEBSCRAPE COUNTER FILE NOT LOADED DUE TO VALUE ERROR\")\n\t\t\tcurrent_counts = {}\n\t\t\tcurrent_counts[f'{current_date}'] = 1\n\t\twith open('dashboard_data/webscrape_counter.json', 'w') as outfile:\n\t\t\t# write updated count to counter file\n\t\t\tjson.dump(current_counts, outfile)\n\t\t\tinfile.close()\n\t\t\toutfile.close()\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "update_product_scrapetime", "data": "def update_product_scrapetime():\n\tfrom datetime import datetime\n\tfrom model import Product\n\n\t#Retrieve crawled asin from review file in crawl_progress folder\n\tasins = set()\n\twith open('crawl_progress/review.txt','r') as f:\n\t\tfor url in f:\n\t\t\tasin = url.split(\"/\")[-2]\n\t\t\tasins.add(asin)\n\n\tif 'product-reviews' in asins:\n\t\tasins.remove('product-reviews')\n\n\t#Iterate asins to update last_scraped of products\n\tfor asin in asins:\n\t\tlast_scraped = datetime.now()\n\t\tproduct = Product.query.filter_by(asin= asin).first()\n\t\tproduct.last_scraped = last_scraped\n\t\tdb.session.commit()\n\t\tprint(\"product:\",product,\"time:\",product.last_scraped )\n\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "update_status", "data": "def update_status(task):\n\twith open('./crawl_progress/status.txt','a') as f:\n\t\tf.write(task + \",\")\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "get_review_profile", "data": "def get_review_profile(config,com_review_output_path,com_review_con_path,com_profile_output_path, com_profile_con_path):\n\tclear_file('./crawl_progress/','review.txt')\n\tclear_file('./crawl_progress/','profile.txt')\n\tupdate_counter_file()\n\n\tm.get_reviews(config)\n\tm.get_outstanding_reviews(config)\n\tm.update_outstanding_reviews(config)\n\tm.combine_reviews(com_review_output_path, com_review_con_path)\n\n\t#Update review status when crawling has been completed\n\tupdate_status('review')\n\n\t#update datetime of products that have been scraped\n\tupdate_product_scrapetime()\n\n\t# Obtain profile urls from scraped reviews in raw\n\tm.get_profile_urls(config)\n\n\t# Scrape profiles\n\tm.get_profiles(config)\n\tm.get_outstanding_profiles(config)\n\tm.update_outstanding_profiles(config)\n\tm.combine_profiles(com_profile_output_path, com_profile_con_path)\n\n\t#Update profile status when crawling has been completed\n\tupdate_status('profile')\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "get_product", "data": "def get_product(config,com_product_output_path, com_product_con_path):\n\tclear_file('./crawl_progress/','product.txt')\n\tm.get_products(config)\n\tm.get_outstanding_products(config)\n\tm.update_outstanding_products(config)\n\tm.combine_products(com_product_output_path, com_product_con_path)\n\n\t#Update product status when crawling has been completed\n\tupdate_status('product')\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "scrape_product", "data": "def scrape_product():\n\n\tfrom model import Setting\n\tfrom model import Directory\n\n\t#Retrieve last record in setting model\n\tset_obj = db.session.query(Setting).order_by(Setting.id.desc()).first()\n\tdir_obj = db.session.query(Directory).order_by(Directory.id.desc()).first()\n\n\tinput_path = dir_obj.input_filepath\n\toutput_path = dir_obj.output_filepath\n\tcon_path = dir_obj.consolidated_filepath\n\tlog_path = dir_obj.log_filepath\n\tno_of_pg_crawl = set_obj.no_of_pg_crawl\n\tno_of_retry = set_obj.no_of_retry\n\n\n\tconfig = {\n\t\t'input_path': input_path,\n\t\t'output_path': output_path,\n\t\t'no_of_pg_crawl': no_of_pg_crawl,\n\t\t'no_of_retry': no_of_retry,\n\t\t'con_path': con_path,\n\t\t'log_path': log_path\n\t}\n\n\t#Retrieve ASIN from selected products and stored it in a list\n\tasin_list = []\n\tif request.method == \"POST\":\n\t\tdata = request.form['myJSONArrs']\n\t\tdata = json.loads(data)\n\t\tprint(data,type(data))\n\n\t\tfor record in data:\n\t\t\tasin = record[1]\n\t\t\tasin_list.append(asin)\n\n\tfrom model import Product\n\n\t# create urls to scrape reviews and products from a csv containing product ASINs\n\tm.create_urls(asin_list)\n\n\t#Find the basepath to this project folder\n\tbasepath = os.path.dirname(__file__)\n\n\t#basepath for all combined funcs (review,product,profile)\n\tcombined_output_basepath = os.path.join(basepath,'amazonreviews',config['output_path'])\n\tcombined_con_basepath = os.path.join(basepath,'amazonreviews',config['con_path'])\n\n\t#paths for combine_review parameters\n\tcom_review_output_path = os.path.join(combined_output_basepath, 'reviews')\n\tcom_review_con_path = os.path.join(combined_con_basepath,'reviews')\n\n\t#paths for combine_profile parameters\n\tcom_profile_output_path = os.path.join(combined_output_basepath, 'profiles')\n\tcom_profile_con_path = os.path.join(combined_con_basepath,'profiles')\n\n\t#clear status in status file which checks for celery task completion\n\tclear_file('./crawl_progress/','status.txt')\n\n\t#invoke review and profile celery task (reviews ->profile)\n\tget_review_profile.apply_async(queue='queue1',args=(config,com_review_output_path,com_review_con_path,com_profile_output_path, com_profile_con_path))\n\n\t#paths for combine_products parameters\n\tcom_product_output_path = os.path.join(combined_output_basepath, 'products')\n\tcom_product_con_path = os.path.join(combined_con_basepath,'products')\n\n\t#invoke product celery task\n\tget_product.apply_async(queue='queue2',args=(config,com_product_output_path,com_product_con_path))\n\n\tmsg = \"Webscrape tool has been successfully activated. It might take a while before web crawling is completed. Please check back again later.\"\n\n\treturn render_template(\"webscrape_progress.html\",name=current_user.name,msg=msg,review_url_count = review_url_count,profile_url_count=profile_url_count,product_url_count=product_url_count)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "status_result", "data": "def status_result():\n\tf = open('./crawl_progress/status.txt','r')\n\tresult = f.read()\n\tresult = result[:-1].split(\",\")\n\treturn result\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "webscrapestatus", "data": "def webscrapestatus():\n\t#Product: no. of product scraped count by ASIN\n\t#Profile: each url corresponds to one profile\n\t#Review: no. of url scraped approx 10 reviews per page\n\tdef countScrapedProductReview(filepath):\n\t\tscrapedList = []\n\t\ta_file = open(filepath,'r')\n\t\tfor line in a_file:\n\t\t\tscrapedList.append(line.strip())\n\n\t\t#Get unique items\n\t\tscrapedList = set(scrapedList)\n\n\t\tscrapedCount = len(scrapedList)\n\n\t\treturn scrapedCount\n\n\tdef countScrapedProfile(filepath):\n\t\t\tscrapedList = []\n\t\t\ta_file = open(filepath,'r')\n\t\t\tmyList = []\n\t\t\tfor line in a_file:\n\t\t\t\turl_asin = line.strip().split(\"/\")[-1].split(\"?\")[0]\n\t\t\t\tif \"ref=cm_cr_dp_d_show_all_btm\" not in url_asin:\n\t\t\t\t\tmyList.append(url_asin)\n\n\t\t\t#Get unique items\n\t\t\tscrapedList = set(scrapedList)\n\n\t\t\tscrapedCount = len(scrapedList)\n\n\t\t\treturn scrapedCount\n\n\tglobal review_url_count\n\tglobal profile_url_count\n\tglobal product_url_count\n\tglobal data_not_uploaded\n\n\treview_url_count = countScrapedProductReview('./crawl_progress/review.txt') * 10\n\tprofile_url_count = countScrapedProfile('./crawl_progress/profile.txt')\n\tproduct_url_count = countScrapedProductReview('./crawl_progress/product.txt')\n\n\tmsg = \"Webscrape tool has been successfully activated. It might take a while before web crawling is completed. Please check back again later.\"\n\tcomplete_msg = \"\"\n\tconfig = {\n\t\"con_path\": \"output/consolidated\",\n\t\"output_path\": \"output/raw\",\n\t\"log_path\": \"output/logs/\"\n\t}\n\n\tresult = status_result()\n\tprint(\"result:\",result)\n\n\t#if 'product' and 'review' and 'profile' in result :\n\tif 'product' and 'review' in result:\n\t\tcomplete_msg = \"Web scraping has been completed.\"\n\t\t\t\n\t\tif data_not_uploaded:\n\t\t\t#invoke upload consolidated csvs and recreate output folder task after queue 1 and queue2\n\t\t\tcwd = os.getcwd()\n\t\t\t#change secret key path based on where you store it\n\t\t\tsecret_key_path = os.path.join(cwd,'credential_file.json')\n\t\t\tm.upload_consolidated_csvs(secret_key_path, 'crafty-chiller-276910', 'scraped_items_test', config)\n\t\t\tm.clear_output_folders(config)\n\t\t\tdata_not_uploaded = False\n\n\tprint(\"check status:\",result)\n\n\treturn render_template(\"webscrape_progress.html\",name=current_user.name,\n\t\t\t\t\t\t   review_url_count = review_url_count,\n\t\t\t\t\t\t   profile_url_count=profile_url_count,\n\t\t\t\t\t\t   product_url_count=product_url_count,\n\t\t\t\t\t\t   msg=msg,complete_msg=complete_msg)\n\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "directory", "data": "def directory():\n\t#Retrieve last setting record\n\tfrom model import Directory\n\tobj = db.session.query(Directory).order_by(Directory.id.desc()).first()\n\n\tinput_path = obj.input_filepath\n\toutput_path = obj.output_filepath\n\ttracker_path = obj.tracker_filepath\n\tcon_path = obj.consolidated_filepath\n\tlog_path = obj.log_filepath\n\n\treturn render_template(\n\t\t\"directory.html\",name=current_user.name,\n\t\tinput_path = input_path,\n\t\toutput_path = output_path,\n\t\tcon_path = con_path,\n\t\tlog_path = log_path,\n\t\ttracker_path = tracker_path)\n\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "setting", "data": "def setting():\n\t#Retrieve last setting record\n\tfrom model import Setting\n\tobj = db.session.query(Setting).order_by(Setting.id.desc()).first()\n\n\tno_of_pg_crawl = obj.no_of_pg_crawl\n\tno_of_retry = obj.no_of_retry\n\n\trotate_proxy = obj.rotate_proxy\n\tfetch_proxies = obj.fetch_proxies\n\trotating_proxy_page_retry = obj.rotating_proxy_page_retry\n\tno_of_concurrent_request = obj.no_of_concurrent_request\n\tdownload_delay = obj.download_delay\n\tdownload_timeout = obj.download_timeout\n\n\treturn render_template(\n\t\t\t\"setting.html\",name=current_user.name,\n\t\t\tno_of_pg_crawl = no_of_pg_crawl,\n\t\t\tno_of_retry = no_of_retry,\n\t\t\trotate_proxy = rotate_proxy,\n\t\t\tfetch_proxies = fetch_proxies,\n\t\t\trotating_proxy_page_retry = rotating_proxy_page_retry,\n\t\t\tno_of_concurrent_request = no_of_concurrent_request,\n\t\t\tdownload_delay = download_delay,\n\t\t\tdownload_timeout = download_timeout\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "insert_directory_record", "data": "def insert_directory_record():\n\tfrom model import Directory\n\n\tinput_path = request.form.get('input_path')\n\toutput_path = request.form.get('output_path')\n\tcon_path = request.form.get('con_path')\n\tlog_path = request.form.get('log_path')\n\ttracker_path = request.form.get('tracker_path')\n\n\t#display result status of inserting new product into db\n\tmsg = \"\"\n\n\tif (check_non_empty_space_in_val(input_path) and check_non_empty_space_in_val(output_path) and\n\t\tcheck_non_empty_space_in_val(con_path) and check_non_empty_space_in_val(log_path) and\n\t\tcheck_non_empty_space_in_val(tracker_path)):\n\n\t\tnew_directory = Directory(input_filepath = input_path,output_filepath = output_path,\n\t\t\t\t\t\t\t\t  consolidated_filepath = con_path,\n\t\t\t\t\t\t\t\t  log_filepath = log_path,tracker_filepath = tracker_path)\n\n\t\t#add new record setting to database\n\t\tdb.session.add(new_directory)\n\t\tdb.session.commit()\n\n\t\tmsg = \"File directories have been successfully modified in the database.\"\n\n\telse:\n\t\tmsg = \"Failed to change file directories into database.\"\n\n\treturn render_template('directory_status.html',msg=msg,name=current_user.name)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "insert_setting_record", "data": "def insert_setting_record():\n\tfrom model import Setting\n\n\tno_of_pg_crawl = int(request.form.get('no_of_pg_crawl'))\n\tno_of_retry = int(request.form.get('no_of_retry'))\n\trotate_proxy = bool(request.form.get('rotate_proxy'))\n\tfetch_proxies = int(request.form.get('fetch_proxies'))\n\trotating_proxy_page_retry = int(request.form.get('rotating_proxy_page_retry'))\n\tno_of_concurrent_request = int(request.form.get('no_of_concurrent_request'))\n\tdownload_delay = int(request.form.get('download_delay'))\n\tdownload_timeout = int(request.form.get('download_timeout'))\n\n\t#display result status of inserting new product into db\n\tmsg = \"\"\n\n\t#Validate inputs are string and integers before inserting records\n\tif (isinstance(no_of_pg_crawl,int) and isinstance(no_of_retry,int) and\n\tisinstance(fetch_proxies,int) and isinstance(rotating_proxy_page_retry,int) and\n\tisinstance(no_of_concurrent_request,int) and isinstance(download_delay,int) and\n\tisinstance(download_timeout,int)):\n\n\t\tnew_setting = Setting(no_of_pg_crawl = no_of_pg_crawl,no_of_retry = no_of_retry,\n\t\t\t\t\t\t\t  rotate_proxy = rotate_proxy, fetch_proxies = fetch_proxies,\n\t\t\t\t\t\t\t  rotating_proxy_page_retry = rotating_proxy_page_retry,no_of_concurrent_request = no_of_concurrent_request,\n\t\t\t\t\t\t\t  download_delay = download_delay, download_timeout = download_timeout)\n\n\n\t\t#add new record setting to database\n\t\tdb.session.add(new_setting)\n\t\tdb.session.commit()\n\n\t\tmsg = \"All webscrap variables have been successfully added to the database.\"\n\n\telse:\n\t\tmsg = \"Failed to insert variable configs into database.\"\n\n\n\treturn render_template('setting_status.html',msg=msg,name=current_user.name)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "report", "data": "def report():\n\treturn render_template(\"report.html\",name=current_user.name)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "create_product", "data": "def create_product():\n\treturn render_template('new_products.html',name=current_user.name)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}, {"term": "def", "name": "new_product", "data": "def new_product():\n\tfrom model import Product\n\n\tasin =request.form.get('asin')\n\tname = request.form.get('name')\n\tcategory = request.form.get('category')\n\tprice = float(request.form.get('price'))\n\n\tprint(\"asin:\",asin,\"name:\",name,\"cat:\",category,\"price:\",price)\n\n\t#display result status of inserting new product into db\n\tmsg = \"\"\n\n\t#check valid value type and string is non-empty/space\n\tif isinstance(price,float) and isinstance(name,str) and check_non_empty_space_in_val(name) and check_non_empty_space_in_val(asin):\n\t\tnew_product = Product(asin=asin,name=name,price=price,category=category)\n\n\t\t#add new product to database\n\t\tdb.session.add(new_product)\n\t\tdb.session.commit()\n\n\t\tinsert_status = \"successful\"\n\t\tmsg = name + \" of price $\"+ str(price) + \" has been successfully added to the database.\"\n\telse:\n\t\tmsg = \"Failed to insert \" + name + \" product.\"\n\n\treturn render_template('product_status.html',msg=msg,name=name)\n", "description": null, "category": "webscraping", "imports": ["from flask_login import login_required, current_user", "from flask import Flask, render_template,request, jsonify", "from amazonreviews import main_func as m", "from datetime import datetime", "import time", "import os", "import json", "import threading", "from extensions import db", "from extensions import migrate", "from extensions import login_manager", "from google.cloud import bigquery", "from auth import auth as auth_blueprint", "from model import model as model_blueprint", "from celery import Celery", "from celery_once import QueueOnce", "\t\tapp.import_name,", "\t\tfrom model import Setting", "\t\tfrom model import Directory", "\tfrom model import User", "\tfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator", "\tfrom PIL import Image", "\timport matplotlib.pyplot as plt", "\timport numpy as np", "\tfrom model import Product", "\tfrom datetime import datetime", "\tfrom model import Product", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Product", "\tfrom model import Directory", "\tfrom model import Setting", "\tfrom model import Directory", "\tfrom model import Setting", "#from model import db", "\tfrom model import Product"]}], [], [{"term": "def", "name": "get_prediction_first", "data": "def get_prediction_first(text):\n\tif stockprice(text):\n\t\tanswer = stockprice_helper(text)\n\t\treturn [answer]\n\n\tif news(text):\n\t\tanswer = get_news()\n\t\treturn [answer]\n\t\n\t# getresponse = predict_response(text)\n\t# return [getresponse]\n\treturn [text]\n\n", "description": null, "category": "webscraping", "imports": ["# from auth import validate_ssn_number, validate_account_number, verify_otp", "# from check import iscustomer", "# from information_helper import extract_creditscore, extract_downpayment, \\", "# from start import start", "# from final import customername, phone", "# from repeat import repeat_last_statement,repeat_question", "# from extra_functionalities import account_balance,account_balance_helper, \\", "# from Models import Bank_Customer_Data", "# from Chatbot.prediction import predict_response, predict_intent", "from Chatbot.newfeatures import stockprice, stockprice_helper, news", "from Chatbot.webscrape.getNews import get_news", "# from Mail_to_HMC import mail", "# from webscrape.getNews import get_news", "# from webscrape.getCity import get_city", "# from VideoCall.zoomlink import create_meeting"]}], [], [{"term": "def", "name": "webscrape_results", "data": "def webscrape_results(\n", "description": null, "category": "webscraping", "imports": ["from json import loads", "from json.decoder import JSONDecodeError", "from time import sleep", "from typing import Any, Optional", "from bs4 import BeautifulSoup", "from requests import get", "from requests.exceptions import HTTPError, RequestException", "from scrape.log import logger"]}], [], [{"term": "class", "name": "classSettings:", "data": "class Settings:\n\tdesired_translations = 1\n\tevent_key = Key.alt_l\t  # key to copy highlight text\n\texit_key = Key.esc\n\n\tdef format_translations_yabla(tag_list): # used by webscraper to format, if webscrape url changed,\n\t\tformatted_tag_list = []\t\t\t  # formatting will need editing\n\t\tfor tag in tag_list:\n\t\t\ttag_string = tag.replace('\\n', ', ')\n\t\t\tformatted_tag_list.append(tag_string)\n\t\treturn ', '.join(formatted_tag_list)\n\n\t# webscrape details\n\turl = 'https://chinese.yabla.com/chinese-english-pinyin-dictionary.php?define='\n\ttag_type = 'div'\n\ttag_attr = 'class'\n\ttag_name = 'meaning'\n\tformat = format_translations_yabla\n\n\n", "description": null, "category": "webscraping", "imports": ["from pynput.keyboard import Key"]}], [{"term": "def", "name": "WebScrape", "data": "def WebScrape():\n\t#Index is used to indicate which story of the page is being used\n\t#Ex. Story 3 of page 1 would be index 4 page 1\n\tindex = 0\n\tpage = 1\n\tdurl = \"https://www.foxla.com/\"\n\turl = 'https://www.foxla.com/tag/crime-publicsafety'\n\tf = open('history.json')\n\tdata = json.load(f)\n\twhile True:\n\t\tif (index > 19):\n\t\t\tprint(\"stop\")\n\t\t\ttime.sleep(2)\n\t\t\t#If the page is done scraping go to the second page.\n\t\t\tpage = page + 1\n\t\t\turl = 'https://www.foxla.com/tag/crime-publicsafety'\n\t\t\turl = url + \"?page=\" + str(page)\n\t\t\tindex = 0\n\n\t\telse:\n\t\t\ttry:\n\t\t\t\tresponse = requests.get(url)\n\t\t\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\t\t\t#Finding title of webpage\n\t\t\t\tprint((soup.findAll('h3', attrs={\"class\": \"title\"})[index].string))\n\t\t\t\tx = (soup.findAll('h3', attrs={\"class\": \"title\"})[index])\n\t\t\t\tfor a in x.find_all('a', href=True):\n\t\t\t\t\tprint(\"Found the URL:\", a['href'])\n\t\t\t\t\t_durl = a['href']\n\t\t\t\turl2 = durl + _durl\n\t\t\t\t#Gets Url\n\t\t\t\tresponse = requests.get(url2)\n\t\t\t\t#Once Url is obtained goes to story url\n\t\t\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\t\t\tname = soup.find('span', {\"class\": \"dateline\"}).text[:-3]\n\t\t\t\t#Name var is the name of the city in California that the crime was comitted in\n\t\t\t\tprint((name))\n\t\t\t\tindex = index + 1\n\t\t\t\t#if name was found in data add 1 else set it as 1\n\t\t\t\tif name in data:\n\t\t\t\t\td = ({name: data[name] + 1})\n\t\t\t\telse:\n\t\t\t\t\td = {name: 1}\n\t\t\t\t#dumps data in json file\n\t\t\t\tdata.update(d)\n\t\t\t\tdata2 = json.dumps(data)\n\t\t\t\tprint(data2)\n\t\t\t\tjsonFile = open(\"history.json\", \"w+\")\n\t\t\t\tjsonFile.write(json.dumps(data))\n\t\t\t\tjsonFile.close()\n\t\t\texcept:\n\t\t\t\t#If the webpage cannot find a crime go to the next story\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import time", "import json"]}], [{"term": "def", "name": "get_cursor", "data": "def get_cursor(file_name):\n\t\"\"\" Connects and returns a cursor to an sqlite output file\n\n\tParameters\n\t----------\n\tfile_name: str\n\t\tname of the sqlite file\n\n\tReturns\n\t-------\n\tsqlite cursor3\n\t\"\"\"\n\tcon = sql.connect(file_name)\n\tcon.row_factory = sql.Row\n\treturn con.cursor()\n\n", "description": " Connects and returns a cursor to an sqlite output file\n\n\tParameters\n\t----------\n\tfile_name: str\n\t\tname of the sqlite file\n\n\tReturns\n\t-------\n\tsqlite cursor3\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "import_pris", "data": "def import_pris(pris_link):\n\t\"\"\" Opens pris_csv using Pandas. Adds Latitude and Longitude\n\tcolumns\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\n\tReturns\n\t-------\n\tpris: pd.Dataframe\n\t\tpris database\n\t\"\"\"\n\tpris = pd.read_csv(pris_link,\n\t\t\t\t\t   delimiter=',',\n\t\t\t\t\t   encoding='iso-8859-1',\n\t\t\t\t\t   skiprows=20,\n\t\t\t\t\t   )\n\n\tpris = pris.rename(columns={pris.columns[2]: 'Country'})\n\tpris = pris[['Country', 'Unit', 'Current Status', 'Type',\n\t\t\t\t 'Model', 'Operator', 'Reactor Supplier', 'Const. Date',\n\t\t\t\t 'Grid Date', 'Shutdown Date', 'RUP [MWe]']]\n\tpris.insert(11, 'Latitude', np.nan)\n\tpris.insert(12, 'Longitude', np.nan)\n\tpris = pris[pris.Unit.notnull()]\n\tpris = pris[pris.Unit != 'Unit']\n\tpris = pris.replace(np.nan, '')\n\treturn pris\n\n", "description": " Opens pris_csv using Pandas. Adds Latitude and Longitude\n\tcolumns\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath to reactors_pris_2016.original.csv file\n\n\tReturns\n\t-------\n\tpris: pd.Dataframe\n\t\tpris database\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "import_webscrape_data", "data": "def import_webscrape_data(scrape_link):\n\t\"\"\" Returns sqlite content of webscrape by performing an\n\tsqlite query\n\n\tParameters\n\t----------\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tcoords: sqlite cursor\n\t\tsqlite cursor containing webscrape data\n\t\"\"\"\n\tcur = get_cursor(scrape_link)\n\tcoords = cur.execute(\"SELECT name, long, lat FROM reactors_coordinates\")\n\treturn coords\n\n", "description": " Returns sqlite content of webscrape by performing an\n\tsqlite query\n\n\tParameters\n\t----------\n\tscrape_link: str\n\t\tpath to webscrape.sqlite file\n\n\tReturns\n\t-------\n\tcoords: sqlite cursor\n\t\tsqlite cursor containing webscrape data\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "get_edge_cases", "data": "def get_edge_cases():\n\t\"\"\" Returns a dictionary of edge cases that fuzzywuzzy is\n\tunable to catch. This could be because PRIS database stores\n\treactor names and Webscrape database fetches power plant names,\n\tor because PRIS reactor names are abbreviated.\n\n\tParameters\n\t----------\n\n\tReturns\n\t-------\n\tothers: dict\n\t\tdictionary of edge cases with \"key=pris_reactor_name, and\n\t\tvalue=webscrape_plant_name\"\n\t\"\"\"\n\tothers = {'OHI-': '\u014ci',\n\t\t\t  'ASCO-': 'Asc\u00f3',\n\t\t\t  'ROVNO-': 'Rivne',\n\t\t\t  'SHIN-KORI-': 'Kori',\n\t\t\t  'ANO-': 'Arkansas One',\n\t\t\t  'HANBIT-': 'Yeonggwang',\n\t\t\t  'FERMI-': 'Enrico Fermi',\n\t\t\t  'BALTIC-': 'Kaliningrad',\n\t\t\t  'COOK-': 'Donald C. Cook',\n\t\t\t  'HATCH-': 'Edwin I. Hatch',\n\t\t\t  'HARRIS-': 'Shearon Harris',\n\t\t\t  'SHIN-WOLSONG-': 'Wolseong',\n\t\t\t  'ST. ALBAN-': 'Saint-Alban',\n\t\t\t  'LASALLE-': 'LaSalle County',\n\t\t\t  'ZAPOROZHYE-': 'Zaporizhzhya',\n\t\t\t  'ROBINSON-': 'H. B. Robinson',\n\t\t\t  'SUMMER-': 'Virgil C. Summer',\n\t\t\t  'FARLEY-': 'Joseph M. Farley',\n\t\t\t  'ST. LAURENT ': 'Saint-Laurent',\n\t\t\t  'HADDAM NECK': 'Connecticut1 Yankee',\n\t\t\t  'FITZPATRICK': 'James A. FitzPatrick',\n\t\t\t  'HIGASHI DORI-1 (TOHOKU)': 'Higashid\u014dri',\n\t\t\t  }\n\treturn others\n\n", "description": " Returns a dictionary of edge cases that fuzzywuzzy is\n\tunable to catch. This could be because PRIS database stores\n\treactor names and Webscrape database fetches power plant names,\n\tor because PRIS reactor names are abbreviated.\n\n\tParameters\n\t----------\n\n\tReturns\n\t-------\n\tothers: dict\n\t\tdictionary of edge cases with \"key=pris_reactor_name, and\n\t\tvalue=webscrape_plant_name\"\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "sanitize_webscrape_name", "data": "def sanitize_webscrape_name(name):\n\t\"\"\" Sanitizes webscrape powerplant names by removing unwanted\n\tstrings (listed in blacklist), applying lower case, and deleting\n\ttrailing whitespace.\n\n\tParameters\n\t----------\n\tname: str\n\t\twebscrape plant name\n\n\tReturns\n\t-------\n\tname: str\n\t\tsanitized name for use with fuzzywuzzy\n\t\"\"\"\n\tblacklist = ['nuclear', 'power',\n\t\t\t\t 'plant', 'generating',\n\t\t\t\t 'station', 'reactor', 'atomic',\n\t\t\t\t 'energy', 'center', 'electric']\n\tname = name.lower()\n\tfor blacklisted in blacklist:\n\t\tname = name.replace(blacklisted, '')\n\tname = name.strip()\n\tname = ' '.join(name.split())\n\treturn name\n\n", "description": " Sanitizes webscrape powerplant names by removing unwanted\n\tstrings (listed in blacklist), applying lower case, and deleting\n\ttrailing whitespace.\n\n\tParameters\n\t----------\n\tname: str\n\t\twebscrape plant name\n\n\tReturns\n\t-------\n\tname: str\n\t\tsanitized name for use with fuzzywuzzy\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "sanitize_pris_name", "data": "def sanitize_pris_name(name):\n\tpris_name = name.lower()\n\tif pris_name.find('-') != -1 and is_int(pris_name[-1]):\n\t\tif pris_name[pris_name.find('-') + 1:].find('-') != -1:\n\t\t\tidx = pris_name.find('-')\n\t\t\tidx += pris_name[pris_name.find('-') + 1:].find('-')\n\t\t\tpris_name = pris_name[:idx]\n\t\telse:\n\t\t\tpris_name = pris_name[:pris_name.find('-')]\n\treturn pris_name\n\n", "description": null, "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "is_int", "data": "def is_int(str):\n\t\"\"\" Checks if input string is a number rather than a letter\n\n\tParameters\n\t----------\n\tstr: str\n\t\tstring to test\n\n\tReturns\n\t-------\n\tanswer: bool\n\t\treturns True if string is a number; False if string is not\n\t\"\"\"\n\tanswer = False\n\ttry:\n\t\tint(str)\n\texcept ValueError:\n\t\treturn answer\n\tanswer = True\n\treturn answer\n\n", "description": " Checks if input string is a number rather than a letter\n\n\tParameters\n\t----------\n\tstr: str\n\t\tstring to test\n\n\tReturns\n\t-------\n\tanswer: bool\n\t\treturns True if string is a number; False if string is not\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "merge_coordinates", "data": "def merge_coordinates(pris_link, scrape_link, data_year):\n\t\"\"\" Obtains coordinates from webscrape.sqlite and\n\twrites them to matching reactors in PRIS reactor file.\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath and name of pris reactor text file\n\tscrape: str\n\t\tpath and name of webscrape sqlite file\n\tdata_year: int\n\t\tyear the data is pulled from\n\n\tReturns\n\t-------\n\tnull\n\t\tWrites pris text file with coordinates\n\t\"\"\"\n\tothers = get_edge_cases()\n\tpris = import_pris(pris_link)\n\tcoords = import_webscrape_data(scrape_link)\n\tfor web in coords:\n\t\tfor i, prs in pris.iterrows():\n\t\t\twebscrape_name = sanitize_webscrape_name(web['name'])\n\t\t\tpris_name = sanitize_pris_name(prs[1])\n\t\t\tif fuzz.ratio(webscrape_name, pris_name) > 78:\n\t\t\t\tprs['Latitude'] = web['lat']\n\t\t\t\tprs['Longitude'] = web['long']\n\t\t\telse:\n\t\t\t\tfor other in others.keys():\n\t\t\t\t\tedge_case_key = other.lower()\n\t\t\t\t\tedge_case_value = others[other].lower()\n\t\t\t\t\tif fuzz.ratio(pris_name, edge_case_key) > 80:\n\t\t\t\t\t\tif fuzz.ratio(webscrape_name, edge_case_value) > 75:\n\t\t\t\t\t\t\tprs['Latitude'] = web['lat']\n\t\t\t\t\t\t\tprs['Longitude'] = web['long']\n\tpris.to_csv(\n\t\t'../database/reactors_pris_' +\n\t\tstr(data_year) +\n\t\t'.csv',\n\t\tindex=False,\n\t\tsep=',')\n\n", "description": " Obtains coordinates from webscrape.sqlite and\n\twrites them to matching reactors in PRIS reactor file.\n\n\tParameters\n\t----------\n\tpris_link: str\n\t\tpath and name of pris reactor text file\n\tscrape: str\n\t\tpath and name of webscrape sqlite file\n\tdata_year: int\n\t\tyear the data is pulled from\n\n\tReturns\n\t-------\n\tnull\n\t\tWrites pris text file with coordinates\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "save_output", "data": "def save_output(pris, data_year):\n\t\"\"\" Saves updated PRIS database as 'reactors_pris_2016.csv'\n\n\tParameters\n\t----------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\tdata_year: int\n\t\tyear the data is pulled from\n\n\tReturns\n\t-------\n\n\t\"\"\"\n\tpris.to_csv('../database/reactors_pris_' + str(data_year) + '.csv',\n\t\t\t\tindex=False,\n\t\t\t\tsep=',',\n\t\t\t\t)\n\n", "description": " Saves updated PRIS database as 'reactors_pris_2016.csv'\n\n\tParameters\n\t----------\n\tpris: pd.DataFrame\n\t\tupdated PRIS database with latitude and longitude info\n\tdata_year: int\n\t\tyear the data is pulled from\n\n\tReturns\n\t-------\n\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "import_csv", "data": "def import_csv(in_csv, delimit=','):\n\t\"\"\" Imports contents of a csv text file to a list of\n\tlists.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath and name of input csv file\n\tdelimit: str\n\t\tdelimiter of the csv file\n\n\tReturns\n\t-------\n\tdata_list: list\n\t\tlist of lists containing the csv data\n\t\"\"\"\n\twith open(in_csv, encoding='utf-8') as source:\n\t\tsourcereader = csv.reader(source, delimiter=delimit)\n\t\tdata_list = []\n\t\tfor row in sourcereader:\n\t\t\tdata_list.append(row)\n\treturn data_list\n\n", "description": " Imports contents of a csv text file to a list of\n\tlists.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath and name of input csv file\n\tdelimit: str\n\t\tdelimiter of the csv file\n\n\tReturns\n\t-------\n\tdata_list: list\n\t\tlist of lists containing the csv data\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "load_template", "data": "def load_template(in_template):\n\t\"\"\" Returns a jinja2 template from file.\n\n\tParameters\n\t---------\n\tin_template: str\n\t\tpath and name of jinja2 template\n\n\tReturns\n\t-------\n\toutput_template: jinja template object\n\t\"\"\"\n\twith open(in_template, 'r') as default:\n\t\toutput_template = jinja2.Template(default.read())\n\treturn output_template\n\n", "description": " Returns a jinja2 template from file.\n\n\tParameters\n\t---------\n\tin_template: str\n\t\tpath and name of jinja2 template\n\n\tReturns\n\t-------\n\toutput_template: jinja template object\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "get_composition_fresh", "data": "def get_composition_fresh(in_list, burnup):\n\t\"\"\" Returns a dictionary of isotope and composition (in mass fraction)\n\tusing vision_recipes for fresh UOX fuel.\n\n\tParameters\n\t---------\n\tin_list: list\n\t\tlist containing vision_recipes\n\tburnup: int\n\t\tburnup\n\n\tReturns\n\t-------\n\tdata_dict: dict\n\t\tdictionary with key=[isotope],\n\t\tand value=[composition]\n\t\"\"\"\n\tdata_dict = {}\n\tfor i in range(len(in_list)):\n\t\tif i > 1:\n\t\t\tif burnup == 33:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][1])})\n\t\t\telif burnup == 51:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][3])})\n\t\t\telse:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][5])})\n\treturn data_dict\n\n", "description": " Returns a dictionary of isotope and composition (in mass fraction)\n\tusing vision_recipes for fresh UOX fuel.\n\n\tParameters\n\t---------\n\tin_list: list\n\t\tlist containing vision_recipes\n\tburnup: int\n\t\tburnup\n\n\tReturns\n\t-------\n\tdata_dict: dict\n\t\tdictionary with key=[isotope],\n\t\tand value=[composition]\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "get_composition_spent", "data": "def get_composition_spent(in_list, burnup):\n\t\"\"\" Returns a dictionary of isotope and composition (in mass fraction)\n\tusing vision_recipes for spent nuclear fuel\n\n\tParameters\n\t---------\n\tin_list: list\n\t\tlist containing vision_recipes data\n\tburnup: int\n\t\tburnup\n\n\tReturns\n\t-------\n\tdata_dict: dict\n\t\tdictionary with key=[isotope],\n\t\tand value=[composition]\n\t\"\"\"\n\tdata_dict = {}\n\tfor i in range(len(in_list)):\n\t\tif i > 1:\n\t\t\tif burnup == 33:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][2])})\n\t\t\telif burnup == 51:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][4])})\n\t\t\telse:\n\t\t\t\tdata_dict.update({nn.id(in_list[i][0]):\n\t\t\t\t\t\t\t\t  float(in_list[i][6])})\n\treturn data_dict\n\n", "description": " Returns a dictionary of isotope and composition (in mass fraction)\n\tusing vision_recipes for spent nuclear fuel\n\n\tParameters\n\t---------\n\tin_list: list\n\t\tlist containing vision_recipes data\n\tburnup: int\n\t\tburnup\n\n\tReturns\n\t-------\n\tdata_dict: dict\n\t\tdictionary with key=[isotope],\n\t\tand value=[composition]\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "write_recipes", "data": "def write_recipes(fresh_dict, spent_dict, in_template,\n\t\t\t\t  burnup, out_path):\n\t\"\"\" Renders jinja template using fresh and spent fuel composition.\n\n\tParameters\n\t---------\n\tfresh_dict: dict\n\t\tdictionary with key=[isotope], and\n\t\tvalue=[composition] for fresh UOX\n\tspent_dict: dict\n\t\tdictionary with key=[isotope], and\n\t\tvalue=[composition] for spent fuel\n\tin_template: jinja template object\n\t\tjinja template object to be rendered\n\tburnup: int\n\t\tamount of burnup\n\tout_path: str\n\t\toutput path for recipe files\n\n\tReturns\n\t-------\n\tnull\n\t\tgenerates recipe files for cyclus.\n\t\"\"\"\n\tpathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n\trendered = in_template.render(fresh=fresh_dict,\n\t\t\t\t\t\t\t\t  spent=spent_dict)\n\twith open(out_path + '/uox_' + str(burnup) + '.xml', 'w') as output:\n\t\toutput.write(rendered)\n\n", "description": " Renders jinja template using fresh and spent fuel composition.\n\n\tParameters\n\t---------\n\tfresh_dict: dict\n\t\tdictionary with key=[isotope], and\n\t\tvalue=[composition] for fresh UOX\n\tspent_dict: dict\n\t\tdictionary with key=[isotope], and\n\t\tvalue=[composition] for spent fuel\n\tin_template: jinja template object\n\t\tjinja template object to be rendered\n\tburnup: int\n\t\tamount of burnup\n\tout_path: str\n\t\toutput path for recipe files\n\n\tReturns\n\t-------\n\tnull\n\t\tgenerates recipe files for cyclus.\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "produce_recipes", "data": "def produce_recipes(in_csv, recipe_template, burnup, out_path):\n\t\"\"\" Generates commodity composition xml input for cyclus.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath and name of recipe file\n\trecipe_template: str\n\t\tpath and name of recipe template\n\tburnup: int\n\t\tamount of burnup\n\tout_path: str\n\t\toutput path for recipe files\n\n\tReturns\n\t-------\n\tnull\n\t\tGenerates commodity composition xml input for cyclus.\n\t\"\"\"\n\trecipe = import_csv(in_csv, ',')\n\twrite_recipes(get_composition_fresh(recipe, burnup),\n\t\t\t\t  get_composition_spent(recipe, burnup),\n\t\t\t\t  load_template(recipe_template), burnup, out_file)\n\n", "description": " Generates commodity composition xml input for cyclus.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath and name of recipe file\n\trecipe_template: str\n\t\tpath and name of recipe template\n\tburnup: int\n\t\tamount of burnup\n\tout_path: str\n\t\toutput path for recipe files\n\n\tReturns\n\t-------\n\tnull\n\t\tGenerates commodity composition xml input for cyclus.\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "confirm_deployment", "data": "def confirm_deployment(row, start_year):\n\t\"\"\" Confirms if reactor is to be deployed for CYCLUS by\n\tchecking if the capacity > 400 and if the commercial date\n\tis a proper date format.\n\n\tParameters\n\t----------\n\tdate_str: str\n\t\tthe commercial date string from PRIS data file\n\tcapacity: str\n\t\tcapacity in MWe from PRIS data file\n\tstart_year: int\n\t\tstart year of the simulation\n\tReturns\n\t-------\n\tis_deployed: bool\n\t\t\tdetermines whether the reactor will be deployed\n\t\t\tin CYCLUS\n\t\"\"\"\n\tis_deployed = False\n\tcapacity = row['RUP [MWe]']\n\tstart_date = str(row['Grid Date'])\n\tend_date = str(row['Shutdown Date'])\n\tif len(start_date) > 4 and float(capacity) > 400:\n\t\tif end_date == 'nan':\n\t\t\ttry:\n\t\t\t\tdate.parse(start_date)\n\t\t\t\tis_deployed = True\n\t\t\texcept BaseException:\n\t\t\t\tpass\n\t\telif date.parse(end_date).year > start_year:\n\t\t\ttry:\n\t\t\t\tdate.parse(start_date)\n\t\t\t\tis_deployed = True\n\t\t\texcept BaseException:\n\t\t\t\tpass\n\treturn is_deployed\n\n", "description": " Confirms if reactor is to be deployed for CYCLUS by\n\tchecking if the capacity > 400 and if the commercial date\n\tis a proper date format.\n\n\tParameters\n\t----------\n\tdate_str: str\n\t\tthe commercial date string from PRIS data file\n\tcapacity: str\n\t\tcapacity in MWe from PRIS data file\n\tstart_year: int\n\t\tstart year of the simulation\n\tReturns\n\t-------\n\tis_deployed: bool\n\t\t\tdetermines whether the reactor will be deployed\n\t\t\tin CYCLUS\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "select_region", "data": "def select_region(in_dataframe, region, start_year):\n\t\"\"\" Returns a list of reactors that will be deployed for\n\tCYCLUS by checking the capacity and commercial date\n\n\tParameters\n\t----------\n\tin_dataframe: DataFrame\n\t\timported csv file in DataFrame format\n\tregion: str\n\t\tname of the region\n\tstart_year: int\n\t\tstart year of simulation\n\n\tReturns\n\t-------\n\treactor_df: DataFrame\n\t\t\tDataFrame of reactors from PRIS\n\t\"\"\"\n\tASIA = {'IRAN, ISLAMIC REPUBLIC OF', 'JAPAN',\n\t\t\t'KAZAKHSTAN',\n\t\t\t'BANGLADESH', 'CHINA', 'INDIA',\n\t\t\t'UNITED ARAB EMIRATES', 'VIETNAM',\n\t\t\t'PAKISTAN', 'PHILIPPINES', 'KOREA, REPUBLIC OF',\n\t\t\t'KAZAKHSTAN', 'ARMENIA', 'TAIWAN, CHINA'\n\t\t\t}\n\tUNITED_STATES = {'UNITED STATES OF AMERICA'}\n\tSOUTH_AMERICA = {'ARGENTINA', 'BRAZIL'}\n\tNORTH_AMERICA = {'CANADA', 'MEXICO', 'UNITED STATES OF AMERICA'}\n\tEUROPE = {'UKRAINE', 'UNITED KINGDOM',\n\t\t\t  'POLAND', 'ROMANIA', 'RUSSIA',\n\t\t\t  'BELARUS', 'BELGIUM', 'BULGARIA',\n\t\t\t  'GERMANY', 'ITALY', 'NETHERLANDS',\n\t\t\t  'SWEDEN', 'SWITZERLAND', 'TURKEY',\n\t\t\t  'SLOVENIA', 'SOVIET UNION', 'SPAIN',\n\t\t\t  'CZECH REPUBLIC', 'FINLAND', 'FRANCE',\n\t\t\t  'SLOVAKIA', 'HUNGARY', 'LITHUANIA'\n\t\t\t  }\n\tAFRICA = {'EGYPT', 'MOROCCO', 'SOUTH AFRICA', 'TUNISIA'}\n\tALL = (SOUTH_AMERICA | NORTH_AMERICA |\n\t\t   EUROPE | ASIA | AFRICA | UNITED_STATES)\n\tregions = {'ASIA': ASIA,\n\t\t\t   'AFRICA': AFRICA,\n\t\t\t   'EUROPE': EUROPE,\n\t\t\t   'SOUTH_AMERICA': SOUTH_AMERICA,\n\t\t\t   'NORTH_AMERICA': NORTH_AMERICA,\n\t\t\t   'UNITED_STATES': UNITED_STATES,\n\t\t\t   'ALL': ALL}\n\tif region.upper() not in regions.keys():\n\t\traise ValueError(region + 'is not a valid region')\n\treactor_df = pd.DataFrame(columns=in_dataframe.columns)\n\tfor index, row in in_dataframe.iterrows():\n\t\tcountry = row['Country']\n\t\tif country.upper() in regions[region.upper()]:\n\t\t\tif confirm_deployment(row, start_year):\n\t\t\t\treactor_df = reactor_df.append(\n\t\t\t\t\tin_dataframe.loc[index], ignore_index=True)\n\treactor_df = reactor_df.replace(np.nan, '')\n\treactor_df = reactor_df.astype(str)\n\n\treturn reactor_df\n\n", "description": " Returns a list of reactors that will be deployed for\n\tCYCLUS by checking the capacity and commercial date\n\n\tParameters\n\t----------\n\tin_dataframe: DataFrame\n\t\timported csv file in DataFrame format\n\tregion: str\n\t\tname of the region\n\tstart_year: int\n\t\tstart year of simulation\n\n\tReturns\n\t-------\n\treactor_df: DataFrame\n\t\t\tDataFrame of reactors from PRIS\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "get_lifetime", "data": "def get_lifetime(in_row, start_year):\n\t\"\"\" Calculates the lifetime of a reactor using first\n\tcommercial date and shutdown date. Defaults to 720 months\n\tif shutdown date is not available.\n\n\tParameters\n\t----------\n\tin_row: list\n\t\tsingle row from PRIS data that contains reactor\n\t\tinformation\n\tstart_year: int\n\t\tstart year for the simulation\n\n\tReturns\n\t-------\n\tlifetime: int\n\t\tlifetime of reactor\n\t\"\"\"\n\tcomm_date = in_row['Grid Date']\n\tif date.parse(comm_date).year < start_year:\n\t\tcomm_date = str(start_year) + '-02-01'\n\tshutdown_date = in_row['Shutdown Date']\n\tif not shutdown_date.strip():\n\t\treturn 720\n\telse:\n\t\tn_days_month = 365.0 / 12\n\t\tdelta = (date.parse(shutdown_date) - date.parse(comm_date)).days\n\t\treturn int(delta / n_days_month)\n\n", "description": " Calculates the lifetime of a reactor using first\n\tcommercial date and shutdown date. Defaults to 720 months\n\tif shutdown date is not available.\n\n\tParameters\n\t----------\n\tin_row: list\n\t\tsingle row from PRIS data that contains reactor\n\t\tinformation\n\tstart_year: int\n\t\tstart year for the simulation\n\n\tReturns\n\t-------\n\tlifetime: int\n\t\tlifetime of reactor\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "write_reactors", "data": "def write_reactors(in_dataframe, out_path, reactor_template, start_year,\n\t\t\t\t   cycle_time=18, refuel_time=1, capacity_factor=1):\n\t\"\"\" Renders CYCAMORE::reactor specifications using jinja2.\n\n\tParameters\n\t----------\n\tin_dataframe: DataFrame\n\t\tDataFrame containing PRIS data\n\tout_path: str\n\t\toutput path for reactor files\n\treactor_template: str\n\t\tpath to reactor template\n\tstart_year: int\n\t\tstart year of the simulation\n\tcycle_time: int\n\t\tcycle length of reactors in months\n\trefuel_time: int\n\t\taverage refuel time in months\n\tcapacity_factor: float\n\t\tcapacity factor to apply to all reactors, as a decimal\n\n\tReturns\n\t-------\n\tnull\n\t\twrites xml files with CYCAMORE::reactor config\n\t\"\"\"\n\tif out_path[-1] != '/':\n\t\tout_path += '/'\n\tpathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n\treactor_template = load_template(reactor_template)\n\tfor index, row in in_dataframe.iterrows():\n\t\tcapacity = float(row['RUP [MWe]'])\n\t\tif capacity >= 400:\n\t\t\tname = row[1].replace(' ', '_')\n\t\t\tassem_per_batch = 0\n\t\t\tassem_no = 0\n\t\t\tassem_size = 0\n\t\t\treactor_type = row['Type']\n\t\t\tlatitude = row['Latitude'] if row['Latitude'] != '' else 0\n\t\t\tlongitude = row['Longitude'] if row['Longitude'] != '' else 0\n\t\t\tif reactor_type in ['BWR', 'ESBWR']:\n\t\t\t\tassem_no = 732\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 138000 / assem_no\n\t\t\telif reactor_type in ['GCR', 'HWGCR']:  # Need batch number\n\t\t\t\tassem_no = 324\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 114000 / assem_no\n\t\t\telif reactor_type == 'HTGR':  # Need batch number\n\t\t\t\tassem_no = 3944\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 39000 / assem_no\n\t\t\telif reactor_type == 'PHWR':\n\t\t\t\tassem_no = 390\n\t\t\t\tassem_per_batch = int(assem_no / 45)\n\t\t\t\tassem_size = 80000 / assem_no\n\t\t\telif reactor_type == 'VVER':  # Need batch number\n\t\t\t\tassem_no = 312\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 41500 / assem_no\n\t\t\telif reactor_type == 'VVER-1200':  # Need batch number\n\t\t\t\tassem_no = 163\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 80000 / assem_no\n\t\t\telse:\n\t\t\t\tassem_no = 241\n\t\t\t\tassem_per_batch = int(assem_no / 3)\n\t\t\t\tassem_size = 103000 / assem_no\n\t\t\tconfig = reactor_template.render(\n\t\t\t\tname=name,\n\t\t\t\tlifetime=get_lifetime(\n\t\t\t\t\trow,\n\t\t\t\t\tstart_year),\n\t\t\t\tcycletime=cycle_time,\n\t\t\t\trefueltime=refuel_time,\n\t\t\t\tassem_size=assem_size,\n\t\t\t\tn_assem_core=assem_no,\n\t\t\t\tn_assem_batch=assem_per_batch,\n\t\t\t\tpower_cap=row['RUP [MWe]'] *\n\t\t\t\tcapacity_factor,\n\t\t\t\tlon=longitude,\n\t\t\t\tlat=latitude)\n\t\t\twith open(out_path + name.replace(' ', '_') + '.xml',\n\t\t\t\t\t  'w') as output:\n\t\t\t\toutput.write(config)\n\n", "description": " Renders CYCAMORE::reactor specifications using jinja2.\n\n\tParameters\n\t----------\n\tin_dataframe: DataFrame\n\t\tDataFrame containing PRIS data\n\tout_path: str\n\t\toutput path for reactor files\n\treactor_template: str\n\t\tpath to reactor template\n\tstart_year: int\n\t\tstart year of the simulation\n\tcycle_time: int\n\t\tcycle length of reactors in months\n\trefuel_time: int\n\t\taverage refuel time in months\n\tcapacity_factor: float\n\t\tcapacity factor to apply to all reactors, as a decimal\n\n\tReturns\n\t-------\n\tnull\n\t\twrites xml files with CYCAMORE::reactor config\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "obtain_reactors", "data": "def obtain_reactors(in_csv, region, reactor_template, out_path, start_year):\n\t\"\"\" Writes xml files for individual reactors in a given\n\tregion.\n\n\tParameters\n\t----------\n\tin_csv: str\n\t\tcsv file name\n\tregion: str\n\t\tregion name\n\treactor_template: str\n\t\tpath to CYCAMORE::reactor config template file\n\tout_path: str\n\t\toutput path for reactor files\n\tstart_year: int\n\t\tstart year of the simulation\n\n\tReturns\n\t-------\n\tnull\n\t\tWrites xml files for individual reactors in region.\n\t\"\"\"\n\tin_data = pd.read_csv(in_csv, ',')\n\treactor_list = select_region(in_data, region, start_year)\n\twrite_reactors(reactor_list, out_path, reactor_template, start_year)\n\n", "description": " Writes xml files for individual reactors in a given\n\tregion.\n\n\tParameters\n\t----------\n\tin_csv: str\n\t\tcsv file name\n\tregion: str\n\t\tregion name\n\treactor_template: str\n\t\tpath to CYCAMORE::reactor config template file\n\tout_path: str\n\t\toutput path for reactor files\n\tstart_year: int\n\t\tstart year of the simulation\n\n\tReturns\n\t-------\n\tnull\n\t\tWrites xml files for individual reactors in region.\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "write_deployment", "data": "def write_deployment(in_dict, out_path, deployinst_template,\n\t\t\t\t\t inclusions_template):\n\t\"\"\" Renders jinja template using dictionary of reactor name and buildtime.\n\tOutputs an xml file that uses xinclude to include the reactor xml files\n\tlocated in cyclus_input/reactors.\n\n\tParameters\n\t---------\n\tin_dict: dictionary\n\t\tdictionary with key=[reactor name], and value=[buildtime]\n\tout_path: str\n\t\toutput path for files\n\tdeployinst_template: str\n\t\tpath to deployinst template\n\tinclusions_template: str\n\t\tpath to inclusions template\n\n\tReturns\n\t-------\n\tnull\n\t\tgenerates input files that have deployment and xml inclusions\n\t\"\"\"\n\tif out_path[-1] != '/':\n\t\tout_path += '/'\n\tpathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n\tdeployinst_template = load_template(deployinst_template)\n\tinclusions_template = load_template(inclusions_template)\n\tcountry_list = {value[0] for value in in_dict.values()}\n\tfor nation in country_list:\n\t\ttemp_dict = {}\n\t\tfor reactor in in_dict.keys():\n\t\t\tif in_dict[reactor][0].upper() == nation.upper():\n\t\t\t\ttemp_dict.update({reactor: in_dict[reactor][1]})\n\t\tpathlib.Path(out_path + nation.replace(' ', '_') +\n\t\t\t\t\t '/').mkdir(parents=True, exist_ok=True)\n\t\tdeployinst = deployinst_template.render(reactors=temp_dict)\n\t\twith open(out_path + nation.replace(' ', '_') +\n\t\t\t\t  '/deployinst.xml', 'w') as output1:\n\t\t\toutput1.write(deployinst)\n\tinclusions = inclusions_template.render(reactors=in_dict)\n\twith open(out_path + 'inclusions.xml', 'w') as output2:\n\t\toutput2.write(inclusions)\n\n", "description": " Renders jinja template using dictionary of reactor name and buildtime.\n\tOutputs an xml file that uses xinclude to include the reactor xml files\n\tlocated in cyclus_input/reactors.\n\n\tParameters\n\t---------\n\tin_dict: dictionary\n\t\tdictionary with key=[reactor name], and value=[buildtime]\n\tout_path: str\n\t\toutput path for files\n\tdeployinst_template: str\n\t\tpath to deployinst template\n\tinclusions_template: str\n\t\tpath to inclusions template\n\n\tReturns\n\t-------\n\tnull\n\t\tgenerates input files that have deployment and xml inclusions\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "get_buildtime", "data": "def get_buildtime(in_list, start_year, path_list):\n\t\"\"\" Calculates the buildtime required for reactor\n\tdeployment in months.\n\n\tParameters\n\t----------\n\tin_list: list\n\t\tlist of reactors\n\tstart_year: int\n\t\tstarting year of simulation\n\tpath_list: list\n\t\tlist of paths to reactor files\n\n\tReturns\n\t-------\n\tbuildtime_dict: dict\n\t\tdictionary with key=[name of reactor], and\n\t\tvalue=[set of country and buildtime]\n\t\"\"\"\n\tbuildtime_dict = {}\n\tfor index, row in in_list.iterrows():\n\t\tcomm_date = date.parse(row['Grid Date'])\n\t\tstart_date = [comm_date.year, comm_date.month, comm_date.day]\n\t\tdelta = ((start_date[0] - int(start_year)) * 12 +\n\t\t\t\t (start_date[1]) +\n\t\t\t\t round(start_date[2] / (365.0 / 12)))\n\t\tif delta < 0:\n\t\t\tdelta = 1\n\t\tfor index, reactor in enumerate(path_list):\n\t\t\tname = row['Unit'].replace(' ', '_')\n\t\t\tcountry = row['Country']\n\t\t\tfile_name = (reactor.replace(\n\t\t\t\tos.path.dirname(path_list[index]), '')).replace('/', '')\n\t\t\tif (name + '.xml' == file_name):\n\t\t\t\tbuildtime_dict.update({name: (country, delta)})\n\treturn buildtime_dict\n\n", "description": " Calculates the buildtime required for reactor\n\tdeployment in months.\n\n\tParameters\n\t----------\n\tin_list: list\n\t\tlist of reactors\n\tstart_year: int\n\t\tstarting year of simulation\n\tpath_list: list\n\t\tlist of paths to reactor files\n\n\tReturns\n\t-------\n\tbuildtime_dict: dict\n\t\tdictionary with key=[name of reactor], and\n\t\tvalue=[set of country and buildtime]\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "deploy_reactors", "data": "def deploy_reactors(in_csv, region, start_year, deployinst_template,\n\t\t\t\t\tinclusions_template, reactors_path, deployment_path):\n\t\"\"\" Generates xml files that specify the reactors that will be included\n\tin a CYCLUS simulation.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath to pris reactor database\n\tregion: str\n\t\tregion name\n\tstart_year: int\n\t\tstarting year of simulation\n\tdeployinst_template: str\n\t\tpath to deployinst template\n\tinclusions_template: str\n\t\tpath to inclusions template\n\treactors_path: str\n\t\tpath containing reactor files\n\tdeployment_path: str\n\t\toutput path for deployinst xml\n\n\tReturns\n\t-------\n\tbuildtime_dict: dict\n\t\tdictionary with key=[name of reactor], and\n\t\tvalue=[set of country and buildtime]\n\t\"\"\"\n\tlists = []\n\tif reactors_path[-1] != '/':\n\t\treactors_path += '/'\n\tfor files in os.listdir(reactors_path):\n\t\tlists.append(reactors_path + files)\n\tin_data = pd.read_csv(in_csv)\n\treactor_list = select_region(in_data, region, start_year)\n\tbuildtime = get_buildtime(reactor_list, start_year, lists)\n\twrite_deployment(buildtime, deployment_path, deployinst_template,\n\t\t\t\t\t inclusions_template)\n\treturn buildtime\n\n", "description": " Generates xml files that specify the reactors that will be included\n\tin a CYCLUS simulation.\n\n\tParameters\n\t---------\n\tin_csv: str\n\t\tpath to pris reactor database\n\tregion: str\n\t\tregion name\n\tstart_year: int\n\t\tstarting year of simulation\n\tdeployinst_template: str\n\t\tpath to deployinst template\n\tinclusions_template: str\n\t\tpath to inclusions template\n\treactors_path: str\n\t\tpath containing reactor files\n\tdeployment_path: str\n\t\toutput path for deployinst xml\n\n\tReturns\n\t-------\n\tbuildtime_dict: dict\n\t\tdictionary with key=[name of reactor], and\n\t\tvalue=[set of country and buildtime]\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}, {"term": "def", "name": "render_cyclus", "data": "def render_cyclus(cyclus_template, region, in_dict,\n\t\t\t\t  out_path, start_year, duration=780, burn_up=50):\n\t\"\"\" Renders final CYCLUS input file with xml base, and institutions\n\tfor each country\n\n\tParameters\n\t----------\n\tcyclus_template: str\n\t\tpath to CYCLUS input file template\n\tregion: str\n\t\tregion chosen for CYCLUS simulation\n\tin_dict: dictionary\n\t\tin_dict should be buildtime_dict from get_buildtime function\n\tout_path: str\n\t\toutput path for CYCLUS input file\n\tstart_year: int\n\t\tstart year for the simulation\n\tduration: int\n\t\tduration for CYCLUS simulation to last in months\n\tburn_up: int\n\t\tburnup in GWd/MTU\n\n\tReturns\n\t-------\n\tnull\n\t\twrites CYCLUS input file in out_path\n\t\"\"\"\n\tif out_path[-1] != '/':\n\t\tout_path += '/'\n\tcyclus_template = load_template(cyclus_template)\n\tcountry_list = {value[0].replace(' ', '_') for value in in_dict.values()}\n\trendered = cyclus_template.render(start_year=start_year,\n\t\t\t\t\t\t\t\t\t  duration=duration,\n\t\t\t\t\t\t\t\t\t  burnup=burn_up,\n\t\t\t\t\t\t\t\t\t  countries=country_list,\n\t\t\t\t\t\t\t\t\t  base_dir=os.path.abspath(out_path) + '/')\n\twith open(out_path + region + '.xml', 'w') as output:\n\t\toutput.write(rendered)\n", "description": " Renders final CYCLUS input file with xml base, and institutions\n\tfor each country\n\n\tParameters\n\t----------\n\tcyclus_template: str\n\t\tpath to CYCLUS input file template\n\tregion: str\n\t\tregion chosen for CYCLUS simulation\n\tin_dict: dictionary\n\t\tin_dict should be buildtime_dict from get_buildtime function\n\tout_path: str\n\t\toutput path for CYCLUS input file\n\tstart_year: int\n\t\tstart year for the simulation\n\tduration: int\n\t\tduration for CYCLUS simulation to last in months\n\tburn_up: int\n\t\tburnup in GWd/MTU\n\n\tReturns\n\t-------\n\tnull\n\t\twrites CYCLUS input file in out_path\n\t", "category": "webscraping", "imports": ["import csv", "import dateutil.parser as date", "import jinja2", "import numpy as np", "import os", "import pathlib", "import pandas as pd", "import sqlite3 as sql", "from fuzzywuzzy import fuzz", "from pyne import nucname as nn", "def import_pris(pris_link):", "def import_webscrape_data(scrape_link):", "\tpris = import_pris(pris_link)", "\tcoords = import_webscrape_data(scrape_link)", "def import_csv(in_csv, delimit=','):", "\trecipe = import_csv(in_csv, ',')", "\t\timported csv file in DataFrame format"]}], [{"term": "def", "name": "CNN_Politics", "data": "def CNN_Politics(url=\"https://search.api.cnn.io/content?q=politics&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n\tdata=[]\n\twith requests.Session() as req:\n\t\tfor item in range(1, 1000, 100):\n\t\t\ttry:\n\t\t\t\tr = req.get(url.format(item)).json()\n\t\t\t\tfor a in r['result']:\n\t\t\t\t\tif 'headline' in a.keys():\n\t\t\t\t\t\ttitle=a['headline']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'   \n\t\t\t\t\tif 'body' in a.keys():\n\t\t\t\t\t\tbody=a['body']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'\n\t\t\t\t\tif 'lastPublishDate' in a.keys():\n\t\t\t\t\t\tdatetime=a['lastPublishDate']\n\t\t\t\t\telse:\n\t\t\t\t\t\t datetime='Nan'\n\t\t\t\t\tif 'url' in a.keys():\n\t\t\t\t\t\turl=a['url']\n\t\t\t\t\telse:\n\t\t\t\t\t\t url='Nan'\n\t\t\t\t\tif 'byLine' in a.keys():\n\t\t\t\t\t\tauthor=a['byLine']\n\t\t\t\t\telse:\n\t\t\t\t\t\t author='Nan'   \n\t\t\t\t\tdata.append([title, body, datetime, author, url])\n\t\t\texcept:\n\t\t\t\tpass\n\tdf_politics = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n\tdf_politics[\"category\"] = 'politics'\n\tbucket.blob('CNN/politics.csv').upload_from_string(df_politics.to_csv(), 'text/csv')\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from io import BytesIO", "import os", "from airflow import DAG", "from google.cloud import storage", "from airflow.operators.python import PythonOperator, task", "from airflow.operators.bash import BashOperator", "from google.oauth2.credentials import Credentials", "import datetime as dt", "import json", "from google.cloud import secretmanager", "import requests"]}, {"term": "def", "name": "CNN_World", "data": "def CNN_World(url=\"https://search.api.cnn.io/content?q=world&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n\tdata=[]\n\twith requests.Session() as req:\n\t\tfor item in range(1, 1000, 100):\n\t\t\ttry:\n\t\t\t\tr = req.get(url.format(item)).json()\n\t\t\t\tfor a in r['result']:\n\t\t\t\t\tif 'headline' in a.keys():\n\t\t\t\t\t\ttitle=a['headline']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'   \n\t\t\t\t\tif 'body' in a.keys():\n\t\t\t\t\t\tbody=a['body']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'\n\t\t\t\t\tif 'lastPublishDate' in a.keys():\n\t\t\t\t\t\tdatetime=a['lastPublishDate']\n\t\t\t\t\telse:\n\t\t\t\t\t\t datetime='Nan'\n\t\t\t\t\tif 'url' in a.keys():\n\t\t\t\t\t\turl=a['url']\n\t\t\t\t\telse:\n\t\t\t\t\t\t url='Nan'\n\t\t\t\t\tif 'byLine' in a.keys():\n\t\t\t\t\t\tauthor=a['byLine']\n\t\t\t\t\telse:\n\t\t\t\t\t\t author='Nan'   \n\t\t\t\t\tdata.append([title, body, datetime, author, url])\n\t\t\texcept:\n\t\t\t\tpass\n\tdf_world = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n\tdf_world[\"category\"] = 'world'\n\tbucket.blob('CNN/world.csv').upload_from_string(df_world.to_csv(), 'text/csv')\n\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from io import BytesIO", "import os", "from airflow import DAG", "from google.cloud import storage", "from airflow.operators.python import PythonOperator, task", "from airflow.operators.bash import BashOperator", "from google.oauth2.credentials import Credentials", "import datetime as dt", "import json", "from google.cloud import secretmanager", "import requests"]}, {"term": "def", "name": "CNN_US", "data": "def CNN_US(url=\"https://search.api.cnn.io/content?q=US&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n\tdata=[]\n\twith requests.Session() as req:\n\t\tfor item in range(1, 1000, 100):\n\t\t\ttry:\n\t\t\t\tr = req.get(url.format(item)).json()\n\t\t\t\tfor a in r['result']:\n\t\t\t\t\tif 'headline' in a.keys():\n\t\t\t\t\t\ttitle=a['headline']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'   \n\t\t\t\t\tif 'body' in a.keys():\n\t\t\t\t\t\tbody=a['body']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'\n\t\t\t\t\tif 'lastPublishDate' in a.keys():\n\t\t\t\t\t\tdatetime=a['lastPublishDate']\n\t\t\t\t\telse:\n\t\t\t\t\t\t datetime='Nan'\n\t\t\t\t\tif 'url' in a.keys():\n\t\t\t\t\t\turl=a['url']\n\t\t\t\t\telse:\n\t\t\t\t\t\t url='Nan'\n\t\t\t\t\tif 'byLine' in a.keys():\n\t\t\t\t\t\tauthor=a['byLine']\n\t\t\t\t\telse:\n\t\t\t\t\t\t author='Nan'   \n\t\t\t\t\tdata.append([title, body, datetime, author, url])\n\t\t\texcept:\n\t\t\t\tpass\n\tdf_us = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n\tdf_us[\"category\"] = 'us'\n\tbucket.blob('CNN/us.csv').upload_from_string(df_us.to_csv(), 'text/csv')\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from io import BytesIO", "import os", "from airflow import DAG", "from google.cloud import storage", "from airflow.operators.python import PythonOperator, task", "from airflow.operators.bash import BashOperator", "from google.oauth2.credentials import Credentials", "import datetime as dt", "import json", "from google.cloud import secretmanager", "import requests"]}, {"term": "def", "name": "CNN_Opinion", "data": "def CNN_Opinion(url=\"https://search.api.cnn.io/content?q=opinion&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n\tdata=[]\n\twith requests.Session() as req:\n\t\tfor item in range(1, 1000, 100):\n\t\t\ttry:\n\t\t\t\tr = req.get(url.format(item)).json()\n\t\t\t\tfor a in r['result']:\n\t\t\t\t\tif 'headline' in a.keys():\n\t\t\t\t\t\ttitle=a['headline']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'   \n\t\t\t\t\tif 'body' in a.keys():\n\t\t\t\t\t\tbody=a['body']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'\n\t\t\t\t\tif 'lastPublishDate' in a.keys():\n\t\t\t\t\t\tdatetime=a['lastPublishDate']\n\t\t\t\t\telse:\n\t\t\t\t\t\t datetime='Nan'\n\t\t\t\t\tif 'url' in a.keys():\n\t\t\t\t\t\turl=a['url']\n\t\t\t\t\telse:\n\t\t\t\t\t\t url='Nan'\n\t\t\t\t\tif 'byLine' in a.keys():\n\t\t\t\t\t\tauthor=a['byLine']\n\t\t\t\t\telse:\n\t\t\t\t\t\t author='Nan'   \n\t\t\t\t\tdata.append([title, body, datetime, author, url])\n\t\t\texcept:\n\t\t\t\tpass\n\tdf_opinion = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n\tdf_opinion[\"category\"] = 'opinion'\n\tbucket.blob('CNN/opinion.csv').upload_from_string(df_opinion.to_csv(), 'text/csv')\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from io import BytesIO", "import os", "from airflow import DAG", "from google.cloud import storage", "from airflow.operators.python import PythonOperator, task", "from airflow.operators.bash import BashOperator", "from google.oauth2.credentials import Credentials", "import datetime as dt", "import json", "from google.cloud import secretmanager", "import requests"]}, {"term": "def", "name": "CNN_Economy", "data": "def CNN_Economy(url=\"https://search.api.cnn.io/content?q=economy&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n\tdata=[]\n\twith requests.Session() as req:\n\t\tfor item in range(1, 1000, 100):\n\t\t\ttry:\n\t\t\t\tr = req.get(url.format(item)).json()\n\t\t\t\tfor a in r['result']:\n\t\t\t\t\tif 'headline' in a.keys():\n\t\t\t\t\t\ttitle=a['headline']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'   \n\t\t\t\t\tif 'body' in a.keys():\n\t\t\t\t\t\tbody=a['body']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'\n\t\t\t\t\tif 'lastPublishDate' in a.keys():\n\t\t\t\t\t\tdatetime=a['lastPublishDate']\n\t\t\t\t\telse:\n\t\t\t\t\t\t datetime='Nan'\n\t\t\t\t\tif 'url' in a.keys():\n\t\t\t\t\t\turl=a['url']\n\t\t\t\t\telse:\n\t\t\t\t\t\t url='Nan'\n\t\t\t\t\tif 'byLine' in a.keys():\n\t\t\t\t\t\tauthor=a['byLine']\n\t\t\t\t\telse:\n\t\t\t\t\t\t author='Nan'   \n\t\t\t\t\tdata.append([title, body, datetime, author, url])\n\t\t\texcept:\n\t\t\t\tpass\n\tdf_economy = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n\tdf_economy[\"category\"] = 'economy'\n\tbucket.blob('CNN/economy.csv').upload_from_string(df_economy.to_csv(), 'text/csv')\n\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from io import BytesIO", "import os", "from airflow import DAG", "from google.cloud import storage", "from airflow.operators.python import PythonOperator, task", "from airflow.operators.bash import BashOperator", "from google.oauth2.credentials import Credentials", "import datetime as dt", "import json", "from google.cloud import secretmanager", "import requests"]}, {"term": "def", "name": "CNN_Tech", "data": "def CNN_Tech(url=\"https://search.api.cnn.io/content?q=tech&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n\tdata=[]\n\twith requests.Session() as req:\n\t\tfor item in range(1, 1000, 100):\n\t\t\ttry:\n\t\t\t\tr = req.get(url.format(item)).json()\n\t\t\t\tfor a in r['result']:\n\t\t\t\t\tif 'headline' in a.keys():\n\t\t\t\t\t\ttitle=a['headline']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'   \n\t\t\t\t\tif 'body' in a.keys():\n\t\t\t\t\t\tbody=a['body']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'\n\t\t\t\t\tif 'lastPublishDate' in a.keys():\n\t\t\t\t\t\tdatetime=a['lastPublishDate']\n\t\t\t\t\telse:\n\t\t\t\t\t\t datetime='Nan'\n\t\t\t\t\tif 'url' in a.keys():\n\t\t\t\t\t\turl=a['url']\n\t\t\t\t\telse:\n\t\t\t\t\t\t url='Nan'\n\t\t\t\t\tif 'byLine' in a.keys():\n\t\t\t\t\t\tauthor=a['byLine']\n\t\t\t\t\telse:\n\t\t\t\t\t\t author='Nan'   \n\t\t\t\t\tdata.append([title, body, datetime, author, url])\n\t\t\texcept:\n\t\t\t\tpass\n\tdf_tech = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n\tdf_tech[\"category\"] = 'tech'\n\tbucket.blob('CNN/tech.csv').upload_from_string(df_tech.to_csv(), 'text/csv')\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from io import BytesIO", "import os", "from airflow import DAG", "from google.cloud import storage", "from airflow.operators.python import PythonOperator, task", "from airflow.operators.bash import BashOperator", "from google.oauth2.credentials import Credentials", "import datetime as dt", "import json", "from google.cloud import secretmanager", "import requests"]}, {"term": "def", "name": "CNN_Health", "data": "def CNN_Health(url=\"https://search.api.cnn.io/content?q=health&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n\tdata=[]\n\twith requests.Session() as req:\n\t\tfor item in range(1, 1000, 100):\n\t\t\ttry:\n\t\t\t\tr = req.get(url.format(item)).json()\n\t\t\t\tfor a in r['result']:\n\t\t\t\t\tif 'headline' in a.keys():\n\t\t\t\t\t\ttitle=a['headline']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'   \n\t\t\t\t\tif 'body' in a.keys():\n\t\t\t\t\t\tbody=a['body']\n\t\t\t\t\telse:\n\t\t\t\t\t\t title='Nan'\n\t\t\t\t\tif 'lastPublishDate' in a.keys():\n\t\t\t\t\t\tdatetime=a['lastPublishDate']\n\t\t\t\t\telse:\n\t\t\t\t\t\t datetime='Nan'\n\t\t\t\t\tif 'url' in a.keys():\n\t\t\t\t\t\turl=a['url']\n\t\t\t\t\telse:\n\t\t\t\t\t\t url='Nan'\n\t\t\t\t\tif 'byLine' in a.keys():\n\t\t\t\t\t\tauthor=a['byLine']\n\t\t\t\t\telse:\n\t\t\t\t\t\t author='Nan'   \n\t\t\t\t\tdata.append([title, body, datetime, author, url])\n\t\t\texcept:\n\t\t\t\tpass\n\tdf_health = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n\tdf_health[\"category\"] = 'health'\n\tbucket.blob('CNN/health.csv').upload_from_string(df_health.to_csv(), 'text/csv')\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from io import BytesIO", "import os", "from airflow import DAG", "from google.cloud import storage", "from airflow.operators.python import PythonOperator, task", "from airflow.operators.bash import BashOperator", "from google.oauth2.credentials import Credentials", "import datetime as dt", "import json", "from google.cloud import secretmanager", "import requests"]}], [{"term": "def", "name": "relative_to_assets", "data": "def relative_to_assets(path: str) -> Path:\r\n\treturn ASSETS_PATH / Path(path)\r\n", "description": null, "category": "webscraping", "imports": ["from pathlib import Path\r", "from tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage\r", "import tkinter as tk\r", "import threading\r", "import pyautogui\r", "import time\r", "import cv2\r", "from pytesseract import *\r", "from PIL import Image\r", "from selenium import webdriver\r", "from webdriver_manager.chrome import ChromeDriverManager\r", "from selenium.webdriver.chrome.service import Service\r", "from selenium.webdriver.common.by import By\r", "import mss\r", "import mss.tools\r"]}, {"term": "def", "name": "fwebscrape", "data": "\tdef webscrape(username, original):\r\n\t\tt = time.time()\r\n\t\r\n\t\toptions = webdriver.ChromeOptions()\r\n\t\toptions.add_argument('--headless')\r\n\t\toptions.add_experimental_option('excludeSwitches', ['enable-logging'])\r\n\t\tdriver = webdriver.Chrome(options=options)\r\n\t\tdriver.get(f'https://cod.tracker.gg/warzone/profile/atvi/{username}/overview')\r\n\t\tpage_title = driver.find_elements(By.CLASS_NAME, 'lead')\r\n\t\t\r\n\t\tif not page_title or page_title[0] == \"WARZONE STATS NOT FOUND\":\r\n\t\t\tprint(\"WARZONE STATS NOT FOUND - Private profile\")\r\n\t\t\tusernameBox.delete(0, tk.END)\r\n\t\t\tusernameBox.insert(0, \"WARZONE STATS NOT FOUND - Private profile\")\r\n\t\t\r\n\t\telse:\r\n\t\t\tusernameBox.delete(0, tk.END)\r\n\t\t\tusernameBox.insert(0, original)\r\n\t\t\tsearch = driver.find_elements(By.CLASS_NAME, 'value')\r\n\t\t\t\r\n\t\t\tif len(search) > 4:\r\n\t\t\t\tprint(\"Wins:\", search[0].text)\r\n\t\t\t\twinsBox.delete(0, tk.END)\r\n\t\t\t\twinsBox.insert(0, search[0].text)\r\n\t\t\t\t\r\n\t\t\t\tprint(\"Win %:\", search[1].text)\r\n\t\t\t\twinPercentageBox.delete(0, tk.END)\r\n\t\t\t\twinPercentageBox.insert(0, search[1].text)\r\n\t\t\t\t\r\n\t\t\t\tprint(\"Kills:\", search[2].text)\r\n\t\t\t\tkillsBox.delete(0, tk.END)\r\n\t\t\t\tkillsBox.insert(0, search[2].text)\r\n\t\t\t\t\r\n\t\t\t\tprint(\"K/D:\", search[3].text)\r\n\t\t\t\tKD_Box.delete(0, tk.END)\r\n\t\t\t\tKD_Box.insert(0, search[3].text)\r\n\t\r\n\t\t\t\tprint(\"Score/min:\", search[4].text)\r\n\t\t\t\tscoreMinBox.delete(0, tk.END)\r\n\t\t\t\tscoreMinBox.insert(0, search[4].text)\r\n\t\t\t\t\r\n\t\t\telse:\r\n\t\t\t\tprint(\"Incorrect name or private profile\")\r\n\t\t\t\t\r\n\t\t\t\tusernameBox.delete(0, tk.END)\r\n\t\t\t\tusernameBox.insert(0, original)\r\n\t\t\t\t\r\n\t\t\t\twinsBox.delete(0, tk.END)\r\n\t\t\t\twinsBox.insert(0, \"-----\")\r\n\t\t\t\t\r\n\t\t\t\twinPercentageBox.delete(0, tk.END)\r\n\t\t\t\twinPercentageBox.insert(0, \"-----\")\r\n\t\t\t\t\r\n\t\t\t\tkillsBox.delete(0, tk.END)\r\n\t\t\t\tkillsBox.insert(0, \"-----\")\r\n\t\t\t\t\r\n\t\t\t\tKD_Box.delete(0, tk.END)\r\n\t\t\t\tKD_Box.insert(0, \"-----\")\r\n\t\t\t\t\r\n\t\t\t\tscoreMinBox.delete(0, tk.END)\r\n\t\t\t\tscoreMinBox.insert(0, \"-----\")\r\n\t\t\t\t\r\n\t\telapsed = time.time() - t\r\n\t\tprint(elapsed, \"Time to webscrape\")\r\n\t\twebscrapeBox.delete(0, tk.END)\r\n\t\twebscrapeBox.insert(0, str(round(elapsed, 2)) + \" seconds\")\r\n\t\t\r\n\t\tdriver.close() \r\n\t\tdriver.quit()\r\n", "description": null, "category": "webscraping", "imports": ["from pathlib import Path\r", "from tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage\r", "import tkinter as tk\r", "import threading\r", "import pyautogui\r", "import time\r", "import cv2\r", "from pytesseract import *\r", "from PIL import Image\r", "from selenium import webdriver\r", "from webdriver_manager.chrome import ChromeDriverManager\r", "from selenium.webdriver.chrome.service import Service\r", "from selenium.webdriver.common.by import By\r", "import mss\r", "import mss.tools\r"]}, {"term": "def", "name": "frunBot", "data": "\tdef runBot():\r\n\t\tglobal run\r\n\t\tglobal lock\r\n\t\tglobal monitor\r\n\t\tprint(\"Waiting for lock\")\r\n\t\tlock.acquire()\r\n\t\tmonitor.wait()\r\n\t\tlock.release()\r\n\t\tprint(\"Acquired the lock!\")\r\n\t\t\r\n\t\ttime.sleep(0.5)\r\n\t\t\r\n\t\twhile run:\r\n\t\t\tcoords = pyautogui.locateOnScreen('1ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n\t\t\tcoords1 = pyautogui.locateOnScreen('2ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n\t\t\tcoords2 = pyautogui.locateOnScreen('3ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n\t\t\tcoords3 = pyautogui.locateOnScreen('6ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n\t\r\n\t\t\tif coords or coords1 or coords2 or coords3:\r\n\t\t\t\twith mss.mss() as sct:\r\n\t\t\t\t\t# The screen part to capture\r\n\t\t\t\t\tregion = {'top': 938, 'left': 696, 'width': 339, 'height': 35}\r\n\t\t\t\t\r\n\t\t\t\t\t# Grab the data\r\n\t\t\t\t\timg = sct.grab(region)\r\n\t\t\t\t\r\n\t\t\t\t\t# Save to the picture file\r\n\t\t\t\t\tmss.tools.to_png(img.rgb, img.size, output='screenshot.png')\r\n\t\t\t\t\r\n\t\t\t\t# Enlarge image for more accurate results\r\n\t\t\t\timg = cv2.imread('screenshot.png')\r\n\t\t\t\timg = cv2.resize(img, dsize=(526, 66), interpolation=cv2.INTER_CUBIC)\r\n\t\t\t\t\r\n\t\t\t\t# Save image and remove any extra spaces\r\n\t\t\t\tcv2.imwrite('screenshot.png', img)\r\n\t\t\t\tresult = pytesseract.image_to_string(img) # Result variable is what the program thinks the username is after conversion to string\r\n\t\t\t\tresult = result.rstrip()\r\n\t\t\t\t\r\n\t\t\t\t# If the username is empty or doesn't have a hashtag, keep scanning/try again\r\n\t\t\t\tif result == '' or '#' not in result:\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t\t\r\n\t\t\t\t# Special condition for when the username has a clantag, remove the clantag\r\n\t\t\t\tif ']' in result:\r\n\t\t\t\t\tslicing = result.find(']')\r\n\t\t\t\t\tnewUsername = result[slicing+1:]\r\n\t\t\t\t\tnewUsername = newUsername.replace('#', '%')\r\n\t\t\t\t\t\r\n\t\t\t\t\t# cod tracker adds a '23' to the links after the percent sign, do the same here\r\n\t\t\t\t\tpcent = newUsername.index('%')\r\n\t\t\t\t\tnewUsername = newUsername[:pcent+1] + '23' + newUsername[pcent+1:]\r\n\t\t\t\t\t\r\n\t\t\t\t\twebscrape(newUsername, result)\r\n\t\t\t\t\r\n\t\t\t\t# If username doesn't have clantag then it's ready for webscraping\r\n\t\t\t\telif ']' not in result:\r\n\t\t\t\t\tnewUsername = result.replace('#', '%')\r\n\t\t\t\t\t\r\n\t\t\t\t\t# cod tracker adds a '23' to the links after the percent sign, do the same here\r\n\t\t\t\t\tpcent = newUsername.index('%')\r\n\t\t\t\t\tnewUsername = newUsername[:pcent+1] + '23' + newUsername[pcent+1:]\r\n\t\t\t\t\t\r\n\t\t\t\t\twebscrape(newUsername, result)\r\n\t\t\t\t\t\r\n\t\t\telse:\r\n\t\t\t\tcontinue\r\n", "description": null, "category": "webscraping", "imports": ["from pathlib import Path\r", "from tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage\r", "import tkinter as tk\r", "import threading\r", "import pyautogui\r", "import time\r", "import cv2\r", "from pytesseract import *\r", "from PIL import Image\r", "from selenium import webdriver\r", "from webdriver_manager.chrome import ChromeDriverManager\r", "from selenium.webdriver.chrome.service import Service\r", "from selenium.webdriver.common.by import By\r", "import mss\r", "import mss.tools\r"]}, {"term": "def", "name": "fstopBot", "data": "\tdef stopBot():\r\n\t\tglobal run\r\n\t\trun = False\r\n", "description": null, "category": "webscraping", "imports": ["from pathlib import Path\r", "from tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage\r", "import tkinter as tk\r", "import threading\r", "import pyautogui\r", "import time\r", "import cv2\r", "from pytesseract import *\r", "from PIL import Image\r", "from selenium import webdriver\r", "from webdriver_manager.chrome import ChromeDriverManager\r", "from selenium.webdriver.chrome.service import Service\r", "from selenium.webdriver.common.by import By\r", "import mss\r", "import mss.tools\r"]}, {"term": "def", "name": "fstartBot", "data": "\tdef startBot():\r\n\t\tglobal run\r\n\t\tglobal lock\r\n\t\tglobal monitor\r\n\t\tprint(\"Starting the bot!\")\r\n\t\trun = True\r\n\t\tlock.acquire()\r\n\t\tmonitor.notify()\r\n\t\tlock.release()\r\n", "description": null, "category": "webscraping", "imports": ["from pathlib import Path\r", "from tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage\r", "import tkinter as tk\r", "import threading\r", "import pyautogui\r", "import time\r", "import cv2\r", "from pytesseract import *\r", "from PIL import Image\r", "from selenium import webdriver\r", "from webdriver_manager.chrome import ChromeDriverManager\r", "from selenium.webdriver.chrome.service import Service\r", "from selenium.webdriver.common.by import By\r", "import mss\r", "import mss.tools\r"]}], [{"term": "def", "name": "processwebdata", "data": "def processwebdata():\r\n\r\n\tprint(\"entering process webdata in scheduler\")\r\n\tevent_list = WebScrape.scrapeweb()\r\n\ttime.sleep(2)\r\n\tDataProcess.saveeventdata(event_list)\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["from .scrape import WebScrape \r", "import schedule \r", "from .service import DataProcess\r", "import time\r"]}, {"term": "def", "name": "deletePastEvents", "data": "def deletePastEvents():\r\n\r\n\tprint('begin delete')\r\n\r\n\tDataProcess.deletePastEvents()\r\n\tprint(\"finish delete\")\r\n\r\n\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from .scrape import WebScrape \r", "import schedule \r", "from .service import DataProcess\r", "import time\r"]}, {"term": "def", "name": "startScehdule", "data": "def startScehdule():\r\n\r\n\r\n\twhile True: \r\n\r\n\t\t# Checks whether a scheduled task  \r\n\t\t# is pending to run or not \r\n\t\tschedule.run_pending() \r\n\t\ttime.sleep(1)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from .scrape import WebScrape \r", "import schedule \r", "from .service import DataProcess\r", "import time\r"]}], [{"term": "def", "name": "Webscrape_divID", "data": "def Webscrape_divID(URL, div_id):\r\n\t'''This function scrapes the website from the URL given to it.\\\r\n\tIt collects the entire website data and stores the data in the html format\\\r\n\t\tAlso it extracts the data segment based on the div_id'''\r\n\t\r\n\tHEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n\t\r\n\t# Making the HTTP Request\r\n\twebpage = requests.get(URL, headers=HEADERS)\r\n  \r\n\t# Creating the Soup Object containing all data\r\n\tsoup = BeautifulSoup(webpage.content, \"html.parser\")\r\n\r\n\tresults = soup.find(id=div_id)\r\n\tst.text(\"Scraping Web......Done!\\n\")\r\n\tst.write('**Exracted text to be analyzed:**')   \r\n\tst.write(results.get_text())\r\n\t\r\n\treturn results.get_text()\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Webscrape_Classname", "data": "def Webscrape_Classname(URL, classname):\r\n\t\r\n\t'''This function scrapes the website from the URL given to it.\\\r\n\tIt collects the entire website data and stores the data in the html format \\\r\n\tAlso it extracts the data segment based on the classname'''\r\n\t\r\n\tHEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n\t\r\n\t# Making the HTTP Request\r\n\twebpage = requests.get(URL, headers=HEADERS)\r\n  \r\n\t# Creating the Soup Object containing all data\r\n\tsoup = BeautifulSoup(webpage.content, \"html.parser\")\r\n   \r\n\tresults = soup.find(\"div\", class_= classname)\r\n\tst.text(\"Scraping Web......Done!\\n\")\r\n\tst.write('**Exracted text to be analyzed:**')   \r\n\tst.write(results.get_text())\r\n\t\r\n\treturn results.get_text()\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Word_Frequency", "data": "def Word_Frequency(spacy_text):\r\n\t'''Visualize the Noun and Verb frequencies in the extracted text'''\r\n\t\r\n\t#Filtering for nouns and verbs only\r\n\tnouns_verbs = [token.text for token in spacy_text if token.pos_ in ('NOUN', 'VERB')]\r\n\t\r\n\tcv = CountVectorizer()\r\n\tX = cv.fit_transform(nouns_verbs)\r\n\tsum_words = X.sum(axis=0)\r\n\twords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\r\n\twords_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n\twf_df = pd.DataFrame(words_freq)\r\n\twf_df.columns = ['word', 'count']\r\n\t\r\n\tsns.barplot(x = 'count', y = 'word', data = wf_df, palette=\"GnBu_r\")\r\n\tst.pyplot()\r\n\t\r\n\tst.write(\"**Word Count(Noun & Verb) of the Extracted Text:\\n**\")\r\n\tst.write(wf_df)\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "POS_Tag", "data": "def POS_Tag(data):\r\n\t'''Tag Parts of Speech to the Extracted data and visualize'''\r\n\t\r\n\tsts.visualize_parser(data)\r\n\tsts.visualize_ner(data, labels=nlp.get_pipe(\"ner\").labels)\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content1", "data": "def Replace_Content1(token):\r\n\t'''Find and replace selected tokens for Usecase 1'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'John Dooley':\r\n\t\treturn '[Your Name]\\n'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'John':\r\n\t\treturn '\\n[Your Name]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jennifer':\r\n\t\treturn \"[Your Manager's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn '[Date]'\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace1", "data": "def FindnReplace1(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content1, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content1a", "data": "def Replace_Content1a(token):\r\n\t'''Find and replace selected tokens for Usecase 1a'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'William J. Jones':\r\n\t\treturn '[Your Name]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'William Jones':\r\n\t\treturn '[Your Signature on the Hard copy]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'ORG':\r\n\t\treturn '[Your Company Name]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Perez':\r\n\t\treturn \"[Your Manager's Name]\"\r\n\tif token.ent_iob !=0 and token.ent_type_ == 'DATE' and token.text == 'between now and September 1, 2013':\r\n\t\treturn \"before [Your task completion timeline]\"\r\n\tif token.ent_iob !=0 and token.ent_type_ == 'DATE' and token.text == 'September 1, 2013 through September 21, 2013':\r\n\t\treturn \"[From Date] through [To Date]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'GPE':\r\n\t\treturn '[Place/Country name]'\r\n\tif token.text == \"Mr.\":\r\n\t\treturn '\\b'\r\n\tif token.text == \"cruise\":\r\n\t\treturn '\\b'\r\n\tif token.text == \"wife\":\r\n\t\treturn \"[self/companion/friend]\"\r\n\t\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace1a", "data": "def FindnReplace1a(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content1a, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content2", "data": "def Replace_Content2(token):\r\n\t'''Find and replace selected tokens for Usecase 2'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'Joe' or token.text == 'Joe Brown'):\r\n\t\treturn '[Your Name]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Steve':\r\n\t\treturn \"[Your Manager's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn '[Sickness Date]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'ORG':\r\n\t\treturn '[Hospital/Clinic Name]'\r\n\tif token.text == 'Joejoe.brown765@email.com555':\r\n\t\treturn '\\n[Your Name]\\n[Your Email ID]'\r\n\tif token.text == '555':\r\n\t\treturn '\\n[Your Contact'\r\n\tif token.text == '5555':\r\n\t\treturn 'Number]'\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace2", "data": "def FindnReplace2(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content2, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content2a", "data": "def Replace_Content2a(token):\r\n\t'''Find and replace selected tokens for Usecase 2a'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jane':\r\n\t\treturn '\\n[Your Name]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jane Doe':\r\n\t\treturn '[Your Name]'\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'Patricia' or token.text == 'Tom'):\r\n\t\treturn \"[Your Colleague's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE' and token.text == 'Friday':\r\n\t\treturn \"on [Meeting day]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn '[Sickness Date]'\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace2a", "data": "def FindnReplace2a(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content2a, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content3", "data": "def Replace_Content3(token):\r\n\t'''Find and replace selected tokens for Usecase 3'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Smith':\r\n\t\treturn \"[Your Colleague's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonny\\n':\r\n\t\treturn \"\\n [Your Name]\\n [Your Designation]\"\r\n\tif token.text == \"Formal\":\r\n\t\treturn 'Formal Birthday Wishes'\r\n\tif token.text == \"Mr.\":\r\n\t\treturn ''\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace3", "data": "def FindnReplace3(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content3, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content3a", "data": "def Replace_Content3a(token):\r\n\t'''Find and replace selected tokens for Usecase 3a'''\r\n\t\r\n\tif token.text == \"company!Have\":\r\n\t\treturn 'company!\\n Have'\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace3a", "data": "def FindnReplace3a(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content3a, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content4", "data": "def Replace_Content4(token):\r\n\t'''Find and replace selected tokens for Usecase 4'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Paul JonesPhoneEmail':\r\n\t\treturn \"\\n\\n Regards,\\n [Your Name]\\n [Your Contact No.]\\n [Your Email ID]\\n\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn \"[Years of Experience]\"\r\n\tif token.text == \"Address\":\r\n\t\treturn \"\"\r\n\tif token.text == \"store\":\r\n\t\treturn \"\\b\"\r\n\tif token.text == \"retail\":\r\n\t\treturn \"\\b\"\r\n\t\t\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace4", "data": "def FindnReplace4(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content4, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content4a", "data": "def Replace_Content4a(token):\r\n\t'''Find and replace selected tokens for Usecase 4a'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'Mary Garcia12' or token.text == 'Mary Garcia'):\r\n\t\treturn \"[Your Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Lee':\r\n\t\treturn \"[Hiring Manager's Name]\"\r\n\tif token.text == \"Lee\":\r\n\t\treturn \"\\n [Hiring Manager's Name]\\n\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Franklin Lee':\r\n\t\treturn \"To: [Hiring Manager's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE' and token.text == 'five years':\r\n\t\treturn \"[Your experience in years]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn \"[Mailing Date]\\n\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'ORG' and token.text == 'CBI Industries39':\r\n\t\treturn \"[Company Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'ORG':\r\n\t\treturn \"[Your leaving Company name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'GPE' and token.text == 'AvenueTownville':\r\n\t\treturn \"[Building No., Street Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'GPE' and token.text == 'New Hampshire':\r\n\t\treturn \"\\n [Area Name, Town Name, Pincode]\\n\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'GPE':\r\n\t\treturn \"[Campus Name]\"\r\n\tif token.text == 'Rogers':\r\n\t\treturn \"\\n\"\r\n\tif token.text == \"Mr.\":\r\n\t\treturn \"\"\r\n\tif token.text == \"03060\":\r\n\t\treturn \"\\n\"\r\n\tif token.text == \"Sincerely\":\r\n\t\treturn \"\\n\\n Sincerely\"\r\n\tif token.text == \"Signature\":\r\n\t\treturn \"\\n [Signature]\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace4a", "data": "def FindnReplace4a(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content4a, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content5", "data": "def Replace_Content5(token):\r\n\t'''Find and replace selected tokens for Usecase 5'''\r\n\t\r\n\tif token.text == \",\":\r\n\t\treturn \"\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Amy':\r\n\t\treturn \"[Your Colleague's Name],\\n\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonathan':\r\n\t\treturn \"\\n\\n [Your Name]\\n [Your Contact number]\"\r\n\tif token.text == \"Sincerely\":\r\n\t\treturn \"\\n Sincerely,\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn \"[Timeline] and [Reason for Appreciation]\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace5", "data": "def FindnReplace5(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content5, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content5a", "data": "def Replace_Content5a(token):\r\n\t'''Find and replace selected tokens for Usecase 5a'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'John':\r\n\t\treturn \"[Your Colleague's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Samantha':\r\n\t\treturn \"\\n [Your Name]\\n [Your Designation]\"\r\n\tif token.text == \"Best\":\r\n\t\treturn \"Best Regards\"\r\n\tif token.text == 'project':\r\n\t\treturn \"[Work of Appreciation]\"\r\n\tif token.text == \"Dear\":\r\n\t\treturn \"\\nDear\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace5a", "data": "def FindnReplace5a(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content5a, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content6", "data": "def Replace_Content6(token):\r\n\t'''Find and replace selected tokens for Usecase 6'''\r\n\t\r\n\tif token.text == \",\":\r\n\t\treturn \"\"\r\n\tif token.text == \"Hello\":\r\n\t\treturn \"Dear [Sender's Name],\\n\"\r\n\tif token.text == \"COLLEAGUE\":\r\n\t\treturn \"[Your Colleague's Name]\"\r\n\tif token.text == \"Regards\":\r\n\t\treturn \"\\n Regards,\"\r\n\tif token.text == \"NAME\":\r\n\t\treturn \"\\n\\n [Your Name]\"\r\n\tif token.text == \"do\":\r\n\t\treturn \"don't\"\r\n\tif token.text == \"n\u2019t\":\r\n\t\treturn \"\\b\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace6", "data": "def FindnReplace6(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content6, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content6a", "data": "def Replace_Content6a(token):\r\n\t'''Find and replace selected tokens for Usecase 6a'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jane Doe':\r\n\t\treturn \"[Your Colleague's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'John Smith':\r\n\t\treturn \"\\n [Your Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n\t\treturn \"[Your Return Date]\"\r\n\tif token.text == \"Thank\":\r\n\t\treturn \"Dear [Sender's Name],\\n\\n\\t Thank\"\r\n\tif token.text == \"She\":\r\n\t\treturn \"He/She\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace6a", "data": "def FindnReplace6a(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content6a, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content7", "data": "def Replace_Content7(token):\r\n\t'''Find and replace selected tokens for Usecase 7'''\r\n\t\r\n\tif token.text == \",\":\r\n\t\treturn \"\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Sam':\r\n\t\treturn \"[Your Partner's Name],\\n\\n\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonathan':\r\n\t\treturn \"\\n\\n[Your Name]\\n[Your Contact Number]\"\r\n\tif token.text == \"Please\":\r\n\t\treturn \"\\nPlease\"\r\n\tif token.text == \"Thank\":\r\n\t\treturn \"\\nThank\"\r\n\tif token.text == \"again!Sincerely\":\r\n\t\treturn \"\\b\\b, again!\\nSincerely,\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace7", "data": "def FindnReplace7(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content7, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "Replace_Content7a", "data": "def Replace_Content7a(token):\r\n\t'''Find and replace selected tokens for Usecase 7a'''\r\n\t\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'James':\r\n\t\treturn \"[Your Colleague's Name]\"\r\n\tif token.ent_iob != 0 and token.ent_type_ == 'DATE' and token.text == 'a whole year':\r\n\t\treturn \"[Number of years]\"\r\n\tif token.text == \"at\":\r\n\t\treturn \"\"\r\n\treturn token.text\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "FindnReplace7a", "data": "def FindnReplace7a(nlp_doc):\r\n\twith nlp_doc.retokenize() as retokenizer:\r\n\t\tfor ent in nlp_doc.ents:\r\n\t\t\tretokenizer.merge(ent)\r\n\ttokens = map(Replace_Content7a, nlp_doc)\r\n\treturn ' '.join(tokens)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL1", "data": "\tdef Process_URL1():\r\n\t\textract1 = Webscrape_divID(URL1, div_id1)\r\n\t\t\r\n\t\tphr = \"As we discussed\"\r\n\t\ttemp = extract1.replace(str(phr), \"\\n\\tAs we discussed\")\r\n\t\tphr1 = \"Dear\"\r\n\t\ttemp2 = temp.replace(str(phr1), \"\\nDear\") \r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text1 = nlp(temp2)\r\n\t\r\n\t\tWord_Frequency(spacy_text1)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text1)\r\n\t\tsts.visualize_tokens(spacy_text1, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\r\n\t\t# Generate Template\r\n\t\ttemp2 = FindnReplace1(spacy_text1)\r\n\t\t\r\n\t\tphr2 = \"I plan\"\r\n\t\ttemp3 = temp2.replace(str(phr2), \"\\nI plan\")\r\n\t\tphr3 = \"I would also\"\r\n\t\ttemp4 = temp3.replace(str(phr3), \"\\nI would also\")\r\n\t\tphr4 = \"Thank you\"\r\n\t\ttemp5 = temp4.replace(str(phr4), \"\\nThank you\")\r\n\t\tphr5 = \"Best\"\r\n\t\ttemplate1 = temp5.replace(str(phr5), \"\\n\\nBest\")\r\n\t\tst.subheader(\"**Your Template for Vacation Leave Email**\\n\")\r\n\t\tst.text(template1)\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL2", "data": "\tdef Process_URL2():\r\n\t\textract2 = Webscrape_Classname(URL2, classname2)\r\n\t\t\r\n\t\tphr1 = \"Yours sincerely,\"\r\n\t\ttemp1 = extract2.replace(str(phr1), \"\\nYours sincerely,\\n\")\r\n\t\tphr2 = \"(555)-555-5555\"\r\n\t\ttemp2 = temp1.replace(str(phr2), \"[Your Contact Number]\")\r\n\t\tphr3 = \"my leave\"\r\n\t\ttemp3 = temp2.replace(str(phr3), \"\\b\\b\\b my leave\")\r\n\t\tphr4 = \"I think you\"\r\n\t\ttemp4 = temp3.replace(str(phr4), \"\\nI think you\")\r\n\t\tphr5 = \"I am planning\"\r\n\t\ttemp5 = temp4.replace(str(phr5), \"\\n\\tI am planning\")\r\n\t\tphr6 = \"If there are\"\r\n\t\ttemp6 = temp5.replace(str(phr6), \"\\nIf there are\")\r\n\t\tphr7 = \"I am writing\"\r\n\t\ttemp7 = temp6.replace(str(phr7), \"\\n\\tI am writing\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text1a = nlp(temp7)\r\n\t\r\n\t\tWord_Frequency(spacy_text1a)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text1a)\r\n\t\tsts.visualize_tokens(spacy_text1a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\t   \r\n\t\t# Generate Template\r\n\t\ttemp = FindnReplace1a(spacy_text1a)\r\n\t\t\r\n\t\tphr4 = \"Assistant Manager\"\r\n\t\ttemplate1a = temp.replace(str(phr4), \"[Your Designation]\")\r\n\t\t\r\n\t\tst.subheader(\"**Your Template for Vacation Leave Email**\\n\")\r\n\t\tst.text(template1a)\r\n\t\t\t\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL1", "data": "\tdef Process_URL1():\r\n\t\textract1 = Webscrape_divID(URL1, div_id1)\r\n\t\t\r\n\t\tphr = \"Dear\"\r\n\t\ttemp = extract1.replace(str(phr), \"\\n\\nDear\") \r\n\t\tphr1 = \"I am writing\"\r\n\t\ttemp1 = temp.replace(str(phr1), \"\\n\\t I am writing\")\r\n\t\tphr2 = \"Please\"\r\n\t\ttemp2 = temp1.replace(str(phr2), \"\\nPlease\")\r\n\t\tphr3 = \"Regards\"\r\n\t\ttemp2 = temp2.replace(str(phr3), \"\\n\\nRegards\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text2 = nlp(temp2)\r\n\t\r\n\t\tWord_Frequency(spacy_text2)\r\n\t\r\n\t\tPOS_Tag(spacy_text2)\r\n\t\tsts.visualize_tokens(spacy_text2, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL2", "data": "\tdef Process_URL2():\r\n\t\textract2 = Webscrape_divID(URL2, div_id2)\r\n\t\t\r\n\t\tphr = \"Dear\"\r\n\t\ttemp = extract2.replace(str(phr), \"\\n\\nDear\")\r\n\t\tphr1 = \"Supervisor Name\"\r\n\t\ttemp1 = temp.replace(str(phr1), \"[Your Manager's Name]\")\r\n\t\tphr2 = \"I've asked\"\r\n\t\ttemp2 = temp1.replace(str(phr2), \"\\nI've asked\")\r\n\t\tphr3 = \"I've come\"\r\n\t\ttemp3 = temp2.replace(str(phr3), \"\\n\\tI've come\")\r\n\t\tphr4 = \"I will try\"\r\n\t\ttemp4 = temp3.replace(str(phr4), \"\\nI will try\")\r\n\t\tphr5 = \"Thank you\"\r\n\t\ttemp5 = temp4.replace(str(phr5), \"\\n\\nThank you\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text2a = nlp(temp5)\r\n\t\r\n\t\tWord_Frequency(spacy_text2a)\r\n\t\r\n\t\tPOS_Tag(spacy_text2a)\r\n\t\tsts.visualize_tokens(spacy_text2a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\t\r\n\t\t# Generate Template\r\n\t\ttemplate2a = FindnReplace2a(spacy_text2a)\r\n\t\tst.subheader(\"**Your Template for Sick Leave Email**\\n\")\r\n\t\tst.text(template2a)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL1", "data": "\tdef Process_URL1():\r\n\t\textract1 = Webscrape_Classname(URL1, classname1)\r\n\t\t\r\n\t\tphr1 = \"I am\"\r\n\t\ttemp1 = extract1.replace(str(phr1),\"\\n\\tI am\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text3 = nlp(temp1)\r\n\t\r\n\t\tWord_Frequency(spacy_text3)\r\n\t\r\n\t\tPOS_Tag(spacy_text3)\r\n\t\tsts.visualize_tokens(spacy_text3, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\r\n\t\t# Generate Template\r\n\t\ttemplate3 = FindnReplace3(spacy_text3)\r\n\t\t \r\n\t\tst.subheader(\"**Your Template for Birthday Wishes Email**\\n\")\r\n\t\tst.text(template3)\r\n\t\t\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL2", "data": "\tdef Process_URL2():\r\n\t\textract2 = Webscrape_divID(URL2, div_id2)\r\n\t\r\n\t\tphr1 = \"birthday!\"\r\n\t\ttemp1 = extract2.replace(str(phr1), \"birthday!\\n\\nSincerely,\\n[Your Name]\\n[Your Designation]\")\r\n\t\t\r\n\t\tphr2 = \"Happy\"\r\n\t\ttemp2 = temp1.replace(str(phr2), \"Dear [Your Colleague's Name],\\n\\tHappy\")\r\n\t\t\r\n\t\tphr3 = \"Your positivity\"\r\n\t\ttemp3 = temp2.replace(str(phr3), \"\\nYour positivity\")\r\n\t\t\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text3a = nlp(temp3)\r\n\t\r\n\t\tWord_Frequency(spacy_text3a)\r\n\t\r\n\t\tPOS_Tag(spacy_text3a)\r\n\t\tsts.visualize_tokens(spacy_text3a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\r\n\t\t# Generate Template\r\n\t\ttemplate3a = FindnReplace3a(spacy_text3a)\r\n\t\t \r\n\t\tst.subheader(\"**Your Template for Birthday Wishes Email**\\n\")\r\n\t\tst.text(template3a)\r\n\t\t\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL1", "data": "\tdef Process_URL1():\r\n\t\textract1 = Webscrape_divID(URL1, div_id1)\r\n\t\r\n\t\tphr1 = \"Store Manager Position\"\r\n\t\ttemp1 = extract1.replace(str(phr1),\"[Role you are applying for]\")\r\n\t\t\r\n\t\tphr2 = \"Your Name\"\r\n\t\ttemp2 = temp1.replace(str(phr2), \"[Your Name]\")\r\n\t\t\r\n\t\tphr3 = \"Store Manager position\"\r\n\t\ttemp3 = temp2.replace(str(phr3),\"[Role you are applying for]\")\r\n\t\t\r\n\t\tphr4 = \"Payroll management, scheduling, reports, and inventory control expertise\"\r\n\t\ttemp4 = temp3.replace(str(phr4),\"\")\r\n\t\t\r\n\t\tphr5 = \"Extensive work with visual standards and merchandising high-ticket items\"\r\n\t\ttemp5 = temp4.replace(str(phr5),\"\")\r\n\t\t\r\n\t\tphr6 = \"retail management\"\r\n\t\ttemp6 = temp5.replace(str(phr6),\"[Your previous role]\")\r\n\t\t\r\n\t\tphr7 = \"XYZ Company:\"\r\n\t\ttemp7 = temp6.replace(str(phr7),\"[Company name you are applying for]:\\n[Your Skill Set]...for example\")\r\n\t\t\r\n\t\tphr8 = \"I read\"\r\n\t\ttemp8 = temp7.replace(str(phr8), \"\\n\\tI read\")\r\n\t\t\r\n\t\tphr9 = \"Dear\"\r\n\t\ttemp9 = temp8.replace(str(phr9), \"\\n\\nDear\")\r\n\t\t\r\n\t\tphr10 = \"I can offer\"\r\n\t\ttemp10 = temp9.replace(str(phr10), \"\\nI can offer\")\r\n\t\t\r\n\t\tphr11 = \"Over\"\r\n\t\ttemp11 = temp10.replace(str(phr11), \"\\nOver\")\r\n\t\t\r\n\t\tphr12 = \"Ability\"\r\n\t\ttemp12 = temp11.replace(str(phr12), \"\\nAbility\")\r\n\t\t\r\n\t\tphr13 = \"In addition\"\r\n\t\ttemp13 = temp12.replace(str(phr13), \"\\n\\nIn addition\")\r\n\t\t\r\n\t\tphr14 = \"My broad\"\r\n\t\ttemp14 = temp13.replace(str(phr14), \"\\nMy broad\")\r\n\t\t\r\n\t\tphr15 = \"I look\"\r\n\t\ttemp15 = temp14.replace(str(phr15), \"\\nI look\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text4 = nlp(temp15)\r\n\t\t\r\n\t\tWord_Frequency(spacy_text4)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text4)\r\n\t\tsts.visualize_tokens(spacy_text4, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\t\r\n\t\t# Generate Template\r\n\t\ttemplate4 = FindnReplace4(spacy_text4)\r\n\t\t\t \r\n\t\tst.subheader(\"**Your Template for Cover Letter Email**\\n\")\r\n\t\tst.text(template4)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL2", "data": "\tdef Process_URL2():\r\n\t\textract2 = Webscrape_divID(URL2, div_id2)\r\n\t\t\r\n\t\tphr = \"Dear\"\r\n\t\ttemp = extract2.replace(str(phr), \"\\nDear\")\r\n\t\t\r\n\t\tphr1 = \"03060555-555-5555mary.garcia@email.com\"\r\n\t\ttemp1 = temp.replace(str(phr1), \"[Your Contact Number and Email ID]\\n\")\r\n\t\t\r\n\t\tphr2 = \"operations assistant/associate\"\r\n\t\ttemp2 = temp1.replace(str(phr2), \"[Role in which you have experience]\")\r\n\t\t\r\n\t\tphr3 = \"operations assistant\"\r\n\t\ttemp3 = temp2.replace(str(phr3), \"[Role you are applying for]\")\r\n\t\t\r\n\t\tphr4 = \"orders, resolved customer issues, ordered supplies, and prepared reports\"\r\n\t\ttemp4 = temp3.replace(str(phr4), \"[Your responsibilities at your leaving company]\")\r\n\t\t\r\n\t\tphr5 = \"bookkeeping, data entry, and sales support\"\r\n\t\ttemp5 = temp4.replace(str(phr5), \"[Your prior job nature]\")\r\n\t\t\r\n\t\tphr6 = \"Strong communication skills, in person, in writing, and on the phone\"\r\n\t\ttemp6 = temp5.replace(str(phr6), \"\\n[Your Skillset]\")\r\n\t\t\r\n\t\tphr7 = \"Excellent attention to detail and organization skills\"\r\n\t\ttemp7 = temp6.replace(str(phr7), \"-\")\r\n\t\t\r\n\t\tphr8 = \"Top-notch customer service\"\r\n\t\ttemp8 = temp7.replace(str(phr8), \"-\")\r\n\t\t\r\n\t\tphr9 = \"Experience in the industry and passion for the product\"\r\n\t\ttemp9 = temp8.replace(str(phr9), \"-\")\r\n\t\t\r\n\t\tphr10 = \"Adept at all the usual professional software, including Microsoft Office Suite\"\r\n\t\ttemp10 = temp9.replace(str(phr10), \"-\")\r\n\t\t\r\n\t\tphr11 = \"I\u2019ve included\"\r\n\t\ttemp11 = temp10.replace(str(phr11), \"\\n\\nI\u2019ve included\")\r\n\t\t\r\n\t\tphr12 = \"Basically\"\r\n\t\ttemp12 = temp11.replace(str(phr12), \"\\nBasically\")\r\n\t\t\r\n\t\tphr13 = \"I was excited\"\r\n\t\ttemp13 = temp12.replace(str(phr13), \"\\n\\tI was excited\")\r\n\t\t\r\n\t\tphr14 = \"CBI Industries39 Main\"\r\n\t\ttemp14 = temp13.replace(str(phr14), \"\")\r\n\t\t\r\n\t\tphr15 = \"In my most\"\r\n\t\ttemp15 = temp14.replace(str(phr15), \"\\nIn my most\")\r\n\t\t\r\n\t\tphr16 = \"My other\"\r\n\t\ttemp16 = temp15.replace(str(phr16), \"\\nMy other\")\r\n\t\t\r\n\t\tphr17 = \"(hard copy letter)\"\r\n\t\ttemp17 = temp16.replace(str(phr17), \"\\n\")\r\n\t\t\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text4a = nlp(temp17)\r\n\t\r\n\t\tWord_Frequency(spacy_text4a)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text4a)\r\n\t\tsts.visualize_tokens(spacy_text4a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\t\r\n\t\t# Generate Template\r\n\t\ttemplate4a = FindnReplace4a(spacy_text4a)\r\n\t\t\t \r\n\t\tst.subheader(\"**Your Template for Cover Letter Email**\\n\")\r\n\t\tst.text(template4a)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL1", "data": "\tdef Process_URL1():\r\n\t\textract1 = Webscrape_divID(URL1, div_id1)\r\n\t\r\n\t\tphr1 = \"You showed\"\r\n\t\ttemp1 = extract1.replace(str(phr1),\"\\nYou showed\")\r\n\t\t\r\n\t\tphr2 = \"I am\"\r\n\t\ttemp2 = temp1.replace(str(phr2),\"\\nI am\")\r\n\t\t\r\n\t\tphr3 = \"Thank you\"\r\n\t\ttemp3 = temp2.replace(str(phr3),\"\\n\\tThank you\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text5 = nlp(temp3)\r\n\t\t\r\n\t\tWord_Frequency(spacy_text5)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text5)\r\n\t\tsts.visualize_tokens(spacy_text5, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\t\r\n\t\t# Generate Template\t\r\n\t\ttemplate5 = FindnReplace5(spacy_text5)\r\n\t\t\r\n\t\tst.subheader(\"**Your Template for Employee Work Appreciation Email**\\n\")\r\n\t\tst.text(template5)\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL2", "data": "\tdef Process_URL2():\r\n\t\textract2 = Webscrape_divID(URL2, div_id2)\r\n\t\r\n\t\tphr1 = \"Subject Line: Thank You Very Much!\"\r\n\t\ttemp1 = extract2.replace(str(phr1), \"Subject Line: Thank You Very Much!\\n\")\r\n\t\t\r\n\t\tphr2 = \"I wanted\"\r\n\t\ttemp2 = temp1.replace(str(phr2), \"\\n\\tI wanted\")\r\n\t\t\r\n\t\tphr3 = \"I know how\"\r\n\t\ttemp3 = temp2.replace(str(phr3), \"\\nI know how\")\r\n\t\t\r\n\t\tphr4 = \"You are a\"\r\n\t\ttemp4 = temp3.replace(str(phr4), \"\\nYou are a\")\r\n\t\t\r\n\t\tphr5 = \"Best\"\r\n\t\ttemp5 = temp4.replace(str(phr5), \"\\n\\nBest\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text5a = nlp(temp5)\r\n\t\t\r\n\t\tWord_Frequency(spacy_text5a)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text5a)\r\n\t\tsts.visualize_tokens(spacy_text5a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\t\r\n\t\t# Generate Template\t\r\n\t\ttemplate5a = FindnReplace5a(spacy_text5a)\r\n\t\t\r\n\t\tst.subheader(\"**Your Template for Employee Work Appreciation Email**\\n\")\r\n\t\tst.text(template5a)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL1", "data": "\tdef Process_URL1():\r\n\t\textract1 = Webscrape_divID(URL1, div_id1)\r\n\t\r\n\t\tphrase = \"Formal out of office reply with referral for customers\"\r\n\t\ttemp = extract1.replace(str(phrase),\"\")\r\n\t\t\r\n\t\tphr1 = \"Feel free\"\r\n\t\ttemp1 = temp.replace(str(phr1),\"\\nFeel free\")\r\n\t\t\r\n\t\tphr2 = \"You can\"\r\n\t\ttemp2 = temp1.replace(str(phr2),\"\\nYou can\")\r\n\t\t\r\n\t\tphr3 = \"Thank you\"\r\n\t\ttemp3 = temp2.replace(str(phr3),\"\\nThank you\")\r\n\t\t\r\n\t\tphr4 = \"Thank you for your message\"\r\n\t\ttemp4 = temp3.replace(str(phr4), \"\\tThank you for your message\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text6 = nlp(temp4)\r\n\t\t\r\n\t\tWord_Frequency(spacy_text6)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text6)\r\n\t\tsts.visualize_tokens(spacy_text6, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\t\r\n\t\t# Generate Template\r\n\t\ttemp1 = FindnReplace6(spacy_text6)\r\n\t\t\r\n\t\tphr = \"MM / DD / YY\"\r\n\t\ttemp2 = temp1.replace(str(phr), \"[Your Date of Return]\")\r\n\t\t\r\n\t\tphr1 = \"( colleague@example.com )\"\r\n\t\ttemp3 = temp2.replace(str(phr1), \"[Your Colleague's Email ID]\")\r\n\t\t\r\n\t\tphr2 = \"( XXX - XXXX )\"\r\n\t\ttemplate6 = temp3.replace(str(phr2), \"[Your Colleague's Contact No.]\")\r\n\t\t\t \r\n\t\tst.subheader(\"**Your Template for Out of Office Email**\\n\")\r\n\t\tst.text(template6)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL2", "data": "\tdef Process_URL2():\r\n\t\textract2 = Webscrape_Classname(URL2, classname2)\r\n\t\r\n\t\tphr1 = \"(555-555-1234)\"\r\n\t\ttemp1 = extract2.replace(str(phr1), \"[Your Colleague's Phone Number]\")\r\n\t\t\r\n\t\tphr2 = \"(jane.doe@example.com)\"\r\n\t\ttemp2 = temp1.replace(str(phr2), \"[Your Colleague's Email ID]\")\r\n\t\t\r\n\t\tphr3 = \"In urgent\"\r\n\t\ttemp3 = temp2.replace(str(phr3), \"\\nIn urgent\")\r\n\t\t\r\n\t\tphr4 = \"Your message\"\r\n\t\ttemp4 = temp3.replace(str(phr4), \"\\nYour message\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text6a = nlp(temp4)\r\n\t\t\r\n\t\tWord_Frequency(spacy_text6a)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text6a)\r\n\t\tsts.visualize_tokens(spacy_text6a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\r\n\t\t# Generate Template\r\n\t\ttemplate6a = FindnReplace6a(spacy_text6a)\r\n\t\r\n\t\tst.subheader(\"**Your Template for Out of Office Email**\\n\")\r\n\t\tst.text(template6a)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL1", "data": "\tdef Process_URL1():\r\n\t\textract1 = Webscrape_divID(URL1, div_id1)\r\n\t\t\r\n\t\tphr1 = \"I\u2019m very\"\r\n\t\ttemp1 = extract1.replace(str(phr1), \"\\tI\u2019m very\")\r\n\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text7 = nlp(temp1)\r\n\t\t\r\n\t\tWord_Frequency(spacy_text7)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text7)\r\n\t\tsts.visualize_tokens(spacy_text7, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\t\r\n\t\t# Generate Template\r\n\t\ttemplate7 = FindnReplace7(spacy_text7)\r\n\t\t\t \r\n\t\tst.subheader(\"**Your Template for Business Deal Closure Email**\\n\")\r\n\t\tst.text(template7)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}, {"term": "def", "name": "fProcess_URL2", "data": "\tdef Process_URL2():\r\n\t\textract2 = Webscrape_divID(URL2, div_id2)\r\n\t\r\n\t\tphr1 = \"great day!Sincerely\"\r\n\t\ttemp1 = extract2.replace(str(phr1), \"great day!\\nSincerely\")\r\n\t\tphr2 = \"Your friends\"\r\n\t\ttemp2 = temp1.replace(str(phr2), \"\\n\\n[Your Name]\")\r\n\t\tphr3 = \"I\u2019m delighted\"\r\n\t\ttemp3 = temp2.replace(str(phr3), \"\\n\\n\\tI\u2019m delighted\")\r\n\t\tphr4 = \"We would\"\r\n\t\ttemp4 = temp3.replace(str(phr4), \"\\nWe would\")\r\n\t\tphr5 = \"You could\"\r\n\t\ttemp5 = temp4.replace(str(phr5), \"\\nYou could\")\r\n\t\tphr6 = \"(your business)\"\r\n\t\ttemp6 = temp5.replace(str(phr6), \"\\n[Your Company Name]\")\r\n\t\t\r\n\t\t# Parse the text with spaCy\r\n\t\tspacy_text7a = nlp(temp6)\r\n\t\t\r\n\t\tWord_Frequency(spacy_text7a)\r\n\t\t\r\n\t\tPOS_Tag(spacy_text7a)\r\n\t\tsts.visualize_tokens(spacy_text7a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n\t\t\r\n\t\t# Generate Template\r\n\t\ttemplate7a = FindnReplace7a(spacy_text7a)\r\n\t\t\t \r\n\t\tst.subheader(\"**Your Template for Business Deal Closure Email**\\n\")\r\n\t\tst.text(template7a)\r\n", "description": null, "category": "webscraping", "imports": ["import streamlit as st\r", "import spacy_streamlit as sts\r", "from PIL import Image\r", "from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import pandas as pd\r", "from sklearn.feature_extraction.text import CountVectorizer\r", "import seaborn as sns\r", "import warnings\r"]}], [{"term": "def", "name": "timed_job", "data": "def timed_job():\n\t# %pip install selenium\n\t# %pip install webdriver_manager\n\t# %pip install pika\n\t\n\tprint(\"Starting webscraper\")\n\t\n\tURL = 'https://www.steamspy.com/search.php'\n\n\tbrowser = webdriver.Chrome(ChromeDriverManager().install())\n\n\tgames = ['Grand Theft Auto V', 'Portal 2', 'The Witcher 3: Wild Hunt', 'Tomb Raider (2013)', 'The Elder Scrolls V: Skyrim',\n\t\t\t 'Left 4 Dead 2', 'Borderlands 2', 'Fallout 4', 'PAYDAY 2', 'Grand Theft Auto IV', 'DOOM (2016)','BioShock',\n\t\t\t 'Half-Life 2', 'Red Dead Redemption 2', 'Limbo','Counter-Strike: Global Offensive', 'Life is Strange',\n\t\t\t 'Team Fortress 2', 'BioShock Infinite']\n\n\t# games = ['Grand Theft Auto V', 'Portal-2']\n\t# games = ['Portal-2']\n\n\tgameInfos = []\n\n\tfor game in games:\n\t\tbrowser.get(URL)\n\t#\t browser.implicitly_wait(1)\n\t\tsearch_field = browser.find_element_by_xpath('/html/body/div[3]/div[2]/div[1]/div[2]/div/div/div/div/div/form/div/input')\n\t\tsearch_field.send_keys(game)\n\t\tsearch_field.submit()\n\t\tgetInfo = browser.find_element_by_xpath('/html/body/div[3]/div[2]/div[1]/div[2]/div[1]/div[1]/div/p').text\n\n\t\tif getInfo.find(\"Price:\") != -1:\n\t\t\tcleanData = getInfo[getInfo.find(\"Price:\"):].split('\\n')\n\t\t\tdel cleanData[1]\n\t\t\twat700 = \"\".join(cleanData).replace(\"Owners\", \";Owners\").replace(': ', \"=\")\n\t\t\tdictionary = dict(subString.split('=') for subString in wat700.split(\";\")) \n\n\t\t\tdictionary[\"Game\"] = game\n\t\t\tgameInfos.append(dictionary)\n\n\tbrowser.close()\n\tjsonData = json.dumps(gameInfos)\n\tprint(jsonData)\n\n\n\tconnection = pika.BlockingConnection(\n\t\tpika.ConnectionParameters(host='localhost'))\n\tchannel = connection.channel()\n\n\tchannel.exchange_declare(exchange='gamerevWebscraper', exchange_type='fanout')\n\n\tchannel.queue_declare(queue='webscrapeData')\n\tchannel.queue_declare(queue='webscrapeData2')\n\n\tchannel.queue_bind(exchange='gamerevWebscraper', queue=\"webscrapeData\")\n\tchannel.queue_bind(exchange='gamerevWebscraper', queue=\"webscrapeData2\")\n\n\tchannel.basic_publish(exchange='gamerevWebscraper', routing_key='', body=jsonData)\n\tprint(\" [x] Sent %r\" % jsonData)\n\tconnection.close()\n\n\tprint(\"Webscraper scheduler job has run at %s\" % datetime.datetime.now())\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.support.ui import Select", "from time import sleep", "import re", "import operator", "from selenium.webdriver.common.keys import Keys", "from webdriver_manager.chrome import ChromeDriverManager", "import pika", "import sys", "import requests", "import bs4", "import json", "import datetime", "from apscheduler.schedulers.blocking import BlockingScheduler"]}], [{"term": "def", "name": "fill_data", "data": "def fill_data(business_name, address_list, source):\n\trow_index = datafile.index[(datafile['Business Name'] == business_name)]\n\tdatafile.at[row_index, 'Source'] = source\n\tdatafile.at[row_index, \"Street Address\"] = address_list[0]\n\tdatafile.at[row_index, \"City\"] = address_list[1]\n\tdatafile.at[row_index, \"State\"] = address_list[2]\n\tdatafile.at[row_index, \"Zip\"] = address_list[3]\n", "description": null, "category": "webscraping", "imports": ["from pandacode import datafile", "from seleniumcode import WebScrape", "import re"]}], [{"term": "class", "name": "classFramework:", "data": "class Framework:\n\t\"\"\"Below is where the various classes already instantiated are called \"\"\"\n\tdef call(self):\n\t\ttry:\n\t\t\tURL = input(\"Enter a website to analyze: \")\n\t\t\tURL_scrape = web_scrape.scrape(URL)\n\t\t\tURL_process = web_process.text_process(URL_scrape)\n\t\t\tword_count = word_counter.word_counter(URL_process)\n\t\t\tbar_chart.plot_bar_chart(word_count)\n\t\t\ttry:\n\t\t\t\tpie_chart.plot_pie_chart(word_count)\n\t\t\texcept ValueError:\n\t\t\t\treturn\n\t\t\tURL_store = open(\"log.csv\", \"a\")\n\t\t\tURL_store.write(f\"{URL}\\n\")\n\t\t\tURL_store.close()\n\t\t\t\"\"\"EXCEPTIONS\"\"\"\n\t\texcept requests.exceptions.MissingSchema:\n\t\t\tprint(f\"Invalid Web URL Format, you entered '{URL}'\")\n\t\texcept requests.exceptions.ConnectTimeout:\n\t\t\tprint(\"Sorry Network issues\")\n\t\texcept requests.exceptions.InvalidSchema:\n\t\t\tprint(f\"Invalid Web URL Format, you entered '{URL}'\")\n\t\texcept requests.exceptions.InvalidURL:\n\t\t\tprint(f\"You entered {URL},  Enter a valid website\")\n\t\texcept requests.exceptions.ConnectionError:\n\t\t\tprint(f\"You entered {URL},  Enter a valid website\")\n\t\t\t\n\t\t\n\n\n", "description": "Below is where the various classes already instantiated are called ", "category": "webscraping", "imports": ["import requests.exceptions", "from web_scrape import WebScrape", "from web_processing import WebProcess", "from web_word_counter import WordCounter", "from bar_chart_plot import BarChart", "from pie_chart_plot import PieChart"]}], [{"term": "def", "name": "webscrape_issues", "data": "def webscrape_issues(numeros):\r\n\tglobal issues_gravados\r\n\turl = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n\tsource = requests.get(url).text\r\n\tsoup = BeautifulSoup(source, 'lxml')\r\n\tlidos = 0\r\n   \r\n\tfor texto in soup.find_all('table', class_='tocArticle'):\r\n\t\tlink = texto.find('a', attrs={'href': re.compile(\"http://\")})\r\n\t\tlink2 = str(link.get('href')[65:70])\r\n\t\ttitulo = texto.find('div', class_='tocTitle').text.strip()\r\n\t\tautor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n\t\tpages = texto.find('div', class_='tocPages').text.strip().replace('\\t', '')\r\n\t\tif autor is not '' and pages is not 'i':\r\n\t\t\tlinks.append(link2)\r\n\t\tprint(f'T\u00edtulo do trabalho: {titulo}')\r\n\t\tprint(f'Autores: {autor}')\r\n\t\tprint(f'Link: {link2}')\r\n\t\tprint(f'P\u00e1ginas: {pages}')\r\n\t\tprint()\r\n\t\tlidos += 1\r\n\t\tissues_gravados += 1\r\n\treturn lidos > 0\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import csv\r", "import re\r"]}, {"term": "def", "name": "webscrape_articles", "data": "def webscrape_articles(numeros):\r\n\tglobal articles_gravados\r\n\turl = f'http://www.uel.br/revistas/uel/index.php/informacao/article/view/{numeros}'\r\n\tsource = requests.get(url).text\r\n\tsoup = BeautifulSoup(source, 'lxml')\r\n\tlidos = 0\r\n   \r\n\tfor texto in soup.find_all('div', id='content'):\r\n\t\ttitulo = texto.find('div', id='articleTitle').text.strip()\r\n\t\tautor = texto.find('div', id='authorString').text.strip()\r\n\t\tresumo = texto.find('div', id='articleAbstract').text.strip().replace('Resumo', '').replace('\\n', '')\r\n\t\tkeywords = texto.find('div', id='articleSubject').text.strip().replace('Palavras-chave', '').replace('\\n', '')\r\n\t\tprint(f'T\u00edtulo do trabalho: {titulo}')\r\n\t\tprint(f'Autores: {autor}')\r\n\t\tprint(f'Resumo: {resumo}')\r\n\t\tprint(f'Palavras-Chave: {keywords}')\r\n\t\tprint()\r\n\t\tcsv_writer.writerow([titulo, autor, resumo, keywords])\r\n\t\tlidos += 1\r\n\t\tarticles_gravados += 1\r\n\treturn lidos > 0\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import csv\r", "import re\r"]}], [{"term": "def", "name": "webscrape_issues", "data": "def webscrape_issues(numeros):\r\n\tglobal issues_gravados\r\n\turl = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n\tsource = requests.get(url).text\r\n\tsoup = BeautifulSoup(source, 'lxml')\r\n\tlidos = 0\r\n   \r\n\tfor texto in soup.find_all('table', class_='tocArticle'):\r\n\t\tlink = texto.find('a', attrs={'href': re.compile(\"http://\")})\r\n\t\tlink2 = str(link.get('href')[65:70])\r\n\t\ttitulo = texto.find('div', class_='tocTitle').text.strip()\r\n\t\tif titulo in 'Informa\u00e7\u00e3o':\r\n\t\t\tprint(titulo)\r\n\t\tautor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n\t\tprint(f'T\u00edtulo do trabalho: {titulo}')\r\n\t\tprint(f'Autores: {autor}')\r\n\t\tprint(f'Link: {link2}')\r\n\t\tprint()\r\n\t\tlidos += 1\r\n\t\tissues_gravados += 1\r\n\treturn lidos > 0\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import csv\r", "import re\r"]}, {"term": "def", "name": "webscrape_articles", "data": "def webscrape_articles(numeros):\r\n\tglobal articles_gravados\r\n\turl = f'http://www.uel.br/revistas/uel/index.php/informacao/article/view/{numeros}'\r\n\tsource = requests.get(url).text\r\n\tsoup = BeautifulSoup(source, 'lxml')\r\n\tlidos = 0\r\n   \r\n\tfor texto in soup.find_all('div', id='content'):\r\n\t\tresumo = 'None'\r\n\t\tkeywords = 'None'\r\n\t\ttitulo = texto.find('div', id='articleTitle').text.strip()\r\n\t\tautor = texto.find('div', id='authorString').text.strip()\r\n\t\tresumo = texto.find('div', id='articleAbstract').text.strip().replace('Resumo', '').replace('\\n', '')\r\n\t\tkeywords = texto.find('div', id='articleSubject').text.strip().replace('Palavras-chave', '').replace('\\n', '')\r\n\t\tprint(f'T\u00edtulo do trabalho: {titulo}')\r\n\t\tprint(f'Autores: {autor}')\r\n\t\tprint(f'Resumo: {resumo}')\r\n\t\tprint(f'Palavras-Chave: {keywords}')\r\n\t\tprint()\r\n\t\tcsv_writer.writerow([titulo, autor, resumo, keywords])\r\n\t\tlidos += 1\r\n\t\tarticles_gravados += 1\r\n\treturn lidos > 0\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import csv\r", "import re\r"]}], [{"term": "class", "name": "Testing", "data": "class Testing(unittest.TestCase):\n\n\tdef test_0_get_unique_results(self):\n\t\ta = {'NAME': ['Kevin', 'Joe'], 'ORGANIZATION': ['Apple','Amazon'], 'LOCATION': ['Canada', 'USA']}\n\t\ttest = 'My name is Kevin and my friends name is Joe.  I work for Apple and my friend works for Amazon.  I live in Canada and my friend lives in USA'\n\t\tmodel = ner_model.Model()\n\t\tner_results = model.ner(test)\n\t\tb = ner_model.get_unique_results(ner_results)\n\t\tself.assertEqual(a,b)\n\n\tdef test_1_web_scraper(self):\n\t\turl = 'https://www.nbcnews.com/politics/biden-says-considering-gas-tax-holiday-rcna34419'\n\t\tscraper = ws_nbc.WebScrape(url)\n\t\tcontent = scraper.scrape_news_article\n\t\tself.assertIsNotNone(content)\n\n\tdef test_2_n_scraper(self):\n\t\turl = 'https://www.nbcnews.com/'\n\t\tscraper = ws_nbc.WebScrape(url)\n\t\tcontent = scraper.scrape_n_articles(num_articles=1)\n\t\tself.assertIsNotNone(content)\n", "description": null, "category": "webscraping", "imports": ["import unittest", "import sys", "from src import ws_nbc, ner_model"]}], [{"term": "class", "name": "classEnvironment:", "data": "class Environment:\n\t#Scrape ACM website and calculate jacquard for each job\n\tdef webscrape(self,stream,area,field,location):\n\t\tag = Agent()\n\t\tglobal page_count_acm\n\t\tglobal dictionary\n\t\tglobal dic_count\n\t\tif area == \"\":\n\t\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'?page='+str(page_count_acm)\n\t\telif stream == \"\": \n\t\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+area+'?page='+str(page_count_acm)\n\t\telse: quote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'/'+area+'?page='+str(page_count_acm)\n\t\tprint quote_page_acm,'\\n'\n\t\tpage_acm = urllib2.urlopen(quote_page_acm)\n\t\tsoup_acm = BeautifulSoup(page_acm, 'html.parser')\n\t\tbox_acm = soup_acm.find_all('div',attrs={'class':'aiResultsMainDiv'})\n\t\ttemp_acm = soup_acm.find('span',attrs={'class':'aiPageTotalTop'}) \n\t\tif  temp_acm == None:\n\t\t\tself.webscrape1(stream,area,field,location)\n\t\tcount_total_acm = int(temp_acm.get_text())\n\t\tfor bx_acm in box_acm:\n\t\t\ttitle_acm = str(bx_acm.find('div',attrs={'class':'aiResultTitle'}).get_text().strip().encode('utf-8'))\n\t\n\t\t\turl_acm = 'http://jobs.acm.org'+str(bx_acm.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href').encode('utf-8'))\n\t\t\n\t\t\tdetails_acm = bx_acm.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')\n\t\t\n\t\t\tcompany_acm = str(details_acm[0].get_text().strip().encode('utf-8'))\n\t\t\n\t\t\tlocation_acm = str(details_acm[1].get_text().strip().encode('utf-8'))\n\t\t\n\t\t\tdate_acm = str(details_acm[2].get_text().strip().encode('utf-8'))\n\t\t\n\t\t\tif bx_acm.find('li',attrs={'id':'searchResultsCategoryDisplay'}) != None:\n\t\t\t\tcategory_acm = str(details_acm[3].get_text().strip().encode('utf-8'))\n\t\t\telse :\n\t\t\t\tcategory_acm = 'None'\n\t\t\tif bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}) == None:\n\t\t\t\tdescription_acm = str(bx_acm.find('div',attrs={'class':'aiResultsDescription'}).get_text().strip().encode('utf-8'))\n\t\t\telse :\n\t\t\t\tdescription_acm = str(bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip().encode('utf-8'))\n\t\t\n\t\t\n\n\t\t\tdictionary[dic_count] = [title_acm,company_acm,location_acm,date_acm,category_acm,description_acm,url_acm,0,'x']\n\t\t\tag.jacard(field,location,dic_count)\n\t\t\tdic_count = dic_count+1\n\n\t\t\n\t\tif (page_count_acm < count_total_acm):\n\t\t\tpage_count_acm = page_count_acm + 1\n\t\t\tself.webscrape(stream,area,field,location)\n\t\n\t\n\n\n\t#scrape Indeed website and calculate jacquard for each job\n\tdef webscrape1(self,stream,area,field,location):\n\t\tag = Agent()\n\t\tglobal page_count_indeed\n\t\tglobal c_indeed\n\t\tglobal test_indeed\n\t\tglobal count_total_indeed \n\t\tglobal counter_indeed\n\t\tglobal cal_page_indeed\n\t\tglobal val_indeed\n\t\tglobal dictionary\n\t\tglobal dic_count\n\t\t\n\t\tquote_page_indeed = 'https://www.indeed.com/jobs?q='+stream+'&l='+area+'&start='+str(page_count_indeed)\n\t\tprint quote_page_indeed,'\\n'\n\t\tpage_indeed = urllib2.urlopen(quote_page_indeed)\n\t\tsoup_indeed = BeautifulSoup(page_indeed, 'html.parser')\n\t\t\n\t\t\n\t\tbox_indeed = soup_indeed.find_all('div',attrs={'class':'row'})\n\t\ttemmp_indeed = soup_indeed.find('div',attrs={'id':'searchCount'})\n\t\tif temmp_indeed == None:\n\t\t\tself.webscrape2(stream,area,field,location)\n\t\tif test_indeed == 0:\n\t\t\tcount_total_indeed = int(temmp_indeed.get_text().split()[5].replace(',',''))\n\t\t\twhile val_indeed <= count_total_indeed:\n\t\t\t\tcal_page_indeed = cal_page_indeed + 1\n\t\t\t\tval_indeed = 25 * cal_page_indeed\n\t\t\ttest_indeed = 1\n\t\t\tcal_page_indeed = cal_page_indeed + 1\n\t\t\tif cal_page_indeed > 50:\n\t\t\t\tcal_page_indeed = 50\n\t\t\n\t\tfor bx_indeed in box_indeed:\n\t\t\ttitle_indeed = str(bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip().encode('utf-8'))\n\t\t\t\n\t\t\tcompany_indeed = str(bx_indeed.find('span',attrs={'class':'company'}).get_text().strip().encode('utf-8'))\n\t\t\t\n\t\t\tlocation_indeed = str(bx_indeed.find('span',attrs={'class':'location'}).get_text().strip().encode('utf-8'))\n\t\t\t\n\t\t\tdescription_indeed = str(bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip().encode('utf-8'))\n\t\t\t\n\t\t\tdate_indeed = 'None'\n\t\t\tcategory_indeed = 'None'\n\t\t\turl_indeed = str(bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get('href').encode('utf-8'))\n\t\t\n\t\t\tdictionary[dic_count] = [title_indeed,company_indeed,location_indeed,date_indeed,category_indeed,description_indeed,url_indeed,0,'x']\n\t\t\tag.jacard(field,location,dic_count)\n\t\t\tdic_count = dic_count +1\n\n\n\t\t\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "from bs4 import BeautifulSoup", "import sys", "from operator import itemgetter", "import random"]}], [{"term": "class", "name": "BackgroundTaskConsumer", "data": "class BackgroundTaskConsumer(SyncConsumer):\n\n\t# This is here for testing purposes\n\tdef test_wait(self, message):\n\t\tif 'wait' not in message.keys():\n\t\t\traise ValueError('message must include wait key')\n\n\t\tif not isinstance(message['wait'], int):\n\t\t\traise ValueError('wait value must be an integer')\n\n\t\tprint(f\"Task: test_wait has begun with wait value: {message['wait']}\")\n\t\tsleep(message['wait'])\n\t\tprint(f\"Task: test_wait has ended\")\n\n\t# This will be used to scrape Linkedin\n\tdef scrape_linkedin(self, message):\n\n\t\tif 'url' not in message.keys():\n\t\t\traise ValueError('message must include url key.')\n\n\t\tprint(f\"Task: Linkedin search scraping has begun with url value: {message['url']}\")\n\t\tlinkedin_search(URL=message['url'], limit=message['limit'])\n\t\tprint(f\"Task: Linkedin search scraping has ended.\")\n", "description": null, "category": "webscraping", "imports": ["from channels.consumer import SyncConsumer", "from time import sleep", "from tasks.webscrape_li import linkedin_search", "from tasks.webscrape_search import definition_search", "from tasks.word_collect import word_collection"]}, {"term": "def", "name": "fscrape_definition", "data": "\tdef scrape_definition(self, message):\n\n\t\tif 'limit' not in message.keys():\n\t\t\traise ValueError('message must include limit key.')\n\n\t\tprint(f\"Task: Definition search scraping has begun with limit: {message['limit']}\")\n\t\tdefinition_search(word_id=message['word_id'], limit=message['limit'])\n\t\tprint(f\"Task: Definition search scraping has ended.\")\n", "description": null, "category": "webscraping", "imports": ["from channels.consumer import SyncConsumer", "from time import sleep", "from tasks.webscrape_li import linkedin_search", "from tasks.webscrape_search import definition_search", "from tasks.word_collect import word_collection"]}, {"term": "def", "name": "fword_collect", "data": "\tdef word_collect(self, message):\n\n\t\tif 'limit' not in message.keys():\n\t\t\traise ValueError('message must include limit key.')\n\n\t\tprint(f\"Task: Word collection has begun with limit: {message['limit']}\")\n\t\tword_collection(limit=message['limit'])\n\t\tprint(f\"Task: Word collection has ended.\")\n\n\n", "description": null, "category": "webscraping", "imports": ["from channels.consumer import SyncConsumer", "from time import sleep", "from tasks.webscrape_li import linkedin_search", "from tasks.webscrape_search import definition_search", "from tasks.word_collect import word_collection"]}], [{"term": "def", "name": "set_dates", "data": "def set_dates():\n\tfor date in dates:\n\t\tprint(' ')\n\t\tprint(date + ': (enter number for option you want)')\n\t\tprint('[0] Any Time')\n\t\tprint('[1] Past Day')\n\t\tprint('[2] Past 2 Days')\n\t\tprint('[3] Past 3 Days')\n\t\tprint('[4] Past Week')\n\t\tprint('[5] Past Month')\n\t\tprint('[6] Past 3 Months')\n\t\tprint('[7] Past Year')\n\t\ttry:\n\t\t\tdates[date] += int(input('Time Period: '))\n\t\texcept:\n\t\t\tprint('ERROR: You must enter a number 0-7')\n\t\t\tdates[date] += int(input('Time Period: '))\n", "description": null, "category": "webscraping", "imports": ["import bs4", "from bs4 import BeautifulSoup as soup", "import pandas as pd", "import numpy as np", "from datetime import datetime", "from selenium import webdriver", "from selenium.webdriver.support.ui import Select", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from selenium.webdriver.common.keys import Keys"]}, {"term": "def", "name": "run_filters", "data": "def run_filters(link, filters, keywords, custom_dict):\n\tdriver.get(link)\n\tif not filters and not keywords and not custom_dict:\n\t\treturn soup(driver.page_source, 'html5lib')\n\tfor i in filters:\n\t\tselect = Select(driver.find_elements_by_xpath('//select[@id=\"time span\"]')[i])\n\t\tselect.select_by_visible_text(option_pairs[filters[i]])\n\t\tif i == 0:\n\t\t\tsubmit_button = driver.find_elements_by_xpath(\n\t\t\t\t'//button[@class=\"sam button tertiary small ng-star-inserted\"]')[2]\n\t\telse:\n\t\t\tsubmit_button = driver.find_elements_by_xpath(\n\t\t\t\t'//button[@class=\"sam button tertiary small ng-star-inserted\"]')[1]\n\n\t\tsubmit_button.click()\n\n\t\ttry:\n\t\t\twait.until(EC.presence_of_element_located((By.CLASS_NAME, 'row')))\n\t\texcept:\n\t\t\tprint(' ')\n\t\t\tprint('No Results For Given Search')\n\t\t\treturn\n\n\t\t#keyword search\n\n\tif type(custom_dict) == dict:\n\n\t\tselect = Select(driver.find_elements_by_xpath('//select[@id=\"time span\"]')[3])\n\t\tselect.select_by_visible_text('Custom Date')\n\n\t\twait.until(EC.element_to_be_clickable((By.XPATH, '//div[@class = \"usa-date-of-birth date-group\"]')))\n\n\t\tdriver.find_elements_by_xpath('//div[@class = \"usa-date-of-birth date-group\"]')[0].click()\n\n\t\tfor time in today_time_dict:\n\t\t\tdriver.find_elements_by_xpath(\"//input[@name = 'date \" + time +  \"']\")[0].send_keys(today_time_dict[time])\n\n\t\tdriver.find_elements_by_xpath('//div[@class = \"usa-date-of-birth date-group\"]')[1].click()\n\n\t\tfor time in custom_dict:\n\t\t\tdriver.find_elements_by_xpath(\"//input[@name = 'date \" + time +  \"']\")[1].send_keys(custom_dict[time])\n\n\t\twhich_box = len(filters) + 5\n\n\t\tfilter_button = driver.find_elements_by_xpath(\n\t\t   '//button[@class=\"sam button tertiary small ng-star-inserted\"]')[which_box]\n\n\t\tfilter_button.click()\n\n\t\ttry:\n\t\t\tprint(driver.page_source)\n\t\t\twait.until(EC.presence_of_element_located((By.CLASS_NAME, 'row')))\n\t\texcept:\n\t\t\tprint(' ')\n\t\t\tprint('No Results For Given Search')\n\t\t\treturn\n\n\tfor kw in keywords:\n\t\twait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'textarea#keywordSearchName-ac-textarea'))).click()\n\t\twait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'textarea#keywordSearchName-ac-textarea'))).send_keys(kw)\n\t\tdriver.find_element_by_id(\"keywordSearchName-ac-textarea\").send_keys(Keys.RETURN)\n\n\t\tprint(' ')\n\n\t\ttry:\n\t\t\twait.until(EC.presence_of_element_located((By.CLASS_NAME, 'row')))\n\t\texcept:\n\t\t\tprint(' ')\n\t\t\tprint('No Results For Given Search')\n\t\t\treturn\n\n\t#due date\n\t#selected_source = driver.page_source\n\t#selected_soup = soup(selected_source, 'html5lib')\n\treturn #selected_soup\n\n", "description": null, "category": "webscraping", "imports": ["import bs4", "from bs4 import BeautifulSoup as soup", "import pandas as pd", "import numpy as np", "from datetime import datetime", "from selenium import webdriver", "from selenium.webdriver.support.ui import Select", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from selenium.webdriver.common.keys import Keys"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(naics, counter_start = 0): #source, counter_start = 0):\n\n\tnaics = int(naics)\n\n\twait\n\n\ttry:\n\t\twait.until(EC.presence_of_element_located((By.XPATH, '//list-results-message')))\n\t\tselected_source = driver.page_source\n\t\tselected_soup = soup(selected_source, 'html5lib')\n\t\tsource = selected_soup\n\t\tresults_shown = source.find('list-results-message').text\n\n\texcept:\n\t\tprint(' ')\n\t\tprint('No Results For Given Search')\n\t\treturn\n\n\t\tprint(driver.current_url)\n\t\tprint('FAILED, TRY AGAIN')\n\t\t#run_filters(dd, time_filters)\n\t\treturn 'FAILED, TRY AGAIN'#webscrape(naics, counter_start)\n\n\tif ' ' in results_shown[12:14]:\n\t\tfirst_results =  int(results_shown[12])\n\t\ttotal_results = int(results_shown[17])\n\t\tnum_pages = 1\n\telse:\n\t\tfirst_results =  int(results_shown[12:14])\n\t\ttotal_results = int(results_shown[18:21])\n\t\tif (int(results_shown[18:21]) % 10) != 0:\n\t\t\tnum_pages = total_results // first_results + 1\n\t\telse:\n\t\t\tnum_pages = total_results // first_results\n\n\tscraped = pd.DataFrame({'Contract Name': [], 'NAICS': [], 'Contract Link':[],\n\t\t\t\t\t\t\t'Description': [], 'Department/Ind. Agency': [], 'Sub-tier': [], 'Office': [],\n\t\t\t\t\t\t\t'Notice ID': [], 'Current Date Offers Due': [], 'Last Updated Date': [],\n\t\t\t\t\t\t\t'Last Published Date': [],'Type': [], 'Current Response Date': [], 'Awardee': [],\n\t\t\t\t\t\t\t'Product Service Code': []\n\t\t\t\t\t\t   })\n\n\tcounter = counter_start\n\n\tprint('Total of ' + str(total_results) + ' results.')\n\tprint(' ')\n\n\tfor page in range(1, num_pages + 1):\n\n\t\tpage_num_len = len(str(page))\n\t\tbeta_sam = driver.current_url.split('page=')[0] + 'page=' + str(page) + driver.current_url.split(\n\t\t\t'page=')[1][page_num_len:]\n\n\t\tdriver.get(beta_sam)\n\n\t\twait.until(EC.presence_of_element_located(\n\t\t\t(By.XPATH, '//list-results-message[@class=\"ng-tns-c3-1 ng-star-inserted\"]')))\n\t\tsam_source = driver.page_source\n\t\tsam_soup = soup(sam_source, 'html5lib')\n\n\t\trows = sam_soup.findAll(class_ = 'sam-ui grid')\n\n\t\tprint(str(round(100 * (page - .5)/num_pages, 2)) + '% Done with ' + str(\n\t\t\tnaics) + ', total of ' + str(num_pages) + ' pages')\n\n\t\tfor row in rows:\n\n\t\t\t#outer page\n\t\t\tscraped.loc[counter, 'Contract Name'] = row.div.div.h3.a.text\n\t\t\tscraped.loc[counter, 'Contract Link'] = ('https://beta.sam.gov' + row.div.div.h3.a['href']).split('?index=')[0]\n\n\t\t\tdescription = [i.text for i in row.div.div.span.findAll('p')]\n\n\t\t\tif len(description) == 1:\n\t\t\t\tdescription = description.pop()\n\t\t\telif len(description) == 0:\n\t\t\t\tdescription = None\n\t\t\telse:\n\t\t\t\tdescription = ', '.join(description)\n\n\t\t\tscraped.loc[counter, 'Description'] = description\n\n\t\t\tfor i in row.findAll(class_ = 'sam-ui small list')[0].findAll('li'):\n\t\t\t\tif i.a:\n\t\t\t\t\tscraped.loc[counter, i.strong.text.strip()] = i.a.text.strip()\n\t\t\t\telse:\n\t\t\t\t\tscraped.loc[counter, i.strong.text.strip()] = i.span.text.strip()\n\n\t\t\tscraped.loc[counter, 'Office'] = row.div.div.ul.findAll('span')[0].text\n\n\t\t\tfor j in row.findAll(class_ = 'four wide column')[0].findAll('li'):\n\t\t\t\tif j.span.text.strip() != 'Contract Opportunities':\n\t\t\t\t\tscraped.loc[counter, j.strong.text.strip()] = j.span.text.strip()\n\n\t\t\t#inner page\n\t\t\tdriver.get(scraped.loc[counter, 'Contract Link'])\n\n\t\t\tWebDriverWait(driver, 10).until(EC.presence_of_element_located(\n\t\t\t\t(By.XPATH, '//div[@class=\"sam-ui padded raised segment\"]')))\n\t\t\tinner_source = driver.page_source\n\t\t\tinner_soup = soup(inner_source, 'html5lib')\n\n\t\t\tscraped.loc[counter, 'Product Service Code'] = inner_soup.find(\"li\", {\n\t\t\t\t\"id\": \"classification-classification-code\"}).text[22:]\n\t\t\tscraped.loc[counter, 'NAICS'] = inner_soup.find(\"li\", {\n\t\t\t\t\"id\": \"classification-naics-code\"}).text[12:]\n\t\t\tscraped.loc[counter, 'Primary Point of Contact (PPOC)'] = inner_soup.find(\"li\", {\n\t\t\t\t\"id\": \"contact-primary-poc-full-name\"}).text\n\t\t\tscraped.loc[counter, 'PPOC Email'] = inner_soup.find(\"li\", {\n\t\t\t\t\"id\": \"contact-primary-poc-email\"}).a['href']\n\t\t\ttry:\n\t\t\t\tscraped.loc[counter, 'PPOC Phone'] = inner_soup.find(\"li\", {\n\t\t\t\t\t\"id\": \"contact-primary-poc-phone\"}).text[16:]\n\t\t\texcept:\n\t\t\t\tscraped.loc[counter, 'PPOC Phone'] = np.nan\n\n\t\t\tdriver.back()\n\n\t\t\tcounter += 1\n\n\t\t#progress report\n\t\tprint(str(round(100 * page/num_pages, 2)) + '% Done with ' + str(naics) + ', total of ' + str(\n\t\t\tnum_pages) + ' pages')\n\n\t#if it is too short\n\tif scraped.shape[0] != int(total_results):\n\t\tprint('ERROR: Trying again now')\n\t\tprint('Length is ' + str(scraped.shape[0]) + ', should be ' + total_results)\n\t\treturn webscrape(naics, counter_start)\n\tscraped.fillna('None Given', inplace = True)\n\treturn scraped\n", "description": null, "category": "webscraping", "imports": ["import bs4", "from bs4 import BeautifulSoup as soup", "import pandas as pd", "import numpy as np", "from datetime import datetime", "from selenium import webdriver", "from selenium.webdriver.support.ui import Select", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from selenium.webdriver.common.keys import Keys"]}, {"term": "def", "name": "run_webscrape", "data": "def run_webscrape(naics):\n\tmaster_scraped = pd.DataFrame({'Contract Name': [], 'NAICS': [], 'Contract Link':[],\n\t\t\t\t\t\t\t\t\t'Description': [], 'Department/Ind. Agency': [], 'Sub-tier': [], 'Office': [],\n\t\t\t\t\t\t\t\t\t'Notice ID': [], 'Current Date Offers Due': [], 'Last Updated Date': [],\n\t\t\t\t\t\t\t\t\t'Last Published Date': [],'Type': [], 'Current Response Date': [], 'Awardee': []\n\t\t\t\t\t\t\t\t   })\n\tif ',' in naics:\n\t\tnaics = [int(i) for i in naics.split(',')]\n\t\tfor code in naics:\n\t\t\tprint(' ')\n\t\t\tprint('Scraping ' + str(code))\n\t\t\tdd = 'https://beta.sam.gov/search?index=opp&naics='+ str(code) +'&page=1'\n\t\t\tselected_soup = run_filters(dd, time_filters, keyword, custom_time_dict)\n\t\t\twait\n\t\t\tmaster_scraped = master_scraped.append(\n\t\t\twebscrape(code, selected_soup, master_scraped.shape[0]))\n\t\treturn master_scraped\n\telse:\n\t\tdd = 'https://beta.sam.gov/search?index=opp&naics='+ naics +'&page=1'\n\t\tselected_soup = run_filters(dd, time_filters, keyword, custom_time_dict)\n\t\treturn webscrape(naics)#, selected_soup)\n\n", "description": null, "category": "webscraping", "imports": ["import bs4", "from bs4 import BeautifulSoup as soup", "import pandas as pd", "import numpy as np", "from datetime import datetime", "from selenium import webdriver", "from selenium.webdriver.support.ui import Select", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from selenium.webdriver.common.keys import Keys"]}], [{"term": "def", "name": "view_album_details", "data": "def view_album_details(album_link):\n\t#clear command line\n\tclear()\n\t#obtain the html for the album page\n\treq = Request(album_link, headers={'User-Agent': 'Mozilla/5.0'})\n\tpage = urlopen(req).read()\n\thtml_album = page.decode(\"utf-8\")\n\t\n\t#create the parser and parse each element\n\tparser = album_parser(html_album)\n\tname = parser.get_name()\n\t\n\tsummary = parser.get_summary()\n\tsummary = summary.replace('', '\\n')\n\tcredit = parser.get_credit()\n\t\n\t#print the results to the command line\n\tprint(name)\n\t#create the formatting for the summary\n\tbody = '\\n\\n'.join(['\\n'.join(textwrap.wrap(line, 100,\n\t\t\t\t break_long_words=False, replace_whitespace=False))\n\t\t\t\t for line in summary.splitlines() if line.strip() != ''])\n\tprint(body)\n\tprint(\"\\n\" + credit + \"\\n\\n\")\n\t\n\t\n\t#prompt the user to view the reviews of the album\n\tanswer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n\t\n\t#while the answer is r\n\twhile (answer == \"r\"):\n\t\t\n\t\talbum_user_reviews.view_album_user_reviews(album_link + \"/user-reviews\")\n\t\t#clear command line output\n\t\tclear()\n\t\t#print the results to the command line\n\t\tprint(name)\n\t\tprint(body)\n\t\tprint(\"\\n\" + credit + \"\\n\\n\")\n\t\t\n\t\t#prompt the user to view the reviews of the movie\n\t\tanswer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n\t\n\t\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen, Request", "import textwrap", "from webscrape.searchresults.categorydetails.albumdetail.albumparse.album_parser import album_parser", "from webscrape.searchresults.categorydetails.albumdetail.albumdisplay import album_user_reviews", "from webscrape import clear"]}], [{"term": "def", "name": "view_game_details", "data": "def view_game_details(game_link):\n\t\n\t#clear command line\n\tclear()\n\t\n\t#obtain the html code for the game page\n\treq = Request(game_link, headers={'User-Agent': 'Mozilla/5.0'})\n\tpage = urlopen(req).read()\n\thtml_game = page.decode(\"utf-8\")\n\t\n\t\n\t#create the parser and parse each element\n\tparser = game_parser(html_game)\n\tname = parser.get_name()\n\t\n\tsummary = parser.get_summary()\n\tsummary = summary.replace('', '\\n')\n\tcredit = parser.get_credit()\n\t\n\t#print the results to the command line\n\tprint(name)\n\t#create the formatting for the summary\n\tbody = '\\n\\n'.join(['\\n'.join(textwrap.wrap(line, 100,\n\t\t\t\t break_long_words=False, replace_whitespace=False))\n\t\t\t\t for line in summary.splitlines() if line.strip() != ''])\n\tprint(body)\n\tprint(\"\\n\" + credit + \"\\n\\n\")\n\t\n\t\n\t#prompt the user to view the reviews of the movie\n\tanswer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n\t\n\t#while the answer is r\n\twhile (answer == \"r\"):\n\t\t\n\t\tgame_user_reviews.view_game_user_reviews(game_link + \"/user-reviews\")\n\t\t#clear the command line\n\t\tclear()\n\t\t#print the results to the command line\n\t\tprint(name)\n\t\tprint(body)\n\t\tprint(\"\\n\" + credit + \"\\n\\n\")\n\t\t\n\t\t#prompt the user to view the reviews of the movie\n\t\tanswer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n\t\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen, Request", "import textwrap", "from webscrape.searchresults.categorydetails.gamedetail.gameparse.game_parser import game_parser", "from webscrape.searchresults.categorydetails.gamedetail.gamedisplay import game_user_reviews", "from webscrape import clear"]}], [{"term": "class", "name": "OpenDeltaCrawler", "data": "class OpenDeltaCrawler(scrapy.Spider):\n\tname = \"forbeCrawler\"\n\n\tdef start_requests(self):\n\t\turl = \"https://www.forbes.com/sites/billybambrough/?sh=28b3aa666a89\"\n\t\t#headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n\t\tyield scrapy.Request(url, callback = self.parse_blog)\n\n\tdef parse_blog(self,response):\n\n\t\t# look for more articles button\n\t\tmore_articles = response.css('').attrib['href']\n\n\n\t\t# Go to the blocks that contain blog posts\n\t\tblog_posts = response.xpath('//h3[contains(@class,\"title list-title m0005\")]')\n\t\t# Go to the blog links\n\t\tblog_links = blog_posts.xpath('./a/@href')\n\t\tprint(blog_links)\n\t\t# Extract the links (as a list of strings)\n\t\tlinks_to_follow = blog_links.extract()\n\n\t\t# look for more articles button\n\t\tmore_articles = response.css('').attrib['href']\n\n\t\t# Follow the links in the next parser\n\t\tfor url in links_to_follow:\n\t\t\t#headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n\t\t\tyield response.follow(url=url, callback=self.parse_pages)\n\n\tdef parse_pages(self, response):\n\t\ti = ItemLoader(item=WebscrapeItem(), selector=response)\n\t\ti.add_css('blog_title','h1.article-headline')\n\t\ti.add_css('blog_excerpt','div.article-excerpt')\n\t\ti.add_xpath('blog_author','//div[@class=\"article-meta grey-text\"]/span[1]/span')\n\t\ti.add_xpath('blog_publish_date','//div[@class=\"article-meta grey-text\"]/span[2]')\n\t\ti.add_xpath('blog_text','//p')\n\t\ti.add_value('blog_url',response.url)\n\t\tyield i.load_item()\n\n\n\t\t# item = WebscrapeItem()\n\t\t# # Direct to the blog title text\n\t\t# blog_title = response.css('h1.article-headline::text')\n\t\t# # Exctract and clean the blog title text\n\t\t# blog_title_clean = blog_title.extract_first().strip()\n\t\t# # Direct to the Exceprt\n\t\t# blog_excerpt = response.css('div.article-excerpt::text')\n\t\t# # Extract and clean the blog exceprt\n\t\t# blog_excerpt_clean = blog_excerpt.extract_first().strip()\n\t\t# # Direct to Blog Author\n\t\t# blog_author = response.xpath('//div[@class=\"article-meta grey-text\"]/span[1]/span/text()')\n\t\t# # Extract and clean the author\n\t\t# blog_author_clean = blog_author.extract_first().strip()\n\t\t# # Direct to publish\n\t\t# blog_publish_date = response.xpath('text()')\n\t\t# # Extract and clean the publish date\n\t\t# blog_publish_date_clean = blog_publish_date.extract_first().strip()\n\t\t# print(blog_publish_date_clean)\n\t\t# print(blog_author_clean)\n\t\t# # code to parse blog posts\n\n\n", "description": null, "category": "webscraping", "imports": ["#import scrappy", "import scrapy", "from ..items import WebscrapeItem", "from scrapy.loader import ItemLoader", "from scrapy.crawler import CrawlerProcess"]}], [{"term": "def", "name": "extractNavigableStrings", "data": "def extractNavigableStrings(context):\n\t\"\"\" from https://stackoverflow.com/questions/29110820/how-to-scrape-between-span-tags-using-beautifulsoup\"\"\"\n\tstrings = []\n\tfor e in context.children:\n\t\tif isinstance(e, NavigableString):\n\t\t\tstrings.append(e)\n\t\tif isinstance(e, Tag):\n\t\t\tstrings.extend(extractNavigableStrings(e))\n\treturn strings\n\n\n", "description": " from https://stackoverflow.com/questions/29110820/how-to-scrape-between-span-tags-using-beautifulsoup", "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}, {"term": "def", "name": "parse_MAl", "data": "def parse_MAl(url):\n\t\"\"\"\n\tParameters\n\t----------\n\turl : string\n\t\tmyanimelist.net url string \n\n\tReturns\n\t-------\n\tdf : DataFrame\n\t\treturns a dataframe with columns \"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"\n\n\t\"\"\"\n\thtml = requests.get(url)\n\tsoup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n\tresults = soup.find_all(class_= \"ranking-list\")\n\t\n\tdf = pd.DataFrame(columns=[\"name\",\"english_name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\", \"url\"])\n\ti = 0\n\tfor result in results:\n\t\t#print(i)\n\t\turl_= result.find(class_=\"hoverinfo_trigger fl-l fs14 fw-b\")[\"href\"]\n\t\thtml_ = requests.get(url_)\n\t\tsoup_ = BeautifulSoup(html_.content, 'html.parser', from_encoding=\"utf-8\")\n\t\t\n\t\tt1name = extractNavigableStrings(soup_.find(class_=\"h1-title\"))\n\t\tif len(t1name) == 1:\n\t\t\tname = t1name[0]\n\t\t\tenglish_name = None\n\t\telif len(t1name) >= 2:\n\t\t\tname=t1name[0]\n\t\t\tenglish_name=t1name[1]\n\t\telse:\n\t\t\tname = None\n\t\t\tenglish_name = None\n\t\t\t\n\t\tType, Dates, members = result.find(class_=\"information di-ib mt4\").text.strip().splitlines()\n\t\ttry:\n\t\t\tmembers = float(\"\".join(members.split()[0].split(\",\")))\n\t\texcept:\n\t\t\tmembers = None\n\t\t\t\n\t\t[Type_, eps, n] = [\", \".join(x.split()) for x in re.split(r'[()]',Type)]\n\t\t\n\t\ttry:\n\t\t\teps = float(eps.split(\",\")[0])\n\t\texcept:\n\t\t\teps = None\n\t\t\n\t\ttry:\n\t\t\tgenres = [genre.text.strip() for genre in soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"genre\")]\n\t\t\t\n\t\texcept:\n\t\t\tgenres = None\n\t\t\n\t\ttry:\n\t\t\tscore = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingValue\")[0].text.strip())\n\t\texcept:\n\t\t\tscore = None\n\t\t#try:\n\t\t#\tscore_members = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingCount\")[0].text.strip())\n\t\t#except:\n\t\t #   score_members = None\n\t\t\n\t\tdf = df.append({\n\t\t\t\"name\": name,\n\t\t\t\"english_name\":english_name,\n\t\t\t\"type\": Type_,\n\t\t\t\"episodes\": eps,\n\t\t\t\"members\": members,\n\t\t\t#\"score_members\": score_members,\n\t\t\t\"rating\": score,\n\t\t\t\"genre\": genres,\n\t\t\t\"dates\": Dates,\n\t\t\t\"url\": url_\n\t\t},ignore_index=True)\n\t\t\n\t\ti+=1\n\treturn df\n\n", "description": "\n\tParameters\n\t----------\n\turl : string\n\t\tmyanimelist.net url string \n\n\tReturns\n\t-------\n\tdf : DataFrame\n\t\treturns a dataframe with columns \"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"\n\n\t", "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}, {"term": "def", "name": "webscrape_MAl", "data": "def webscrape_MAl(anime_limit=16750, start=0):\n\turl_template = \"https://myanimelist.net/topanime.php?limit={}\"\n\tdf = pd.DataFrame(columns=[\"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"])\n\tfor limit in tqdm(range(start,anime_limit, 50)): # iterate in steps of 50\n\t\turl = url_template.format(limit)\n\t\tdf_temp = parse_MAl(url)\n\t\tif df_temp[\"name\"].isnull().sum() >= 40:\n\t\t\tprint(\"Number of missing names, for limit {} = {}\".format(limit, df_temp[\"name\"].isnull().sum()))\n\t\t\tprint(\"--------Halting---------\")\n\t\t\traise SystemExit()\n\t\tsave_mal_temp(df_temp, limit)\n\t\t\n\t\t# I think MAL has a limit on the number of conenctions per minute/second/hour\n\t\t# and after 200-400, the site blocks access. Adding the pause for 1 minute below soleves the issue\n\t\tsleep(60) # pause the loop for 1 minute. \n\t\t\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}, {"term": "def", "name": "save_mal_temp", "data": "def save_mal_temp(df, limit):\n\tcsvTemp = \"temp/MAL_start_{}.csv\".format(limit)\n\tdf.to_csv(csvTemp)\n\t\t\n\t#print(\"Number of missing names, for limit {} = {}\".format(limit, df[\"name\"].isnull().sum()))\n\t\n\t\n", "description": null, "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}], [{"term": "class", "name": "wikiScraper", "data": "class wikiScraper(Scraper):\n\t\"\"\"class holding input variables and retrieving results from wikipedia\"\"\"\n\n\tdef __str__(self):\n\t\tattrDict = str(self.__dict__)\n\t\treturn f'wiki Scraper object named {self.journalName} with the following attributes {attrDict}'\n\n\tdef webscrapeWikipedia(self):\n\t\t\"\"\"wrapping function to scrape wikipedia\"\"\"\n\t\tself.checkForMainAttr()\n\t\tif self.journalName == 'wikipedia':\n\t\t\tself.wikiPage = self.buildWikiPage(self.plantName)\n\t\t\tself.sectionContents = self.getWikiSections()\n\t\t\tif \"Summary\" in self.sectionTitles:\n\t\t\t\tself.sectionContents[\"Summary\"] = self.wikiPage.summary\n\t\t\telse:\n\t\t\t\tpass\n\t\telse:\n\t\t\tprint(\"journal is not wikipedia\")\n\n\n\tdef buildWikiPage(self, plantName):\n\t\t\"\"\"build wikipage using wikip. api with checks\"\"\"\n\t\tplantName= self.plantName\n\t\twiki_wiki = wiki.Wikipedia('en')\n\t\tpage_py = wiki_wiki.page(plantName)\n\t\tif page_py.exists():\n\t\t\treturn page_py\n\t\telse:\n\t\t\tprint(f'could not find wikipediaPage for {plantName}')\n\t\t\treturn False\n\n\tdef getWikiSections(self):\n\t\t\"\"\"extract specific section from wiki page\"\"\"\n\t\twikiOnlineSections = [i for i in self.wikiPage.sections if i.title.title() in self.sectionTitles]\n\t\tdic = {}\n\t\tfor v in wikiOnlineSections:\n\t\t\tk = v.title\n\t\t\tdic[k] = v.text\n\t\treturn dic\n", "description": "class holding input variables and retrieving results from wikipedia", "category": "webscraping", "imports": ["from utils import Scraper", "from utils import wikiSectionTitles", "import wikipediaapi as wiki"]}], [{"term": "def", "name": "blackbox_webscrape", "data": "def blackbox_webscrape(sim_num, endstep,intervals):\n\tcurrent_step = 1 # starting step #\n\n\turl = \"https://www.informatics.indiana.edu/jbollen/I501F18/blackbox/BlackBox_N.php\" # Blackbox URL\n\tdriver = webdriver.Chrome('C:/Users/Kyrie/GitHub/I501Blackbox/chromedriver')  # Location of chrome driver on Kyrie's Desktop\n\t#driver = webdriver.Chrome('C:/Users/kyrie/OneDrive/Documents/GitHub/I501Blackbox/chromedriver')  # Location of chrome driver on Kyrie's Laptop\n\t#driver = webdriver.Firefox(executable_path='/usr/local/bin/geckodriver') #this is for Becca's Macbook. Comment this out and do the other drivers for Kyrie.\n\t\n\tdriver.get(url) # Get URL\n\t\n\tdriver.find_element_by_name(\"cycles\").clear() # clear current interval amount\n\tdriver.find_element_by_name(\"cycles\").send_keys(intervals) # Set to chosen interval amount\n\t\t\n\twhile current_step < endstep: # Run until end step \n\n\t\tsoup = BeautifulSoup(driver.page_source) # read page source\n\t\t\n\t\ttable1 = soup.find_all('table',id=\"system\")[0] # find blackbox table in source\n\t\t\n\t\tfind_step = soup.find_all('p') # find location of current step in source\n\t\tcurrent_step = int(find_step[3].contents[0][13:]) # Finds current step in integer format\n\t\t\n\t\t#Find all of the numbers in the 20 x 20 grid in the page source and store to my_table\n\t\trows = table1.findChildren(['th', 'tr'])\n\t\t\n\t\tmy_table = []\n\t\ti = 0\n\t\tj = 0\n\n\t\tfor row in rows:\n\t\t\ti += 1\n\t\t\tcells = row.findChildren('td')\n\t\t\tfor cell in cells:\n\t\t\t\tj += 1\n\t\t\t\tif j == 21: # Reset to 1 after 20\n\t\t\t\t\tj= 1\n\t\t\t\tvalue = cell.string\n\t\t\t\tmy_table.append(value)\n\t\t\n\t\tmy_table2 = np.reshape(my_table, (20,20)).astype(int) # Reshape to 20 x 20 array\n\t\t\n\t\t# Save array to text file with simulation # and step # in filename\n\t\toutput_file = 'sim' +str(sim_num)+ '_step' + str(current_step) + '_int' + str(intervals) + '.txt'\n\t\tnp.savetxt(output_file, my_table2, delimiter=',',fmt= '%d') \n\t\t\n\t\t# Print status\n\t\tprint('Simulation ' + str(sim_num) + ': Step ' + str(current_step) + ' file saved.')\n\t\t\n\t\ttime.sleep(5) # wait 5 seconds to allow for saving file\n\t\t\n\t\t# Click next button on page\n\t\tbutton = driver.find_element(By.XPATH, '//button[text()=\"Next n Step\"]')\n\t\tbutton.click()\n\t\t\n\t\ttime.sleep(5) # wait 5 seconds to wait after clicking Next Step\n\t\t\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from bs4 import BeautifulSoup", "import numpy as np"]}], [{"term": "def", "name": "index", "data": "def index():\n\treturn render_template(\"index.html\")\n\n", "description": null, "category": "webscraping", "imports": ["import logging", "import os", "from flask import (", "from webscraper.selenium_scrape import setup_webdriver, selenium_clean_up", "from webscraper.webscraper import single_site_scrape, mass_webscrape", "from webscraper.data_manipulation import file_management as fm"]}, {"term": "def", "name": "single_submission", "data": "def single_submission():\n\treturn render_template(\"single_submission.html\")\n\n", "description": null, "category": "webscraping", "imports": ["import logging", "import os", "from flask import (", "from webscraper.selenium_scrape import setup_webdriver, selenium_clean_up", "from webscraper.webscraper import single_site_scrape, mass_webscrape", "from webscraper.data_manipulation import file_management as fm"]}, {"term": "def", "name": "multi_submission", "data": "def multi_submission():\n\treturn render_template(\"multi_submission.html\")\n\n", "description": null, "category": "webscraping", "imports": ["import logging", "import os", "from flask import (", "from webscraper.selenium_scrape import setup_webdriver, selenium_clean_up", "from webscraper.webscraper import single_site_scrape, mass_webscrape", "from webscraper.data_manipulation import file_management as fm"]}, {"term": "def", "name": "single_api_submission", "data": "def single_api_submission():\n\tbase_url = request.args.get(\"base_url\")\n\tlogging.info(f\"Searching for {base_url}\")\n\tselenium_driver = setup_webdriver()\n\toutput = jsonify(single_site_scrape(base_url, selenium_driver))\n\tselenium_clean_up(selenium_driver)\n\treturn make_response(output, 200)\n\n", "description": null, "category": "webscraping", "imports": ["import logging", "import os", "from flask import (", "from webscraper.selenium_scrape import setup_webdriver, selenium_clean_up", "from webscraper.webscraper import single_site_scrape, mass_webscrape", "from webscraper.data_manipulation import file_management as fm"]}, {"term": "def", "name": "upload_file", "data": "def upload_file():\n\t# check if the post request has the file part\n\tuploaded_file = request.files[\"file\"]\n\tfile = uploaded_file.filename\n\tfile_name = os.path.splitext(file)[0]\n\tif file != \"\":  # this is done on the renderer side too but just for precaution\n\t\tfile_extension = os.path.splitext(file)[1]\n\t\tif file_extension not in current_app.config[\"UPLOAD_EXTENSIONS\"]:\n\t\t\tabort(400)\n\t\tuploaded_file.save(file)\n\tdf = fm.read_excel_get_url_series(file)\n\t# output = fm.data_dict_to_pandas(webscraper.dry_run(df))  # for testing, add this in and remove the mass webscrape\n\tos.remove(file)  # remove the uploaded file\n\n\t# do the scraping\n\tselenium_driver = setup_webdriver()\n\toutput = fm.data_dict_to_pandas(mass_webscrape(df, selenium_driver))\n\tselenium_clean_up(selenium_driver)\n\n\t# generate the output\n\tfile_to_download = f\"output/{file_name}_output{file_extension}\"\n\tfm.output_excel_file(\n\t\tf\"{file_to_download}\", output\n\t)  # generate the file for the user to download\n\treturn render_template(\n\t\t\"dataframe_template.html\",\n\t\ttables=output.to_html(),\n\t\ttitles=file_name,\n\t\tfile_to_download=file_to_download,\n\t)\n\n", "description": null, "category": "webscraping", "imports": ["import logging", "import os", "from flask import (", "from webscraper.selenium_scrape import setup_webdriver, selenium_clean_up", "from webscraper.webscraper import single_site_scrape, mass_webscrape", "from webscraper.data_manipulation import file_management as fm"]}, {"term": "def", "name": "download_file", "data": "def download_file(file_to_download):\n\t@after_this_request\n\tdef remove_file(response):\n\t\t# FIXME: if the file isn't downloaded then this stays in the output folder\n\t\ttry:\n\t\t\tos.remove(f\"output/{file_to_download}\")\n\t\texcept Exception as e:\n\t\t\tapp.logger.error(\"Error removing or closing downloaded file handle\", e)\n\t\treturn response\n\n\ttry:\n\t\tpath = f\"output/{file_to_download}\"\n\t\treturn send_file(path, download_name=file_to_download, as_attachment=True)\n\texcept FileNotFoundError:\n\t\tpass\n\t\treturn \"\"\"\n\t\t\n\t\t I've seen data you people wouldn't believe... Unstructured data in production databases... I watched users abuse\n\t\t column typing using only varchar. Like the file you requested a second time, those moments will be lost in\n\t\t  time, like tears in rain\n\t\t  Go back to the home page\n\t\t\t   \n\t\t\n\t\t\"\"\"\n\texcept Exception as error:\n\t\treturn app.logger.error(\"Unable to serve the requested file\", error)\n\n", "description": "\n\t\t\n\t\t I've seen data you people wouldn't believe... Unstructured data in production databases... I watched users abuse\n\t\t column typing using only varchar. Like the file you requested a second time, those moments will be lost in\n\t\t  time, like tears in rain\n\t\t  Go back to the home page\n\t\t\t   \n\t\t\n\t\t", "category": "webscraping", "imports": ["import logging", "import os", "from flask import (", "from webscraper.selenium_scrape import setup_webdriver, selenium_clean_up", "from webscraper.webscraper import single_site_scrape, mass_webscrape", "from webscraper.data_manipulation import file_management as fm"]}, {"term": "def", "name": "health", "data": "def health():\n\tresponse = {\"healthcheck\": \"I'm working here\"}\n\treturn make_response(jsonify(response), 200)\n\n", "description": null, "category": "webscraping", "imports": ["import logging", "import os", "from flask import (", "from webscraper.selenium_scrape import setup_webdriver, selenium_clean_up", "from webscraper.webscraper import single_site_scrape, mass_webscrape", "from webscraper.data_manipulation import file_management as fm"]}, {"term": "def", "name": "favicon", "data": "def favicon():\n\treturn send_from_directory(\n\t\tos.path.join(app.root_path, \"static\"),\n\t\t\"favicon.ico\",\n\t\tmimetype=\"image/vnd.microsoft.icon\",\n\t)\n\n", "description": null, "category": "webscraping", "imports": ["import logging", "import os", "from flask import (", "from webscraper.selenium_scrape import setup_webdriver, selenium_clean_up", "from webscraper.webscraper import single_site_scrape, mass_webscrape", "from webscraper.data_manipulation import file_management as fm"]}], [{"term": "class", "name": "CentrisScraper", "data": "class CentrisScraper(d.WebScrape):\n\tdef __init__(self, url, searched):\n\t\tsuper().__init__(url, searched)\n\t\n\tdef findApparts(self):\n\n\t\tfile_rows, address_column, price_column, link_column, img_column = [], [], [], [], []\n\t\tlink_start = \"https://centris.ca\"\n\t\tpage = self.parsePage()\n\t\tapparts = page.findAll(\"div\", {\"class\":\"shell\"})\n\n\t\tfor appart in apparts:\n\n\t\t\taddress = appart.find(\"span\", {\"class\":\"address\"}).text.strip()\n\t\t\taddress_column.append(address)\n\t\t\tprice = appart.find(\"span\", {\"itemprop\":\"price\"}).text.strip()\n\t\t\tprice_column.append(price)\n\t\t\tlink = link_start+appart.find(\"a\")[\"href\"]\n\t\t\tlink_column.append(link)\n\t\t\timg = appart.find(\"img\", {\"itemprop\":\"image\"})[\"src\"]\n\t\t\timg_column.append(img)\n\n\t\t\tprint(\"--------------------------------\")\n\t\t\tprint(f\"{address}\")\n\t\t\tprint(f\"Prix: {price}\")\n\t\t\tprint(f\"Lien: {link}\")\n\t\t\tprint(f\"Image: {img}\")\n\t\t\tprint(\"--------------------------------\")\n\t\t\n\t\t\n\t\tfile_rows.append(address_column)\n\t\tfile_rows.append(price_column)\n\t\tfile_rows.append(link_column)\n\t\tfile_rows.append(img_column)\n\n", "description": null, "category": "webscraping", "imports": ["import webscrape_module as d", "from bs4 import BeautifulSoup as soup"]}], [{"term": "class", "name": "WebscrapeAppConfig", "data": "class WebscrapeAppConfig(AppConfig):\n\tname = 'webscrape_app'\n", "description": null, "category": "webscraping", "imports": ["from django.apps import AppConfig"]}], [{"term": "def", "name": "WebScrape", "data": "def WebScrape(startPage, endPage):\r\n\t# fileName = 'super_magic_god_of_harry_potter_langCheck'+ str(startPage) + '_' + str(endPage) +'.txt'\r\n\tf= open(\"super_magic_god_of_harry_potter_langCheck751_796.txt\",\"a+\",encoding=\"utf-8\")\r\n\r\n\tmyUrl = 'http://lnmtl.com/chapter/super-magic-god-of-harry-potter-chapter-'\r\n\r\n\r\n\tfor page in range(startPage, endPage):\r\n\t\t#traverse the Site\r\n\t\tprint(page)\r\n\t\tsrhUrl =  myUrl + str(page) + \"/\"\r\n\t\ttry:\r\n\t\t\tuClient =  requests.get(srhUrl)\r\n\t\t\tuClient.encoding = \"utf-8\"\r\n\t\t\thtml_content = soup(uClient.content, 'html.parser')\r\n\r\n\t\t\tBody = html_content.findAll(\"div\",{\"class\":'chapter-body'})\r\n\r\n\t\t\ttranslatedBody = Body[0].findAll(\"sentence\",{\"class\":'translated'})\r\n\t\t\tcount = 0\r\n\t\t\tfor transdiv in translatedBody:\r\n\t\t\t\ttemp  = transdiv.text\r\n\t\t\t\ttemp = temp.replace(\"\\t\", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\r\n\t\t\t\ttemp = temp.replace(\"\u201e\",\"\\\"\").replace(\"\u201d\",\"\\\"\")\r\n\t\t\t\tmatches = tool.check(temp)\r\n\t\t\t\t#print(len(matches))\r\n\t\t\t\tif(len(matches) > 0):\r\n\t\t\t\t\ttemp = language_check.correct(temp, matches)\r\n\t\t\t\t#print(temp)\r\n\t\t\t\tif count == 0:\r\n\t\t\t\t\tf.write(temp)\r\n\t\t\t\t\tf.write(\"\\n\")\r\n\t\t\t\t\tcount += 1\r\n\t\t\t\telse:\r\n\t\t\t\t\tf.write(temp)\r\n\t\t\t\tf.write(\"\\n\")\r\n\r\n\t\t\tf.write(\"\\n\\n\")\r\n\t\t\t# f1.write(\"\\n\\n\")\r\n\t\t\tprint(\"Completed page\")\r\n\t\texcept:\r\n\t\t\tcontinue\r\n\t\tnum = random.randint(5, 8)\r\n\r\n\t\ttime.sleep(num)\r\n\r\n\tf.close()\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from multiprocessing import Process\r", "import requests\r", "from bs4 import BeautifulSoup as soup\r", "import language_check\r", "import time\r", "import random\r"]}], [{"term": "def", "name": "webscrape_presale", "data": "def webscrape_presale(parsepage):\n\t'''This function will take a page and scrape its data for sale lot information\n\tIt will also download sale shapefile and move it to directory of this script file\n\t'''\n\n\t#webscrape sale page - gathering lot serial numbers from html \n\tserialnums = parsepage.find_all(\"span\", \"lot-name\")\n\t#ist comprehension - appending text into serialnums\n\tserialnums = [i.text for i in serialnums]\n\t\n\t#storing all data from tag 'td's with clas name \"lot-legal\n\t#this html container/tag has 3 pieces of information\n\tlegalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n\t\n\tlotclosings = parsepage.find_all(\"td\", \"lot-closing\")\n\t\n\t\n\tclosingInfo = []\n\tfor item in lotclosings:\n\t\t#finding date from lot closing element\n\t\tclosingDate = re.search(\"\\d+/\\d+/\\d+\", item.text)[0]\n\t\t\n\t\t#finding opening time from lot closing element - then calculate closing time\n\t\topeningTime = re.search(\"\\d+:\\d+\", item.text)[0]\n\t\t#calculating closing time \n\t\tclosingTime = str(int(openingTime.split(\":\")[0]) + bidDuration)+\":\" + openingTime.split(\":\")[1]\n\t\t\n\t\tdateAndTime = closingDate + \" \" + closingTime\n\t\tclosingInfo.append(dateAndTime)\n\t\n\t#initializing empty arrays\n\tacres = []\n\tdesc = []\n\tcounty = []\n\t\n\tfor item in legalinfo:\n\t\tcounty.append(item.contents[0].text)\n\t\tdesc.append(item.contents[1].text)\n\t\t#getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n\t\tacres.append(float(re.split(\":\\W\",item.contents[2].text)[1].replace(',','')))\n\n\t##getting shapefile from webpage  \n\t#clicking link of where shapefile is stored on sale page  \n\ttry:\n\t\tdriver.find_element_by_link_text(\"GIS Data WGS84\").click()\n\t\ttime.sleep(2)\n\texcept:\n\t\tprint(\"shapefile download unable to be clicked from webscraper\")\n\t\t\n\ttry:\n\t\tdriver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n\texcept:\n\t\tprint(\"Sale notice pdf was unable to be clicked\")\n\t#getting list of filenames in downloads\n\ttry:\n\t\tdownloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir)\n\texcept:\n\t\tdownloaddir = \"C:/Users/mishaun/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir) \n\t\t\n\t#pattern will find downloaded file name of shapefile\n\tpattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n\t\n\ttry:\n\t\t#searching through filenames in downlaods folder\n\t\tfinds = []\n\t\tfor file in downloads:\n\t\t\tif re.findall(pattern, file):\n\t\t\t\tfinds.append(file)\n\t\t\t\tbreak\n\t\t\t\n\t\t#moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n\t\tshutil.copy(downloaddir + finds[0], filepath)\n\texcept:\n\t\tprint(\"moving shapefile unable to be moved due to not finding file and/or not able to be clicked\")\n\t\t\n\t\t\n\ttry:\n\t\tos.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n\texcept:\n\t   pass\n   \n\treturn acres, desc, county, serialnums, closingInfo\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "fillexcel", "data": "def fillexcel():\n\t'''\n\tThis function will take scraped (global) values for lots and insert into sale spreadsheet \n\t'''\t\n\t\n\t#opening template sale notebook for modifications\n\t#preserving vba to keep formatting of workbook preserved - also keeping formulas \n\twb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n\tsheet = wb.active\n\t\n\t#updating sheet title to sale title\n\tsheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n\t\n\t#inserting values from webscrape into spreadsheet -8th row is where data rows begin\n\tfor i in range(0,len(serials)):\n\t\tsheet.cell(row = 8+i, column = 2, value = serials[i])\n\t\tsheet.cell(row = 8+i, column = 5, value = closing[i])\n\t\tsheet.cell(row = 8+i, column = 6, value = acres[i])\n\t\tsheet.cell(row = 8+i, column = 7, value = counties[i])\n\t\tsheet.cell(row = 8+i, column = 8, value = descriptions[i])\n\t\n\t#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n\tif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n\t\tprint(\"File already exists - Preventing overwrite of changes in excel file\")\n\telse:\n\t\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\t\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "fillwinnings", "data": "def fillwinnings():\n\t'''This function will take ourwinnings dictionary and add values to created spreadsheet\n\t'''\n\t\n\t#### insert our winnings into sale spreadsheet\n\twb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n\tsheet = wb.active\n\t\n\tfor i in range(0,len(ourwinnings)):\n\t\t#row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n\t\n\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "write_fillable_pdf", "data": "def write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n\t'''\n\tThis function will fill in pdf's forms based on a form pdf\n\t'''\n\t\n\ttemplate_pdf = pdfrw.PdfReader(input_pdf_path)\n\tannotations = template_pdf.pages[0][ANNOT_KEY]\n\tfor annotation in annotations:\n\t\tif annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n\t\t\tif annotation[ANNOT_FIELD_KEY]:\n\t\t\t\tkey = annotation[ANNOT_FIELD_KEY][1:-1]\n\t\t\t\tif key in data_dict.keys():\n\t\t\t\t\tannotation.update(\n\t\t\t\t\t\tpdfrw.PdfDict(V='{}'.format(data_dict[key]))\n\t\t\t\t\t)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "wonlotsDF", "data": "def wonlotsDF():\n\t'''\n\tThis function will create a dataframe of the won lots by reading information\n\tfrom completed sale note spreadsheet\n\tThe dataframe will then be used to parse pdf's \n\t'''\n\t#using openpyxl in order to read formulated values from spreadsheet\n\t# NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n\tdata_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n\tdataSheet = data_onlyWB.active\n\t\n\t#covnerting spreadsheet into dataframe\n\tdf = pd.DataFrame(dataSheet.values)\n\t\n\t#slicing the dataframe to get only relevant data\n\tdf = df.iloc[6:,1:25]\n\t#setting columns to first row of dataframe\n\tdf.columns = df.iloc[0]\n\t#dropping the repeated row with column names\n\tdf = df.drop(index =[6])\n\t\n\t#filtering data frame with values only won by magnum\n\twonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n\treturn wonlotsdf\n\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "createBidSheets", "data": "def createBidSheets():\n\t'''\n\tThis function will take a template pdf and generate pdf's based on wonlots dataframe\n\t'''\n\t#calling funciton wonlotsDF in ordre for bid sheets to be created \n\twonlotsdf = wonlotsDF()\n\n\ttemplatePDF = 'bidsheet template.pdf'\n\t\n\tfor i in range(0,len(wonlotsdf.index)):\n\t\n\t\tOutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n\t\t\n\t\tfields = {\n\t\t\t\t\"State\": stinitials,\n\t\t\t\t\"Date of Sale\": date,\n\t\t\t\t'Check Box for Oil and Gas' : \"x\",\n\t\t\t\t\"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n\t\t\t\t\"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n\t\t\t\t\"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n\t\t\t\t\"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n\t\t\t\t\"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n\t\t\t\t\"City\" : \"Corpus Christi\",\n\t\t\t\t\"State_2\": \"TX\",\n\t\t\t\t\"Zip Code\" : \"78401\"\n\t\t\t\t}\n\t\t\n\t\twrite_fillable_pdf(templatePDF, OutputPath, fields)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "openDI", "data": "def openDI():\n\t'''This function will open up DrillingInfo and log user in\n\t'''\n\t\n\tdriver = webdriver.Chrome(filepath + \"/chromedriver\")\n\twait = WebDriverWait(driver, 20)\n\t\t\n\tdriver.get(\"https://app.drillinginfo.com/gallery/\")\n\tuserfield = driver.find_element_by_name(\"username\")\n\tpassfield = driver.find_element_by_name(\"password\")\n\t\n\tuserfield.click()\n\tuserfield.send_keys(\"mbhaktamgm\")\n\tpassfield.click()\n\tpassfield.send_keys(\"itheCwe\")\n\tpassfield.send_keys(Keys.RETURN)\n\t\n\tmyworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n\tmyworkspaces.click()\n\t\n\tdefault_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n\tdefault_workspace.click()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}], [{"term": "def", "name": "webscrape_presale", "data": "def webscrape_presale(parsepage):\n\t'''This function will take a page and scrape its data for sale lot information\n\tIt will also download sale shapefile and move it to directory of this script file\n\t'''\n\n\t#webscrape sale page - gathering lot serial numbers from html \n\tserialnums = parsepage.find_all(\"span\", \"lot-name\")\n\t#ist comprehension - appending text into serialnums\n\tserialnums = [i.text for i in serialnums]\n\t\n\t#storing all data from tag 'td's with clas name \"lot-legal\n\t#this html container/tag has 3 pieces of information\n\tlegalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n\t\n\tlotclosings = parsepage.find_all(\"td\", \"lot-closing\")\n\t\n\t\n\tclosingInfo = []\n\tfor item in lotclosings:\n\t\t#finding date from lot closing element\n\t\tclosingDate = re.search(\"\\d+/\\d+/\\d+\", item.text)[0]\n\t\t\n\t\t#finding opening time from lot closing element - then calculate closing time\n\t\topeningTime = re.search(\"\\d+:\\d+\", item.text)[0]\n\t\t#calculating closing time \n\t\tclosingTime = str(int(openingTime.split(\":\")[0]) + bidDuration)+\":\" + openingTime.split(\":\")[1]\n\t\t\n\t\tdateAndTime = closingDate + \" \" + closingTime\n\t\tclosingInfo.append(dateAndTime)\n\t\n\t#initializing empty arrays\n\tacres = []\n\tdesc = []\n\tcounty = []\n\t\n\tfor item in legalinfo:\n\t\tcounty.append(item.contents[0].text)\n\t\tdesc.append(item.contents[1].text)\n\t\t#getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n\t\tacres.append(float(re.split(\":\\W\",item.contents[2].text)[1].replace(',','')))\n\n\t##getting shapefile from webpage  \n\t#clicking link of where shapefile is stored on sale page\t\n\tdriver.find_element_by_link_text(\"GIS Data WGS84\").click()\n\ttime.sleep(2)\n\ttry:\n\t\tdriver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n\texcept:\n\t\tprint(\"Sale notice pdf was unable to be clicked\")\n\t#getting list of filenames in downloads\n\ttry:\n\t\tdownloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir)\n\texcept:\n\t\tdownloaddir = \"C:/Users/mishaun/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir) \n\t\t\n\t#pattern will find downloaded file name of shapefile\n\tpattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n\t\n\t#searching through filenames in downlaods folder\n\tfinds = []\n\tfor file in downloads:\n\t\tif re.findall(pattern, file):\n\t\t\tfinds.append(file)\n\t\t\tbreak\n\t\t\n\t#moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n\tshutil.copy(downloaddir + finds[0], filepath)\n\t\n\ttry:\n\t\tos.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n\texcept:\n\t   pass\n   \n\treturn acres, desc, county, serialnums, closingInfo\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "fillexcel", "data": "def fillexcel():\n\t'''\n\tThis function will take scraped (global) values for lots and insert into sale spreadsheet \n\t'''\t\n\t\n\t#opening template sale notebook for modifications\n\t#preserving vba to keep formatting of workbook preserved - also keeping formulas \n\twb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n\tsheet = wb.active\n\t\n\t#updating sheet title to sale title\n\tsheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n\t\n\t#inserting values from webscrape into spreadsheet -8th row is where data rows begin\n\tfor i in range(0,len(serials)):\n\t\tsheet.cell(row = 8+i, column = 2, value = serials[i])\n\t\tsheet.cell(row = 8+i, column = 5, value = closing[i])\n\t\tsheet.cell(row = 8+i, column = 6, value = acres[i])\n\t\tsheet.cell(row = 8+i, column = 7, value = counties[i])\n\t\tsheet.cell(row = 8+i, column = 8, value = descriptions[i])\n\t\n\t#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n\tif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n\t\tprint(\"File already exists - Preventing overwrite of changes in excel file\")\n\telse:\n\t\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\t\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "fillwinnings", "data": "def fillwinnings():\n\t'''This function will take ourwinnings dictionary and add values to created spreadsheet\n\t'''\n\t\n\t#### insert our winnings into sale spreadsheet\n\twb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n\tsheet = wb.active\n\t\n\tfor i in range(0,len(ourwinnings)):\n\t\t#row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n\t\n\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "write_fillable_pdf", "data": "def write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n\t'''\n\tThis function will fill in pdf's forms based on a form pdf\n\t'''\n\t\n\ttemplate_pdf = pdfrw.PdfReader(input_pdf_path)\n\tannotations = template_pdf.pages[0][ANNOT_KEY]\n\tfor annotation in annotations:\n\t\tif annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n\t\t\tif annotation[ANNOT_FIELD_KEY]:\n\t\t\t\tkey = annotation[ANNOT_FIELD_KEY][1:-1]\n\t\t\t\tif key in data_dict.keys():\n\t\t\t\t\tannotation.update(\n\t\t\t\t\t\tpdfrw.PdfDict(V='{}'.format(data_dict[key]))\n\t\t\t\t\t)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "wonlotsDF", "data": "def wonlotsDF():\n\t'''\n\tThis function will create a dataframe of the won lots by reading information\n\tfrom completed sale note spreadsheet\n\tThe dataframe will then be used to parse pdf's \n\t'''\n\t#using openpyxl in order to read formulated values from spreadsheet\n\t# NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n\tdata_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n\tdataSheet = data_onlyWB.active\n\t\n\t#covnerting spreadsheet into dataframe\n\tdf = pd.DataFrame(dataSheet.values)\n\t\n\t#slicing the dataframe to get only relevant data\n\tdf = df.iloc[6:,1:25]\n\t#setting columns to first row of dataframe\n\tdf.columns = df.iloc[0]\n\t#dropping the repeated row with column names\n\tdf = df.drop(index =[6])\n\t\n\t#filtering data frame with values only won by magnum\n\twonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n\treturn wonlotsdf\n\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "createBidSheets", "data": "def createBidSheets():\n\t'''\n\tThis function will take a template pdf and generate pdf's based on wonlots dataframe\n\t'''\n\t#calling funciton wonlotsDF in ordre for bid sheets to be created \n\twonlotsdf = wonlotsDF()\n\n\ttemplatePDF = 'bidsheet template.pdf'\n\t\n\tfor i in range(0,len(wonlotsdf.index)):\n\t\n\t\tOutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n\t\t\n\t\tfields = {\n\t\t\t\t\"State\": stinitials,\n\t\t\t\t\"Date of Sale\": date,\n\t\t\t\t'Check Box for Oil and Gas' : \"x\",\n\t\t\t\t\"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n\t\t\t\t\"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n\t\t\t\t\"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n\t\t\t\t\"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n\t\t\t\t\"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n\t\t\t\t\"City\" : \"Corpus Christi\",\n\t\t\t\t\"State_2\": \"TX\",\n\t\t\t\t\"Zip Code\" : \"78401\"\n\t\t\t\t}\n\t\t\n\t\twrite_fillable_pdf(templatePDF, OutputPath, fields)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "openDI", "data": "def openDI():\n\t'''This function will open up DrillingInfo and log user in\n\t'''\n\t\n\tdriver = webdriver.Chrome(filepath + \"/chromedriver\")\n\twait = WebDriverWait(driver, 20)\n\t\t\n\tdriver.get(\"https://app.drillinginfo.com/gallery/\")\n\tuserfield = driver.find_element_by_name(\"username\")\n\tpassfield = driver.find_element_by_name(\"password\")\n\t\n\tuserfield.click()\n\tuserfield.send_keys(\"mbhaktamgm\")\n\tpassfield.click()\n\tpassfield.send_keys(\"itheCwe\")\n\tpassfield.send_keys(Keys.RETURN)\n\t\n\tmyworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n\tmyworkspaces.click()\n\t\n\tdefault_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n\tdefault_workspace.click()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}], [{"term": "def", "name": "webscrape_presale", "data": "def webscrape_presale(parsepage):\n\t'''This function will take a page and scrape its data for sale lot information\n\tIt will also download sale shapefile and move it to directory of this script file\n\t'''\n\n\t#webscrape sale page - gathering lot serial numbers from html \n\tserialnums = parsepage.find_all(\"span\", \"lot-name\")\n\t#ist comprehension - appending text into serialnums\n\tserialnums = [i.text for i in serialnums]\n\t\n\t#storing all data from tag 'td's with clas name \"lot-legal\n\t#this html container/tag has 3 pieces of information\n\tlegalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n\t\n\tlotclosings = parsepage.find_all(\"td\", \"lot-closing\")\n\t\n\t\n\tclosingInfo = []\n\tfor item in lotclosings:\n\t\t#finding date from lot closing element\n\t\tclosingDate = re.search(\"\\d+/\\d+/\\d+\", item.text)[0]\n\t\t\n\t\t#finding opening time from lot closing element - then calculate closing time\n\t\topeningTime = re.search(\"\\d+:\\d+\", item.text)[0]\n\t\t#calculating closing time \n\t\tclosingTime = str(int(openingTime.split(\":\")[0]) + bidDuration)+\":\" + openingTime.split(\":\")[1]\n\t\t\n\t\tdateAndTime = closingDate + \" \" + closingTime\n\t\tclosingInfo.append(dateAndTime)\n\t\n\t#initializing empty arrays\n\tacres = []\n\tdesc = []\n\tcounty = []\n\t\n\tfor item in legalinfo:\n\t\tcounty.append(item.contents[0].text)\n\t\tdesc.append(item.contents[1].text)\n\t\t#getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n\t\tacres.append(float(re.split(\":\\W\",item.contents[2].text)[1].replace(',','')))\n\n\t##getting shapefile from webpage  \n\t#clicking link of where shapefile is stored on sale page\t\n\tdriver.find_element_by_link_text(\"GIS Data WGS84\").click()\n\ttime.sleep(2)\n\ttry:\n\t\tdriver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n\texcept:\n\t\tprint(\"Sale notice pdf was unable to be clicked\")\n\t#getting list of filenames in downloads\n\ttry:\n\t\tdownloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir)\n\texcept:\n\t\tdownloaddir = \"C:/Users/mishaun/Downloads/\"\n\t\tdownloads = os.listdir(downloaddir) \n\t\t\n\t#pattern will find downloaded file name of shapefile\n\tpattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n\t\n\t#searching through filenames in downlaods folder\n\tfinds = []\n\tfor file in downloads:\n\t\tif re.findall(pattern, file):\n\t\t\tfinds.append(file)\n\t\t\tbreak\n\t\t\n\t#moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n\tshutil.copy(downloaddir + finds[0], filepath)\n\t\n\ttry:\n\t\tos.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n\texcept:\n\t   pass\n   \n\treturn acres, desc, county, serialnums, closingInfo\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "fillexcel", "data": "def fillexcel():\n\t'''\n\tThis function will take scraped (global) values for lots and insert into sale spreadsheet \n\t'''\t\n\t\n\t#opening template sale notebook for modifications\n\t#preserving vba to keep formatting of workbook preserved - also keeping formulas \n\twb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n\tsheet = wb.active\n\t\n\t#updating sheet title to sale title\n\tsheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n\t\n\t#inserting values from webscrape into spreadsheet -8th row is where data rows begin\n\tfor i in range(0,len(serials)):\n\t\tsheet.cell(row = 8+i, column = 2, value = serials[i])\n\t\tsheet.cell(row = 8+i, column = 5, value = closing[i])\n\t\tsheet.cell(row = 8+i, column = 6, value = acres[i])\n\t\tsheet.cell(row = 8+i, column = 7, value = counties[i])\n\t\tsheet.cell(row = 8+i, column = 8, value = descriptions[i])\n\t\n\t#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n\tif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n\t\tprint(\"File already exists - Preventing overwrite of changes in excel file\")\n\telse:\n\t\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\t\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "fillwinnings", "data": "def fillwinnings():\n\t'''This function will take ourwinnings dictionary and add values to created spreadsheet\n\t'''\n\t\n\t#### insert our winnings into sale spreadsheet\n\twb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n\tsheet = wb.active\n\t\n\tfor i in range(0,len(ourwinnings)):\n\t\t#row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n\t\tsheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n\t\n\twb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n\twb.close()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "write_fillable_pdf", "data": "def write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n\t'''\n\tThis function will fill in pdf's forms based on a form pdf\n\t'''\n\t\n\ttemplate_pdf = pdfrw.PdfReader(input_pdf_path)\n\tannotations = template_pdf.pages[0][ANNOT_KEY]\n\tfor annotation in annotations:\n\t\tif annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n\t\t\tif annotation[ANNOT_FIELD_KEY]:\n\t\t\t\tkey = annotation[ANNOT_FIELD_KEY][1:-1]\n\t\t\t\tif key in data_dict.keys():\n\t\t\t\t\tannotation.update(\n\t\t\t\t\t\tpdfrw.PdfDict(V='{}'.format(data_dict[key]))\n\t\t\t\t\t)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "wonlotsDF", "data": "def wonlotsDF():\n\t'''\n\tThis function will create a dataframe of the won lots by reading information\n\tfrom completed sale note spreadsheet\n\tThe dataframe will then be used to parse pdf's \n\t'''\n\t#using openpyxl in order to read formulated values from spreadsheet\n\t# NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n\tdata_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n\tdataSheet = data_onlyWB.active\n\t\n\t#covnerting spreadsheet into dataframe\n\tdf = pd.DataFrame(dataSheet.values)\n\t\n\t#slicing the dataframe to get only relevant data\n\tdf = df.iloc[6:,1:25]\n\t#setting columns to first row of dataframe\n\tdf.columns = df.iloc[0]\n\t#dropping the repeated row with column names\n\tdf = df.drop(index =[6])\n\t\n\t#filtering data frame with values only won by magnum\n\twonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n\treturn wonlotsdf\n\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "createBidSheets", "data": "def createBidSheets():\n\t'''\n\tThis function will take a template pdf and generate pdf's based on wonlots dataframe\n\t'''\n\t#calling funciton wonlotsDF in ordre for bid sheets to be created \n\twonlotsdf = wonlotsDF()\n\n\ttemplatePDF = 'bidsheet template.pdf'\n\t\n\tfor i in range(0,len(wonlotsdf.index)):\n\t\n\t\tOutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n\t\t\n\t\tfields = {\n\t\t\t\t\"State\": stinitials,\n\t\t\t\t\"Date of Sale\": date,\n\t\t\t\t'Check Box for Oil and Gas' : \"x\",\n\t\t\t\t\"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n\t\t\t\t\"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n\t\t\t\t\"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n\t\t\t\t\"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n\t\t\t\t\"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n\t\t\t\t\"City\" : \"Corpus Christi\",\n\t\t\t\t\"State_2\": \"TX\",\n\t\t\t\t\"Zip Code\" : \"78401\"\n\t\t\t\t}\n\t\t\n\t\twrite_fillable_pdf(templatePDF, OutputPath, fields)\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}, {"term": "def", "name": "openDI", "data": "def openDI():\n\t'''This function will open up DrillingInfo and log user in\n\t'''\n\t\n\tdriver = webdriver.Chrome(filepath + \"/chromedriver\")\n\twait = WebDriverWait(driver, 20)\n\t\t\n\tdriver.get(\"https://app.drillinginfo.com/gallery/\")\n\tuserfield = driver.find_element_by_name(\"username\")\n\tpassfield = driver.find_element_by_name(\"password\")\n\t\n\tuserfield.click()\n\tuserfield.send_keys(\"mbhaktamgm\")\n\tpassfield.click()\n\tpassfield.send_keys(\"itheCwe\")\n\tpassfield.send_keys(Keys.RETURN)\n\t\n\tmyworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n\tmyworkspaces.click()\n\t\n\tdefault_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n\tdefault_workspace.click()\n", "description": null, "category": "webscraping", "imports": ["import os, re, shutil, time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait", "from selenium.webdriver.support import expected_conditions as EC", "from bs4 import BeautifulSoup", "import openpyxl", "import pandas as pd", "import pdfrw"]}], [], [{"term": "def", "name": "webscrape", "data": "def webscrape(link):\n\ttry:\n\t\tpage = requests.get(link)\n\texcept Exception as e:\n\t\terror_type,error_obj,error_info = sys.exc_info()\n\t\tprint(\"error\")\n\t\n\tsoup = BeautifulSoup(page.content,'html.parser')\n\tbody  = soup.find('script',attrs={'id':'__NEXT_DATA__'})\n\tpydict = json.loads(body.text)\n\trestaurant = pydict['props']['pageProps']['initialMenuState']['restaurant']\n\tresult = {}\n\tresult[\"restaurant_name\"]=restaurant[\"name\"]\n\tresult[\"restaurant_logo\"] = restaurant['logo']\n\tresult[\"latitude\"] = float(restaurant['latitude'])\n\tresult[\"longitude\"] = float(restaurant['longitude'])\n\tresult[\"cuisine_tags\"] = restaurant['cuisineString'].split(\", \")\n\tmenu_items = pydict['props']['pageProps']['initialMenuState']['menuData']['items']\n\titems = []\n\tfor i in range(len(menu_items)):\n\t\titem = menu_items[i]\n\t\titem_name = item[\"name\"]\n\t\titem_price = float(item[\"price\"])\n\t\titem_description = item[\"description\"]\n\t\titem_image = item[\"image\"]\n\t\titems.append({\"item_name\":item_name,\"item_description\":item_description,\"item_price\":item_price,\"item_image\":item_image})\n\tresult[\"menu_items\"]= items\n", "description": null, "category": "webscraping", "imports": ["import time", "import requests", "from bs4 import BeautifulSoup", "import sys", "import json"]}], [{"term": "class", "name": "AmazonSpiderSpider", "data": "class AmazonSpiderSpider(scrapy.Spider):\n\tname = 'amazon'\n\tstart_urls = ['https://www.amazon.com/s?k=masks+50pcs&ref=nb_sb_noss_1']\n\tpageNumber = 2\n\n\tdef parse(self, response):\n\t\titems = WebscrapeItem()\n\n\t\tproduct_name = response.css('.a-color-base.a-text-normal').css('::text').extract()\n\t\tproduct_price = response.css('.a-price-whole::text').extract()\n\n\t\titems['product_name'] = product_name\n\t\titems['product_price'] = product_price\n\n\t\tyield items\n\n\t\tnextPage = 'https://www.amazon.com/s?k=masks+50pcs&page=' + str(AmazonSpiderSpider.pageNumber)\n\n\t\tif AmazonSpiderSpider.pageNumber <= 100:\n\t\t\tAmazonSpiderSpider.pageNumber += 1\n", "description": null, "category": "webscraping", "imports": ["import scrapy", "from ..items import WebscrapeItem"]}], [], [], [], [{"term": "class", "name": "classwebscrape:", "data": "class webscrape:\n\tdef __init__(self):\n\t\tpass\n\t\n\tdef scape_url(self, url):\n\t\tresp = requests.get(url)\n\t\tdata = resp.json()\n\t\tissues = pd.DataFrame(data, columns=['number', 'title',\n\t\t\t\t\t\t\t\t\t 'labels', 'state'])\n\t\tprint (data)\n\t\t\n", "description": null, "category": "webscraping", "imports": ["import sys", "import numpy as np", "import pandas as pd", "import requests ", "import matplotlib.pyplot as plt "]}, {"term": "def", "name": "main", "data": "def main():\n\turl = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n\tscraper = webscrape()\n\tscraper.scape_url(url)\n", "description": null, "category": "webscraping", "imports": ["import sys", "import numpy as np", "import pandas as pd", "import requests ", "import matplotlib.pyplot as plt "]}], [], [{"term": "def", "name": "shot", "data": "def shot(s_type):\n\tif \"jump shot\" in s_type.lower():\n\t\treturn \"jump\"\n\telif \"layup shot\" in s_type.lower():\n\t\treturn \"layup\"\n\telse:\n\t\treturn \"else\"\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pandas as pd", "import numpy as np", "from sklearn.cross_validation import train_test_split", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier", "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV", "from nba_py import player", "import webscrape"]}, {"term": "def", "name": "shot_dist", "data": "def shot_dist(dist):\n\tif dist < 8:\n\t\treturn \"less than 8\"\n\telif dist < 16:\n\t\treturn \"8-16\"\n\telif dist < 24:\n\t\treturn \"16-24\"\n\telse:\n\t\treturn \"24+\"\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pandas as pd", "import numpy as np", "from sklearn.cross_validation import train_test_split", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier", "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV", "from nba_py import player", "import webscrape"]}, {"term": "def", "name": "def_dist", "data": "def def_dist(dist):\n\tif dist < 2:\n\t\treturn \"0-2\"\n\telif dist < 4:\n\t\treturn \"2-4\"\n\telif dist < 6:\n\t\treturn \"4-6\"\n\telse:\n\t\treturn \"6+\"\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pandas as pd", "import numpy as np", "from sklearn.cross_validation import train_test_split", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier", "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV", "from nba_py import player", "import webscrape"]}, {"term": "def", "name": "transform_web", "data": "def transform_web(data):\n\tdata = data.copy()\n\tshot_type = pd.get_dummies(data[\"Shot Type\"].apply(shot))\n\tdata[\"Shot Dist.\"] = data[\"Shot Dist.\"].apply(lambda x : float(x.replace(\"ft.\", \"\")))\n\tshot_clock = data[\"Shot Clock\"].apply(lambda x: float(x))\n\ttouch_time = data[\"Touch Time\"].apply(lambda x: float(x))\n\tdrib = data[\"Drib.\"].apply(lambda x: int(x))\n\tdata[\"Def Dist.\"] = data[\"Def Dist.\"].apply(lambda x: float(x))\n\n\tdef_dist_c = pd.get_dummies(data[\"Def Dist.\"].apply(def_dist))\n\n\t\n\tshot_dist_c = pd.get_dummies(data[\"Shot Dist.\"].apply(shot_dist))\n\t# if \"24+\" not in shot_dist_c.columns:\n\t#\t shot_dist_c[\"24+\"] = 0\n\t\n\tcon = [shot_type, shot_clock, data[\"Shot Dist.\"],touch_time, drib, data[\"Def Dist.\"],def_dist_c, shot_dist_c, (data[\"Made?\"]==\"Yes\").astype(int)]\n\tnew_shot_chart = pd.concat(con , axis=1)\n\n\t# pred = ['16-24', '24+', '8-16', 'less than 8', 'else', 'jump', 'layup', 'Made?']\n\tpred = ['Shot Dist.','Def Dist.', 'else', 'jump', 'layup', 'Made?']\n\n\treturn new_shot_chart[pred]\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pandas as pd", "import numpy as np", "from sklearn.cross_validation import train_test_split", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier", "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV", "from nba_py import player", "import webscrape"]}, {"term": "def", "name": "predictor", "data": "def predictor(athlete, season):\n\n\tweb_df = webscrape.getData(athlete, season)\n\ttransformed_web = transform_web(web_df)\n\tlogistic = LogisticRegression()\n\n\tpredictors = transformed_web.columns[:-1]\n\n\tlogistic.fit(transformed_web[predictors], transformed_web[\"Made?\"])\n\treturn logistic\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pandas as pd", "import numpy as np", "from sklearn.cross_validation import train_test_split", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier", "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV", "from nba_py import player", "import webscrape"]}, {"term": "def", "name": "transform_big", "data": "def transform_big(data):\n\n\tdata = data.copy()\n\tshot_type = pd.get_dummies(data[\"Shot Type\"].apply(shot))\n\tdata[\"Shot Dist.\"] = data[\"Shot Dist.\"].apply(lambda x : x.replace(\"ft.\", \"\"))\n\tdata[\"Shot Dist.\"] = data[\"Shot Dist.\"].apply(lambda x : 0 if x== \"\" else float(x))\n\t\n\t# shot_clock = data[\"Shot Clock\"].apply(lambda x: 0 if x == \"\" else float(x))\n\t# touch_time = data[\"Touch Time\"].apply(lambda x: float(x))\n\t# drib = data[\"Drib.\"].apply(lambda x: int(x))\n\tdata[\"Def Dist.\"] = data[\"Def Dist.\"].apply(lambda x: float(x))\n\n\t# def_dist_c = pd.get_dummies(data[\"Def Dist.\"].apply(def_dist))\n\t\n\tplayer_c = pd.get_dummies(data[\"Player\"])\n\n\tshot_dist_c = pd.get_dummies(data[\"Shot Dist.\"].apply(shot_dist))\n\t\n\tcon = [player_c, shot_type , data[\"Def Dist.\"],\n\t\t\tshot_dist_c, data[\"Shot Dist.\"],(data[\"Made?\"]==\"Yes\").astype(int)]\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pandas as pd", "import numpy as np", "from sklearn.cross_validation import train_test_split", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier", "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV", "from nba_py import player", "import webscrape"]}, {"term": "def", "name": "getDataFrame", "data": "def getDataFrame(lst):\n\tapp = []\n\tfor player in lst:\n\t\tapp.append(webscrape.getData(player, \"2014\"))\n\tprint \"done\"\n\tdf_a = pd.concat(app)\n\tdf_a.reset_index(drop=True, inplace =True)\n\treturn transform_big(df_a)\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pandas as pd", "import numpy as np", "from sklearn.cross_validation import train_test_split", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier", "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV", "from nba_py import player", "import webscrape"]}, {"term": "def", "name": "large_model", "data": "def large_model(lst):\n\tdf_guards = getDataFrame(lst)\n\t\n\tlogistic = LogisticRegression()\n\n\tpredictors = df_guards.columns[:-1]\n\n\tlogistic.fit(df_guards[predictors], df_guards[\"Made?\"])\n\treturn logistic, predictors\n", "description": null, "category": "webscraping", "imports": ["import requests", "import pandas as pd", "import numpy as np", "from sklearn.cross_validation import train_test_split", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier", "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV", "from nba_py import player", "import webscrape"]}], [], [{"term": "def", "name": "WebScrape", "data": "def WebScrape(startPage, endPage):\r\n\tfileName = 'super_magic_god_of_harry_potter_langCheck'+ str(startPage) + '_' + str(endPage) +'.txt'\r\n\tf= open(fileName,\"w+\",encoding=\"utf-8\")\r\n\r\n\tmyUrl = 'http://lnmtl.com/chapter/super-magic-god-of-harry-potter-chapter-'\r\n\r\n\r\n\tfor page in range(startPage, endPage):\r\n\t\t#traverse the Site\r\n\t\tprint(page)\r\n\t\tsrhUrl =  myUrl + str(page) + \"/\"\r\n\t\tuClient =  requests.get(srhUrl)\r\n\t\tuClient.encoding = \"utf-8\"\r\n\t\thtml_content = soup(uClient.content, 'html.parser')\r\n\r\n\t\tBody = html_content.findAll(\"div\",{\"class\":'chapter-body'})\r\n\r\n\t\ttranslatedBody = Body[0].findAll(\"sentence\",{\"class\":'translated'})\r\n\t\tcount = 0\r\n\t\tfor transdiv in translatedBody:\r\n\t\t\ttemp  = transdiv.text\r\n\t\t\ttemp = temp.replace(\"\\t\", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\r\n\t\t\ttemp = temp.replace(\"\u201e\",\"\\\"\").replace(\"\u201d\",\"\\\"\")\r\n\t\t\tmatches = tool.check(temp)\r\n\t\t\t#print(len(matches))\r\n\t\t\tif(len(matches) > 0):\r\n\t\t\t\ttemp = language_check.correct(temp, matches)\r\n\t\t\t#print(temp)\r\n\t\t\tif count == 0:\r\n\t\t\t\tf.write(temp)\r\n\t\t\t\tf.write(\"\\n\")\r\n\t\t\t\tcount += 1\r\n\t\t\telse:\r\n\t\t\t\tf.write(temp)\r\n\t\t\tf.write(\"\\n\")\r\n\r\n\r\n\t\tf.write(\"\\n\\n\")\r\n\t\t# f1.write(\"\\n\\n\")\r\n\t\tprint(\"Completed page\")\r\n\r\n\t\tnum = random.randint(5, 8)\r\n\r\n\t\ttime.sleep(num)\r\n\r\n\tf.close()\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from multiprocessing import Process\r", "import requests\r", "from bs4 import BeautifulSoup as soup\r", "import language_check\r", "import time\r", "import random\r"]}], [{"term": "def", "name": "WebScrape", "data": "def WebScrape( myUrl, fileName, startPage, endPage):\r\n\r\n\tif os.path.exists(fileName):\r\n\t\tprint(\"Append TO existing file\")\r\n\t\tf= open(fileName,\"a+\",encoding=\"utf-8\")\r\n\telse:\r\n\t\tprint(\"Create new file\")\r\n\t\tf= open(fileName,\"w+\",encoding=\"utf-8\")\r\n\t#myUrl = 'https://comrademao.com/mtl/all-attributes-martial-path/all-attributes-martial-path-chapter-'\r\n\r\n\tfor page in range(startPage, endPage):\r\n\t\t#traverse the Site\r\n\t\tprint(page)\r\n\t\tsrhUrl =  myUrl + str(page) + \"/\"\r\n\t\ttry:\r\n\t\t\tuClient =  requests.get(srhUrl)\r\n\t\t\tuClient.encoding = \"utf-8\"\r\n\t\t\thtml_content = soup(uClient.content, 'html.parser')\r\n\r\n\t\t\tBody = html_content.findAll(\"article\",{\"class\":'status-publish'})\r\n\r\n\t\t\ttranslatedBody = Body[0].findAll(\"p\")\r\n\t\t\ttransaltedText = translatedBody[0].find_all(\"p\",class_=False)\r\n\t\t\tcount = 0\r\n\t\t\tfor transdiv in transaltedText:\r\n\t\t\t\ttemp  = transdiv.text\r\n\t\t\t\ttemp = temp.replace(\"\\t\", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\r\n\t\t\t\ttemp = temp.replace(\"\u201e\",\"\\\"\").replace(\"\u201d\",\"\\\"\").replace(\"\u201c\",\"\\\"\").replace(\"\u201d\",\"\\\"\")\r\n\t\t\t\tmatches = tool.check(temp)\r\n\t\t\t\t#print(len(matches))\r\n\t\t\t\tif(len(matches) > 0):\r\n\t\t\t\t\ttemp = language_check.correct(temp, matches)\r\n\t\t\t\t#print(temp)\r\n\t\t\t\tif count == 0:\r\n\t\t\t\t\tf.write(temp)\r\n\t\t\t\t\tf.write(\"\\n\")\r\n\t\t\t\t\tcount += 1\r\n\t\t\t\telse:\r\n\t\t\t\t\tf.write(temp)\r\n\t\t\t\tf.write(\"\\n\")\r\n\r\n\t\t\tf.write(\"\\n\\n\")\r\n\t\t\tprint(\"Completed page\")\r\n\t\texcept:\r\n\t\t\tcontinue\r\n\t\tnum = random.randint(5, 8)\r\n\r\n\t\ttime.sleep(num)\r\n\r\n\tf.close()\r\n\r\n", "description": null, "category": "webscraping", "imports": ["import os\r", "import requests\r", "from bs4 import BeautifulSoup as soup\r", "import language_check\r", "import time\r", "import random\r"]}], [{"term": "class", "name": "classproteinProduct:", "data": "class proteinProduct:\n\tdef __init__(self, name, desc='', proteinPercentage=0, proteinPerItem=0, price=0, ppk=0, itemsInBatch=0, site=''):\n\t\tself.name = name\n\t\tself.description = desc\n\t\tself.priceTotal = priceTotal\n\t\tself.proteinPercentage = proteinPercentage\n\t\tself.proteinPerItem = proteinPerItem\n\t\tself.itemsInBatch = itemsInBatch\n\t\tself.proteinTotal = self.ItemsInBatch * self.proteinPerItem\n\t\tself.ppk = self.proteinTotal / self.priceTotal\n\t\tself.site = site\n\n\t\t# ppk = proteinTotal / priceTotal\n\t\t# ProteinTotal = ItemsInBatch * proteinPerItem\n\n\t\t# * Name, Description(desc, proteininneh\u00e5ll, j\u00e4mf\u00f6r med gainomax bar), price, protein per krona\n\n", "description": null, "category": "webscraping", "imports": ["#from . import itemToData as i2d", "from webscraper import webscrapeGymgrossisten as wsg", "from webscraper import webscrapeMatsmart as wsm", "import json"]}, {"term": "def", "name": "ppk", "data": "def ppk():\n\tpass\n\n", "description": null, "category": "webscraping", "imports": ["#from . import itemToData as i2d", "from webscraper import webscrapeGymgrossisten as wsg", "from webscraper import webscrapeMatsmart as wsm", "import json"]}, {"term": "def", "name": "wsgToJson", "data": "def wsgToJson():\n\tjsona = json.dumps([vars(item) for item in wsg.getProducts()], indent=4)\n\tf = open(\"data/gymgrossisten.json\", \"w\")\n\tf.write(jsona)\n\tf.close()\n\n", "description": null, "category": "webscraping", "imports": ["#from . import itemToData as i2d", "from webscraper import webscrapeGymgrossisten as wsg", "from webscraper import webscrapeMatsmart as wsm", "import json"]}, {"term": "def", "name": "wsmToJson", "data": "def wsmToJson():\n\tjsona = json.dumps([vars(item) for item in wsm.getProducts()], indent=4)\n\tf = open(\"data/matsmart.json\", \"w\")\n\tf.write(jsona)\n\tf.close()\n\n", "description": null, "category": "webscraping", "imports": ["#from . import itemToData as i2d", "from webscraper import webscrapeGymgrossisten as wsg", "from webscraper import webscrapeMatsmart as wsm", "import json"]}, {"term": "def", "name": "bigFunc", "data": "def bigFunc():\n\tpass\n\t# WHAT WE WANT:\n\t# * pull json (A)\n\t# * pull scrape (B)\n\t# * Turn all in A which is in B \"active\". All in A which is not in B \"inactive\"\n\t# * All in B which are not in A, write to file with basic values\n\n\t# WHAT INFORMATION DO WE WANT IN BIG FILE:\n\t# * Name, Description(desc, proteininneh\u00e5ll, j\u00e4mf\u00f6r med gainomax bar), price, protein per krona\n", "description": null, "category": "webscraping", "imports": ["#from . import itemToData as i2d", "from webscraper import webscrapeGymgrossisten as wsg", "from webscraper import webscrapeMatsmart as wsm", "import json"]}], [], [{"term": "class", "name": "classWebscrapeTrails:", "data": "class WebscrapeTrails:\n\tdef __init__(self):\n\n\t\tself.CURRENT_DIRECTORY = os.getcwd()\n\n\t\tself.MAIN_URL = \"https://jollyturns.com/resort/united-states-of-america\"\n\n\t\tself.ALPINE_MEADOWS_URL = f\"{self.MAIN_URL}/alpine-meadows/\"\n\t\tself.ARAPAHOE_BASIN_URL = f\"{self.MAIN_URL}/arapahoe-basin/\"\n\t\tself.ASPEN_SNOWMASS_URL = f\"{self.MAIN_URL}/aspen-snowmass/\"\n\t\tself.BALD_MOUNTAIN_URL = f\"{self.MAIN_URL}/bald-mountain/\"\n\t\tself.BEAVER_CREEK_URL = f\"{self.MAIN_URL}/beaver-creek-resort/\"\n\t\tself.COPPER_URL = f\"{self.MAIN_URL}/copper-mountain-resort/\"\n\t\tself.CRESTED_BUTTE_URL = f\"{self.MAIN_URL}/crested-butte-mountain-resort/\"\n\t\tself.DIAMOND_PEAK_URL = f\"{self.MAIN_URL}/diamond-peak/\"\n\t\tself.ELDORA_URL = f\"{self.MAIN_URL}/eldora-mountain-resort/\"\n\t\tself.JACKSON_HOLE_URL = f\"{self.MAIN_URL}/jackson-hole/\"\n\t\tself.LOVELAND_URL = f\"{self.MAIN_URL}/loveland-ski-area/\"\n\t\tself.MONARCH_URL = f\"{self.MAIN_URL}/monarch-ski-area/\"\n\t\tself.STEAMBOAT_URL = f\"{self.MAIN_URL}/steamboat-ski-resort/\"\n\t\tself.TAOS_URL = f\"{self.MAIN_URL}/taos-ski-valley/\"\n\t\tself.TELLURIDE_URL = f\"{self.MAIN_URL}/telluride-ski-resort/\"\n\t\tself.VAIL_URL = f\"{self.MAIN_URL}/vail-ski-resort/\"\n\t\tself.WINTER_PARK_URL = f\"{self.MAIN_URL}/winter-park-resort/\"\n\t\tself.WOLF_CREEK_URL = f\"{self.MAIN_URL}/wolf-creek-ski-area/\"\n\n\t\tself.URLs = [\n\t\t\tself.ALPINE_MEADOWS_URL,\n\t\t\tself.ARAPAHOE_BASIN_URL,\n\t\t\tself.ASPEN_SNOWMASS_URL,\n\t\t\tself.BALD_MOUNTAIN_URL,\n\t\t\tself.BEAVER_CREEK_URL,\n\t\t\tself.COPPER_URL,\n\t\t\tself.CRESTED_BUTTE_URL,\n\t\t\tself.DIAMOND_PEAK_URL,\n\t\t\tself.ELDORA_URL,\n\t\t\tself.JACKSON_HOLE_URL,\n\t\t\tself.LOVELAND_URL,\n\t\t\tself.MONARCH_URL,\n\t\t\tself.STEAMBOAT_URL,\n\t\t\tself.TAOS_URL,\n\t\t\tself.TELLURIDE_URL,\n\t\t\tself.VAIL_URL,\n\t\t\tself.WINTER_PARK_URL,\n\t\t\tself.WOLF_CREEK_URL,\n\t\t]\n\n\t\tself.browser_options = webdriver.ChromeOptions()\n\t\tself.browser_options.add_argument(\"--no-sandbox\")\n\t\tself.browser_options.add_argument(\"--headless\")\n\t\tself.browser_options.add_argument(\"--disable-gpu\")\n\n\t\tself.browser = webdriver.Chrome(options=self.browser_options)\n\n\t\tself.lst_run_difficulty = [\n\t\t\t\"skiruns-green\",\n\t\t\t\"skiruns-blue\",\n\t\t\t\"skiruns-black\",\n\t\t\t\"skiruns-double-black\",\n\t\t]\n\n\t\tself.blank_value = \"__NA__\"\n\n\tdef make_tables(self, URL: str) -> pd.core.frame.DataFrame:\n\t\t\"\"\"\n\t\tInputs:\n\t\t\tURL from URLs (str)\n\t\tOutputs:\n\t\t\tPandas DataFrame of ski resort information\n\t\t\"\"\"\n\n\t\tself.browser.get(URL)\n\n\t\ttime.sleep(3)\n\n\t\tsoup = BeautifulSoup(self.browser.page_source, \"html.parser\")\n\n\t\tX_web_trail = soup.select(\"table.table-striped tr\")\n\n\t\tlst_rows = [x.text.strip() for x in X_web_trail]\n\t\tlst_rows = [i.replace(\"  \", \"|\") for i in lst_rows]\n\t\tlst_rows = [i.replace(\" ft\", \"|\") for i in lst_rows]\n\t\tlst_rows = [i.replace(\" mi\", \"|\") for i in lst_rows]\n\n\t\tlst_cols = [\n\t\t\t\"Name Bottom Top Vertical rise\",\n\t\t\t\"Name Bottom Top Vertical drop Length\",\n\t\t\t\"Name Elevation\",\n\t\t]\n\n\t\t# Indices where headers start, separating runs by difficulty\n\t\tidx_headers = [i for i, j in enumerate(lst_rows) if j in lst_cols]\n\n\t\t# Create DataFrame from rows\n\t\tdf_trails = pd.DataFrame(lst_rows, columns=[\"trail_data\"]).reset_index(\n\t\t\tdrop=True\n\t\t)\n\n\t\t# Expand DataFrame values into separate columns\n\t\tdf_trails = df_trails[\"trail_data\"].str.split(\"|\", expand=True)\n\t\tdf_trails.columns = [\n\t\t\t\"Trail Name\",\n\t\t\t\"Bottom Elev (ft)\",\n\t\t\t\"Top Elev (ft)\",\n\t\t\t\"Vertical Drop (ft)\",\n\t\t\t\"Length (mi)\",\n\t\t\t\"Blank\",\n\t\t]\n\n\t\tdf_trails.drop(\"Blank\", axis=1, inplace=True)\n\n\t\tlst_difficulty = soup.select(\"h4\")\n\t\tlst_difficulty = [l.text.strip() for l in lst_difficulty]\n\t\tlst_difficulty = [i.replace(\"Ski runs: \", \"\") for i in lst_difficulty]\n\n\t\tdf_difficulties = pd.DataFrame(lst_difficulty, index=idx_headers)\n\n\t\tdf_combined = pd.merge(\n\t\t\tdf_trails, df_difficulties, left_index=True, right_index=True, how=\"outer\"\n\t\t)\n\n\t\tdf_combined.rename(columns={0: \"Difficulty\"}, inplace=True)\n\n\t\tdf_combined[\"Difficulty\"].fillna(method=\"ffill\", inplace=True)\n\n\t\t# Remove rows which are not trail names\n\t\tdf_combined = df_combined[\n\t\t\t~df_combined[\"Trail Name\"].isin(lst_cols)\n\t\t].reset_index(drop=True)\n\n\t\t# Remove runs with no name and trails with __NA__ value\n\t\tdf_combined = df_combined[df_combined[\"Trail Name\"].notnull()].reset_index(\n\t\t\tdrop=True\n\t\t)\n\t\tdf_combined = df_combined[\n\t\t\tdf_combined[\"Trail Name\"] != self.blank_value\n\t\t].reset_index(drop=True)\n\n\t\t# Remove Lifts, Restaurants, and Terrain Park\n\t\tdf_combined = df_combined[\n\t\t\t~df_combined[\"Difficulty\"].isin([\"Lifts\", \"Restaurants\", \"terrain park\"])\n\t\t].reset_index(drop=True)\n\n\t\t# Add URL\n\t\tdf_combined[\"URL\"] = URL\n\n\t\treturn df_combined\n\n\tdef rename_resorts(self, df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n\t\t\"\"\"\n\t\tRename resorts for recommendation system\n\n\t\tINPUT\n\t\t\tdf: Pandas DataFrame\n\t\tOUTPUT\n\t\t\tPandas DataFrame, with resort_name column altered\n\t\t\"\"\"\n\n\t\tdf[\"Resort\"] = df[\"URL\"].str.split(\"united-states-of-america/\", 1, expand=True)[\n\t\t\t1\n\t\t]\n\t\tdf[\"Resort\"] = df[\"Resort\"].str.split(\"/\", 1, expand=True)[0]\n\n\t\tdict_webscrape_trail_names = {\n\t\t\t\"alpine-meadows\": \"Alpine Meadows\",\n\t\t\t\"arapahoe-basin\": \"Arapahoe Basin\",\n\t\t\t\"aspen-snowmass\": \"Aspen Snowmass\",\n\t\t\t\"bald-mountain\": \"Bald Mountain\",\n\t\t\t\"beaver-creek-resort\": \"Beaver Creek\",\n\t\t\t\"copper-mountain-resort\": \"Copper\",\n\t\t\t\"crested-butte-mountain-resort\": \"Crested Butte\",\n\t\t\t\"diamond-peak\": \"Diamond Peak\",\n\t\t\t\"eldora-mountain-resort\": \"Eldora\",\n\t\t\t\"jackson-hole\": \"Jackson Hole\",\n\t\t\t\"loveland-ski-area\": \"Loveland\",\n\t\t\t\"monarch-ski-area\": \"Monarch\",\n\t\t\t\"steamboat-ski-resort\": \"Steamboat\",\n\t\t\t\"taos-ski-valley\": \"Taos\",\n\t\t\t\"telluride-ski-resort\": \"Telluride\",\n\t\t\t\"vail-ski-resort\": \"Vail\",\n\t\t\t\"winter-park-resort\": \"Winter Park\",\n\t\t\t\"wolf-creek-ski-area\": \"Wolf Creek\",\n\t\t}\n\n\t\tdf[\"Resort\"] = df[\"Resort\"].map(dict_webscrape_trail_names).fillna(df[\"Resort\"])\n\n\t\t# Drop URL column\n\t\tdf.drop(\"URL\", axis=1, inplace=True)\n\n\t\t# Reset index\n\t\tdf = df.reset_index(drop=True)\n\n\t\treturn df\n\n\tdef save_trail_data(self, df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n\t\t\"\"\"\n\t\tSave trail data to Parquet file\n\t\t\"\"\"\n\n\t\tcurrent_date = date.today().strftime(\"%Y%m%d\")\n\n\t\tdf.to_parquet(\n\t\t\tf\"{self.CURRENT_DIRECTORY}/data/trail_data_{current_date}.parquet\",\n\t\t\tindex=False,\n\t\t)\n\n", "description": "\n\t\tInputs:\n\t\t\tURL from URLs (str)\n\t\tOutputs:\n\t\t\tPandas DataFrame of ski resort information\n\t\t", "category": "webscraping", "imports": ["import os", "import time", "from datetime import date", "import pandas as pd", "import requests", "from bs4 import BeautifulSoup", "from selenium import webdriver", "from tqdm import tqdm"]}], [{"term": "def", "name": "view_movie_details", "data": "def view_movie_details(movie_link):\n\t#clear the command line output\n\tclear()\n\t#obtain the html code for the movie page\n\treq = Request(movie_link, headers={'User-Agent': 'Mozilla/5.0'})\n\tpage = urlopen(req).read()\n\thtml_movie = page.decode(\"utf-8\")\n\t\n\t#create the parser and parse each element\n\tparser = movie_parser(html_movie)\n\tname = parser.get_name()\n\tcast = parser.get_cast()\n\tsummary = parser.get_summary()\n\tsummary = summary.replace('', '\\n')\n\tcredit = parser.get_credit()\n\t\n\t#print the results to the command line\n\tprint(name)\n\tprint(cast + \"\\n\")\n\t#create the formatting for the summary\n\tbody = '\\n\\n'.join(['\\n'.join(textwrap.wrap(line, 100,\n\t\t\t\t break_long_words=False, replace_whitespace=False))\n\t\t\t\t for line in summary.splitlines() if line.strip() != ''])\n\t\n\tprint(body)\n\tprint(\"\\n\" + credit + \"\\n\\n\")\n\t\n\t#prompt the user to view the reviews of the movie\n\tanswer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n\t\n\t#while the\n\twhile (answer == \"r\"):\n\t\t\n\t\tmovie_user_reviews.view_movie_user_reviews(movie_link + \"/user-reviews\")\n\t\t#clear the command line output\n\t\tclear()\n\t\t#print the results to the command line\n\t\tprint(name)\n\t\tprint(cast + \"\\n\")\n\t\tprint(body)\n\t\tprint(\"\\n\" + credit + \"\\n\\n\")\n\t\t\n\t\t#prompt the user to view the reviews of the movie\n\t\tanswer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n\t\n\t\n\t\n\t\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen, Request", "import textwrap", "from webscrape.searchresults.categorydetails.moviedetail.movieparse.movie_parser import movie_parser", "from webscrape.searchresults.categorydetails.moviedetail.moviedisplay import movie_user_reviews", "from webscrape import clear"]}], [], [{"term": "def", "name": "get_search_url", "data": "def get_search_url(args):\n\targs = \"+\".join(args)\n\tsearch_url = f\"https://www.imdb.com/find?q={args}&s=tt&ttype=ft\"\n\treturn search_url\n\n", "description": null, "category": "webscraping", "imports": ["import sys", "import requests, bs4, click"]}, {"term": "def", "name": "get_search_html", "data": "def get_search_html(url):\n\tres = requests.get(url)\n\tres.raise_for_status()\n\tsearch_html = bs4.BeautifulSoup(res.text, features='html.parser')\n\treturn search_html\n\n", "description": null, "category": "webscraping", "imports": ["import sys", "import requests, bs4, click"]}, {"term": "def", "name": "get_search_titles", "data": "def get_search_titles(html):\n\ttitle_elems = html.select(\"#main td.result_text\")\n\ttitles = []\n\tfor title in (title_elems):\n\t\ttitles.append(title.getText().strip())\n\treturn titles\n\n", "description": null, "category": "webscraping", "imports": ["import sys", "import requests, bs4, click"]}, {"term": "def", "name": "get_search_ids", "data": "def get_search_ids(html):\n\tid_elems = html.select(\"#main td.result_text a\")\n\tids = []\n\tfor id_elem in (id_elems):\n\t\ttitle_id = id_elem.attrs['href']\t\n\t\tids.append(title_id)\n\treturn ids \n\n", "description": null, "category": "webscraping", "imports": ["import sys", "import requests, bs4, click"]}, {"term": "def", "name": "get_nudity_list", "data": "def get_nudity_list(title_id):\n\tparents_guide_url = f'https://www.imdb.com{title_id}parentalguide'\n\tres = requests.get(parents_guide_url)\n\tres.raise_for_status()\n\tparents_guide_html = bs4.BeautifulSoup(res.text, features='html.parser')\n\tnudity_elems = parents_guide_html.select('#advisory-nudity li.ipl-zebra-list__item')\n\tnudity_list = []\n\tfor _, list_item in enumerate(nudity_elems):\n\t\tnudity_list.append(list_item.getText().strip().rstrip('Edit').strip())\n\treturn nudity_list\n\n", "description": null, "category": "webscraping", "imports": ["import sys", "import requests, bs4, click"]}, {"term": "def", "name": "show_nudity_info", "data": "def show_nudity_info(nudity_list, title):\n\tclick.echo(title + \"\\n\")\n\tfor _, item in enumerate(nudity_list):\n\t\tclick.echo(\"* \" + item)\n\n", "description": null, "category": "webscraping", "imports": ["import sys", "import requests, bs4, click"]}, {"term": "def", "name": "show_title_list", "data": "def show_title_list(search_titles):\n\twhile True:\n\t\tselections = [\"q\"]\n\t\tprint()\n\t\tfor i, title in enumerate(search_titles[:10]):\n\t\t\tprint(f\"{i+1}) {title}\")\n\t\t\tselections.append(str(i+1))\n\t\tprint()\n\t\ttitle_selection = input(\"Enter [title #] or [q] to quit: \")\n\t\tif title_selection in selections:\n\t\t\tbreak  \n\n\t# Validate selection choice\n\tif title_selection.lower() == \"q\":\n\t\texit()\n\telse:\n\t\treturn (int(title_selection)-1)\n\t\n", "description": null, "category": "webscraping", "imports": ["import sys", "import requests, bs4, click"]}, {"term": "def", "name": "main", "data": "def main(movie_title, l):\n\t\"\"\"Display nudity information for given movie title.\"\"\"\n\n\tsearch_url = get_search_url(movie_title)\n\tsearch_html = get_search_html(search_url)\n\tsearch_titles = get_search_titles(search_html)\n\tsearch_ids = get_search_ids(search_html)\n\n\tif l:\n\t\ttitle_index = show_title_list(search_titles)\n\telse:\n\t\ttitle_index = 0\n\n\tnudity_list = get_nudity_list(search_ids[title_index])\n\tshow_nudity_info(nudity_list, search_titles[title_index])\n\n", "description": "Display nudity information for given movie title.", "category": "webscraping", "imports": ["import sys", "import requests, bs4, click"]}], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape:\n\t'''\n\tThis web scrapping class allows you to scrape a single news article of your choice,\n\tor you can scrape multiple news article and store it to a dataframe.  You need to initialize the \n\tclass with the specific url and depending on your choice, run to scrape one article or more than one. \n\tKeep in mind that the scrape_N_articles() also allows you to scrape one article, however it is a\n\trandom article from the landing page of NBC News.\n\n\tdef __init__()\t\t\t  ---->   Initialize requests and obtains content from landing page of URL\n\n\tdef scrape_news_article()   ---->   Web Scrapping of a single news article\n\n\tdef scrap_N_articles()\t  ---->   Web Scrapping of multipl articles of the NBC home page\n\n\t'''\n\tdef __init__(self, url):\n\t\tself.url = url\n\t\tself.request = requests.get(self.url)\n\n\t\t# We'll save in coverpage the cover page content\n\t\tself.coverpage = self.request.content\n\n\t\t# Soup creation\n\t\tself.soup = BeautifulSoup(self.coverpage, 'html5lib')\n\n\tdef scrape_news_article(self):\n\t\ttitle = self.soup.find('h1').get_text()\n\t\tx = self.soup.find_all('p', {'class':['','endmark']})\n\n\t\t# Unifying the paragraphs\n\t\tlist_paragraphs = []\n\t\tfor p in np.arange(0, len(x)):\n\t\t\tparagraph = x[p].get_text()\n\t\t\tlist_paragraphs.append(paragraph)\n\t\t\tfinal_article = \" \".join(list_paragraphs)\n\n\t\tarticle_dict = {'article link': self.url, 'article title' : title, 'article content': final_article}\n\n\t\treturn article_dict\n\n\tdef scrape_n_articles(self, num_articles=1):\n\t\t# News identification\n\t\tcoverpage_news = []\n\t\tfor tag in self.soup.find_all('h2', class_='styles_headline__ice3t'):\n\t\t\tfor anchor in tag.find_all('a'):\n\t\t\t\tcoverpage_news.append(anchor)\n\n\t\tprint('Number of articles found: {}'.format(len(coverpage_news)))\n\n\t\t## Let's extract the text from the article\n\t\t# Empty lists for content, links and titles\n\t\tnews_contents = []\n\t\tlist_links = []\n\t\tlist_titles = []\n\n\t\tfor n in np.arange(0, num_articles):\n\t\t\t\t\n\t\t\t# Getting the link of the article\n\t\t\tlink = coverpage_news[n]['href']\n\t\t\tlist_links.append(link)\n\t\t\t\n\t\t\t# Getting the title\n\t\t\ttitle = coverpage_news[n].get_text()\n\t\t\tlist_titles.append(title)\n\t\t\t\n\t\t\t# Reading the content (it is divided in paragraphs)\n\t\t\tarticle = requests.get(link)\n\t\t\tarticle_content = article.content\n\t\t\tsoup_article = BeautifulSoup(article_content, 'html5lib')\n\t\t\tx = soup_article.find_all('p', {'class':['','endmark']})\n\t\t\t\n\t\t\t# Unifying the paragraphs\n\t\t\tlist_paragraphs = []\n\t\t\tfinal_article = \"\"\n\t\t\tfor p in np.arange(0, len(x)):\n\t\t\t\tparagraph = x[p].get_text()\n\t\t\t\tlist_paragraphs.append(paragraph)\n\t\t\t\tfinal_article = \" \".join(list_paragraphs)\n\t\t\t\t\n\t\t\tnews_contents.append(final_article)\n\n\t\t# df_show_info\n\t\tnbc_articles = pd.DataFrame({\n\t\t\t# 'Article Title': list_titles,\n\t\t\t'article link': list_links,\n\t\t\t'article content': news_contents})\n\n\t\treturn nbc_articles\n", "description": null, "category": "webscraping", "imports": ["import requests", "import numpy as np", "import pandas as pd", "import argparse", "from bs4 import BeautifulSoup"]}], [], [{"term": "class", "name": "classflights_webscrape:\r", "data": "class flights_webscrape:\r\n\r\n\tdef __init__(self,url,years):\r\n\t\tself.url  = url\r\n\t\tself.years = years\r\n\t\t\r\n\tdef get_chrome_driver(self,driver_path):\r\n\t\t\"\"\"\r\n\t\tcreate a chrome driver object for Selenium from the specified driver file\r\n\t\t\"\"\"\r\n\t\tdriver = webdriver.Chrome(driver_path)\r\n\t\treturn driver\r\n\t\r\n\tdef create_param_list(self):\r\n\t\t\"\"\"\r\n\t\tcreate list of parameters to pull from the url \r\n\t\t\"\"\"\r\n\t\t# Time Period Parameters\r\n\t\tTparam = ['Year','Month','DayofMonth','DayOfWeek','FlightDate']\r\n\t\t# Airline Parameters\r\n\t\tAparam = ['Reporting_Airline','Tail_Number','Flight_Number_Reporting_Airline']\r\n\t\t# Origin Parameters\r\n\t\tOparam = ['OriginAirportID','Origin','OriginCityName','OriginStateName','OriginWac']\r\n\t\t# Destination Parameters\r\n\t\tDparam = ['DestAirportID','Dest','DestCityName','DestStateName','DestWac']\r\n\t\t# Departure Performance\r\n\t\tDPfparam = ['CRSDepTime','DepTime','DepDelay','DepartureDelayGroups','TaxiOut','WheelsOff']\r\n\t\t# Arrival Performance\r\n\t\tAPfparam = ['CRSArrTime','ArrTime','ArrDelay','ArrivalDelayGroups','TaxiIn','WheelsOn']\r\n\t\t# Cancellations\r\n\t\tCparam = ['Cancelled','CancellationCode','Diverted']\r\n\t\t# Flight Summary\r\n\t\tFparam = ['AirTime','Flights','Distance']\r\n\t\t# Cause of Delay\r\n\t\tCDparam = ['CarrierDelay','WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay']\r\n\t\t# Gate Return Delay\r\n\t\tGparam = ['FirstDepTime','TotalAddGTime','LongestAddGTime']\r\n\t\t# Diverted Landings\r\n\t\tDvparam = ['DivAirportLandings']\r\n\t\t\r\n\t\tparam_list = Tparam + Aparam + Oparam + Dparam + DPfparam + APfparam + Cparam + Fparam + CDparam + Gparam + Dvparam\r\n\t\treturn param_list\r\n\t\r\n\tdef download_datasets(self,driver,param_list):\r\n\t\t\"\"\"\r\n\t\tSelect the list of parameters and year on the url and download the flights data for given year\r\n\t\t\"\"\"\r\n\t\t## go to specified web url\r\n\t\tdriver.get(self.url)\r\n\r\n\t\t## select all the parameters of interest on the css selector\r\n\t\tfor param in param_list:\r\n\t\t\tschk = driver.find_element_by_css_selector('input[Title='+param+']')\r\n\t\t\twebdriver.ActionChains(driver).move_to_element(schk).click(schk).perform()\r\n\t\t\r\n\t\t## scroll up to top of window\r\n\t\tdriver.execute_script(\"window.scrollTo(0, 0)\")\r\n\r\n\t\t## Pull Data by Year\r\n\t\tfor year in self.years:\r\n\t\t\tfor month in np.arange(1,13):\r\n\t\t\t\tselectY = Select(driver.find_element_by_id('XYEAR'))\r\n\t\t\t\tselectY.select_by_value(year)\r\n\t\t\t\tselectM = Select(driver.find_element_by_id('FREQUENCY'))\r\n\t\t\t\tselectM.select_by_value(str(month))\r\n\t\t\t\t## click the download button (files go to browser downloads folder)\r\n\t\t\t\tsbtn = driver.find_element_by_css_selector('button[onclick=\"tryDownload()\"]')\r\n\t\t\t\tsbtn.click()\r\n\t\t\t\ttime.sleep(45)\r\n\t\t\r\n\t\t## close the driver once download is complete\r\n\t\tdriver.close()\r\n\r\n\tdef write_parquet(self,read_path,write_path):\r\n\t\t\"\"\"\r\n\t\tconvert csvs in zip files to parquet for faster spark queries\r\n\t\t\"\"\"\r\n\t\timport pandas as pd\r\n\t\timport os\r\n\t\timport pyarrow as pa\r\n\t\timport pyarrow.parquet as pq\r\n\t\t\r\n\t\tfilelist = [f for f in os.listdir(read_path) if 'ONTIME_REPORTING' in f]\r\n\t\t\r\n\t\tfor file in filelist:\r\n\t\t\ttemp = pd.read_csv(read_path + file,compression='zip')\r\n\t\t\ttable = pa.Table.from_pandas(temp)\r\n\t\t\t# Local dataset write\r\n\t\t\tpq.write_to_dataset(table, root_path=write_path,partition_cols=['YEAR', 'MONTH'])\r\n\t\treturn\r\n\r\n\r\n\t\r\n\r\n", "description": "\r\n\t\tcreate a chrome driver object for Selenium from the specified driver file\r\n\t\t", "category": "webscraping", "imports": ["import numpy as np\r", "import pandas as pd\r", "from selenium import webdriver\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium.webdriver.support.ui import Select\r", "import time\r", "\t\timport pandas as pd\r", "\t\timport os\r", "\t\timport pyarrow as pa\r", "\t\timport pyarrow.parquet as pq\r"]}], [], [], [], [{"term": "def", "name": "make_tables", "data": "def make_tables():\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"CREATE TABLE IF NOT EXISTS users(\"\n\t\t\t   + \"id integer PRIMARY KEY,\"\n\t\t\t   + \"username text,\"\n\t\t\t   + \"email text,\"\n\t\t\t   + \"password text);\")\n\tcursor.execute(command)\n\tcommand = (\"CREATE TABLE IF NOT EXISTS wishlists(\"\n\t\t\t   + \"id integer PRIMARY KEY,\"\n\t\t\t   + \"user_id integer,\"\n\t\t\t   + \"book1 text DEFAULT null,\"\n\t\t\t   + \"book2 text DEFAULT null,\"\n\t\t\t   + \"book3 text DEFAULT null,\"\n\t\t\t   + \"book4 text DEFAULT null,\"\n\t\t\t   + \"book5 text DEFAULT null,\"\n\t\t\t   + \"book6 text DEFAULT null,\"\n\t\t\t   + \"book7 text DEFAULT null,\"\n\t\t\t   + \"book8 text DEFAULT null,\"\n\t\t\t   + \"book9 text DEFAULT null,\"\n\t\t\t   + \"book10 text DEFAULT null,\"\n\t\t\t   + \"book11 text DEFAULT null,\"\n\t\t\t   + \"book12 text DEFAULT null,\"\n\t\t\t   + \"book13 text DEFAULT null,\"\n\t\t\t   + \"book14 text DEFAULT null,\"\n\t\t\t   + \"book15 text DEFAULT null);\")\n\tcursor.execute(command)\n\tconn.commit()\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "register", "data": "def register(username, email, password):\n\tif username_exists(username) or email_exists(email):\n\t\treturn False\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"INSERT INTO users(username, email, password)\"\n\t\t\t   + \"VALUES(?, ?, ?);\")\n\tcursor.execute(command, (username, email, password,))\n\tconn.commit()\n\treturn True\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "login", "data": "def login(username, password):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"SELECT * FROM users WHERE username=?\")\n\tcursor.execute(command, (username,))\n\trow = cursor.fetchall()\n\tif len(row) == 0:\n\t\treturn (False, False)\n\tif row[0][3] == password:\n\t\treturn (True, True)\n\treturn (True, False)\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "username_exists", "data": "def username_exists(username):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"SELECT * FROM users WHERE username=?;\")\n\tcursor.execute(command, (username,))\n\trows = cursor.fetchall()\n\tif len(rows) == 0:\n\t\treturn False\n\treturn True\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "email_exists", "data": "def email_exists(email):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"SELECT * FROM users WHERE email=?;\")\n\tcursor.execute(command, (email,))\n\trows = cursor.fetchall()\n\tif len(rows) == 0:\n\t\treturn False\n\treturn True\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "update_user", "data": "def update_user(user_id, email, username, password):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"UPDATE users\"\n\t\t\t   + \"SET email=?,\"\n\t\t\t   + \"username=?,\"\n\t\t\t   + \"password=?\"\n\t\t\t   + \"WHERE id=?;\")\n\tcursor.execute(command, (email, username, password, user_id,))\n\tconn.commit()\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "get_user_id", "data": "def get_user_id(email='', username=''):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tif not username_exists(username) and not email_exists(email):\n\t\treturn False\n\tif email != '':\n\t\tcommand = (\"SELECT * FROM users WHERE email=?\")\n\t\tcursor.execute(command, (email,))\n\t\trow = cursor.fetchall()[0]\n\t\treturn row[0]\n\tcommand = (\"SELECT * FROM users WHERE username=?\")\n\tcursor.execute(command, (username,))\n\trow = cursor.fetchall()[0]\n\treturn row[0]\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "wishlist_is_full", "data": "def wishlist_is_full(user_id):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"SELECT * FROM wishlists WHERE user_id=?;\")\n\tcursor.execute(command, (user_id,))\n\trow = cursor.fetchall()[0]\n\tfor i in range(len(row)):\n\t\tif row[i] is None:\n\t\t\treturn i\n\treturn True\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "create_wishlist", "data": "def create_wishlist(user_id):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"INSERT INTO wishlists(user_id)\"\n\t\t\t   + \"VALUES(?);\")\n\tcursor.execute(command, (user_id,))\n\tconn.commit()\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "insert_book", "data": "def insert_book(user_id, title):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tindex = wishlist_is_full(user_id)\n\tif index is True:\n\t\treturn False\n\tbook_num = \"book\" + str(index-1)\n\tcommand = (\"UPDATE wishlists \"\n\t\t\t   + \"SET \" + book_num + \"=? \"\n\t\t\t   + \"WHERE user_id=?\")\n\tprint(command)\n\tcursor.execute(command, (title, user_id,))\n\tconn.commit()\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "get_index", "data": "def get_index(user_id, title):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"SELECT * FROM wishlists WHERE user_id=?;\")\n\tcursor.execute(command, (user_id,))\n\trow = cursor.fetchall()[0]\n\tfor i in range(len(row)):\n\t\tif row[i] == title:\n\t\t\treturn i-1\n\treturn 16\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "delete_book", "data": "def delete_book(user_id, title):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tindex = get_index(user_id, title)\n\tbook_num = \"book\" + str(index)\n\tprint(book_num)\n\tcommand = (\"UPDATE wishlists \"\n\t\t\t   + \"SET \" + book_num + \"=null \"\n\t\t\t   + \"WHERE user_id=?;\")\n\tcursor.execute(command, (user_id,))\n\tconn.commit()\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}, {"term": "def", "name": "get_wishlist", "data": "def get_wishlist(user_id):\n\tconn = sqlite3.connect('database.db')\n\tcursor = conn.cursor()\n\tcommand = (\"SELECT * FROM wishlists WHERE user_id=?\")\n\tcursor.execute(command, (user_id,))\n\twishlist = list(cursor.fetchall()[0])[2:]\n\twhile None in wishlist:\n\t\twishlist.remove(None)\n\tbooks = set()\n\tfor book in wishlist:\n\t\tbooks.update(webscrape.cheapest_textbooks(book))\n\t\tbooks.update(webscrape.thriftbooks(book))\n\treturn (wishlist, books)\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import webscrape"]}], [{"term": "def", "name": "root", "data": "def root():\r\n\tif request.method=='GET':\r\n\t\t\treturn render_template('home.html')\r\n\telif request.method=='POST':\r\n\t\tstartspider()\r\n\t\tcon = sqlite3.connect(r\"C:\\Users\\VARUN\\Desktop\\flask\\Web Scraper\\Webscrape\\Webscrape\\mydata.db\")\r\n\t\tcur = con.cursor()\r\n\t\tcur.execute(\"SELECT * FROM data_tb\")\r\n\t\tdata = cur.fetchall()\r\n\t\t#d=data['Title']\r\n\t\t#print(d)\r\n\t\treturn render_template('home.html', data=data)\r\n\t\t\r\n\t\t#p=['1','2','3','4','5']\r\n\t\t\r\n\t\t#return  render_template('home.html',proxies=p)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from ast import parse\r", "from re import A\r", "import sqlite3\r", "from urllib import response\r", "from urllib.request import ProxyDigestAuthHandler\r", "from flask import Flask\r", "from flask import render_template\r", "from flask import request\r", "#from flask_sqlalchemy import SQLAlchemy\r", "from datetime import datetime\r", "import scrapy\r", "from tech import *\r"]}], [{"term": "def", "name": "create", "data": "def create():\n\tcon=sqlite3.connect('sql.db')\n\tcur=con.cursor()\n\tcur.execute('''CREATE TABLE IF NOT EXISTS CLASSES(\n\t\tSUBJECT TEXT NOT NULL,\n\t\tDAY INTEGER NOT NULL,\n\t\tTIME INTEGER NOT NULL,\n\t\tSKIP INTEGER DEFAULT 0,\n\t\tREASON TEXT);''')\n\tcon.commit()\n\n\tcur.execute('''CREATE TABLE IF NOT EXISTS ASSIGNMENTS(\n\t\tSUBJECT TEXT NOT NULL,\n\t\tDATE INTEGER NOT NULL,\n\t\tMONTH INTEGER NOT NULL,\n\t\tYEAR INTEGER NOT NULL,\n\t\tTIME INTEGER NOT NULL,\n\t\tDESCRIPTION TEXT);''')\n\tcon.commit()\n\n\tcur.execute('''CREATE TABLE IF NOT EXISTS EVENTS(\n\t\tEVENT TEXT NOT NULL,\n\t\tETYPE TEXT NOT NULL,\n\t\tDATE INTEGER NOT NULL,\n\t\tMONTH INTEGER NOT NULL,\n\t\tYEAR INTEGER NOT NULL,\n\t\tTIME INTEGER NOT NULL,\n\t\tDESCRIPTION TEXT);''')\n\tcon.commit()\n\n\tcon.close()\n\treturn\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import requests"]}, {"term": "def", "name": "skip", "data": "def skip():\n\tskipp=0\n\treasons=[\"MASS BUNK\",\"CANCELLATION OF CLASS\",\"HOLIDAY\"]\n\treason=\"\"\n\tcon=sqlite3.connect('sql.db')\n\tcur=con.cursor()\n\tcur.execute('''SELECT rowid,* FROM CLASSES''')\n\toutput = cur.fetchall()\n\tfor row in output:\n\t\tprint(f'{row[0]} : {row[1]} at {row[3]} and Status is {row[4]}')\n\tcon.commit()\n\tassgi = input(\"Select The Class To Skip \") # 30 seconds to reply\n\trowi=int(assgi)\n\treas = input(\"Select The Reason To Skip \") # 30 seconds to reply\n\treason=reasons[int(reas)]\n\tcur.execute('''UPDATE CLASSES set SKIP = 1, REASON=? where rowid = ?''',(reason,rowi))\n\tcon.commit()\n\tcon.close()\n\t\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import requests"]}, {"term": "def", "name": "delc", "data": "def delc():\n\tskipp=0\n\tcon=sqlite3.connect('sql.db')\n\tcur=con.cursor()\n\tcur.execute('''SELECT rowid,* FROM CLASSES''')\n\toutput = cur.fetchall()\n\tfor row in output:\n\t\tprint(f'{row[0]} : {row[1]} at {row[3]} and Status is {row[4]}')\n\tcon.commit()\n\tassgi = input(\"Select The Class To Delete \") # 30 seconds to reply\n\trowi=int(assgi)\n\tcur.execute('''DELETE FROM CLASSES WHERE rowid = ?''',(rowi,))\n\tcon.commit()\n\tcon.close()\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import requests"]}, {"term": "def", "name": "addc", "data": "def addc():\n\tcon=sqlite3.connect('sql.db')\n\tcur=con.cursor()\n\tsubject=\"\"\n\tday=\"\"\n\ttime=\"\"\n\tsubjecti = input(\"Enter the Subject \") # 30 seconds to reply\n\tsubject=subjecti\n\n\tdayi = input(\"What is the Day? \") # 30 seconds to reply\n\tday=int(dayi)\n\n\ttimei = input(\"What is the Time(HH) ? \") # 30 seconds to reply\n\ttime=int(timei)\n\t\n\t\n\tcur.execute('''INSERT INTO CLASSES(SUBJECT,DAY,TIME) VALUES(\n\t\t?,?,?)''',(subject,day,time))\n\tcon.commit()\n\tcon.close()\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import requests"]}, {"term": "def", "name": "classes", "data": "def classes():\n\tcon=sqlite3.connect('sql.db')\n\tcur=con.cursor()\n\tcur.execute('''SELECT rowid,* FROM CLASSES''')\n\toutput = cur.fetchall()\n\tfor row in output:\n\t\tprint(f'{row[0]} : {row[1]} at {row[3]} and Status is {row[4]}')\n\tcon.commit()\n\tcon.close()\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import requests"]}], [{"term": "def", "name": "webscrape_results", "data": "def webscrape_results(\n", "description": null, "category": "webscraping", "imports": ["from json import loads", "from json.decoder import JSONDecodeError", "from time import sleep", "from typing import Any", "import requests", "from requests.exceptions import HTTPError, RequestException", "from scrape.log import logger  # type: ignore"]}], [], [{"term": "def", "name": "get_text", "data": "def get_text(row):\n\t# Get URL\n\turl = row['URL']\n\tquestionable_stuff = ['doc', 'pdf']\n\t\n\tif not pd.isnull(url) and not any(x in url for x in questionable_stuff):\n\t\ttry:\n\t\t\t# Extract from website\n\t\t\tsource = requests.get(url).text\n\t\t\tsoup = BeautifulSoup(source, 'lxml')\n\n\t\t\t# Parse through text of site\n\t\t\tparagraph =  soup.find_all('div', class_ =\"main-content main-content--two\")[1]\n\t\t\t#print(paragraph.prettify())\n\n\t\t\tlines = paragraph.find_all(class_ = \"ma__rich-text\")[1]\n\t\t\ttext = lines.get_text(separator=' ')\n\t\t\ttext= re.sub(r\"(\\n( ?))+\", \"\\n\", text)\n\t\t\ttext = re.sub(r\"   +\", \"  \", text)\n\t\t\ttext = text.replace(\"\\n\",\" \")\n\t\t\treturn text\n\n\t\texcept Exception as exception:\n\t\t\tprint(url)\n\t\t\treturn exception.__class__.__name__\n\treturn np.nan\n", "description": null, "category": "webscraping", "imports": ["# Handling imports", "from bs4 import BeautifulSoup", "import requests", "import pandas as pd", "import numpy as np", "import re", "import sys"]}], [{"term": "class", "name": "TestTask", "data": "class TestTask(TestCase):\n\tdef setUp(self):\n\t\tself.web_scrape = WebScrape()\n\t\tself.web_process = WebProcess()\n\t\tself.count = WordCounter()\n\n\tdef test_web_scrape(self):\n\t\tresult =  self.web_scrape.scrape(\"https://www.python.org\")\n\t\tself.assertIsNotNone(result)\n\t\tself.assertIsInstance(result, str)\n\t\tself.assertNotIsInstance(result, dict)\n\t\tself.assertNotIsInstance(result, list)\n\t\tself.assertTrue(result)\n\t\tself.assertGreater(len(result), 1)\n\n\tdef test_web_processing(self):\n\t\tresult = self.web_process.text_process(\"python software is a 1 2 3 4 class\")\n\t\tself.assertIsNotNone(result)\n\t\tself.assertIsInstance(result, list)\n\t\tself.assertNotIsInstance(result, dict)\n\t\tself.assertNotIsInstance(result, dict)\n\t\tself.assertTrue(result)\n\t\tself.assertEqual(result, [\"Python\" , \"Software\", \"Class\"])\n\n\tdef test_web_counter(self):\n\t\tresult = self.count.word_counter([\"python\", \"software\", \"train\", \"sound\", \"music\", \"punch\", \"dance\"])\n\t\tself.assertIsNotNone(result)\n\t\tself.assertIsInstance(result, dict)\n\t\tself.assertNotIsInstance(result, list)\n\t\tself.assertNotIsInstance(result, str)\n\t\tself.assertEqual(len(result), 7)\n\t\tself.assertEqual(result, {\"python\": 1, \"software\": 1, \"train\" : 1, \"sound\" : 1, \"music\" : 1, \"punch\" : 1, \"dance\" : 1})\n\t\tself.assertEqual(sum(list(result.values())), 7)\n\t\tself.assertEqual(list(result.keys()), [\"python\", \"software\", \"train\", \"sound\", \"music\", \"punch\", \"dance\"])\n\n\tdef tearDown(self):\n\t\tself.web_scrape = None\n\t\tself.web_process = None\n\t\tself.count = None\n\t\n\t   \n\n\n\n\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from unittest import TestCase", "from web_scrape import WebScrape", "from web_processing import WebProcess", "from web_word_counter import WordCounter"]}], [], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape:\n\tdef GoToWeb(StartingIValue):\n\t\tr.init()\n\n\t\ti = StartingIValue\n\t\tchunk = 100\n\t\tRunUntil = i + chunk\n\n\t\tGTG = True\n\t\tlistPigs = []\n\n\t\twhile GTG == True:\n\n\t\t\ttry:\n\t\t\t\tnewUrl = f'https://www.cpspedigrees.com/poland/pigs/show/{i}'\n\t\t\t\tr.url(newUrl)\n\n\t\t\t\tpigInstance = Pig()\n\n\t\t\t\tpigInfo = r.read('ul')\n\n\t\t\t\tif pigInfo == '':\n\t\t\t\t\t#GTG = False\n\t\t\t\t\ti += 1\n\t\t\t\t\tcontinue\n\n\t\t\t\tregNum = re.search('Registration#:\\n(.*)\\n', pigInfo)\n\t\t\t\tregNum = regNum.group(1).strip()\n\t\t\t\tpigInstance.regNum = regNum\n\n\t\t\t\tSex = re.search('Sex:\\n(.*)\\n', pigInfo)\n\t\t\t\tSex = Sex.group(1).strip()\n\t\t\t\tpigInstance.Sex = Sex\n\n\t\t\t\tpigInstance.FullName = r.read('show-pig-fullname')\n\n\t\t\t\tDOB = re.search('Farrow Date:\\n(.*)\\n', pigInfo)\n\t\t\t\tDOB = DOB.group(1).strip()\n\t\t\t\tpigInstance.DOB = DOB\n\n\t\t\t\tOwner = re.search('Owner:\\n(.*)\\n', pigInfo)\n\t\t\t\tOwner = Owner.group(1).strip()\n\t\t\t\tpigInstance.Owner = Owner\n\n\t\t\t\tBreeder = re.search('Breeder:\\n(.*)\\n', pigInfo)\n\t\t\t\tBreeder = Breeder.group(1).strip()\n\t\t\t\tpigInstance.Breeder = Breeder\n\n\t\t\t\tTotalBorn = re.search('Total Born:\\n(.*)\\n', pigInfo)\n\t\t\t\tTotalBorn = TotalBorn.group(1).strip()\n\t\t\t\tpigInstance.TotalBorn = TotalBorn\n\n\t\t\t\tBornAlive = re.search('Born Alive:\\n(.*)\\n', pigInfo)\n\t\t\t\tBornAlive = BornAlive.group(1).strip()\n\t\t\t\tpigInstance.BornAlive = BornAlive\n\n\t\t\t\tSireRegNum = r.read(\n\t\t\t\t\t'/html/body/div[1]/div[4]/div/div[2]/div[2]/div/div/strong[1]')\n\t\t\t\tpigInstance.SireRegNum = SireRegNum\n\n\t\t\t\tDamRegNum = r.read(\n\t\t\t\t\t'/html/body/div[1]/div[4]/div/div[2]/div[6]/div/div/strong[1]')\n\t\t\t\tpigInstance.DamRegNum = DamRegNum\n\t\t\t\tpigInstance.webID = i\n\n\t\t\t\tlistPigs.append(pigInstance)\n\n\t\t\t\tif i >= RunUntil:\n\t\t\t\t\tExportPigListToCSV(listPigs, \"Pigs.csv\")\n\t\t\t\t\tlistPigs.clear()\n\t\t\t\t\tRunUntil = i + chunk\n\t\t\t\ti += 1\n\n\t\t\texcept Exception:\n\t\t\t\tGTG == False\n\t\ttry:\n\t\t\tExportPigListToCSV(listPigs, \"Pigs.csv\")\n\t\texcept BaseException as e:\n\t\t\tprint(f'BaseException: {e}')\n\t\telse:\n\t\t\tprint('Data has been loaded successfully !')\n\n", "description": null, "category": "webscraping", "imports": ["import rpa as r", "import urllib as urllib", "import re", "import datetime", "import csv", "from multiprocessing import Process", "import multiprocessing"]}, {"term": "def", "name": "ExportPigListToCSV", "data": "def ExportPigListToCSV(listPigs, filename):\n\twith open(filename, 'a') as f:\n\t\twriter = csv.writer(f)\n\t\tj = 0\n\t\tfor item in listPigs:\n\t\t\tj += 1\n\t\t\twriter.writerow([item.webID, item.FullName, item.regNum, item.Sex, item.DOB, item.Owner,\n\t\t\t\t\t\t\titem.Breeder, item.TotalBorn, item.BornAlive, item.SireRegNum, item.DamRegNum])\n\treturn filename\n\n", "description": null, "category": "webscraping", "imports": ["import rpa as r", "import urllib as urllib", "import re", "import datetime", "import csv", "from multiprocessing import Process", "import multiprocessing"]}, {"term": "def", "name": "StringToFile", "data": "def StringToFile(text, output):\n\tf = open(output, \"a\")\n\tf.write(text)\n\tf.close()\n\n", "description": null, "category": "webscraping", "imports": ["import rpa as r", "import urllib as urllib", "import re", "import datetime", "import csv", "from multiprocessing import Process", "import multiprocessing"]}, {"term": "def", "name": "IngestWebContent", "data": "def IngestWebContent(urlPath):\n\tfp = urllib.urlopen(urlPath)\n\tmyBytes = fp.read()\n\n\tmyStr = myBytes.decode('utf8')\n\n\tfp.close()\n\tStringToFile(myStr, )\n\n\t###main entry point###\n\n", "description": null, "category": "webscraping", "imports": ["import rpa as r", "import urllib as urllib", "import re", "import datetime", "import csv", "from multiprocessing import Process", "import multiprocessing"]}, {"term": "class", "name": "classPig:", "data": "class Pig:\n\tdef __init__(self):\n\t\tself.regNum = 0\n\t\tself.Sex = \"\"\n\t\tself.DOB = datetime.datetime.now()\n\t\tself.Owner = \"\"\n\t\tself.Breeder = \"\"\n\t\tself.TotalBorn = 0\n\t\tself.BornAlive = 0\n\t\tself.SireRegNum = 0\n\t\tself.DamRegNum = 0\n\t\tself.FullName = \"\"\n", "description": null, "category": "webscraping", "imports": ["import rpa as r", "import urllib as urllib", "import re", "import datetime", "import csv", "from multiprocessing import Process", "import multiprocessing"]}, {"term": "def", "name": "main", "data": "def main():\n\tWebScrape.GoToWeb(302982)\n\t# argumentList = [100, 200, 300]\n\t# procs = []\n\n\t# for argument in argumentList:\n\t#\t proc = multiprocessing.Process(\n\t#\t\t target=WebScrape.GoToWeb, args=(argument,))\n\t#\t procs.append(proc)\n\t#\t proc.start()\n\n\t# for proc in procs:\n\t#\t proc.join()\n\n", "description": null, "category": "webscraping", "imports": ["import rpa as r", "import urllib as urllib", "import re", "import datetime", "import csv", "from multiprocessing import Process", "import multiprocessing"]}], [{"term": "def", "name": "convert_to_date", "data": "def convert_to_date(date: str) -> str:\n\t'''\n\tConverts date in format 'Jan 03, 2021' to 1/3/21\n\t'''\n\tformatted_date = ''\n\tdate_conversion_dict = {'Jan' : 1, 'Feb' : 2, 'Mar' : 3, 'Apr' : 4,\n\t\t\t\t\t\t\t'May' : 5, 'Jun' : 6, 'Jul' : 7, 'Aug' : 8,\n\t\t\t\t\t\t\t'Sep' : 9, 'Oct' : 10, 'Nov' : 11, 'Dec' : 12}\n\tdate = date.split()\n\n\tif date[0] not in date_conversion_dict:\n\t\traise AttributeError(f'{date[0]} not in date_conversion_dict')\n\telse:\n\t\tformatted_date += str(date_conversion_dict[date[0]]) + '/'\n\t\tformatted_date += str(int(date[1][:-1])) + '/'\n\t\tformatted_date += date[2][2:]\n\t\n\treturn formatted_date\n\t\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "set_source", "data": "def set_source(physician: physician.Physician, source: str) -> None:\n\t'''\n\tSets source during webscrape\n\t'''\n\tphysician.set_source(source)\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "set_tier", "data": "def set_tier(physician: physician.Physician) -> None:\n\t'''\n\tSets tier based on residency\n\t'''\n\ttier_dict = {'T20_Rsrch' : ['Harvard', 'Harvard University', 'Harvard Medical School', 'Grossman', 'Columbia', 'Hopkins', 'Johns Hopkins Hospital', 'Johns Hopkins University', 'Francisco', \n\t\t\t\t\t\t\t\t'Duke', 'Duke University Health System', 'Duke University Hospital', 'Perelman', 'Stanford', 'Stanford University', 'University of Washington', 'Washington University/Barnes Hospital', 'University of Washington Medical Center', \n\t\t\t\t\t\t\t\t'Yale', 'Icahn', 'Icahn School of Medicine at Mount Sinai', 'Washington University', 'Washington University School of Medicine in St. Louis', 'Washington University in St. Louis', 'Washington University in St. Louis', 'Vanderbilt', 'Vanderbilt University', \n\t\t\t\t\t\t\t\t'Cornell', 'Mayo', 'Mayo Medical School', 'University of Pittsburgh', 'University of Pittsburg', 'University of Pittsburgh Medical Center', 'University Pittsburgh Medical Center Hospitals', 'Northwestern', 'McGaw Medical Center of Northwestern University', \n\t\t\t\t\t\t\t\t'Northwestern University, Feinberg School of Medicine', 'University of Michigan', 'University of Michigan Health System', 'Los Angeles', 'San Diego', 'Pritzker'],\n\t\t\t\t'T50_Rsrch' : ['Baylor', 'Baylor College of Medicine', 'Baylor University Medical Center', 'Emory', 'Case Western', 'Case Western University', 'University of North Carolina', 'University of Michigan Hospitals and Health Centers', \n\t\t\t\t\t\t\t\t'Southwestern', 'Colorado', 'Southern California', 'University of Southern California', 'USC Medical Center', 'Maryland', 'LAC + USC Medical Center',  \n\t\t\t\t\t\t\t\t'Ohio State', 'The Ohio State Wexner Medical Center', 'University of Virginia', 'Boston', 'Oregon Health and Science', 'Oregon Health and Science University', 'Oregon Health Sciences University',  \n\t\t\t\t\t\t\t\t'Alabama', 'University of Alabama at Birmingham', 'Brown', 'University of Utah', 'University of Utah, Salt Lake City', 'University of Utah Medical Center', 'Albert Einstein', \n\t\t\t\t\t\t\t\t'University of Florida', 'University of Rochester', 'University of Rochester School of Medicine', 'University of Wisconsin', \n\t\t\t\t\t\t\t\t'Indiana University', 'University of Iowa', 'University of Iowa Hospitals and Clinics', 'University of Iowa, College of Medicine', 'University of Cincinnati', \n\t\t\t\t\t\t\t\t'University of Miami', 'University of Minnesota', 'University of South Florida', 'University of South Florida Morsani School of Medicine, Mayo School of Graduate Medical Education', 'USF College of Medicine', 'University of South Florida College of Medicine', \n\t\t\t\t\t\t\t\t'Dartmouth', 'University of Massachusetts', 'Texas Health and Science', \n\t\t\t\t\t\t\t\t'Wake Forest', 'Davis'],\n\t\t\t\t'T20_PC' : ['University of Washington', 'Francisco', 'Minnesota', 'Oregon Health and Science', \n\t\t\t\t\t\t\t'University of North Carolina', 'University of North Carolina - Chapel Hill', 'University of North Carolina at Chapel Hill', 'University of Colorado', 'University of Colorado School of Medicine', 'University of Nebraska', 'University of Nebraska Medical Center', \n\t\t\t\t\t\t\t'Davis', 'Harvard', 'University of Kansas', 'University of Kansas Medical Center', 'University of Massachusetts', \n\t\t\t\t\t\t\t'University of Pittsburgh', 'University of Pittsburgh School of Medicine', 'University of Pittsburgh School of Dental Medicine', 'University of Pittsburg Medical Center', 'Los Angeles', 'Brown', 'Maryland', 'University of Maryland Medical Center', 'Baylor', 'Iowa', \n\t\t\t\t\t\t\t'New Mexico', 'Texas Southwestern', 'University of Michigan', 'University of Michicagn', 'University of Pennsylvania', 'University of Pennsylvania School of Medicine', 'Hospital of the University of Pennsylvania', \n\t\t\t\t\t\t\t'University of Pennsylvania Health System', 'Hospital of The University of PA', 'University of Wisconsin'],\n\t\t\t\t'T50_PC' : ['Indiana University', 'University of Hawaii', 'University of Utah', 'East Carolina University', \n\t\t\t\t\t\t\t'University of Alabama', 'University of Alabama Medical Center', 'University of Rochester', 'University of Tennessee', 'University of Tennessee Health Science Center', 'Stanford', \n\t\t\t\t\t\t\t'Pritzker', 'Ohio State', 'Ohio State UMC', 'San Diego', 'University of California, San Diego', 'University of California - San Diego', 'University of Vermont', 'The University of Vermont Medical Center', 'Virginia', 'Boston University', 'Boston Medical Center',\n\t\t\t\t\t\t\t'Dartmouth', 'Mayo', 'University of Arkansas', 'University of North Texas', \n\t\t\t\t\t\t\t'University of Texas Health Science Center', 'University of Texas Medical Branch', 'Emory', 'Emory University', 'Northwestern', 'Vanderbilt', 'Cornell', \n\t\t\t\t\t\t\t'Tufts', 'Boston University/Tufts University', 'University of Oklahoma', 'University of Oklahoma, University of Colorado Health Science Center', 'University Of Oklahoma College of Medicine', 'Grossman', 'Texas Tech University Health Sciences Center', \n\t\t\t\t\t\t\t'University of Florida', 'Virginia Commonwealth', 'Tufts University School of Medicine'],\n\t\t\t\t'T20_Resi' : ['Memorial Sloan', 'Massachusetts General', 'Hopkins', 'Mayo', 'San Fran', 'University of California, San Francisco', 'Penn Presbyterian', \n\t\t\t\t\t\t\t\t'Ohio State', 'University of Michigan', 'Vanderbilt', 'MD Anderson', 'Los Angeles', 'UCLA', 'UCLA Medical Center',  'University of California, Los Angeles School of Medicine', 'University of California, Los Angeles School of Medicine', \n\t\t\t\t\t\t\t\t'Stanford', 'MUSC Health', 'OHSU', 'Presbyterian', 'University of Kansas', 'Cedars-Sinai', \n\t\t\t\t\t\t\t\t'Brigham', 'Barnes-Jewish', 'University of California, Los Angeles School of Medicine']\n\t\t\t\t}\n\n\tif physician.get_residency() == 'N/A':\n\t\tphysician.set_tier('N/A')\n\t\tprint('1')\n\t\treturn\n\n\tfound = False\n\tfor key, _ in tier_dict.items():\n\t\tif physician.get_residency() in tier_dict[key]:\n\t\t\tphysician.set_tier(key)\n\t\t\tfound = True\n\t\t\tbreak\n\t\n\tif found == False and physician.get_residency() not in ['University Hospital of Louvain at Mont-Godinne', 'Thomas Jefferson University Hospitals']:\n\t\tphysician.set_tier('N/A')\n\t\tprint(3)\n\t\n\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "set_overall_rating_healthgrades", "data": "def set_overall_rating_healthgrades(physician: physician.Physician, soup: BeautifulSoup) -> None:\n\t'''\n\tSets overall rating during webscrape\n\t'''\n\toverall_rating = soup.select('div.overall-rating-wrapper strong')\n\ttry:\n\t\tphysician.set_overall_rating('Healthgrades', float(overall_rating[0].text.strip())) # Converts the html to float\n\texcept IndexError:\n\t\tphysician.set_overall_rating('Healthgrades', 'N/A')\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "set_comment_healthgrades", "data": "def set_comment_healthgrades(physician: physician.Physician, soup: BeautifulSoup) -> None:\n\t'''\n\tSets the comment as a 3-tuple of (date, rating, comment)\n\t'''\n\t# API GET request\n\turl = 'https://www.healthgrades.com/api4/providerprofile/comments'\n\tpwid = physician.get_link('Healthgrades')[-5:]\n\tpage_num = 1\n\tdata = {'currentPage': page_num, 'includeAllAnswers': True, 'perPage': 5, 'pwid': pwid, 'sortOption': 1}\n\theaders = {\"accept\": \"*/*\",\n\t\"accept-language\": \"en-US,en;q=0.9\",\n\t\"content-type\": \"application/x-www-form-urlencoded\",\n\t\"sec-ch-ua\": \"\\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Chromium\\\";v=\\\"101\\\", \\\"Google Chrome\\\";v=\\\"101\\\"\",\n\t\"sec-ch-ua-mobile\": \"?0\",\n\t\"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n\t\"sec-fetch-dest\": \"empty\",\n\t\"sec-fetch-mode\": \"cors\",\n\t\"sec-fetch-site\": \"same-origin\",\n\t\"Referer\": physician.get_link('Healthgrades'),\n\t\"Referrer-Policy\": \"strict-origin-when-cross-origin\"}\n\n\trequest = requests.post(url, data = data, headers=headers)\n\n\tjson_data = json.loads(request.text)\n\n\ttotal_comment_count = json_data['totalCommentCount']\n\tnum_loops = math.ceil(total_comment_count / 5)\n\n\tfor i in range(0, num_loops):\n\t\t# time.sleep(random.random()*10)\n\t\tpage_num += 1\n\n\t\tdata = {'currentPage': page_num, 'includeAllAnswers': True, 'perPage': 5, 'pwid': pwid, 'sortOption': 1}\n\t\theaders = {\"accept\": \"*/*\",\n\t\t\"accept-language\": \"en-US,en;q=0.9\",\n\t\t\"content-type\": \"application/x-www-form-urlencoded\",\n\t\t\"sec-ch-ua\": \"\\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Chromium\\\";v=\\\"101\\\", \\\"Google Chrome\\\";v=\\\"101\\\"\",\n\t\t\"sec-ch-ua-mobile\": \"?0\",\n\t\t\"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n\t\t\"sec-fetch-dest\": \"empty\",\n\t\t\"sec-fetch-mode\": \"cors\",\n\t\t\"sec-fetch-site\": \"same-origin\",\n\t\t\"Referer\": physician.get_link('Healthgrades'),\n\t\t\"Referrer-Policy\": \"strict-origin-when-cross-origin\"}\n\t\t\n\t\tfor j in range(0, len(json_data['results'])):\n\t\t\tcomment_text = json_data['results'][j]['commentText'].strip()\n\t\t\tcomment_date = convert_to_date(json_data['results'][j]['submittedDate'])\n\t\t\tcomment_rating = json_data['results'][j]['overallScore']\n\t\t\tuseful_count = json_data['results'][j]['helpfulCount']\n\n\t\t\tphysician.add_comment((comment_date, 'Healthgrades', comment_rating, comment_text, useful_count))\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "webscrape_healthgrades", "data": "def webscrape_healthgrades(physician: physician.Physician) -> None:\n\t'''\n\tWebscrapes healthgrades.com for the given physician\n\t'''\n\turl = physician.get_link('Healthgrades')\n\n\tif url != 'N/A':\n\t\tset_source(physician, 'Healthgrades')\n\t\t# set_tier(physician)\n\n\t\trequest = requests.get(url) # Gets HTML of website\n\t\tsoup = BeautifulSoup(request.content, 'html.parser')\n\n\t\tset_overall_rating_healthgrades(physician, soup)\n\t\tset_comment_healthgrades(physician, soup)\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "set_overall_rating_vitals", "data": "def set_overall_rating_vitals(physician: physician.Physician, soup: BeautifulSoup) -> None:\n\t'''\n\tSets overall rating during webscrape\n\t'''\n\ttime.sleep(random.random()*10)\n\toverall_rating = soup.select('#app > div.top-section > div.profile-header-container.loc-vs-prvdr > header > div > div > div.header-info > div > div.name-info > div > div.rating-section > a.ratings > span.rating-score')\n\ttry:\n\t\tphysician.set_overall_rating('Vitals', float(overall_rating[0].text.strip())) # Converts the html to float\n\texcept IndexError:\n\t\tphysician.set_overall_rating('Vitals', 'N/A')\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "webscrape_vitals", "data": "def webscrape_vitals(physician: physician.Physician) -> None:\n\t'''\n\tWebscrapes vitals.com for the given physician\n\t'''\n\turl = physician.get_link('Vitals')\n\tpage = 1\n\n\t# Redirects url to the reviews page\n\tif '.html' in url:\n\t\turl = url[:-5]\n\t\turl += f'/reviews?page={page}&sort=updated_at_dt%20desc'\n\n\tif url != 'N/A':\n\t\tset_source(physician, 'Vitals')\n\n\t#\t headers = {'authority': 'www.vitals.com',\n\t#\t\t\t\t 'method': 'GET',\n\t#\t\t\t\t 'path': '/doctors/Dr_Adam_Luginbuhl/reviews',\n\t#\t\t\t\t 'scheme': 'https',\n\t#\t\t\t\t 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n\t#\t\t\t\t 'accept-encoding': 'gzip, deflate, br',\n\t#\t\t\t\t 'accept-language': 'en-US,en;q=0.9',\n\t#\t\t\t\t 'cache-control': 'max-age=0',\n\t#\t\t\t\t 'cookie': 'gtinfo={\"ct\":\"Irvine\",\"c\":\"Orange\",\"cc\":\"6059\",\"st\":\"CA\",\"sc\":\"5\",\"z\":\"92697\",\"lat\":\"33.65\",\"lon\":\"-117.84\",\"dma\":\"803\",\"cntr\":\"usa\",\"cntrc\":\"840\",\"tz\":null,\"ci\":\"169.234.45.66\"}; notice_behavior=implied,us; _ga=GA1.1.280728103.1653438674; AMCVS_16AD4362526701720A490D45%40AdobeOrg=1; usprivacy=1YNN; s_cc=true; aam_uuid=91305347862693971774061348772953748111; _ibp=0:l3kup39v:f95c0d05-b5f2-4e1c-aea8-40a51ae5865c; TapAd_DID=7b3647c3-c14a-43c4-8917-d452f5028cec; notice_preferences=2:; notice_gdpr_prefs=0,1,2:; aam=aam%3D999996%2C529440%2C32964%2C32920%2C318069%2C663590%2C32539%2C617784%2C18091421%2C18292951%2C21558705%2C22156106%2C22833965%2C22876027%2C23269831%2C23376301%2C23421578%2C24060393; initial_url_path={%22url%22:%22%2Fdoctors%2FDr_Adam_Luginbuhl%2Freviews%22}; s_sq=%5B%5BB%5D%5D; __cfruid=4ddc06e72654592012cac55754a9a775651e3c9e-1653707609; __cf_bm=5cc70kut0gXZ76qYiSMZCN9f7y7GGEhnimld8nfb2fQ-1653707610-0-ARwjyKakQlDYoM83NBqkbqJpH0QvQAMTn2Iy+VhQwb97pO7i4UO24ttK1sod7/mimw3HqBT20E+dvOXVHqUiysyzp62o48BjOzaA1V+hUTwAn5OAZQ9W6OFEbKhBwCcpyLHSOoc/gHVasL0z6IYc+BW5eMOPbpEqzBHlnRISBohbSyGe04cI7LbHs1WPjPFULg==; mnet_session_depth=1%7C1653707611077; _ga_3ZVJC9H4TB=GS1.1.1653707611.5.0.1653707611.0; ui={%22expmatch%22:1%2C%22vtime%22:27561793}; fpci={%22iafValue%22:1%2C%22url%22:%22www.vitals.com%2Fdoctors%2FDr_Adam_Luginbuhl%2Freviews%22}; AMCV_16AD4362526701720A490D45%40AdobeOrg=-432600572%7CMCIDTS%7C19141%7CMCMID%7C91511255161485384144041336488334773784%7CMCAAMLH-1654312411%7C9%7CMCAAMB-1654312411%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1653714811s%7CNONE%7CMCAID%7CNONE%7CvVersion%7C4.5.2; _ibs=0:l3kup39x:87126a4d-080b-40f0-8864-c106f5a314d6',\n\t#\t\t\t\t 'referer': 'https://www.vitals.com/doctors/Dr_Adam_Luginbuhl.html',\n\t#\t\t\t\t 'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\", \"Google Chrome\";v=\"101\"',\n\t#\t\t\t\t 'sec-ch-ua-mobile': '?0',\n\t#\t\t\t\t 'sec-ch-ua-platform': \"Windows\",\n\t#\t\t\t\t 'sec-fetch-dest': 'document',\n\t#\t\t\t\t 'sec-fetch-mode': 'navigate',\n\t#\t\t\t\t 'sec-fetch-site': 'same-origin',\n\t#\t\t\t\t 'sec-fetch-user': '?1',\n\t#\t\t\t\t 'upgrade-insecure-requests': '1',\n\t#\t\t\t\t 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36}'\n\t#\t\t\t\t }   \n\t\t\n\t#\t request = requests.post(url, headers=headers)\n\t#\t json_data = json.loads(request.text)\n\n\n\t\t# with requests.session() as s:\n\n\t\t#\t # load cookies:\n\t\t#\t s.get(url, headers=headers)\n\n\t\t#\t # get data:\n\t\t#\t data = s.get(url, headers=headers).json()\n\n\n\t\t# headers = {'User-agent': 'Mozilla/5.0'}\n\n\t\t# request = requests.get('https://www.vitals.com/doctors/Dr_Adam_Luginbuhl/reviews', headers = headers) # Gets HTML of website\n\t\t# page = urllib.request.urlopen(request)\n\t\t# soup = BeautifulSoup(request.content, 'html.parser')\n\n\t\t# set_overall_rating_vitals(physician, soup)\n\t\t# set_comment_vitals(physician, soup)\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "webscrape_ratemds", "data": "def webscrape_ratemds(physician: physician.Physician) -> None:\n\t'''\n\tWebscrapes ratemds.com for the given physician\n\t'''\n\tpage_num = 1\n\turl = physician.get_link('RateMDs') + f'/?page={page_num}'\n\n\tif url != 'N/A':\n\t\tset_source(physician, 'RateMDs')\n\t\t# set_tier(physician)\n\t\ttime.sleep(random.random()*10)\n\n\t\trequest = requests.get(url) # Gets HTML of website\n\t\tsoup = BeautifulSoup(request.content, 'html.parser')\n\n\t\t# set_overall_rating_healthgrades(physician, soup)\n\t\t# set_comment_healthgrades(physician, soup)\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "set_overall_rating_yelp", "data": "def set_overall_rating_yelp(physician: physician.Physician, soup: BeautifulSoup) -> None:\n\t'''\n\tSets overall rating during webscrape\n\t'''\n\toverall_rating = soup.select('#main-content > div.margin-b3__09f24__l9v5d.border-color--default__09f24__NPAKY > div > div > div.arrange__09f24__LDfbs.gutter-1-5__09f24__vMtpw.vertical-align-middle__09f24__zU9sE.margin-b2__09f24__CEMjT.border-color--default__09f24__NPAKY > div:nth-child(1) > span > div')\n\n\ttry:\n\t\tphysician.set_overall_rating('Yelp', float(overall_rating[0]['aria-label'][0])) # Converts the html to float\n\texcept IndexError:\n\t\tphysician.set_overall_rating('Yelp', 'N/A')\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "set_comment_yelp", "data": "def set_comment_yelp(physician: physician.Physician, soup: BeautifulSoup) -> None:\n\t'''\n\tSets comment during webscrape\n\t'''\n\tcomments = soup.find_all('div', {'review__09f24__oHr9V border-color--default__09f24__NPAKY'})\n\n\tfor comment in comments:\n\t\tcomment_tag = comment.find('span', {'raw__09f24__T4Ezm'})\n\t\tcomment_text = comment_tag.text\n\n\n\n\t\trating_tag = comment.find('div', {'i-stars__09f24__M1AR7 i-stars--regular-5__09f24__tKNMk border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG', \n\t\t\t\t\t\t\t\t\t\t\t'i-stars__09f24__M1AR7 i-stars--regular-1__09f24__o88Iy border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG',\n\t\t\t\t\t\t\t\t\t\t\t'i-stars__09f24__M1AR7 i-stars--regular-2__09f24__mq_AY border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG',\n\t\t\t\t\t\t\t\t\t\t\t'i-stars__09f24__M1AR7 i-stars--regular-4__09f24__qui79 border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG',\n\t\t\t\t\t\t\t\t\t\t\t'i-stars__09f24__M1AR7 i-stars--regular-3__09f24__sRNTp border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG'})\n\t\trating = rating_tag['aria-label'][0]\n\n\t\tcomment_date = comment.find('span', {'css-chan6m'}).text\n\n\n\n\t\ttry:\n\t\t\tuseful_count = comment.find('span', {'css-1lr1m88'}).text.strip()\n\t\t\tphysician.add_comment((comment_date, 'Yelp', rating, comment_text, useful_count))\n\t\texcept AttributeError:\n\t\t\tuseful_count = 0\n\t\t\tphysician.add_comment((comment_date, 'Yelp', rating, comment_text, useful_count))\n\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "webscrape_yelp", "data": "def webscrape_yelp(physician: physician.Physician) -> None:\n\t'''\n\tWebscrapes yelp.com for the given physician\n\t'''\n\ttime.sleep(random.random()*10)\n\tnum_queries = 0\n\turl = physician.get_link('Yelp')\n\t# url = f'https://www.yelp.com/biz/chatime-irvine-irvine-2?start={num_queries}'\n\trequest = requests.get(url) # Gets HTML of website\n\tsoup = BeautifulSoup(request.content, 'html.parser')\n\ttotal_num_queries = soup.select('#main-content > div.margin-b3__09f24__l9v5d.border-color--default__09f24__NPAKY > div > div > div.arrange__09f24__LDfbs.gutter-1-5__09f24__vMtpw.vertical-align-middle__09f24__zU9sE.margin-b2__09f24__CEMjT.border-color--default__09f24__NPAKY > div.arrange-unit__09f24__rqHTg.arrange-unit-fill__09f24__CUubG.border-color--default__09f24__NPAKY.nowrap__09f24__lBkC2 > span')\n\t\n\tif len(total_num_queries) > 0:\n\t\tnum_queries_left = int(total_num_queries[0].text[0])\t\n\telse:\n\t\tnum_queries_left = 1\n\n\twhile num_queries_left > 0:\n\t\t# url = f'https://www.yelp.com/biz/chatime-irvine-irvine-2?start={num_queries}'\n\t\turl = physician.get_link('Yelp') + f'?start={num_queries}'\n\n\t\tif url != 'N/A':\n\t\t\tset_source(physician, 'Yelp')\n\t\t\t# set_tier(physician)\n\n\t\t\trequest = requests.get(url) # Gets HTML of website\n\t\t\tsoup = BeautifulSoup(request.content, 'html.parser')\n\n\t\t\tset_overall_rating_yelp(physician, soup)\n\t\t\tset_comment_yelp(physician, soup)\n\t\t\n\t\tnum_queries_left -= 10\n\t\tnum_queries += 10\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "webscrape_links", "data": "def webscrape_links(physician: physician.Physician) -> None:\n\t'''\n\tCalls the webscrape on each link in the physican\n\t'''\n\t# if physician.get_link('Healthgrades') != 'N/A':\n\t#\t webscrape_healthgrades(physician)\n\n\t# if physician.get_link('Vitals') != 'N/A': # change .html to /reviews?page=1&sort=updated_at_dt%20desc\n\t#\t webscrape_vitals(physician)\n\n\t# if physician.get_link('RateMDs') != 'N/A': # Does by page num in link\n\t#\t webscrape_ratemds(physician)\n\n\t# if physician.get_link('Yelp') != 'N/A': # Does by num queries in link\n\t#\t webscrape_yelp(physician)\n\t\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}, {"term": "def", "name": "webscrape_list", "data": "def webscrape_list(list_of_physicians: list[physician.Physician]) -> None:\n\t'''\n\tCalls webscrape on all physicians\n\t'''\n\tfor physician in list_of_physicians:\n\t\tset_tier(physician)\n\t\twebscrape_links(physician)\n\n", "description": null, "category": "webscraping", "imports": ["import physician", "import requests", "from bs4 import BeautifulSoup", "import json", "import math", "import random", "import time"]}], [{"term": "def", "name": "blackbox_webscrape", "data": "def blackbox_webscrape():\n\tcurrent_step = 1 # starting step #\n\t\n\t# Manually adjust simulation number, endsteps and intervals here\n\tsim_num = 8\n\tendstep = 7000\n\tintervals = 1\n\t\n\tprint(\"Starting to run code. The date/time is: \" + str(x))\n\n\turl = \"https://www.informatics.indiana.edu/jbollen/I501F18/blackbox/BlackBox_N.php\" # Blackbox URL\n\tdriver = webdriver.Chrome('C:/Users/Kyrie/GitHub/I501Blackbox/chromedriver')  # Location of chrome driver on Kyrie's Desktop\n\t#driver = webdriver.Chrome('C:/Users/kyrie/OneDrive/Documents/GitHub/I501Blackbox/chromedriver')  # Location of chrome driver on Kyrie's Laptop\n\t#driver = webdriver.Firefox(executable_path='/usr/local/bin/geckodriver') #this is for Becca's Macbook. Comment this out and do the other drivers for Kyrie.\n\t\n\tdriver.get(url) # Get URL\n\t\n\tdriver.find_element_by_name(\"cycles\").clear() # clear current interval amount\n\tdriver.find_element_by_name(\"cycles\").send_keys(intervals) # Set to chosen interval amount\n\t\t\n\twhile current_step < endstep: # Run until end step \n\n\t\tsoup = BeautifulSoup(driver.page_source) # read page source\n\t\t\n\t\ttable1 = soup.find_all('table',id=\"system\")[0] # find blackbox table in source\n\t\t\n\t\tfind_step = soup.find_all('p') # find location of current step in source\n\t\tcurrent_step = int(find_step[3].contents[0][13:]) # Finds current step in integer format\n\t\t\n\t\t#Find all of the numbers in the 20 x 20 grid in the page source and store to my_table\n\t\trows = table1.findChildren(['th', 'tr'])\n\t\t\n\t\tmy_table = []\n\t\ti = 0\n\t\tj = 0\n\n\t\tfor row in rows:\n\t\t\ti += 1\n\t\t\tcells = row.findChildren('td')\n\t\t\tfor cell in cells:\n\t\t\t\tj += 1\n\t\t\t\tif j == 21: # Reset to 1 after 20\n\t\t\t\t\tj= 1\n\t\t\t\tvalue = cell.string\n\t\t\t\tmy_table.append(value)\n\t\t\n\t\tmy_table2 = np.reshape(my_table, (20,20)).astype(int) # Reshape to 20 x 20 array\n\t\t\n\t\t# Save array to text file with simulation # and step # in filename\n\t\toutput_file = 'sim' +str(sim_num)+ '_step' + str(current_step) + '_int' + str(intervals) + '.txt'\n\t\tnp.savetxt(output_file, my_table2, delimiter=',',fmt= '%d') \n\t\t\n\t\t# Print status\n\t\tprint('Simulation ' + str(sim_num) + ': Step ' + str(current_step) + ' file saved.')\n\t\t\n\t\ttime.sleep(5) # wait 5 seconds to allow for saving file\n\t\t\n\t\t# Click next button on page\n\t\tbutton = driver.find_element(By.XPATH, '//button[text()=\"Next n Step\"]')\n\t\tbutton.click()\n\t\t\n\t\ttime.sleep(5) # wait 5 seconds to wait after clicking Next Step\n\t\t\t  \n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.by import By", "from bs4 import BeautifulSoup", "import numpy as np", "from datetime import datetime", "from threading import Timer"]}], [{"term": "def", "name": "scrape_summary_data", "data": "def scrape_summary_data(symbol):\n\t\"\"\"Uses BeautifulSoup and Requests to webscrape summary data of stock from Yahoo Finance\n\n\tParameters:\n\tsymbol: The stock symbol (str)\n\n\tReturns:\n\tTwo Pandas Dataframe in a Tuple with up to date summary statistics of stock\n\t\"\"\"\n\t# Check symbol is a (str), raise type error otherwise\n\tvalid_type(symbol, str, True)\n\n\ttry:\n\t\t# Request data from yahoo finance\n\t\tr = requests.get(f\"https://finance.yahoo.com/quote/{symbol}?p={symbol}\")\n\n\t\tsoup = BeautifulSoup(r.text, \"lxml\")\n\n\t\t# Create summary stats dictionary and add current price\n\t\tsummary = {\n\t\t\t\"Price\": soup.find_all(\"div\", {\"class\": \"My(6px) Pos(r) smartphone_Mt(6px)\"})[0]\n\t\t\t.find(\"span\")\n\t\t\t.text\n\t\t}\n\n\texcept:\n\t\t# Raise error if stock symbol input is not valid\n\t\traise ValueError(\"Did not enter valid stock symbol\")\n\n\t# Use html class strings from inspecting yahoo finance website\n\tclass1_str = (\n\t\t\"D(ib) W(1/2) Bxz(bb) Pend(12px) Va(t) ie-7_D(i) smartphone_D(b) smartphone_W(100%)\"\n\t\t+ \" smartphone_Pend(0px) smartphone_BdY smartphone_Bdc($seperatorColor)\"\n\t)\n\n\t# Locate summary statistics and add to summary dictionary\n\ttablesoup = soup.find_all(\"div\", {\"class\": class1_str})[0].find_all(\"td\")\n\tfor i in range(7):\n\t\tsummary[tablesoup[2 * i].text] = tablesoup[2 * i + 1].text\n\n\t# Return Pandas dataframe of summary stats\n\treturn pd.DataFrame(summary, index=[0])\n\n", "description": "Uses BeautifulSoup and Requests to webscrape summary data of stock from Yahoo Finance\n\n\tParameters:\n\tsymbol: The stock symbol (str)\n\n\tReturns:\n\tTwo Pandas Dataframe in a Tuple with up to date summary statistics of stock\n\t", "category": "webscraping", "imports": ["from luigi import ExternalTask, Parameter, build", "import dask.dataframe as dd", "import yfinance as yf", "from bs4 import BeautifulSoup", "import requests", "import pandas as pd", "from csci_utils.Validation.validater import valid_type", "from csci_utils.luigi.task import Requirement, Requires, TargetOutput", "from csci_utils.luigi.dask.target import ParquetTarget", "from .utils import get_range"]}, {"term": "class", "name": "GetHistoricalData", "data": "class GetHistoricalData(ExternalTask):\n\t\"\"\"Uses the yfinance package to webscrape stock data from yahoo finance\n\tread in as a dask dataframe, and writes to parquet.\n\n\t\tParameters:\n\t\tsymbol: The stock symbol (str)\n\t\tinterval: The unit of time per row (str)\n\t\"\"\"\n\n\t# Task Parameters\n\tsymbol = Parameter(default=\"AAPL\")\n\tinterval = Parameter(default=\"1d\")\n\n\t# Target Output as descriptor\n\toutput = TargetOutput(\n\t\tfile_pattern=\"data/{symbol}/{interval}/rawdata/\", target_class=ParquetTarget\n\t)\n\n\tdef run(self):\n\t\tstock = yf.Ticker(self.symbol)\n\t\tdf = stock.history(\n\t\t\tinterval=self.interval, period=get_range(self.interval)\n\t\t).iloc[:, :5]\n\t\tdf = df.dropna()\n\t\tddf = dd.from_pandas(df, chunksize=500)\n\t\tself.output().write_dask(ddf, compression=\"gzip\")\n\n\n", "description": "Uses the yfinance package to webscrape stock data from yahoo finance\n\tread in as a dask dataframe, and writes to parquet.\n\n\t\tParameters:\n\t\tsymbol: The stock symbol (str)\n\t\tinterval: The unit of time per row (str)\n\t", "category": "webscraping", "imports": ["from luigi import ExternalTask, Parameter, build", "import dask.dataframe as dd", "import yfinance as yf", "from bs4 import BeautifulSoup", "import requests", "import pandas as pd", "from csci_utils.Validation.validater import valid_type", "from csci_utils.luigi.task import Requirement, Requires, TargetOutput", "from csci_utils.luigi.dask.target import ParquetTarget", "from .utils import get_range"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape():\n\tfor page in range(1, 10):\n\t\tfuel_type = 'Petrol'\n\t\tdistance = 40\n\t\tr = requests.get(f'https://www.autotrader.co.uk/van-search?sort=relevance&postcode=ig26sl&radius={distance}&include-delivery-option=on&fuel-type={fuel_type}&page={page}', headers=headers)\n\t\tsoup = BeautifulSoup(r.text, 'lxml')\n\t\t#print(r.status_code)\n\n\t\tvanlist = soup.find_all('li', class_='search-page__result')\n\t\t\n\t\tfor van in vanlist:\n\t\t\tfor link in van.find_all('a', class_='js-click-handler listing-fpa-link tracking-standard-link', href=True):\n\t\t\t\t\n\t\t\t\tvan_title = van.find('h3', class_='product-card-details__title').text.strip()\n\t\t\t\tvan_price = van.find('div', class_='product-card-pricing__price').text.strip()\n\t\t\t\tvan_specs = van.find('p', class_='product-card-details__subtitle').text.strip()\n\t\t\t\tvan_link = baseurl + link['href']\n\t\t\t\tvan = {\n\t\t\t\t'name': van_title,\n\t\t\t\t'price': van_price,\n\t\t\t\t'specs': van_specs,\n\t\t\t\t'link': van_link\n\t\t\t\t}\n\t\t\t\tc.execute('''INSERT INTO vans VALUES(?,?,?,?)''', (van['name'], van['price'], van['specs'], van['link']))\n\n\t\t\t\tvans.append(van)\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import re", "import pandas as pd", "import sqlite3"]}], [{"term": "class", "name": "KijijiScraper", "data": "class KijijiScraper(d.WebScrape):\n\tdef __init__(self, url, searched):\n\t\tsuper().__init__(url, searched)\n\t\n\tdef findApparts(self):\n\t\tlink_start = \"https://kijiji.ca\"\n\t\tfile_rows, title_column, price_column, description_column, location_column, img_column, link_column = [], [], [], [], [], [], []\n\t\tpage = self.parsePage()\n\t\tapparts = page.findAll(\"div\", {\"class\":\"search-item\"})\n\n\t\tfor appart in apparts:\t\n\t\t\ttitle = appart.find(\"div\", {\"class\":\"title\"}).text.strip()\n\t\t\ttitle_column.append(title)\n\t\t\tlink = link_start + appart.find(\"div\", {\"class\":\"title\"}).find(\"a\")[\"href\"]\n\t\t\tlink_column.append(link)\n\t\t\tprice = appart.find(\"div\", {\"class\":\"price\"}).text.strip()\n\t\t\tprice_column.append(price)\n\t\t\tdescription = appart.find(\"div\", {\"class\":\"description\"}).text.strip()\n\t\t\tdescription_column.append(description)\n\t\t\taddress = appart.find(\"div\", {\"class\":\"location\"}).text.strip()\n\t\t\tlocation_column.append(address)\n\t\t\timg = appart.find(\"div\", {\"class\":\"image\"}).find(\"img\")[\"src\"]\n\t\t\timg_column.append(img)\n\n\t\t\tprint(\"--------------------------------\")\n\t\t\tprint(f\"Location: {address}\")\n\t\t\tprint(f\"Lien: {link}\")\n\t\t\tprint(f\"Prix: {price}\")\n\t\t\tprint(f\"Description: {description}\")\n\t\t\tprint(f\"Image: {img}\")\n\t\t\tprint(\"--------------------------------\")\n\t\t\n\t\n\t\tfile_rows.append(title_column)\n\t\tfile_rows.append(link_column)\n\t\tfile_rows.append(price_column)\n\t\tfile_rows.append(description_column)\n\t\tfile_rows.append(location_column)\n\t\tfile_rows.append(img_column)\n\n\t\treturn file_rows\n\t\t\n\n\n\n\n\t\t\t\n\n\t\t\n\n\n\n\n\t\n\t\n", "description": null, "category": "webscraping", "imports": ["import webscrape_module as d", "from bs4 import BeautifulSoup as soup"]}], [{"term": "def", "name": "update_urlfile", "data": "def update_urlfile(df_urls, index_row):\n\t\"\"\"\n\tAfter downloading html or scraping content the process status has to be updated.\n\t\"\"\"\n\tdf_urls.loc[index] = index_row\n\tdf_urls.to_csv(\"Input/urls_scrape.csv\", index=False)\n\n", "description": "\n\tAfter downloading html or scraping content the process status has to be updated.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import numpy as np", "from tqdm import tqdm", "import webscrape_script as wss", "from datetime import date"]}], [{"term": "def", "name": "pages", "data": "def pages(URL, numpages): #takes URL, numpages, returns pagelist of numpages URL's of reviews\n\n\tpagelist = []\n\n\tfor i in range(len(URL)):\n\t\tif URL[i] == 'e' and URL[i+1] == '1':\n\t\t\tvar = i+1\n\tfor j in range(numpages):\n\t\tbefore = URL[0:(var)]\n\t\tafter = URL[(var+1):]\n\t\tpagenum = str(j+1)\n\t\tpage = before + pagenum + after\n\t\t#print(pagenum)\n\t\tpagelist.append(page)\n\n\treturn pagelist\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}, {"term": "def", "name": "reviewlist", "data": "def reviewlist(pagelist):\n\n\treviewlist = []\n\tfor i in range(len(pagelist)):\n\t\t\n\t\tURL = pagelist[i]\n\t\tpage = requests.get(URL)\n\t\tstatus = page.status_code #if 200, page is accessible\n\t\tcontents = page.text\n\t\tsoup = BeautifulSoup(contents, 'html.parser')\n\t\t\n\t\tmain = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n\t\t#finds specific reviews 2nd review starts at 2...index of reviews\n\t\treview = main.find_all('div', recursive=False) \n\t\t\n\n\t\tfor j in range(len(review)):\n\t\t\tk = j+1\n\t\t\tif k > 10:  #if k is greater than the number of reviews on page\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treview = main.find_all('div', recursive=False)[k]\n\t\t\t\tbody = review.find_all('p')[0]\n\t\t\t\tstring = body.text\n\t\t\t\tstring2 = string\n\t\t\t\treviewlist.append(string2)\n\n\treturn reviewlist\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}, {"term": "def", "name": "generalrating", "data": "def generalrating(pagelist):\n\t\n\trating_list = []\n\n\tfor i in range(len(pagelist)):\n\t\t\n\t\tURL = pagelist[i]\n\t\tpage = requests.get(URL)\n\t\tstatus = page.status_code #if 200, page is accessible\n\t\tcontents = page.text\n\t\tsoup = BeautifulSoup(contents, 'html.parser')\n\t\t\n\t\tmain = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n\t\t#finds specific reviews 2nd review starts at 2...index of reviews\n\t\treview = main.find_all('div', recursive=False) \n\n\t\tfor j in range(len(review)):\n\t\t\tk = j+1\n\t\t\tif k > 10:  #if k is greater than the number of reviews on page\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treview = main.find_all('div', recursive=False)[k]\n\t\t\t\trating_1 = review.find_all('div')[4]\n\t\t\t\trating_2 = rating_1['class']\n\n\t\t\t\t#[2] is the class review rating\n\t\t\t\trating_3 = rating_2[2]  #turning the rating class into a string\n\t\t\t\tscore = float(rating_3[7] + '.' + rating_3[8]) # score is review score out of 5\n\t\t\t\t#index goes from 1-11 : 10 reviews per page\n\n\t\t\t\trating_list.append(score)\n\t\t\t\n\treturn rating_list\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}, {"term": "def", "name": "specificratings", "data": "def specificratings(pagelist):\n\tbreakdownlist = []\n\tfor i in range(len(pagelist)):\n\t\t\n\t\t#range(len(pagelist)):\n\t\tURL = pagelist[i]\n\t\tpage = requests.get(URL)\n\t\tstatus = page.status_code #if 200, page is accessible\n\t\tcontents = page.text\n\t\tsoup = BeautifulSoup(contents, 'html.parser')\n\t\t\n\t\tmain = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n\t\t#finds specific reviews 2nd review starts at 2...index of reviews\n\t\treview = main.find_all('div', recursive=False) \n\t\tfor j in range(len(review)):\n\t\t\tk = j+1\n\t\t\tif k > 10:  #if k is greater than the number of reviews on page\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\t#finds specific reviews 2nd review starts at 2...index of reviews\n\n\t\t\t\treview = main.find_all('div', recursive=False)[k]\n\t\t\t\trating_1 = review.find_all('div')[10] #rating breakdown\n\n\t\t\t\tbreakdownindex = [2, 5, 8, 11, 14] # index of location of specific breakdown scores\n\t\t\t\t\t# 2 'Customer Service\n\t\t\t\t\t# 5 'Quality of work\n\t\t\t\t\t# 8 'Friendliness\n\t\t\t\t\t# 11 is Pricing\n\t\t\t\t\t# 14 is overall excperience\n\n\t\t\t\tavglist = []\n\n\t\t\t\tfor l in breakdownindex: # 5 is the number of specific ratings\n\t\t\t\t\trating_2 = rating_1.find_all('div')[0] \n\n\t\t\t\t\t#finds individual rating sets\n\t\t\t\t\trating_3 = rating_2.find_all('div')[l]\n\t\t\t\t\t\n\t\t\t\t\trating_4 = rating_3['class']\n\t\t\t\t\trating_5 = rating_4[1]\n\t\t\t\t\tscore = float(rating_5[7] + '.' + rating_5[8]) # score is review score out of 5\n\n\t\t\t\t\tavglist.append(score)\n\t\t\t\t\n\t\t\t\taverage = sum(avglist) / len(avglist)\n\t\t\t\tbreakdownlist.append(average)\n\n\treturn breakdownlist\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}, {"term": "def", "name": "positivewords", "data": "def positivewords(reviewlist):\n\t\n\t#initialize lists\n\tsentimentscore, pos_words = ([] for i in range(2))\n\t\n\tfor j in range(len(reviewlist)):\n\n\t\t#initialize variables\n\t\tpos_score = 0\n\t\trsp = []\n\n\t\tind_review = reviewlist[j]\n\t\tsplitwords = ind_review.split(' ')  #individual review list\n\n\t\tfor i in range(len(splitwords)):\n\t\t\tfor k in range(len(posdict)):\n\n\t\t\t\tif (splitwords[i] == posdict[k]) or (splitwords[i] == posdictstrip[k]):\n\n\t\t\t\t\tpos_score +=1\n\t\t\t\t\trsp.append(splitwords[i])\n\n\t\tif pos_score == 0:\n\t\t\trsp.append(0)\n\n\t\tpos_words.append(rsp)\n\n\t\tsentimentscore.append(pos_score)\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}], [{"term": "def", "name": "pages", "data": "def pages(URL, numpages): #takes URL, numpages, returns pagelist of numpages URL's of reviews\n\n\tpagelist = []\n\n\tfor i in range(len(URL)):\n\t\tif URL[i] == 'e' and URL[i+1] == '1':\n\t\t\tvar = i+1\n\tfor j in range(numpages):\n\t\tbefore = URL[0:(var)]\n\t\tafter = URL[(var+1):]\n\t\tpagenum = str(j+1)\n\t\tpage = before + pagenum + after\n\t\t#print(pagenum)\n\t\tpagelist.append(page)\n\n\treturn pagelist\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}, {"term": "def", "name": "reviewlist", "data": "def reviewlist(pagelist):\n\n\treviewlist = []\n\tfor i in range(len(pagelist)):\n\t\t\n\t\tURL = pagelist[i]\n\t\tpage = requests.get(URL)\n\t\tstatus = page.status_code #if 200, page is accessible\n\t\tcontents = page.text\n\t\tsoup = BeautifulSoup(contents, 'html.parser')\n\t\t\n\t\tmain = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n\t\t#finds specific reviews 2nd review starts at 2...index of reviews\n\t\treview = main.find_all('div', recursive=False) \n\t\t\n\n\t\tfor j in range(len(review)):\n\t\t\tk = j+1\n\t\t\tif k > 10:  #if k is greater than the number of reviews on page\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treview = main.find_all('div', recursive=False)[k]\n\t\t\t\tbody = review.find_all('p')[0]\n\t\t\t\tstring = body.text\n\t\t\t\tstring2 = string\n\t\t\t\treviewlist.append(string2)\n\n\treturn reviewlist\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}, {"term": "def", "name": "generalrating", "data": "def generalrating(pagelist):\n\t\n\trating_list = []\n\n\tfor i in range(len(pagelist)):\n\t\t\n\t\tURL = pagelist[i]\n\t\tpage = requests.get(URL)\n\t\tstatus = page.status_code #if 200, page is accessible\n\t\tcontents = page.text\n\t\tsoup = BeautifulSoup(contents, 'html.parser')\n\t\t\n\t\tmain = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n\t\t#finds specific reviews 2nd review starts at 2...index of reviews\n\t\treview = main.find_all('div', recursive=False) \n\n\t\tfor j in range(len(review)):\n\t\t\tk = j+1\n\t\t\tif k > 10:  #if k is greater than the number of reviews on page\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treview = main.find_all('div', recursive=False)[k]\n\t\t\t\trating_1 = review.find_all('div')[4]\n\t\t\t\trating_2 = rating_1['class']\n\n\t\t\t\t#[2] is the class review rating\n\t\t\t\trating_3 = rating_2[2]  #turning the rating class into a string\n\t\t\t\tscore = float(rating_3[7] + '.' + rating_3[8]) # score is review score out of 5\n\t\t\t\t#index goes from 1-11 : 10 reviews per page\n\n\t\t\t\trating_list.append(score)\n\t\t\t\n\treturn rating_list\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}, {"term": "def", "name": "specificratings", "data": "def specificratings(pagelist):\n\tbreakdownlist = []\n\tfor i in range(len(pagelist)):\n\t\t\n\t\t#range(len(pagelist)):\n\t\tURL = pagelist[i]\n\t\tpage = requests.get(URL)\n\t\tstatus = page.status_code #if 200, page is accessible\n\t\tcontents = page.text\n\t\tsoup = BeautifulSoup(contents, 'html.parser')\n\t\t\n\t\tmain = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n\t\t#finds specific reviews 2nd review starts at 2...index of reviews\n\t\treview = main.find_all('div', recursive=False) \n\t\tfor j in range(len(review)):\n\t\t\tk = j+1\n\t\t\tif k > 10:  #if k is greater than the number of reviews on page\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\t#finds specific reviews 2nd review starts at 2...index of reviews\n\n\t\t\t\treview = main.find_all('div', recursive=False)[k]\n\t\t\t\trating_1 = review.find_all('div')[10] #rating breakdown\n\n\t\t\t\tbreakdownindex = [2, 5, 8, 11, 14] # index of location of specific breakdown scores\n\t\t\t\t\t# 2 'Customer Service\n\t\t\t\t\t# 5 'Quality of work\n\t\t\t\t\t# 8 'Friendliness\n\t\t\t\t\t# 11 is Pricing\n\t\t\t\t\t# 14 is overall excperience\n\n\t\t\t\tavglist = []\n\n\t\t\t\tfor l in breakdownindex: # 5 is the number of specific ratings\n\t\t\t\t\trating_2 = rating_1.find_all('div')[0] \n\n\t\t\t\t\t#finds individual rating sets\n\t\t\t\t\trating_3 = rating_2.find_all('div')[l]\n\t\t\t\t\t\n\t\t\t\t\trating_4 = rating_3['class']\n\t\t\t\t\trating_5 = rating_4[1]\n\t\t\t\t\tscore = float(rating_5[7] + '.' + rating_5[8]) # score is review score out of 5\n\n\t\t\t\t\tavglist.append(score)\n\t\t\t\t\n\t\t\t\taverage = sum(avglist) / len(avglist)\n\t\t\t\tbreakdownlist.append(average)\n\n\treturn breakdownlist\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}, {"term": "def", "name": "positivewords", "data": "def positivewords(reviewlist):\n\t\n\t#initialize lists\n\tsentimentscore, pos_words = ([] for i in range(2))\n\t\n\tfor j in range(len(reviewlist)):\n\n\t\t#initialize variables\n\t\tpos_score = 0\n\t\trsp = []\n\n\t\tind_review = reviewlist[j]\n\t\tsplitwords = ind_review.split(' ')  #individual review list\n\n\t\tfor i in range(len(splitwords)):\n\t\t\tfor k in range(len(posdict)):\n\n\t\t\t\tif (splitwords[i] == posdict[k]) or (splitwords[i] == posdictstrip[k]):\n\n\t\t\t\t\tpos_score +=1\n\t\t\t\t\trsp.append(splitwords[i])\n\n\t\tif pos_score == 0:\n\t\t\trsp.append(0)\n\n\t\tpos_words.append(rsp)\n\n\t\tsentimentscore.append(pos_score)\n\n", "description": null, "category": "webscraping", "imports": ["import requests # to request website data", "from bs4 import BeautifulSoup # to parse HTML and find content", "from dictionary import dictionary"]}], [], [], [{"term": "class", "name": "classAmazonWebscrapeSpiderMiddleware:", "data": "class AmazonWebscrapeSpiderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the spider middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_spider_input(self, response, spider):\n\t\t# Called for each response that goes through the spider\n\t\t# middleware and into the spider.\n\n\t\t# Should return None or raise an exception.\n\t\treturn None\n\n\tdef process_spider_output(self, response, result, spider):\n\t\t# Called with the results returned from the Spider, after\n\t\t# it has processed the response.\n\n\t\t# Must return an iterable of Request, or item objects.\n\t\tfor i in result:\n\t\t\tyield i\n\n\tdef process_spider_exception(self, response, exception, spider):\n\t\t# Called when a spider or process_spider_input() method\n\t\t# (from other spider middleware) raises an exception.\n\n\t\t# Should return either None or an iterable of Request or item objects.\n\t\tpass\n\n\tdef process_start_requests(self, start_requests, spider):\n\t\t# Called with the start requests of the spider, and works\n\t\t# similarly to the process_spider_output() method, except\n\t\t# that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n\t\t# Must return only requests (not items).\n\t\tfor r in start_requests:\n\t\t\tyield r\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}, {"term": "class", "name": "classAmazonWebscrapeDownloaderMiddleware:", "data": "class AmazonWebscrapeDownloaderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the downloader middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_request(self, request, spider):\n\t\t# Called for each request that goes through the downloader\n\t\t# middleware.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this request\n\t\t# - or return a Response object\n\t\t# - or return a Request object\n\t\t# - or raise IgnoreRequest: process_exception() methods of\n\t\t#   installed downloader middleware will be called\n\t\treturn None\n\n\tdef process_response(self, request, response, spider):\n\t\t# Called with the response returned from the downloader.\n\n\t\t# Must either;\n\t\t# - return a Response object\n\t\t# - return a Request object\n\t\t# - or raise IgnoreRequest\n\t\treturn response\n\n\tdef process_exception(self, request, exception, spider):\n\t\t# Called when a download handler or a process_request()\n\t\t# (from other downloader middleware) raises an exception.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this exception\n\t\t# - return a Response object: stops process_exception() chain\n\t\t# - return a Request object: stops process_exception() chain\n\t\tpass\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}], [], [], [{"term": "def", "name": "search", "data": "def search(recipe):\n\tif recipe == '' or recipe == ' ':\n\t\traise ValueError(\"Error! Need recipe name.\")\n\t\t\n\telse:\n\t\t#Credit rjdang for google search code\n\t\tsearch = recipe\n\t\tsite = \"food network\"\n\t\tsearch_string = site + ' ' + search\n\t\turl = \"https://www.google.com/search?\" \n\t\tquery_encoded = urllib.parse.urlencode({\"q\":search_string})\n\t\tquery = url + query_encoded\n\t\t\n\t\t#spaces %20\n\t\t#apostrophe %27\n\n\t\turl_request = Request(query, headers = {\"User-Agent\": \"Mozilla/5.0\"})\n\t\tuclient = uReq(url_request)\n\t\tresponse = uclient.read()\n\t\tuclient.close()\n\t\tpage_soup = soup(response)\n\t\tred = page_soup.find_all('a')\n\t\t\n\t\t\n\t\tIPython.embed()\n\n\t\t\n\t\t\t\t\n", "description": null, "category": "webscraping", "imports": ["import IPython", "from urllib.request import urlopen as uReq", "from urllib.request import Request", "from bs4 import BeautifulSoup as soup", "import keys", "import urllib", "import json", "import sys"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(myurl):\n\t#open connection and grab page\n\tuClient = uReq(myurl)\n\tpage_html = uClient.read()\n\tuClient.close()\n\tpage_soup = soup(page_html, \"html.parser\")\n\tcontainer = page_soup.find(\"div\",{\"class\":\"o-Ingredients__m-Body\"})\n\thold = container.ul.findAll('li')\n\tingredients = []\n\tfor c in range(0,len(hold)):\n\t\tingredients.append(hold[c].label.string)\n\treturn ingredients\n\t# groceryList = []\n\t# quantity = []\n\t# for d in range(0,len(ingredients)):\n\t\t# ingredient = ingredients[d].split()\t\n\t\t# groceryList.append(ingredient[len(ingredient)-1])\n\t\t# amount = []\n\t\t# for y in range(0,len(ingredient)-1):\n\t\t\t# amount.append(ingredient[y])\n\t\t# quantity.append(amount)\n\t# return (groceryList, quantity)\n", "description": null, "category": "webscraping", "imports": ["import IPython", "from urllib.request import urlopen as uReq", "from urllib.request import Request", "from bs4 import BeautifulSoup as soup", "import keys", "import urllib", "import json", "import sys"]}], [{"term": "class", "name": "TestDataAccess", "data": "class TestDataAccess(unittest.TestCase):\n\n\tdef test_Billboard_webscrape(self):\n\t\t#1. test Billboard webscrape\n\t\tscrape_inst = get_billboard_data()\n\t\tself.assertEqual(len(scrape_inst), 100)\n\n\tdef test_Ticketmaster_Json(self):\n\t\t#2. test Json data retrieval\n\t\tapi_inst = get_events_data('Ed Sheeran')\n\t\tself.assertEqual(len(api_inst['_embedded']['events']), 10)\n\n", "description": null, "category": "webscraping", "imports": ["import unittest", "from proj4 import *"]}, {"term": "class", "name": "TestClass", "data": "class TestClass(unittest.TestCase):\n\n\tdef testConstructor(self):\n\t\t#3. test class constructor\n\t\ta = TopArtists('Cardi B', 1, 6, 1, 39)\n\t\tself.assertEqual(a.artist, 'Cardi B')\n\t\t#4. test class constructor\n\t\tself.assertEqual(a.current, 1)\n\n\n\tdef testString(self):\n\t\t#5. test class string\n\t\ta = TopArtists('Cardi B', 1, 6, 1, 39)\n\t\tself.assertEqual(a.__str__(), 'Cardi B, 1, 6, 1, 39')\n\n", "description": null, "category": "webscraping", "imports": ["import unittest", "from proj4 import *"]}, {"term": "class", "name": "TestDatabase", "data": "class TestDatabase(unittest.TestCase):\n\n\tdef test_artist_table(self):\n\t\tconn = sqlite3.connect(DBNAME)\n\t\tcur = conn.cursor()\n\n\t\tsql = 'SELECT Artist FROM Artists'\n\t\tresults = cur.execute(sql)\n\t\tresult_list = results.fetchall()\n\t\tartist_list=[]\n\t\tfor tuple in result_list:\n\t\t\tartist_list.append(tuple[0])\n\n\t\t#6. test length of Artist Database\n\t\tself.assertEqual(len(result_list), 100)\n\t\t#7. test artist in Database\n\t\tself.assertIn('The Weeknd', artist_list)\n\n\tdef test_events_table(self):\n\t\tconn = sqlite3.connect(DBNAME)\n\t\tcur = conn.cursor()\n\n\t\tsql = 'SELECT EventId FROM Events'\n\t\tresults = cur.execute(sql)\n\t\tresults_list = results.fetchall()\n\n\t\t#8. test length of Events Database\n\t\tself.assertEqual(len(results_list), 720)\n\n\t\tsql = '''\n\t\t\tSELECT Events.Artist, COUNT(Events.EventId)\n\t\t\tFROM Events\n\t\t\tWHERE Events.Artist = 'The Weeknd'\n\t\t'''\n\t\tresults = cur.execute(sql)\n\t\tresults_list = results.fetchall()\n\n\t\t#9. test event sql count in Database\n\t\tself.assertEqual(len(results_list), 1)\n\n\t\tsql = 'SELECT Artist FROM Events'\n\t\tresults = cur.execute(sql)\n\t\tresults_list = results.fetchall()\n\t\tartist_list = []\n\t\tfor tuple in results_list:\n\t\t\tartist_list.append(tuple[0])\n\n\t\t#10. test lookup_events - filter out 2 artists with no concert/no referential integrity\n\t\tself.assertNotIn('Cardi B', artist_list)\n", "description": null, "category": "webscraping", "imports": ["import unittest", "from proj4 import *"]}, {"term": "class", "name": "TestDataProcessing", "data": "class TestDataProcessing(unittest.TestCase):\n\n\tdef test_commands(self):\n\t\tresults = process_command('artists')\n\t\t#11 test process_command('artists') - bar chart\n\t\tself.assertEqual(results[0], '21 Savage')\n\n\tdef test_bar(self):\n\t\t#12 test bar graph\n\t\ttry:\n\t\t\tbar_venue('CA')\n\t\texcept:\n\t\t\tself.fail()\n\n\tdef test_map(self):\n\t\t#13 test map\n\t\ttry:\n\t\t\tmap_concerts(3)\n\t\texcept:\n\t\t\tself.fail()\n\n\t\t#14 test map command, artist with no events\n\t\tself.assertEqual(map_concerts(1), 0)\n\n\tdef test_pie(self):\n\t\t#15 test pie chart\n\t\ttry:\n\t\t\tpie_genre('US')\n\t\texcept:\n\t\t\tself.fail()\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import unittest", "from proj4 import *"]}], [], [{"term": "class", "name": "classWebscrape:", "data": "class Webscrape:\n\tdef __init__(self, price, type, trendIndicator, trendNum, source):\n\t\tself.price = price\n\t\tself.type = type\n\t\tself.trendIndicator = trendIndicator\n\t\tself.trendNum = trendNum\n\t\tself.source = source\n\t\tpass\n\tdef priceBitcoin(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/bitcoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tself.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\treturn self.price\n\tdef typeBitcoin(self):\n\t\turl = \"https://coinmarketcap.com/currencies/bitcoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\treturn coin\n\n\tdef trendIndicatorBitcoin(self):\n\t\turl = \"https://coinmarketcap.com/currencies/bitcoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\treturn trendIndicator\n\tdef trendNumberBitcoin(self):\n\t\turl = \"https://coinmarketcap.com/currencies/bitcoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\treturn trendNumber\n\tdef sourceBitcoin(self):\n\t\turl = \"https://coinmarketcap.com/currencies/bitcoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\treturn source\n\n\tdef priceTrackBitcoin(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/bitcoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tprice = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\t# if not down then list will be empty and it will have to be going up\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\tob = Webscrape(price,coin,trendIndicator,trendNumber,source)\n\t\treturn ob\n\n\n\tdef priceEterium(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/ethereum/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tself.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\treturn self.price\n\tdef typeEterium(self):\n\t\turl = \"https://coinmarketcap.com/currencies/ethereum/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\treturn coin\n\n\tdef trendIndicatorEterium(self):\n\t\turl = \"https://coinmarketcap.com/currencies/ethereum/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\treturn trendIndicator\n\tdef trendNumberEterium(self):\n\t\turl = \"https://coinmarketcap.com/currencies/ethereum/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\treturn trendNumber\n\tdef sourceEterium(self):\n\t\turl = \"https://coinmarketcap.com/currencies/ethereum/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\treturn source\n\tdef priceTrackEthereum(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/ethereum/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tprice = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\t# if not down then list will be empty and it will have to be going up\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\tob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n\t\treturn ob\n\n\tdef priceDodgecoin(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/dogecoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tself.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\treturn self.price\n\tdef typeDodgecoin(self):\n\t\turl = \"https://coinmarketcap.com/currencies/dogecoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\treturn coin\n\n\tdef trendIndicatorDodgecoin(self):\n\t\turl = \"https://coinmarketcap.com/currencies/dogecoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\treturn trendIndicator\n\tdef trendNumberDodgecoin(self):\n\t\turl = \"https://coinmarketcap.com/currencies/dogecoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\treturn trendNumber\n\tdef sourceDodgecoin(self):\n\t\turl = \"https://coinmarketcap.com/currencies/dogecoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\treturn source\n\tdef priceTrackDodgecoin(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/dogecoin/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tprice = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\t# if not down then list will be empty and it will have to be going up\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\tob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n\t\treturn ob\n\n\tdef priceTether(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/tether/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tself.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\treturn self.price\n\tdef typeTether(self):\n\t\turl = \"https://coinmarketcap.com/currencies/tether/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\treturn coin\n\n\tdef trendIndicatorTether(self):\n\t\turl = \"https://coinmarketcap.com/currencies/tether/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\treturn trendIndicator\n\tdef trendNumberTether(self):\n\t\turl = \"https://coinmarketcap.com/currencies/tether/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\treturn trendNumber\n\tdef sourceTether(self):\n\t\turl = \"https://coinmarketcap.com/currencies/tether/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\treturn source\n\tdef priceTrackTether(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/tether/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tprice = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\t# if not down then list will be empty and it will have to be going up\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\tob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n\t\treturn ob\n\n\tdef priceCatGirl(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/catgirl/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tself.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\treturn self.price\n\tdef typeCatGirl(self):\n\t\turl = \"https://coinmarketcap.com/currencies/catgirl/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\treturn coin\n\n\tdef trendIndicatorCatGirl(self):\n\t\turl = \"https://coinmarketcap.com/currencies/catgirl/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\treturn trendIndicator\n\tdef trendNumberCatGirl(self):\n\t\turl = \"https://coinmarketcap.com/currencies/catgirl/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\treturn trendNumber\n\tdef sourceCatGirl(self):\n\t\turl = \"https://coinmarketcap.com/currencies/catgirl/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\treturn source\n\tdef priceTrackCatGirl(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/catgirl/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tprice = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\t#if not down then list will be empty and it will have to be going up\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\tob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n\t\treturn ob\n\n\tdef priceCelsius(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/celsius/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tself.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\treturn self.price\n\tdef typeCelsius(self):\n\t\turl = \"https://coinmarketcap.com/currencies/celsius/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\treturn coin\n\n\tdef trendIndicatorCelsius(self):\n\t\turl = \"https://coinmarketcap.com/currencies/celsius/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\treturn trendIndicator\n\tdef trendNumberCelsius(self):\n\t\turl = \"https://coinmarketcap.com/currencies/celsius/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\treturn trendNumber\n\tdef sourceCelsius(self):\n\t\turl = \"https://coinmarketcap.com/currencies/celsius/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\treturn source\n\tdef priceTrackCelsius(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/celsius/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tprice = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\t#if not down then list will be empty and it will have to be going up\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\tob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n\t\treturn ob\n\n\tdef priceBitbook(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/bitbook/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tself.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\treturn self.price\n\tdef typeBitbook(self):\n\t\turl = \"https://coinmarketcap.com/currencies/bitbook/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\treturn coin\n\n\tdef trendIndicatorBitbook(self):\n\t\turl = \"https://coinmarketcap.com/currencies/bitbook/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\treturn trendIndicator\n\tdef trendNumberBitbook(self):\n\t\turl = \"https://coinmarketcap.com/currencies/bitbook/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\treturn trendNumber\n\tdef sourceBitbook(self):\n\t\turl = \"https://coinmarketcap.com/currencies/bitbook/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\treturn source\n\tdef priceTrackBitbook(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/bitbook/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tprice = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\t#if not down then list will be empty and it will have to be going up\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\tob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n\t\treturn ob\n\n\tdef priceSandbox(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tself.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\treturn self.price\n\tdef typeSandbox(self):\n\t\turl = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\treturn coin\n\n\tdef trendIndicatorSandbox(self):\n\t\turl = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\treturn trendIndicator\n\tdef trendNumberSandbox(self):\n\t\turl = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\treturn trendNumber\n\tdef sourceSandbox(self):\n\t\turl = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\treturn source\n\tdef priceTrackSandbox(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tprice = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\t#if not down then list will be empty and it will have to be going up\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\tob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n\t\treturn ob\n\n\tdef priceM7v2(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/m7v2/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tself.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\treturn self.price\n\tdef typeM7v2(self):\n\t\turl = \"https://coinmarketcap.com/currencies/m7v2/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\treturn coin\n\n\tdef trendIndicatorM7v2(self):\n\t\turl = \"https://coinmarketcap.com/currencies/m7v2/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n\t\treturn trendIndicator\n\tdef trendNumberM7v2(self):\n\t\turl = \"https://coinmarketcap.com/currencies/m7v2/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\treturn trendNumber\n\tdef sourceM7v2(self):\n\t\turl = \"https://coinmarketcap.com/currencies/m7v2/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\treturn source\n\tdef priceTrackM7v2(self):\n\n\t\turl = \"https://coinmarketcap.com/currencies/m7v2/\"\n\t\tresponse = requests.get(url)\n\n\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\t\tprice = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n\t\tsource = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n\t\tcoin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n\t\t#if not down then list will be empty and it will have to be going up\n\t\ttrendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n\t\ttrendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n\t\tob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n\t\treturn ob\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup"]}], [{"term": "class", "name": "pcGamesPages", "data": "class pcGamesPages( WebScrape.WebScrape ):\n\t\n\tdef __init__(self):\n\t\tcontinue\n\t\n\n\t#------------------------------------------------------\n\t#\tHTML EDITIONS\n\t#------------------------------------------------------\n\n\tdef deleteTags( self ):\n\t\t\n\t\tprint(\"Editing -> Deleting Tags... \")\n\n\t\tlistDelete = []\n\n\t\t# Delete scripts tags, analytics and pops BY TEXT inside tag\t\n\t\tlistDelete.append( [self.soup.find(lambda tag:tag.name==\"script\" and \"GoogleAnalyticsObject\" in tag.text)] )\n\t\tlistDelete.append( [self.soup.find(lambda tag:tag.name==\"script\" and \"_pop\" in tag.text)] )\n\n\t\t# Delete  TAG  google fonts CSS\n\t\tlistDelete.append( self.soup.select(\"link[id=baskerville_googleFonts-css]\") )\t\n\n\t\t# Delete Content Layout\n\t\tlistDelete.append( self.soup.findAll(\"h3\", {\"class\": \"blog-description\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"navigation\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"metaslider\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"sidebar\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"post-meta-container\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"comments\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"iframe\", {\"id\": \"mgiframe\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"footer\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"credits\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"yarpp-related\"}) )\t\t\n\t\tlistDelete.append( self.soup.findAll(\"img\", {\"class\": \"avatar\"}) )\n\n\t\t\n\t\t# DELETING TAGS\n\t\tfor listItems in listDelete:\n\t\t\tfor tag in listItems:\n\t\t\t\ttag.decompose()\n\n\n\n\n\t\t\n\n\tdef editNumNav( self ):\n\n\t\tprint(\"Editing -> Changing numbers nav menu links... \")\n\n\t\tnav = self.soup.find(\"div\", {\"class\": \"wp-pagenavi\"})\t\n\t\tbuttonsList = nav.findAll(\"a\")\n\n\t\tfor a in buttonsList:\t\t\t\n\n\t\t\tif not \"page\" in a['href']:\n\t\t\t\taUrl = self.folder + \"page1.html\"\n\t\t\telse:\n\t\t\t\taUrl = \"page\"+a['href'].split(\"/\")[-2] + \".html\"\n\n\t\t\ta['href'] = aUrl\n\n\n\n\tdef editPostUrl( self ):\n\t\t\n\t\tprint(\"Editing -> Changing post games links... \")\n\n\t\tpostList = self.soup.find(\"div\", {\"class\": \"posts\"})\n\t\tbuttonsPosts = postList.findAll(\"a\")\n\n\t\tfor a in buttonsPosts:\n\t\t\taUrl = a['href'][:-1] + \".html\"\n\t\t\ta['href'] = aUrl\n\t\t\n\t\n\n\t#------------------------------------------------------\n\t#\tGetters and Setters\n\t#------------------------------------------------------\n\n\tdef getUrlPosts( self ):\n\t\treturn self.soup.findAll(\"a\", {\"class\": \"more-link\"})\n\n\n\tdef setNumberPage( self, n):\n\t\tself.nPage = n\n\n\t\n\t#------------------------------------------------------\n\t#\tSaving Posts\n\t#------------------------------------------------------\n\n\tdef savePosts( self ):\n\n\t\tappPost = pcGamesPages( urlDomain = \"https://pcgames-download.com/\", folder = \"../../\")\n\n\t\tpagesList = self.getUrlPosts()\n\t\t\n\n\t\tfor gameUrlSrc in pagesList:\n\n\t\t\tgameUrl = gameUrlSrc[\"href\"]\t\t\t\n\t\t\tgameUrlData = gameUrl.replace(\"https://pcgames-download.com/\",\"\").split(\"/\")\n\t\t\t\n\t\t\tfolderPost = gameUrlData[0] + \"/\" + gameUrlData[1] + \"/\"\n\t\t\tnamePost = gameUrlData[2] + \".html\"\n\t\t\t\n\t\t\tprint( \"-\"*30+\"\\n[\"+self.nPage+\"] -> Downloading post: \" + namePost)\n\n\n\t\t\t# Create another instance for post pages\n\t\t\tappPost.getHtmlCode( gameUrl )\n\t\t\tappPost.deleteTags()\n\t\t\tappPost.saveHtml( folderPost, namePost )\t\n\t\t\tappPost.saveImages( )\t\n\n\t\t\tprint( \"-\"*30)\n\n\n", "description": null, "category": "webscraping", "imports": ["from edtools import *", "from edtools.Scrape import WebScrape"]}, {"term": "def", "name": "initCustomPcGames", "data": "def initCustomPcGames( ini, fin, mult, folderPath):\n\n\n\tini = ini\n\tfin = fin\t\n\n\n\tdef pInit():\n\t\tprint(\"\\n\"*2 +\"=\"*80+\"\\n\" + \"\\n STARTED\\n\\n\" + \"=\"*80+ \"\\n\" )\n\t\n\tdef pEnd():\n\t\tprint(\"\\n\"*5 + \"=\"*80+\"\\n\" + \"\\n END\\n\\n\" + \"=\"*80 + \"\\n\")\n\n\tdef pPage(str):\n\t\tprint(\"\\n\"*3+\"=\"*80+\"\\n\" +  \"Working page: \" + str + \"\\n\" + \"=\"*80)\n\n\n\n\tdef customSoup(soup):\n\t\tsoupPosts = soup.find(\"div\", {\"class\": \"posts\"})\n\t\tsoupPosts.clear() # Delete content from sopupTag\n\t\treturn soupPosts\n\n\n\tdef appSinlePage():\n\n\n\t\tapp = pcGamesPages( urlDomain = \"https://pcgames-download.com/\", folder = folderPath )\n\n\n\t\tfor i in range(ini,fin+1):\n\t\t\n\t\t\ti = str(i)\n\t\t\turl = 'https://pcgames-download.com/page/' + i + '/'\n\t\t\tnamePage = \"page\" + i + \".html\"\n\n\t\t\tpPage(namePage)\t\n\n\n\t\t\t#-------------------------------------------------------\n\t\t\t# Scraping Procces\n\t\t\t#-------------------------------------------------------\n\n\t\t\t# Get html code from web\n\t\t\tapp.getHtmlCode( url )\t\t\t\n\n\t\t\t'''\n\t\t\t# Editing html\n\t\t\tapp.deleteTags()\t\t\n\t\t\tapp.editPostUrl()\n\t\t\tapp.editNumNav()\n\n\n\t\t\t# Save html and images\n\t\t\tapp.saveHtml( \"\", namePage)\n\t\t\tapp.saveImages()\n\n\t\t\t'''\n\t\t\t\n\t\t\t# Download all Post per page\n\t\t\t#app.resetSoup() # Reseting soup changes\n\t\t\tapp.setNumberPage(str(i))\t\t\t\n\t\t\tapp.savePosts()\n\n\n\t\t\t#-------------------------------------------------------\n\t\t\t# END Scraping Procces\n\t\t\t#-------------------------------------------------------\n\n\n\n\tdef appMultiplePageBy( mult ):\n\n\t\tmult = mult\n\t\t\n\t\tbigList = []\n\n\t\tapp = pcGamesPages( urlDomain = \"https://pcgames-download.com/\", folder = folderPath )\n\n\t\tfor i in range(ini,fin+1):\n\t\t\n\t\t\ti = str(i)\n\t\t\turl = 'https://pcgames-download.com/page/' + i + '/'\n\t\t\tnamePage = \"page\" + i + \".html\"\n\n\t\t\tpPage(namePage)\t\n\n\n\t\t\t\n\n\t\t\t#-------------------------------------------------------\n\t\t\t# Scraping Procces\n\t\t\t#-------------------------------------------------------\n\n\t\t\t# Get html code from web\n\t\t\tapp.getHtmlCode( url )\n\n\t\t\t# save images\n\t\t\tapp.saveImages()\n\n\t\t\t#-------------------------------------------------------\n\t\t\t# END Scraping Procces\n\t\t\t#-------------------------------------------------------\n\n\n\t\t\tbigList.append( app.getSoup().findAll(\"div\", {\"class\": \"post-container\"}) )\n\n\n\t\t\t# Multiplos de 10\n\t\t\tif (int(i)%mult) == 0 or int(i) == fin:\n\n\t\t\t\tprint(\"Editing html data...\")\n\n\t\t\t\tnamePage = \"big\" + i + \".html\"\n\t\t\t\tsoupPosts = app.getSoup().find(\"div\", {\"class\": \"posts\"})\n\t\t\t\tnav = app.getSoup().find(\"div\", {\"class\": \"wp-pagenavi\"})\n\n\t\t\t\tsoupPosts.clear() # Delete content from sopupTag\n\n\n\t\t\t\tfor itemBiglist in bigList:\n\t\t\t\t\tfor itemPageBotton in itemBiglist:\n\t\t\t\t\t\tsoupPosts.append(itemPageBotton)\n\n\n\n\t\t\t\tnav.clear() # Delete content from nav\n\n\t\t\t\tfor j in range(ini,fin+1):\n\n\t\t\t\t\tj = str(j)\n\n\t\t\t\t\tif (int(j)%mult) == 0 or int(j) == fin:\n\t\t\t\t\t\tif j == i:\n\t\t\t\t\t\t\tnav.append( BeautifulSoup(''+j+'', 'html.parser') )\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tnav.append( BeautifulSoup(''+j+'', 'html.parser') )\n\n\t\t\t\t\n\n\t\t\t\t# Editing html\n\t\t\t\tapp.deleteTags()\t\t\n\t\t\t\tapp.editPostUrl()\t\t\t\t\n\n\t\t\t\t# Save html and images\n\t\t\t\tapp.saveHtml( \"\", namePage)\n\n\t\t\t\t\n\t\t\t\tbigList = []\n\n\t\t\t\t\t\n\n\n\t\n\n\n\n\tpInit()\n\n\tappSinlePage()\n\t#appMultiplePageBy(mult)\n\n\tpEnd()\n\n\n", "description": null, "category": "webscraping", "imports": ["from edtools import *", "from edtools.Scrape import WebScrape"]}], [{"term": "def", "name": "get_child", "data": "def get_child(html, pos):\n\treturn [i for i in html[pos].children][0]\n", "description": null, "category": "webscraping", "imports": ["from ipaddress import ip_address", "from itertools import count", "import json", "from re import T", "import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from datetime import date", "import gspread", "import logging", "import yaml", "import boto3", "import os", "import pandas as pd", "import io"]}, {"term": "def", "name": "extract_books", "data": "def extract_books(page):\n\n\tbook_list = page.find_all('span', {'class' : 'zg-bdg-text'})\n\n\tassert len(book_list) > 0, 'no books found'\n\tbooks = {}\n\n\tfor book_ranking in book_list:\n\t\t\n\t\tbook_rank_scrape = [i for i in book_ranking][0]\n\n\t\tbook = {\n\t\t\t'book_rank_scrape' : book_rank_scrape\n\t\t}\n\n\t\tbook_box = book_ranking.parent.parent.parent\n\n\n\t\tbook_info = [i for i in [i for i in [i for i in book_box.children][1]][0]]\n\n\t\tif len(book_info) != 6:\n\t\t\tif len(book_info) == 5:\n\t\t\t\t# gonna guess that 5 means no star rating TODO improve later\n\n\t\t\t\timg = book_info[0]\n\t\t\t\tbook_title = book_info[1]\n\t\t\t\tauthor = book_info[2]\n\t\t\t\tbook_type = book_info[3]\n\t\t\t\tprice = book_info[4]\n\n\t\t\t\t# book title\n\t\t\t\tbook['book_title_txt'] = [i for i in [i for i in [i for i in book_title.children][0].children][0].children][0]\n\t\t\t\tbook['author_txt'] = [i for i in [i for i in [i for i in author][0]][0]][0]\n\t\t\t\tbook['star_rating_txt' ] = 'unknown'\n\t\t\t\tbook['book_type_txt'] = [i for i in [i for i in book_type][0]][0]\n\t\t\t\tbook['price_txt'] = [i for i in [i for i in [i for i in [i for i in price][0]][0]][0]][0]\n\n\t\t\telse:\n\t\t\t\t# no idea jk :P but dict always needs 5 keys\n\t\t\t\tbook['book_title_txt'] = 'unknown'\n\t\t\t\tbook['author_txt'] = 'unknown'\n\t\t\t\tbook['star_rating_txt' ] = 'unknown'\n\t\t\t\tbook['book_type_txt'] = 'unknown'\n\t\t\t\tbook['price_txt'] = 'unknown'\n\t\telse:\n\t\t\t# expected 6 catorgories\n\t\t\timg = book_info[0]\n\t\t\tbook_title = book_info[1]\n\t\t\tauthor = book_info[2]\n\t\t\tstar_rating = book_info[3]\n\t\t\tbook_type = book_info[4]\n\t\t\tprice = book_info[5]\n\n\t\t\t# book title\n\t\t\tbook['book_title_txt'] = [i for i in [i for i in [i for i in book_title.children][0].children][0].children][0]\n\t\t\tbook['author_txt'] = [i for i in [i for i in [i for i in author][0]][0]][0]\n\t\t\tbook['star_rating_txt' ]= [i for i in [i for i in [i for i in [i for i in [i for i in star_rating][0]][0]][0]][0]][0]\n\t\t\tbook['book_type_txt'] = [i for i in [i for i in book_type][0]][0]\n\t\t\tbook['price_txt'] = [i for i in [i for i in [i for i in [i for i in price][0]][0]][0]][0]\n\n\t\tbooks[book_rank_scrape] = book\n\n\treturn books\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from ipaddress import ip_address", "from itertools import count", "import json", "from re import T", "import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from datetime import date", "import gspread", "import logging", "import yaml", "import boto3", "import os", "import pandas as pd", "import io"]}, {"term": "def", "name": "write_gs", "data": "def write_gs(df):\n\n\tgc = gspread.service_account(\"webscrape-346716-e7082b6f73c5.json\")\n\n\tsh = gc.open(\"Amazon Data\")\n\n\tsheetName = date.today().strftime(\"%d_%m_%Y\") \n\n\ttry:\n\t\tsh.add_worksheet(sheetName, rows = 20, cols = 10)\n\texcept:\n\t\tsheetName = sheetName + '_1'\n\t\tsh.add_worksheet(sheetName, rows = 20, cols = 10)\n\n\tlogging.info(f'Created excel sheetnamer was: {sheetName}')\n\n\tworksheet = sh.worksheet(sheetName)\n\n\tworksheet.update([df.columns.values.tolist()] + df.values.tolist())\n\n", "description": null, "category": "webscraping", "imports": ["from ipaddress import ip_address", "from itertools import count", "import json", "from re import T", "import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from datetime import date", "import gspread", "import logging", "import yaml", "import boto3", "import os", "import pandas as pd", "import io"]}, {"term": "def", "name": "get_webpage", "data": "def get_webpage(url):\n\n\tapi_key =  os.getenv('proxy_api')\n\tassert api_key is not None, \"envrioment var proxy_api not found\"\n\n\tapi_url = \"https://api.webscrapingapi.com/v1\"\n\n\tparams = {\n\t\t\"api_key\": api_key['api_key'],\n\t\t\"url\": url\n\t}\n\n\tsuccess_status = False\n\tattept = 1\n\tmax_attepts = 10\n\n\twhile success_status is False and attept < max_attepts:\n\t\t\n\t\ttry:\n\t\t\tresponse = requests.request(\"GET\", api_url, params=params)\n\t\t\tlogging.info(f'response status {response.status_code}')\n\n\t\t\tif response.status_code == 401:\n\t\t\t\traise Exception(\"401 error\")\n\n\t\t\tif response.status_code == 422:\n\t\t\t\traise Exception(\"422 error\")\n\n\t\t\tsuccess_status = True\n\t\texcept:\n\t\t\tattept += 1\n\t\t\tpass\n\n\tpage = BeautifulSoup(response.content, features= \"html.parser\")\n\n\treturn page\n\n\n", "description": null, "category": "webscraping", "imports": ["from ipaddress import ip_address", "from itertools import count", "import json", "from re import T", "import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from datetime import date", "import gspread", "import logging", "import yaml", "import boto3", "import os", "import pandas as pd", "import io"]}, {"term": "def", "name": "load_all_data", "data": "def load_all_data():\n\n\tgc = gspread.service_account(\"webscrape-346716-e7082b6f73c5.json\")\n\n\tsh = gc.open(\"Amazon Data\")\n\n\tworksheet_list = sh.worksheets()\n\n\tsheet_dfs = {}\n\n\tfor sheet in worksheet_list:\n\t\tdataframe = pd.DataFrame(sheet.get_all_records())\n\t\tdataframe['date'] = sheet.title\n\t\tsheet_dfs[sheet.title] = dataframe\n\n\n\tall_data =  pd.concat(sheet_dfs.values(), ignore_index=True)\n\treturn all_data\n\n", "description": null, "category": "webscraping", "imports": ["from ipaddress import ip_address", "from itertools import count", "import json", "from re import T", "import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from datetime import date", "import gspread", "import logging", "import yaml", "import boto3", "import os", "import pandas as pd", "import io"]}, {"term": "def", "name": "write_books_s3", "data": "def write_books_s3(book_dict, filename):\n\t\n\n\taws_access_key = os.environ.get('access_key')\n\taws_secret_access = os.environ.get('secret')\n\n\tif aws_access_key is None or aws_secret_access is None:\n\t\traise Exception(\"Missing envrioment variables to access s3- access_key and secret\")\n\n\ts3 = boto3.client(\n\t\tservice_name = 's3',\n\t\tregion_name = 'eu-west-2',\n\t\taws_access_key_id = aws_access_key,\n\t\taws_secret_access_key = aws_secret_access\n\t)\n\n\ts3_bucket_name = 'books-webscrape'\n\n\tresponse = s3.put_object(\n\t\tBody = json.dumps(book_dict),\n\t\tBucket = s3_bucket_name,\n\t\tKey = filename\n\t)\n\n\tstatus = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n\n\tif status == 200:\n\t\tlogging.info(f\"Successful S3 put_object response. Status - {status}\")\n\telse:\n\t\tlogging.info(f\"Unsuccessful S3 put_object response. Status - {status}\")\n\n", "description": null, "category": "webscraping", "imports": ["from ipaddress import ip_address", "from itertools import count", "import json", "from re import T", "import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from datetime import date", "import gspread", "import logging", "import yaml", "import boto3", "import os", "import pandas as pd", "import io"]}, {"term": "def", "name": "convert_to_pd", "data": "def convert_to_pd(book_pages: Dict, date: chr):\n\n\tbooks_dfs = {}\n\n\tfor cato in book_pages.keys():\n\n\t\tcato_df = pd.DataFrame.from_dict(book_pages[cato], orient='index')\n\t\tcato_df['catagory'] = cato\n\n\t\tbooks_dfs[cato] = cato_df\n\n\tday_df = pd.concat(books_dfs.values(), ignore_index=True)\n\tday_df['date'] = date\n\t\n\treturn day_df\n", "description": null, "category": "webscraping", "imports": ["from ipaddress import ip_address", "from itertools import count", "import json", "from re import T", "import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from datetime import date", "import gspread", "import logging", "import yaml", "import boto3", "import os", "import pandas as pd", "import io"]}, {"term": "def", "name": "write_books_s3", "data": "def write_books_s3(book_pages_df, today):\n\n\taws_access_key = os.getenv('access_key')\n\taws_secret_access = os.getenv('secret')\n\n\tif aws_access_key is None or aws_secret_access is None:\n\t\traise Exception(\"Missing envrioment variables to access s3- access_key and secret\")\n\n\ts3 = boto3.client(\n\t\tservice_name = 's3',\n\t\tregion_name = 'eu-west-2',\n\t\taws_access_key_id = aws_access_key,\n\t\taws_secret_access_key = aws_secret_access\n\t)\n\n\ts3_bucket_name = 'books-webscrape'\n\n\tresponse = s3.get_object(Bucket=s3_bucket_name, Key=\"books.csv\")\n\n\tstatus = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n\n\tif status == 200:\n\t\tlogging.info(f\"Successful S3 get_object response. Status - {status}\")\n\t\tbooks_df = pd.read_csv(response.get(\"Body\"))\n\telse:\n\t\tlogging.info(f\"Unsuccessful S3 get_object response. Status - {status}\")\n\n\n\tif today in books_df.date.unique():\n\t\tlogging.info(\"Data already exists, not saving\")\n\t\treturn\n\telse:\n\t\tlogging.info(\"Data doesn't exists, adding to total\")\n\t\tcombined_data = pd.concat([books_df, book_pages_df])\n\t\t\n\t\t\n\twith io.StringIO() as csv_buffer:\n\t\tcombined_data.to_csv(csv_buffer, index=False)\n\n\t\tresponse = s3.put_object(\n\t\t\tBucket=s3_bucket_name, Key=\"books.csv\", Body=csv_buffer.getvalue()\n\t\t)\n\n\t\tstatus = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n\n\t\tif status == 200:\n\t\t\tlogging.info(f\"Successful S3 put_object response. Status - {status}\")\n\t\telse:\n\t\t\tlogging.info(f\"Unsuccessful S3 put_object response. Status - {status}\")\n", "description": null, "category": "webscraping", "imports": ["from ipaddress import ip_address", "from itertools import count", "import json", "from re import T", "import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from datetime import date", "import gspread", "import logging", "import yaml", "import boto3", "import os", "import pandas as pd", "import io"]}], [], [{"term": "def", "name": "addTextToJson", "data": "def addTextToJson(jsonElement, textJsonName, imagePath):\n\tim = Image.open(str(imagePath))\n\tcontent = pt.image_to_string(im, lang=\"nld\")\n\tjsonElement[str(textJsonName)] = content\n\tprint(\"json text added from \" + imagePath)\n\n", "description": null, "category": "webscraping", "imports": ["from PIL import Image", "import pytesseract as pt", "import os", "import glob", "import json"]}, {"term": "def", "name": "addCategoriesToJson", "data": "def addCategoriesToJson(jsonElement, textJsonName, imagePath):\n\ttry:\n\t\tim = Image.open(str(imagePath))\n\t\tcontent = pt.image_to_string(im, lang=\"nld\")\n\t\t# print(content)\n\texcept KeyError:\n\t\tprint(\"No linkedIn screenshot\")\n\n", "description": null, "category": "webscraping", "imports": ["from PIL import Image", "import pytesseract as pt", "import os", "import glob", "import json"]}], [{"term": "def", "name": "check4Urgent", "data": "def check4Urgent(msg):\n\ttry:\n\t\treturn msg.find(\"img\").attrs['alt'] == \"High Priority\"\n\texcept:\n\t\treturn False\n", "description": null, "category": "webscraping", "imports": ["import os,re", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "check4Movie", "data": "def check4Movie(msg):\n\ttext = \"\"\n\ttry:\n\t\ttext = str(msg.find(\"span\",attrs={'class':\"Fragment\"}))\n\texcept:\n\t\treturn False\n\tmatches = re.search(r\"[M|m]ovie[s]?\", text, re.MULTILINE)\n\tif matches != None: return True\n\treturn False\n", "description": null, "category": "webscraping", "imports": ["import os,re", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "webscrape", "data": "def webscrape():\n\twith requests.Session() as s:\n\t\turl = \"https://mail.nitrkl.ac.in/\"\n\t\ttry:\n\t\t\tr= s.get(url,headers=headers)\n\t\texcept:\n\t\t\treturn _ERROR\n\t\tsoup = BeautifulSoup(r.content,features=\"html.parser\")\n\t\tlogin_data[\"login_csrf\"] = soup.find('input',attrs={'name':'login_csrf'})['value']\n\t\t\n\t\tr= s.post(url,data=login_data,headers=headers)\n\t\tsoup = BeautifulSoup(r.content)\n\t\tinbox = soup.find(\"tbody\", {\"id\": \"mess_list_tbody\"})\n\t\tmessage_list =list(inbox.find_all(\"tr\",{\"class\":\"Unread\"}))\n", "description": null, "category": "webscraping", "imports": ["import os,re", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "mailDetails", "data": "def mailDetails():\n\tdetail = dict()\n\n\tmgs = webscrape()\n\n\tif mgs == _ERROR:\n\t\tdetail[\"error\"] = ER_WEBPAGE_UNAVAILABLE\n\t\treturn detail\n\n\turgent = len(list(filter(check4Urgent,mgs)))\n\tmovie  = len(list(filter(check4Movie,mgs)))\n\t\n\tdetail[\"mails\"]  = len(mgs)\n\tdetail[\"urgent\"] = urgent\n\tdetail[\"movie\"]  = movie\n\n\treturn detail\n", "description": null, "category": "webscraping", "imports": ["import os,re", "import requests", "from bs4 import BeautifulSoup"]}], [{"term": "class", "name": "database", "data": "class database(object):\n\t\"\"\"Handles all database connectsion, inputs, and outputs\n\t\n\tClass constructor initiates sqlite3 database connection. If used in WITH statement\n\tthe connection will cleanly close after the statement is finished. If there are\n\tuncommitted transactions they will be rolled back prior to connection closure.\n\t\n\t\"\"\"\n\tdef __init__(self, test_database=None):\n\t\tif test_database:\n\t\t\tself.__db_connection = test_database\n\t\t\tself.cur = self.__db_connection.cursor()\n\t\t# elif os.path.exists(Path(os.environ['WEBSCRAPE_DB']) / 'webscraper.db'): #os.environ['WEBSCRAPE_DB']:\n\t\t#\t print(os.path.exists(Path(os.environ['WEBSCRAPE_DB']) / 'webscraper.db'))\n\t\t#\t print(os.path.exists(prod_database / 'webscraper.db'))\n\t\t#\t __DB_LOCATION = prod_database / 'webscraper.db'\n\t\t#\t print(__DB_LOCATION)\n\n\t\telse:\n\t\t\t__DB_LOCATION = (\n\t\t\t\tPath.home() \n\t\t\t\t/ \"py_apps\" \n\t\t\t\t/ \"_appdata\" \n\t\t\t\t/ \"webscraper\"\n\t\t\t\t/ \"webscraper.db\"\n\t\t\t)\n\t\t\tif os.path.exists(__DB_LOCATION):\n\t\t\t\tself.__db_connection = sqlite3.connect(str(__DB_LOCATION))\n\t\t\t\tself.cur = self.__db_connection.cursor()\n\t\t\telse:\n\t\t\t\tPath(\n\t\t\t\t\tPath.home() / \"py_apps\" / \"_appdata\" / \"webscraper\"\n\t\t\t\t).mkdir(parents=True, exist_ok=True)\n\t\t\t\tself.__db_connection = sqlite3.connect(str(__DB_LOCATION))\n\t\t\t\tself.cur = self.__db_connection.cursor()\n\n\tdef __del__(self):\n\t\tself.__db_connection.close()\n\n\tdef __enter__(self):\n\t\treturn self\n\n\tdef __exit__(self, ext_type, exc_value, traceback):\n\t\tself.cur.close()\n\t\tif isinstance(exc_value, Exception):\n\t\t\tself.__db_connection.rollback()\n\t\telse:\n\t\t\tself.__db_connection.commit()\n\t\tself.__db_connection.close()\n\t\n\n\n\tdef query_table_ids_all(self, table: str, where_column=None, table_id=None, last_date_ascending=False) -> list:\n\t\tid_list = []\n\t\tif table_id == None:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT id FROM {table} \n\t\t\t\t\t\t\t\t\t\t  ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\").fetchall()\n\t\telse:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT id FROM {table} \n\t\t\t\t\t\t\t\t\t\t  WHERE {where_column} = {table_id} \n\t\t\t\t\t\t\t\t\t\t  ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\").fetchall()\n\t\t# print(f\"\"\"SELECT id FROM {table} \n\t\t#\t\t\t\t\t\t\t\t   WHERE {where_column} = {table_id} \n\t\t#\t\t\t\t\t\t\t\t   ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\")\n\t\t\n\t\tfor i in result:\n\t\t\tid_list.append(i[0])\n\t\treturn id_list\n\n\n\tdef query_websites(self, name=None, url=None, website_id=None):\n\t\tif name:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE name = '{name}'\"\"\").fetchone()\n\t\telif url:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE url = '{url}'\"\"\").fetchone()\n\t\telif website_id:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE id = '{website_id}'\"\"\").fetchone()\n\t\telse:\n\t\t\treturn 'query_websites() requires name or url'\n\t\tif result:\n\t\t\treturn table.website(id\t\t  = result[0], # id\n\t\t\t\t\t\t   name\t\t= result[1], # name\n\t\t\t\t\t\t   url\t\t = result[2], # url\n\t\t\t\t\t\t   last_date   = result[3], # last_date\n\t\t\t\t\t\t   last_user   = result[4], # last_user\n\t\t\t\t\t\t   create_date = result[5], # create_date\n\t\t\t\t\t\t   create_user = result[6]) # create_user\n\t\telse:\n\t\t\treturn None\n\n\n\tdef query_categories(self, name=None, url=None, category_id=None):\n\t\tif category_id:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE id = '{category_id}'\"\"\").fetchone()\n\t\telif name:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE name = '{name}'\"\"\").fetchone()\n\t\telif url:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE url = '{url}'\"\"\").fetchone()\n\t\tif result:\n\t\t\treturn table.category(id\t\t  = result[0], # id\n\t\t\t\t\t\t\tname\t\t= result[1], # name\n\t\t\t\t\t\t\tcontext\t = result[2], # context\n\t\t\t\t\t\t\turl\t\t = result[3], # url\n\t\t\t\t\t\t\twebsite_id  = result[4], # website_id\n\t\t\t\t\t\t\tlast_date   = result[5], # last_date\n\t\t\t\t\t\t\tlast_user   = result[6], # last_user\n\t\t\t\t\t\t\tcreate_date = result[7], # create_date\n\t\t\t\t\t\t\tcreate_user = result[8]) # create_user)\n\t\t\t#return result\n\t\telse:\n\t\t\treturn None\n\n\t\n\tdef query_blogs(self, name=None, url=None, blog_id=None) -> object:   # blog object\n\t\tif blog_id:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE id = '{blog_id}'\"\"\").fetchone()\n\t\telif name:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE name = '{name}'\"\"\").fetchone()\n\t\telif name:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE url = '{url}'\"\"\").fetchone()\n\t\tif result:\n\t\t\treturn table.blog(id\t\t  = result[0], # id\n\t\t\t\t\t\tauthor\t  = result[1], # name\n\t\t\t\t\t\tname\t\t= result[2], # context\n\t\t\t\t\t\turl\t\t = result[3], # url\n\t\t\t\t\t\tcategory_id = result[4], # website_id\n\t\t\t\t\t\tlast_date   = result[5], # last_date\n\t\t\t\t\t\tlast_user   = result[6], # last_user\n\t\t\t\t\t\tcreate_date = result[7], # create_date\n\t\t\t\t\t\tcreate_user = result[8]) # create_user)\n\t\telse:\n\t\t\treturn None\n\n\t\n\tdef execute(self, new_data: str) -> tuple:\n\t\t\"\"\"Executes an valid SQL statement passed through as a string.\n\n\t\tArugments:\n\t\t\tnew_data (string): Valid SQL statement\n\n\t\t\"\"\"\n\t\treturn self.cur.execute(new_data).fetchall()\n\t\n\n\tdef insert_update_site_pages(self, page_number: int, site_id: int):\n\t\t\"\"\"Inserts a website record. Designed for use with the website class.\n\n\t\tArguments:\n\t\t\twebsite (website class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\t# existing_url = self.insert_update_website(url=website.url)\n\t\t# new = True if existing_url == None else False\n\t\ttoday = date.today()\n\t\ttry:\n\t\t\tresult = self.cur.execute(f\"SELECT number FROM site_pages WHERE site_id = {site_id}\").fetchall()\n\t\t\t#print(result)\n\t\t\tif result[0]:\n\t\t\t\tself.cur.execute(\n\t\t\t\t\tf\"\"\"UPDATE site_pages\n\t\t\t\t\t\tSET number = '{page_number}',\n\t\t\t\t\t\t\tlast_date = '{today}',\n\t\t\t\t\t\t\tlast_user = 'user'\n\t\t\t\t\t\tWHERE site_id = '{site_id}'\n\t\t\t\t\t\"\"\"\n\t\t\t\t)\n\t\t\t#print('updated or same')\n\t\texcept IndexError:\n\t\t\tnext_id = self.cur.execute(\"\"\"SELECT MAX(id) FROM site_pages\"\"\").fetchone()[0]\n\t\t\tnext_id = next_id + 1 if next_id else 1\n\t\t\tself.cur.execute(\n\t\t\t\tf\"\"\"INSERT INTO site_pages\n\t\t\t\t\t\t\t\tVALUES (\n\t\t\t\t\t\t\t\t\t\tNULL,\n\t\t\t\t\t\t\t\t\t\t\"{page_number}\",\n\t\t\t\t\t\t\t\t\t\t\"{site_id}\",\n\t\t\t\t\t\t\t\t\t\t\"{today}\",\n\t\t\t\t\t\t\t\t\t\t\"user\",\n\t\t\t\t\t\t\t\t\t\t\"{today}\",\n\t\t\t\t\t\t\t\t\t\t\"user\"\n\t\t\t\t\t\t\t)\"\"\"\n\t\t\t).fetchall()\n\t\t\t#print('new')\n\t\t# else:\n\t\t#\t self.cur.execute(\n\t\t#\t\t f\"\"\"UPDATE websites\n\t\t#\t\t\t SET number = '{page_number}',\n\t\t#\t\t\t\t last_date = '{today}',\n\t\t#\t\t\t\t last_user = 'user'\n\t\t#\t\t\t WHERE website_id = '{website_id}'\n\t\t#\t\t \"\"\"\n\t\t#\t ).fetchall()\n\t\t#\t return self.query_websites(url=website.url)\n\t\t# else:\n\t\t#\t return f'Not inserted. If exists, then {existing_url.url} is already present, named {existing_url.name}.'\n\n\n\tdef insert_website(self, website: object) -> object:\n\t\t\"\"\"Inserts a website record. Designed for use with the website class.\n\n\t\tArguments:\n\t\t\twebsite (website class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\texisting_url = self.query_websites(url=website.url)\n\t\tnew = True if existing_url == None else False\n\t\tif new == True:\n\t\t\tnext_id = self.cur.execute(\"\"\"SELECT MAX(id) FROM websites\"\"\").fetchone()[0]\n\t\t\tif next_id:\n\t\t\t\tnext_id = next_id + 1 if next_id else 1\n\t\t\ttoday = date.today()\n\t\t\tself.cur.execute(\n\t\t\t\tf\"\"\"INSERT INTO websites\n\t\t\t\t\t\t\t\tVALUES (\n\t\t\t\t\t\t\t\t\t\tNULL,\n\t\t\t\t\t\t\t\t\t\t\"{website.name}\",\n\t\t\t\t\t\t\t\t\t\t\"{website.url}\",\n\t\t\t\t\t\t\t\t\t\t\"{today}\",\n\t\t\t\t\t\t\t\t\t\t\"user\",\n\t\t\t\t\t\t\t\t\t\t\"{today}\",\n\t\t\t\t\t\t\t\t\t\t\"user\"\n\t\t\t\t\t\t\t)\"\"\"\n\t\t\t).fetchall()\n\t\t\treturn self.query_websites(url=website.url)\n\t\telse:\n\t\t\treturn f'Not inserted. If exists, then {existing_url.url} is already present, named {existing_url.name}.'\n\n\t\n\tdef insert_category(self, category):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tcategory.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM categories\"\"\").fetchone()[0]\n\t\tcategory.id = category.id + 1 if category.id else 1\n\t\treturn self.cur.execute(\n\t\t\tf\"\"\"INSERT INTO categories\n\t\t\t\t\t\t\t VALUES (\n\t\t\t\t\t\t\t\t\t\t\"{category.id}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.name}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.context}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.url}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.website_id}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.last_date}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.last_user}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.create_date}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.create_user}\"\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\n\t\n\tdef insert_blog(self, blog):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tblog.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM blogs\"\"\").fetchone()[0]\n\t\tblog.id = blog.id + 1 if blog.id else 1\n\t\tblog.name = blog.name.replace('\"', '')\n\t\treturn self.cur.execute(\n\t\t\tf\"\"\"INSERT INTO blogs VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n\t\t\t\t\t\t\t\t\t   (blog.id,\n\t\t\t\t\t\t\t\t\t\tblog.author,\n\t\t\t\t\t\t\t\t\t\tblog.name,\n\t\t\t\t\t\t\t\t\t\tblog.url,\n\t\t\t\t\t\t\t\t\t\tblog.category_id,\n\t\t\t\t\t\t\t\t\t\tblog.last_date,\n\t\t\t\t\t\t\t\t\t\tblog.last_user,\n\t\t\t\t\t\t\t\t\t\tblog.create_date,\n\t\t\t\t\t\t\t\t\t\tblog.create_user)\n\t\t)\n\n\n\tdef update_date_blog(self, blog):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tlast_date = datetime.now()\n\t\treturn self.cur.execute(\n\t\t\tf\"\"\"UPDATE blogs\n\t\t\t\tSET last_date = '{last_date}'\n\t\t\t\tWHERE id = {blog.id}\"\"\"\n\t\t)\n\n\t\n\tdef update_date_category(self, category):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tlast_date = datetime.now()\n\t\treturn self.cur.execute(\n\t\t\tf\"\"\"UPDATE categories\n\t\t\t\tSET last_date = '{last_date}'\n\t\t\t\tWHERE id = {category.id}\"\"\"\n\t\t)\n\n\t\t\n\tdef insert_post(self, post):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tpost.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM posts\"\"\").fetchone()[0]\n\t\tpost.id = post.id + 1 if post.id else 1\n\t\tpost.content = post.content.replace('\"', '\\\"')\n\t\tpost.content = post.content.replace(\"'\", \"\\'\")\n\t\tpost.tags = ','.join(post.tags)\n\t\tself.cur.execute(\n\t\t\t \"\"\"INSERT INTO posts\n\t\t\t\t\t\t\t\tVALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n\t\t\t\t\t\t\t\t\t   (post.id,\n\t\t\t\t\t\t\t\t\t\tpost.title,\n\t\t\t\t\t\t\t\t\t\tpost.author,\n\t\t\t\t\t\t\t\t\t\tpost.date,\n\t\t\t\t\t\t\t\t\t\tpost.tags,\n\t\t\t\t\t\t\t\t\t\tpost.content,\n\t\t\t\t\t\t\t\t\t\tpost.content_html,\n\t\t\t\t\t\t\t\t\t\tpost.url,\n\t\t\t\t\t\t\t\t\t\tpost.blog_id,\n\t\t\t\t\t\t\t\t\t\tpost.last_date,\n\t\t\t\t\t\t\t\t\t\tpost.last_user,\n\t\t\t\t\t\t\t\t\t\tpost.create_date,\n\t\t\t\t\t\t\t\t\t\tpost.create_user)\n\t\t)\n\n\n\tdef insert_error_url(self, url_error: object):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\turl_error.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM url_error_list\"\"\").fetchone()[0]\n\t\turl_error.id = url_error.id + 1 if url_error.id else 1\n\t\tself.cur.execute(\n\t\t\t\"\"\"INSERT INTO url_error_list\n\t\t\t\t\t\t\t\tVALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n\t\t\t\t\t\t\t\t\t\t(url_error.id,\n\t\t\t\t\t\t\t\t\t\turl_error.url,\n\t\t\t\t\t\t\t\t\t\turl_error.url_type,\n\t\t\t\t\t\t\t\t\t\turl_error.parent_id,\n\t\t\t\t\t\t\t\t\t\turl_error.resolved,\n\t\t\t\t\t\t\t\t\t\turl_error.last_date,\n\t\t\t\t\t\t\t\t\t\turl_error.last_user,\n\t\t\t\t\t\t\t\t\t\turl_error.create_date,\n\t\t\t\t\t\t\t\t\t\turl_error.create_user)\n\t\t)\n\n\t\n\tdef create_tables(self):\n\t\t\"\"\"This function confirms the existence of or creates the path, database, and tables.\n\t\t\n\t\tCan be used by calling the function directly, but is designed to by used by install.py, which is called by the install.bat file.\n\t\t\n\t\t\"\"\"\n\t\tif (\n\t\t\tPath.home() / \"py_apps\" / \"_appdata\" / \"webscrape_patheos\" / \"patheos.db\"\n\t\t):\n\t\t\tpass\n\t\telse:\n\t\t\tPath(Path.home() / \"py_apps\" / \"_appdata\" / \"webscrape_patheos\" / \"patheos.db\").mkdir(\n\t\t\t\tparents=True, exist_ok=True\n\t\t\t)\n\n\t\t\"\"\"create a database table if it does not exist already\"\"\"\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS \n\t\t\t\t\t\t\tsite_pages (\n\t\t\t\t\t\t\t\tid\t\t   INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n\t\t\t\t\t\t\t\tnumber\t   INTEGER NOT NULL,\n\t\t\t\t\t\t\t\tsite_id\t  INTEGER NOT NULL UNIQUE,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS \n\t\t\t\t\t\t\turl_error_list (\n\t\t\t\t\t\t\t\tid\t\t   INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n\t\t\t\t\t\t\t\turl\t\t  TEXT NOT NULL,\n\t\t\t\t\t\t\t\turl_type\t TEXT NOT NULL,\n\t\t\t\t\t\t\t\tparent_id\tINTEGER NOT NULL,\n\t\t\t\t\t\t\t\tresolved\t TEXT NOT NULL,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS \n\t\t\t\t\t\t\twebsites (\n\t\t\t\t\t\t\t\tid\t\t   INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n\t\t\t\t\t\t\t\tname\t\t VARCHAR(100) NOT NULL, \n\t\t\t\t\t\t\t\turl\t\t  VARCHAR(2000) NOT NULL,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS \n\t\t\t\t\t\t\tcategories (\n\t\t\t\t\t\t\t\tid\t\t   INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n\t\t\t\t\t\t\t\tname\t\t VARCHAR(100) NOT NULL, \n\t\t\t\t\t\t\t\tcontext\t  VARCHAR(100), \n\t\t\t\t\t\t\t\turl\t\t  VARCHAR(2000) NOT NULL,\n\t\t\t\t\t\t\t\twebsite_id   INTEGER NOT NULL,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS\n\t\t\t\t\t\t\tblogs (\n\t\t\t\t\t\t\t\tid\t\t   INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n\t\t\t\t\t\t\t\tauther\t   VARCHAR(255),\n\t\t\t\t\t\t\t\tname\t\t VARCHAR(255), \n\t\t\t\t\t\t\t\turl\t\t  TEXT NOT NULL,\n\t\t\t\t\t\t\t\tcategory_id  INTEGER NOT NULL, \n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS\n\t\t\t\t\t\t\tposts (\n\t\t\t\t\t\t\t\tid\t\t   INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n\t\t\t\t\t\t\t\ttitle\t\tVARCHAR(255) NOT NULL,\n\t\t\t\t\t\t\t\tauthor\t   VARCHAR(255),\n\t\t\t\t\t\t\t\tdate\t\t TIMESTAMP, \n\t\t\t\t\t\t\t\ttags\t\t VARCHAR(255), \n\t\t\t\t\t\t\t\tcontent\t  TEXT, \n\t\t\t\t\t\t\t\tcontent_html TEXT,\n\t\t\t\t\t\t\t\turl\t\t  TEXT NOT NULL,\n\t\t\t\t\t\t\t\tblog_id\t  INTEGER NOT NULL,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t)\"\"\"\n\t\t)\n\n\n\tdef check_url_new(self, table: str, url: str) -> bool:\n\t\t\"\"\"Check a URL against a database table to see if it already exists.\n\t\t\n\t\tArguments:\n\t\t\turl (str): webpage page address to check against database table\n\t\t\ttable (str): database table to check against\n\n\t\tReturn\n\t\t\tbool: True if url exists in given table; False if it doesn't exist.\n\t\t\n\t\t\"\"\"\n\t\texisting_url = self.cur.execute(f'SELECT url FROM {table} WHERE url = \\'{url}\\'')\n\t\tif len(existing_url) == 0:\n\t\t\treturn True\n\t\telse: \n\t\t\treturn False\n\n\tdef commit(self):\n\t\t\"\"\"Use after any other database class function to commit changes.\n\t\tThis function is separated from initial transactions to enable the __exit__ function to rollback changes in the case that errors are encountered.\n\t\t\"\"\"\n\t\tself.__db_connection.commit()\n", "description": "Handles all database connectsion, inputs, and outputs\n\t\n\tClass constructor initiates sqlite3 database connection. If used in WITH statement\n\tthe connection will cleanly close after the statement is finished. If there are\n\tuncommitted transactions they will be rolled back prior to connection closure.\n\t\n\t", "category": "webscraping", "imports": ["import sqlite3", "import os", "from pathlib import Path", "from datetime import datetime", "from datetime import date", "from interface_db.reference import table_classes as table"]}], [{"term": "class", "name": "classdb_connect_postgres:", "data": "class db_connect_postgres:\n\tdef __init__(self, user: str, password: str, host: str, port: int, environment=None):\n\t\tself.user\t\t= user\n\t\tself.password\t= password\n\t\tself.host\t\t= host\n\t\tself.port\t\t= port\n\t\tself.environment = environment\n\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import os", "from pathlib import Path", "from datetime import datetime", "from datetime import date", "import psycopg2", "from interface_db.reference import table_classes as table"]}, {"term": "class", "name": "database", "data": "class database(object):\n\t\"\"\"Handles all database connectsion, inputs, and outputs\n\t\n\tClass constructor initiates sqlite3 database connection. If used in WITH statement\n\tthe connection will cleanly close after the statement is finished. If there are\n\tuncommitted transactions they will be rolled back prior to connection closure.\n\t\n\t\"\"\"\n\tdef __init__(self, test_database=None):\n\t\tif test_database:\n\t\t\tself.__db_connection = test_database\n\t\t\tself.cur = self.__db_connection.cursor()\n\t\t# elif os.path.exists(Path(os.environ['WEBSCRAPE_DB']) / 'webscraper.db'): #os.environ['WEBSCRAPE_DB']:\n\t\t#\t print(os.path.exists(Path(os.environ['WEBSCRAPE_DB']) / 'webscraper.db'))\n\t\t#\t print(os.path.exists(prod_database / 'webscraper.db'))\n\t\t#\t __DB_LOCATION = prod_database / 'webscraper.db'\n\t\t#\t print(__DB_LOCATION)\n\t\telse:\n\t\t\tself.__db_connection = psycopg2.connect(\n\t\t\t\tuser='postgres',\n\t\t\t\tpassword='postgrest',\n\t\t\t\thost='192.168.86.108',\n\t\t\t\tport='32834',\n\t\t\t\t#database='postgres_db'\n\t\t\t)\n\t\t\tself.cur = self.__db_connection.cursor()\n\tdef __del__(self):\n\t\tself.__db_connection.close()\n\n\tdef __enter__(self):\n\t\treturn self\n\n\tdef __exit__(self, ext_type, exc_value, traceback):\n\t\tself.cur.close()\n\t\tif isinstance(exc_value, Exception):\n\t\t\tself.__db_connection.rollback()\n\t\telse:\n\t\t\tself.__db_connection.commit()\n\t\tself.__db_connection.close()\n\t\n\n\n\tdef query_table_ids_all(self, table: str, where_column=None, table_id=None, last_date_ascending=False) -> list:\n\t\tid_list = []\n\t\tif table_id == None:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT id FROM {table} \n\t\t\t\t\t\t\t\t\t\t  ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\").fetchall()\n\t\telse:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT id FROM {table} \n\t\t\t\t\t\t\t\t\t\t  WHERE {where_column} = {table_id} \n\t\t\t\t\t\t\t\t\t\t  ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\").fetchall()\n\t\t# print(f\"\"\"SELECT id FROM {table} \n\t\t#\t\t\t\t\t\t\t\t   WHERE {where_column} = {table_id} \n\t\t#\t\t\t\t\t\t\t\t   ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\")\n\t\t\n\t\tfor i in result:\n\t\t\tid_list.append(i[0])\n\t\treturn id_list\n\n\n\tdef query_websites(self, name=None, url=None, website_id=None):\n\t\tif name:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE name = '{name}'\"\"\").fetchone()\n\t\telif url:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE url = '{url}'\"\"\")\n\t\t\tprint(result)\n\t\telif website_id:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE id = '{website_id}'\"\"\").fetchone()\n\t\telse:\n\t\t\treturn 'query_websites() requires name or url'\n\t\tif result:\n\t\t\treturn table.website(id\t\t  = result[0], # id\n\t\t\t\t\t\t   name\t\t= result[1], # name\n\t\t\t\t\t\t   url\t\t = result[2], # url\n\t\t\t\t\t\t   last_date   = result[3], # last_date\n\t\t\t\t\t\t   last_user   = result[4], # last_user\n\t\t\t\t\t\t   create_date = result[5], # create_date\n\t\t\t\t\t\t   create_user = result[6]) # create_user\n\t\telse:\n\t\t\treturn None\n\n\n\tdef query_categories(self, name=None, url=None, category_id=None):\n\t\tif category_id:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE id = '{category_id}'\"\"\").fetchone()\n\t\telif name:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE name = '{name}'\"\"\").fetchone()\n\t\telif url:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE url = '{url}'\"\"\").fetchone()\n\t\tif result:\n\t\t\treturn table.category(id\t\t  = result[0], # id\n\t\t\t\t\t\t\tname\t\t= result[1], # name\n\t\t\t\t\t\t\tcontext\t = result[2], # context\n\t\t\t\t\t\t\turl\t\t = result[3], # url\n\t\t\t\t\t\t\twebsite_id  = result[4], # website_id\n\t\t\t\t\t\t\tlast_date   = result[5], # last_date\n\t\t\t\t\t\t\tlast_user   = result[6], # last_user\n\t\t\t\t\t\t\tcreate_date = result[7], # create_date\n\t\t\t\t\t\t\tcreate_user = result[8]) # create_user)\n\t\t\t#return result\n\t\telse:\n\t\t\treturn None\n\n\t\n\tdef query_blogs(self, name=None, url=None, blog_id=None) -> object:   # blog object\n\t\tif blog_id:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE id = '{blog_id}'\"\"\").fetchone()\n\t\telif name:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE name = '{name}'\"\"\").fetchone()\n\t\telif name:\n\t\t\tresult = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE url = '{url}'\"\"\").fetchone()\n\t\tif result:\n\t\t\treturn table.blog(id\t\t  = result[0], # id\n\t\t\t\t\t\tauthor\t  = result[1], # name\n\t\t\t\t\t\tname\t\t= result[2], # context\n\t\t\t\t\t\turl\t\t = result[3], # url\n\t\t\t\t\t\tcategory_id = result[4], # website_id\n\t\t\t\t\t\tlast_date   = result[5], # last_date\n\t\t\t\t\t\tlast_user   = result[6], # last_user\n\t\t\t\t\t\tcreate_date = result[7], # create_date\n\t\t\t\t\t\tcreate_user = result[8]) # create_user)\n\t\telse:\n\t\t\treturn None\n\n\t\n\tdef execute(self, new_data: str) -> tuple:\n\t\t\"\"\"Executes an valid SQL statement passed through as a string.\n\n\t\tArugments:\n\t\t\tnew_data (string): Valid SQL statement\n\n\t\t\"\"\"\n\t\treturn self.cur.execute(new_data).fetchall()\n\t\n\n\tdef insert_update_site_pages(self, page_number: int, site_id: int):\n\t\t\"\"\"Inserts a website record. Designed for use with the website class.\n\n\t\tArguments:\n\t\t\twebsite (website class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\t# existing_url = self.insert_update_website(url=website.url)\n\t\t# new = True if existing_url == None else False\n\t\ttoday = date.today()\n\t\ttry:\n\t\t\tresult = self.cur.execute(f\"SELECT number FROM site_pages WHERE site_id = {site_id}\").fetchall()\n\t\t\t#print(result)\n\t\t\tif result[0]:\n\t\t\t\tself.cur.execute(\n\t\t\t\t\tf\"\"\"UPDATE site_pages\n\t\t\t\t\t\tSET number = '{page_number}',\n\t\t\t\t\t\t\tlast_date = '{today}',\n\t\t\t\t\t\t\tlast_user = 'user'\n\t\t\t\t\t\tWHERE site_id = '{site_id}'\n\t\t\t\t\t\"\"\"\n\t\t\t\t)\n\t\t\t#print('updated or same')\n\t\texcept IndexError:\n\t\t\tnext_id = self.cur.execute(\"\"\"SELECT MAX(id) FROM site_pages\"\"\").fetchone()[0]\n\t\t\tnext_id = next_id + 1 if next_id else 1\n\t\t\tself.cur.execute(\n\t\t\t\tf\"\"\"INSERT INTO site_pages\n\t\t\t\t\t\t\t\tVALUES (\n\t\t\t\t\t\t\t\t\t\tNULL,\n\t\t\t\t\t\t\t\t\t\t\"{page_number}\",\n\t\t\t\t\t\t\t\t\t\t\"{site_id}\",\n\t\t\t\t\t\t\t\t\t\t\"{today}\",\n\t\t\t\t\t\t\t\t\t\t\"user\",\n\t\t\t\t\t\t\t\t\t\t\"{today}\",\n\t\t\t\t\t\t\t\t\t\t\"user\"\n\t\t\t\t\t\t\t)\"\"\"\n\t\t\t).fetchall()\n\t\t\t#print('new')\n\t\t# else:\n\t\t#\t self.cur.execute(\n\t\t#\t\t f\"\"\"UPDATE websites\n\t\t#\t\t\t SET number = '{page_number}',\n\t\t#\t\t\t\t last_date = '{today}',\n\t\t#\t\t\t\t last_user = 'user'\n\t\t#\t\t\t WHERE website_id = '{website_id}'\n\t\t#\t\t \"\"\"\n\t\t#\t ).fetchall()\n\t\t#\t return self.query_websites(url=website.url)\n\t\t# else:\n\t\t#\t return f'Not inserted. If exists, then {existing_url.url} is already present, named {existing_url.name}.'\n\n\n\tdef insert_website(self, website: object) -> object:\n\t\t\"\"\"Inserts a website record. Designed for use with the website class.\n\n\t\tArguments:\n\t\t\twebsite (website class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\texisting_url = self.query_websites(url=website.url)\n\t\tnew = True if existing_url == None else False\n\t\tif new == True:\n\t\t\tnext_id = self.cur.execute(\"\"\"SELECT MAX(id) FROM websites\"\"\").fetchone()[0]\n\t\t\tif next_id:\n\t\t\t\tnext_id = next_id + 1 if next_id else 1\n\t\t\ttoday = date.today()\n\t\t\tself.cur.execute(\n\t\t\t\tf\"\"\"INSERT INTO websites\n\t\t\t\t\t\t\t\tVALUES (\n\t\t\t\t\t\t\t\t\t\tNULL,\n\t\t\t\t\t\t\t\t\t\t\"{website.name}\",\n\t\t\t\t\t\t\t\t\t\t\"{website.url}\",\n\t\t\t\t\t\t\t\t\t\t\"{today}\",\n\t\t\t\t\t\t\t\t\t\t\"user\",\n\t\t\t\t\t\t\t\t\t\t\"{today}\",\n\t\t\t\t\t\t\t\t\t\t\"user\"\n\t\t\t\t\t\t\t)\"\"\"\n\t\t\t).fetchall()\n\t\t\treturn self.query_websites(url=website.url)\n\t\telse:\n\t\t\treturn f'Not inserted. If exists, then {existing_url.url} is already present, named {existing_url.name}.'\n\n\t\n\tdef insert_category(self, category):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tcategory.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM categories\"\"\").fetchone()[0]\n\t\tcategory.id = category.id + 1 if category.id else 1\n\t\treturn self.cur.execute(\n\t\t\tf\"\"\"INSERT INTO categories\n\t\t\t\t\t\t\t VALUES (\n\t\t\t\t\t\t\t\t\t\t\"{category.id}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.name}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.context}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.url}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.website_id}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.last_date}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.last_user}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.create_date}\",\n\t\t\t\t\t\t\t\t\t\t\"{category.create_user}\"\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\n\t\n\tdef insert_blog(self, blog):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tblog.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM blogs\"\"\").fetchone()[0]\n\t\tblog.id = blog.id + 1 if blog.id else 1\n\t\tblog.name = blog.name.replace('\"', '')\n\t\treturn self.cur.execute(\n\t\t\tf\"\"\"INSERT INTO blogs VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n\t\t\t\t\t\t\t\t\t   (blog.id,\n\t\t\t\t\t\t\t\t\t\tblog.author,\n\t\t\t\t\t\t\t\t\t\tblog.name,\n\t\t\t\t\t\t\t\t\t\tblog.url,\n\t\t\t\t\t\t\t\t\t\tblog.category_id,\n\t\t\t\t\t\t\t\t\t\tblog.last_date,\n\t\t\t\t\t\t\t\t\t\tblog.last_user,\n\t\t\t\t\t\t\t\t\t\tblog.create_date,\n\t\t\t\t\t\t\t\t\t\tblog.create_user)\n\t\t)\n\n\n\tdef update_date_blog(self, blog):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tlast_date = datetime.now()\n\t\treturn self.cur.execute(\n\t\t\tf\"\"\"UPDATE blogs\n\t\t\t\tSET last_date = '{last_date}'\n\t\t\t\tWHERE id = {blog.id}\"\"\"\n\t\t)\n\n\t\n\tdef update_date_category(self, category):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tlast_date = datetime.now()\n\t\treturn self.cur.execute(\n\t\t\tf\"\"\"UPDATE categories\n\t\t\t\tSET last_date = '{last_date}'\n\t\t\t\tWHERE id = {category.id}\"\"\"\n\t\t)\n\n\t\t\n\tdef insert_post(self, post):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\tpost.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM posts\"\"\").fetchone()[0]\n\t\tpost.id = post.id + 1 if post.id else 1\n\t\tpost.content = post.content.replace('\"', '\\\"')\n\t\tpost.content = post.content.replace(\"'\", \"\\'\")\n\t\tpost.tags = ','.join(post.tags)\n\t\tself.cur.execute(\n\t\t\t \"\"\"INSERT INTO posts\n\t\t\t\t\t\t\t\tVALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n\t\t\t\t\t\t\t\t\t   (post.id,\n\t\t\t\t\t\t\t\t\t\tpost.title,\n\t\t\t\t\t\t\t\t\t\tpost.author,\n\t\t\t\t\t\t\t\t\t\tpost.date,\n\t\t\t\t\t\t\t\t\t\tpost.tags,\n\t\t\t\t\t\t\t\t\t\tpost.content,\n\t\t\t\t\t\t\t\t\t\tpost.content_html,\n\t\t\t\t\t\t\t\t\t\tpost.url,\n\t\t\t\t\t\t\t\t\t\tpost.blog_id,\n\t\t\t\t\t\t\t\t\t\tpost.last_date,\n\t\t\t\t\t\t\t\t\t\tpost.last_user,\n\t\t\t\t\t\t\t\t\t\tpost.create_date,\n\t\t\t\t\t\t\t\t\t\tpost.create_user)\n\t\t)\n\n\n\tdef insert_error_url(self, url_error: object):\n\t\t\"\"\"Inserts a category record. Designed for use with the category class.\n\n\t\tArguments:\n\t\t\tcategory (category class): class or dictionary containing the following values:\n\t\t\t\t-\n\n\t\t\"\"\"\n\t\turl_error.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM url_error_list\"\"\").fetchone()[0]\n\t\turl_error.id = url_error.id + 1 if url_error.id else 1\n\t\tself.cur.execute(\n\t\t\t\"\"\"INSERT INTO url_error_list\n\t\t\t\t\t\t\t\tVALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n\t\t\t\t\t\t\t\t\t\t(url_error.id,\n\t\t\t\t\t\t\t\t\t\turl_error.url,\n\t\t\t\t\t\t\t\t\t\turl_error.url_type,\n\t\t\t\t\t\t\t\t\t\turl_error.parent_id,\n\t\t\t\t\t\t\t\t\t\turl_error.resolved,\n\t\t\t\t\t\t\t\t\t\turl_error.last_date,\n\t\t\t\t\t\t\t\t\t\turl_error.last_user,\n\t\t\t\t\t\t\t\t\t\turl_error.create_date,\n\t\t\t\t\t\t\t\t\t\turl_error.create_user)\n\t\t)\n\n\t\n\tdef create_tables(self):\n\t\t\"\"\"This function confirms the existence of or creates the path, database, and tables.\n\t\t\n\t\tCan be used by calling the function directly, but is designed to by used by install.py, which is called by the install.bat file.\n\t\t\n\t\t\"\"\"\n\t\tif (\n\t\t\tPath.home() / \"py_apps\" / \"_appdata\" / \"webscrape_patheos\" / \"patheos.db\"\n\t\t):\n\t\t\tpass\n\t\telse:\n\t\t\tPath(Path.home() / \"py_apps\" / \"_appdata\" / \"webscrape_patheos\" / \"patheos.db\").mkdir(\n\t\t\t\tparents=True, exist_ok=True\n\t\t\t)\n\n\t\t\"\"\"create a database table if it does not exist already\"\"\"\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS \n\t\t\t\t\t\t\tsite_pages (\n\t\t\t\t\t\t\t\tid\t\t   BIGSERIAL PRIMARY KEY, \n\t\t\t\t\t\t\t\tnumber\t   INTEGER NOT NULL,\n\t\t\t\t\t\t\t\tsite_id\t  INTEGER NOT NULL UNIQUE,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS \n\t\t\t\t\t\t\turl_error_list (\n\t\t\t\t\t\t\t\tid\t\t   BIGSERIAL PRIMARY KEY,\n\t\t\t\t\t\t\t\turl\t\t  TEXT NOT NULL,\n\t\t\t\t\t\t\t\turl_type\t TEXT NOT NULL,\n\t\t\t\t\t\t\t\tparent_id\tINTEGER NOT NULL,\n\t\t\t\t\t\t\t\tresolved\t TEXT NOT NULL,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS \n\t\t\t\t\t\t\twebsites (\n\t\t\t\t\t\t\t\tid\t\t   BIGSERIAL PRIMARY KEY, \n\t\t\t\t\t\t\t\tname\t\t VARCHAR(100) NOT NULL, \n\t\t\t\t\t\t\t\turl\t\t  VARCHAR(2000) NOT NULL,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS \n\t\t\t\t\t\t\tcategories (\n\t\t\t\t\t\t\t\tid\t\t   BIGSERIAL PRIMARY KEY,\n\t\t\t\t\t\t\t\tname\t\t VARCHAR(100) NOT NULL, \n\t\t\t\t\t\t\t\tcontext\t  VARCHAR(100), \n\t\t\t\t\t\t\t\turl\t\t  VARCHAR(2000) NOT NULL,\n\t\t\t\t\t\t\t\twebsite_id   INTEGER NOT NULL,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS\n\t\t\t\t\t\t\tblogs (\n\t\t\t\t\t\t\t\tid\t\t   BIGSERIAL PRIMARY KEY,\n\t\t\t\t\t\t\t\tauther\t   VARCHAR(255),\n\t\t\t\t\t\t\t\tname\t\t VARCHAR(255), \n\t\t\t\t\t\t\t\turl\t\t  TEXT NOT NULL,\n\t\t\t\t\t\t\t\tcategory_id  INTEGER NOT NULL, \n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t)\"\"\"\n\t\t)\n\t\tself.cur.execute(\n\t\t\t\"\"\"CREATE TABLE IF NOT EXISTS\n\t\t\t\t\t\t\tposts (\n\t\t\t\t\t\t\t\tid\t\t   BIGSERIAL PRIMARY KEY,\n\t\t\t\t\t\t\t\ttitle\t\tVARCHAR(255) NOT NULL,\n\t\t\t\t\t\t\t\tauthor\t   VARCHAR(255),\n\t\t\t\t\t\t\t\tdate\t\t TIMESTAMP, \n\t\t\t\t\t\t\t\ttags\t\t VARCHAR(255), \n\t\t\t\t\t\t\t\tcontent\t  TEXT, \n\t\t\t\t\t\t\t\tcontent_html TEXT,\n\t\t\t\t\t\t\t\turl\t\t  TEXT NOT NULL,\n\t\t\t\t\t\t\t\tblog_id\t  INTEGER NOT NULL,\n\t\t\t\t\t\t\t\tlast_date\tTIMESTAMP,\n\t\t\t\t\t\t\t\tlast_user\tVARCHAR(100),\n\t\t\t\t\t\t\t\tcreate_date  TIMESTAMP,\n\t\t\t\t\t\t\t\tcreate_user  VARCHAR(100)\n\t\t\t\t\t)\"\"\"\n\t\t)\n\n\n\tdef check_url_new(self, table: str, url: str) -> bool:\n\t\t\"\"\"Check a URL against a database table to see if it already exists.\n\t\t\n\t\tArguments:\n\t\t\turl (str): webpage page address to check against database table\n\t\t\ttable (str): database table to check against\n\n\t\tReturn\n\t\t\tbool: True if url exists in given table; False if it doesn't exist.\n\t\t\n\t\t\"\"\"\n\t\texisting_url = self.cur.execute(f'SELECT url FROM {table} WHERE url = \\'{url}\\'')\n\t\tif len(existing_url) == 0:\n\t\t\treturn True\n\t\telse: \n\t\t\treturn False\n\n\tdef commit(self):\n\t\t\"\"\"Use after any other database class function to commit changes.\n\t\tThis function is separated from initial transactions to enable the __exit__ function to rollback changes in the case that errors are encountered.\n\t\t\"\"\"\n\t\tself.__db_connection.commit()\n", "description": "Handles all database connectsion, inputs, and outputs\n\t\n\tClass constructor initiates sqlite3 database connection. If used in WITH statement\n\tthe connection will cleanly close after the statement is finished. If there are\n\tuncommitted transactions they will be rolled back prior to connection closure.\n\t\n\t", "category": "webscraping", "imports": ["import sqlite3", "import os", "from pathlib import Path", "from datetime import datetime", "from datetime import date", "import psycopg2", "from interface_db.reference import table_classes as table"]}], [{"term": "class", "name": "classResults:", "data": "class Results:\n\tdef __init__(self):\n\t\tself.blacklist_count = 0\n\t\tself.whitelist_count = 0\n\t\tself.list = []\n\t\tself.result = namedtuple('Result', 'order, type, link')\n\n\tdef add_blacklist(self):\n\t\tself.blacklist_count += 1\n\n\tdef add_whitelist(self):\n\t\tself.blacklist_count += 1\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "save_ctl", "data": "def save_ctl():\n\t\"\"\"\n\t\tCANCEL: webscrape has full access to sql to see what has been\n\t\t\t\tdownloaded in the past.\n\n\t\tSave Control file containing list of dictionaries\n\n\t\tUSED by mserve and webscrape.py\n\t\t\tmserve passes previous list of names with flags to scrape.\n\t\t\twebscrape.py passes back name of website that was scraped.\n\t\t\twebscrape.py passes names of websites that CAN BE scraped.\n\t\"\"\"\n\twith open(SCRAPE_CTL_FNAME, \"w\") as ctl:\n\t\tctl.write(json.dumps(CTL_LIST))\n\n", "description": "\n\t\tCANCEL: webscrape has full access to sql to see what has been\n\t\t\t\tdownloaded in the past.\n\n\t\tSave Control file containing list of dictionaries\n\n\t\tUSED by mserve and webscrape.py\n\t\t\tmserve passes previous list of names with flags to scrape.\n\t\t\twebscrape.py passes back name of website that was scraped.\n\t\t\twebscrape.py passes names of websites that CAN BE scraped.\n\t", "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "load_ctl", "data": "def load_ctl():\n\t\"\"\"\n\t\tReturn contents of CTL file or empty list of dictionaries\n\t\"\"\"\n\tdata = CTL_LIST\n\tif os.path.isfile(SCRAPE_CTL_FNAME):\n\t\twith open(SCRAPE_CTL_FNAME, \"r\") as ctl:\n\t\t\tdata = json.loads(ctl.read())\n\n\treturn data\n\n", "description": "\n\t\tReturn contents of CTL file or empty list of dictionaries\n\t", "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "check_files", "data": "def check_files(select='all'):\n\t\"\"\" NOT USED \"\"\"\n\tif select == 'all' or select == 'list':\n\t\tif not os.path.isfile(SCRAPE_LIST_FNAME):\n\t\t\treturn False\n\n\tif select == 'all' or select == 'lyrics':\n\t\tif not os.path.isfile(SCRAPE_LYRICS_FNAME):\n\t\t\treturn False\n\n\treturn True\n\n", "description": " NOT USED ", "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "delete_files", "data": "def delete_files(select='all'):\n\t\"\"\" USED by mserve and webscrape.py \"\"\"\n\tif select == 'all' or select == 'list':\n\t\ttry:\n\t\t\tos.remove(SCRAPE_LIST_FNAME)\n\t\texcept OSError:\n\t\t\tpass\n\n\tif select == 'all' or select == 'lyrics':\n\t\ttry:\n\t\t\tos.remove(SCRAPE_LYRICS_FNAME)\n\t\texcept OSError:\n\t\t\tpass\n\n", "description": " USED by mserve and webscrape.py ", "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "google_search", "data": "def google_search(search):\n\t\"\"\" Get google search results for \"song lyrics\" + Artist + Song\n\t\tTODO: Accept parameter for which of 8 websites to focus on getting:\n\t\t\t1. Metrolyrics\n\t\t\t2. AZ Lyrics\n\t\t\t3. Lyrics.com\n\t\t\t4. LyricsMode\n\t\t\t5. Letsingit\n\t\t\t6. Genius\n\t\t\t7. Musixmatch\n\t\t\t8. LyricsPlanet\n\t\"\"\"\n\tglobal WS_DICT, CTL_LIST\n\tglobal MEGALOBIZ, AZLYRICS, LYRICS, LYRICSMODE, LETSSINGIT\n\tglobal GENIUS, MUSIXMATCH, LYRICSPLANET, list_output\n\n\t# If we try to print normally an error occurs when launched in background\n\t# print(\"CTL_LIST start search:\", CTL_LIST, file=sys.stderr)\n\n\t# print('requests header:', requests.utils.default_headers())\n\t# Avoid google robot detection\n\t# noinspection SpellCheckingInspection\n\tuser_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1)' + \\\n\t\t\t\t 'AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14'\n\theaders = {'User-Agent': user_agent,\n\t\t\t   'Accept': 'text/html,application/xhtml+xml,application/' +\n\t\t\t\t\t\t 'xml;q=0.9,image/webp,*/*;q=0.8'}\n\t# inspection SpellCheckingInspection\n\n\tresults = 100  # valid options 10, 20, 30, 40, 50, and 100\n\t#\tpage = requests.get(f\"https://www.google.com/search?q={search}&num={results}\")\n\tquery = \"https://www.google.com/search?q={search}&num={results}\". \\\n\t\tformat(search=search, results=results, headers=headers)\n\t# print('query:', query)\n\n\t# noinspection PyBroadException\n\ttry:\n\t\tpage = requests.get(query)\n\texcept:\n\t\tlist_output.append('Internet not available.')\n\t\t# print('Internet not available.')\n\t\treturn\n\n\tsoup = BeautifulSoup(page.content, \"html5lib\")\n\t# print('soup:', soup)\n\tlinks = soup.findAll(\"a\")\n\tfor link in links:\n\t\tlink_href = link.get('href')\n\t\tif \"url?q=\" in link_href and \"webcache\" not in link_href:\n\t\t\tt = (link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0])\n\t\t\t# if 'youtube' in t or 'wikipedia' in t or 'facebook' in t \\\n\t\t\t# or 'pinterest' in t:\n\t\t\tif any(s in t for s in BLACK_LIST):\n\t\t\t\t# print('skipping youtube, wikipedia, facebook & pinterest')\n\t\t\t\t# TODO: keep these but put in blacklist\n\t\t\t\tadd_blacklist(t)\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tlist_output.append(t)\n\t\t\t\tadd_whitelist(t)\n\n\t\t\t# TODO: look up in list and get ranking\n\t\t\tif 'www.metalobiz.com' in t:\n\t\t\t\tMEGALOBIZ = t\n\t\t\tif 'www.metrolyrics.com' in t:\n\t\t\t\tMETROLYRICS = t\n\t\t\tif 'www.azlyrics.com' in t:\n\t\t\t\tAZLYRICS = t\n\t\t\tif 'www.lyrics.com' in t:\n\t\t\t\tLYRICS = t\n\t\t\tif 'www.lyricsmode.com' in t:\n\t\t\t\tLYRICSMODE = t\n\t\t\tif 'www.letssingit.com' in t:\n\t\t\t\tLETSSINGIT = t\n\t\t\tif '//genius.com' in t:  # Trap out //dekgenius.com\n\t\t\t\tGENIUS = t\n\t\t\tif 'www.musixmatch.com' in t:\n\t\t\t\tMUSIXMATCH = t\n\t\t\tif 'www.lyricsplanet.com' in t:  # Not sure if '//' or 'www.' prefix\n\t\t\t\tLYRICSPLANET = t\n\n\t\t\t# New method February 21/2021\n\t\t\tfor i, website in enumerate(WEBSITE_LIST):\n\t\t\t\tif website in t:\n\t\t\t\t\t# If we try to print normally an error occurs when launched in background\n\t\t\t\t\t# print(\"\\ni, website in t:\", i, website, NAMES_LIST[i], t, file=sys.stderr, sep=\" | \")\n\t\t\t\t\t# noinspection PyDictCreation\n\t\t\t\t\tWS_DICT = {}  # Reset dynamic link to earlier elements\n\t\t\t\t\tWS_DICT['name'] = NAMES_LIST[i]\n\t\t\t\t\tWS_DICT['website'] = website\n\t\t\t\t\tWS_DICT['link'] = t\n\t\t\t\t\tWS_DICT['flag'] = \"available\"\n\t\t\t\t\tCTL_LIST[i] = WS_DICT\n\t\t\t\t\tbreak\n\n\t# Save files\n\twith open(SCRAPE_LIST_FNAME, \"w\") as outfile:\n\t\tfor line in list_output:\n\t\t\toutfile.write(line + \"\\n\")\n\n\t# If we try to print normally an error occurs when launched in background\n\t# print(\"\\nCTL_LIST after search:\", CTL_LIST, file=sys.stderr)\n\n\tsave_ctl()\n\n\n", "description": " Get google search results for \"song lyrics\" + Artist + Song\n\t\tTODO: Accept parameter for which of 8 websites to focus on getting:\n\t\t\t1. Metrolyrics\n\t\t\t2. AZ Lyrics\n\t\t\t3. Lyrics.com\n\t\t\t4. LyricsMode\n\t\t\t5. Letsingit\n\t\t\t6. Genius\n\t\t\t7. Musixmatch\n\t\t\t8. LyricsPlanet\n\t", "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "add_blacklist", "data": "def add_blacklist(t):\n\tglobal BLACK_LIST_COUNT\n\tBLACK_LIST_COUNT += 1\n\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "add_whitelist", "data": "def add_whitelist(t):\n\tglobal WHITE_LIST_COUNT\n\tWHITE_LIST_COUNT += 1\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "scrape", "data": "def scrape(search):\n\tglobal lyrics_output\n\tif GENIUS:\n\t\tget_from_genius()\n\n\t# We didn't find anything in genius.com\n\tif len(lyrics_output) == 0:\n\t\tif MEGALOBIZ:\n\t\t\tget_from_megalobiz()\n\n\t# We didn't find anything in megalobiz.com\n\tif len(lyrics_output) == 0:\n\t\tif AZLYRICS:\n\t\t\tget_from_azlyrics()\n\n\t# We didn't find anything in azlyrics.com\n\tif len(lyrics_output) == 0:\n\t\tif METROLYRICS:\n\t\t\tget_from_metrolyrics()\n\n\tif len(lyrics_output) == 0:\n\t\t# We didn't find anything in genius.com or azlyrics.com\n\t\tlyrics_output.append('No lyrics found for: ' + search)\n\t\tlyrics_output.append('Popular sites search results:')\n\n\t\tif METROLYRICS:\n\t\t\tlyrics_output.append(METROLYRICS)\n\t\tif AZLYRICS:\n\t\t\tlyrics_output.append(AZLYRICS)\n\t\tif LYRICS:\n\t\t\tlyrics_output.append(LYRICS)\n\t\tif LYRICSMODE:\n\t\t\tlyrics_output.append(LYRICSMODE)\n\t\tif LETSSINGIT:\n\t\t\tlyrics_output.append(LETSSINGIT)\n\t\tif GENIUS:\n\t\t\tlyrics_output.append(GENIUS)\n\t\tif MUSIXMATCH:\n\t\t\tlyrics_output.append(MUSIXMATCH)\n\t\tif LYRICSPLANET:\n\t\t\tlyrics_output.append(LYRICSPLANET)\n\n\t\tlyrics_output.append('Or consider scraping following sites for lyrics:')\n\t\tfor line in list_output:\n\t\t\t# Write possible lyrics websites as song lyrics to display\n\t\t\tlyrics_output.append(line)\n\n\t# Save file\n\twith open(SCRAPE_LYRICS_FNAME, \"w\") as outfile:\n\t\tfor line in lyrics_output:\n\t\t\toutfile.write(line + \"\\n\")\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "get_from_genius", "data": "def get_from_genius():\n\tglobal lyrics_output\n\n\turl = GENIUS\n\t# noinspection PyBroadException\n\ttry:\n\t\tsoup = BeautifulSoup(requests.get(url).content, 'lxml')\n\t\tfor tag in soup.select('div[class^=\"Lyrics__Container\"], \\\n\t\t\t\t\t\t\t   .song_body-lyrics p'):\n\t\t\tt = tag.get_text(strip=True, separator='\\n')\n\t\t\tif t:\n\t\t\t\t# print(t)\n\t\t\t\tlyrics_output.append(t)\n\texcept:\n\t\tlyrics_output.append('Error occurred retrieving genius.com lyrics')\n\t\tlyrics_output.append(url)\n\t\tlyrics_output.append('Search String: ' + search)\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "get_from_azlyrics", "data": "def get_from_azlyrics():\n\tglobal lyrics_output\n\n\turl = AZLYRICS\n\n\t# noinspection PyBroadException\n\ttry:\n\t\thtml_page = urllib.request.urlopen(url)\n\t\tsoup = BeautifulSoup(html_page, 'html.parser')\n\t\thtml_pointer = soup.find('div', attrs={'class': 'ringtone'})\n\t\t# song_name = html_pointer.find_next('b').contents[0].strip()\n\t\tlyrics = html_pointer.find_next('div').text.strip()\n\t\tlyrics_output.append(lyrics)\n\texcept:\n\t\tlyrics_output.append('Error occurred retrieving azlyrics.com lyrics')\n\t\tlyrics_output.append(url)\n\t\tlyrics_output.append('Search String: ' + SEARCH)\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "get_from_metrolyrics", "data": "def get_from_metrolyrics():\n\tfrom lxml import html\n\tglobal lyrics_output\n\n\turl = METROLYRICS\n\t\"\"\"Load the lyrics from MetroLyrics.\"\"\"\n\tpage = requests.get(url)\n\n\tif page.status_code > 200:\n\t\t# raise TswiftError(\"No lyrics available for requested song\")\n\t\treturn\n\n\t# Forces utf-8 to prevent character mangling\n\tpage.encoding = 'utf-8'\n\n\ttree = html.fromstring(page.text)\n\ttry:\n\t\tlyric_div = tree.get_element_by_id('lyrics-body-text')\n\t\tverses = [c.text_content() for c in lyric_div.find_class('verse')]\n\texcept KeyError:\n\t\t# raise \"No lyrics available for requested song\"\n\t\treturn\n\telse:\n\t\t# Not sure what do do with following line just yet (Feb 21 2021)\n\t\t# lyrics = '\\n\\n'.join(verses)\n\t\tpass\n\n\treturn verses\n\n", "description": "Load the lyrics from MetroLyrics.", "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "get_from_megalobiz", "data": "def get_from_megalobiz():\n\t\"\"\" Song # 878 Creed - My Sacrifice\n\n\t\tLyrics are split up into two lines versus one in Genius.com\n\t\t.LRC times are to 100th of a second\n\t\"\"\"\n\n\tfrom lxml import html\n\tglobal lyrics_output\n\n\turl = MEGALOBIZ\n\t\"\"\"Load the lyrics from MetroLyrics.\"\"\"\n\tpage = requests.get(url)\n\n\tif page.status_code > 200:\n\t\t# raise TswiftError(\"No lyrics available for requested song\")\n\t\treturn\n\n\t# Forces utf-8 to prevent character mangling\n\tpage.encoding = 'utf-8'\n\n\ttree = html.fromstring(page.text)\n\n\ttry:\n\t\tlyric_div = tree.get_element_by_id('lyrics_details entity_more_info')\n\t\tverses = [c.text_content() for c in lyric_div.find_class('verse')]\n\texcept KeyError:\n\t\t# raise \"No lyrics available for requested song\"\n\t\treturn\n\telse:\n\t\t# Not sure what do do with following line just yet (Feb 21 2021)\n\t\t# lyrics = '\\n\\n'.join(verses)\n\t\tpass\n\n\treturn verses\n\n", "description": " Song # 878 Creed - My Sacrifice\n\n\t\tLyrics are split up into two lines versus one in Genius.com\n\t\t.LRC times are to 100th of a second\n\t", "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "def", "name": "no_parameters", "data": "def no_parameters():\n\t\"\"\" Called with no parameters \"\"\"\n\tglobal root\n\tsql.open_db()\n\troot = tk.Tk()  # Create \"very top\" toplevel for all top levels\n\troot.withdraw()  # Remove default window because we have treeview\n\t# background From: https://stackoverflow.com/a/11342481/6929343\n\tdefault_bg = root.cget('bg')\n\tprint('default_bg:', default_bg)  # d9d9d9 It's a little bright\n\n\t''' Set font style for all fonts including tkSimpleDialog.py '''\n\timg.set_font_style()  # Make messagebox text larger for HDPI monitors\n\t''' Set program icon in taskbar '''\n\timg.taskbar_icon(root, 64, 'black', 'green', 'red', char='W')\n\n\t''' console splash message '''\n\tprint(r'  ######################################################')\n\tprint(r' //////////////\t\t\t\t\t\t\t\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\n\tprint(r'<<<<<<<<<<<<<<   webscrape.py - Get Lyrics  >>>>>>>>>>>>>>')\n\tprint(r' \\\\\\\\\\\\\\\\\\\\\\\\\\\\\t\t\t\t\t\t\t//////////////')\n\tprint(r'  ######################################################')\n\n\tHistoryTree()  # Build treeview of sql history\n\n\troot.mainloop()  # When splash screen calls us there is mainloop\n\n", "description": " Called with no parameters ", "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}, {"term": "class", "name": "classHistoryTree:", "data": "class HistoryTree:\n\t\"\"\" Create self.his_tree = tk.Treeview() via CheckboxTreeview()\n\n\t\tResizeable, Scroll Bars, select songs, play songs.\n\n\t\tIf toplevel is not None then it is the splash screen to destroy.\n\n\t\"\"\"\n\n\tdef __init__(self, sbar_width=12):\n\n\t\t# Define self. variables in backups() where play_top frame is used.\n\t\tself.bup_top = None  # Toplevel for Backups\n\t\tself.bup_top_is_active = None  # Is backups top level open?\n\t\tself.bup_view = None  # Treeview using data dictionary\n\t\tself.bup_view_btn1 = None\n\t\tself.bup_view_btn2 = None\n\t\tself.bup_view_btn3 = None\n\t\tself.bup_view_btn4 = None\n\t\tself.bup_view_btn5 = None\n\t\tself.bup_view_btn6 = None\n\t\tself.bup_view_btn7 = None\n\t\tself.bup_search = None  # Searching for trash, etc?\n\n\t\tself.hdr_top = None  # Toplevel for gmail message header\n\t\tself.hdr_top_is_active = False  # Displaying gmail message header?\n\t\tself.scrollbox = None  # Holds pretty print dictionary\n\n\t\t''' HistoryTree Instances '''\n\t\tself.view = None  # = lib_view or bup_view\n\t\tself.region = None  # 'heading', 'separator' or 'cell'\n\t\tself.bup_gen = None  # Backup generations\n\t\tself.subject_list = []  # GMAIL_SUBJECT.split('!')\n\t\tself.iid = None\n\n\t\t''' get parameters in SQL setup by mserve '''\n\t\tnow = time.time()\n\t\tlast_time = sql.hist_last_time('scrape', 'parm')\n\t\tif last_time is None:\n\t\t\tlast_time = now\n\t\thist_row = sql.hist_get_row(sql.HISTORY_ID)\n\t\tlag = now - last_time\n\t\tif lag > 1.0:\n\t\t\tprint('It took more than 1 second for webscrape to start:', lag)\n\t\telse:\n\t\t\tprint('webscrape start up time:', lag)\n\t\t\tpass\n\t\tprint(hist_row)\n\n\t\t''' TODO: Relocate dtb '''\n\t\tdtb = message.DelayedTextBox(title=\"Building history table view\",\n\t\t\t\t\t\t\t\t\t toplevel=None, width=1000)\n\n\t\t# If we are started by splash screen get object, else it will be None\n\t\t# self.splash_toplevel = toplevel\n\t\tself.splash_toplevel = tk.Toplevel()\n\n\t\tself.splash_removed = False  # Did we remove splash screen?\n\n\t\t# Create our tooltips pool (hover balloons)\n\t\tself.tt = toolkit.ToolTips()\n\n\t\t''' Create toplevel and set program icon in taskbar '''\n\t\tself.his_top = tk.Toplevel()\n\t\tself.his_top.title(\"SQL History Table - webscrape\")\n\t\timg.taskbar_icon(self.his_top, 64, 'black', 'green', 'red', char='W')\n\n\t\t''' Initial size of Window 75% of HD monitor size '''\n\t\t_w = int(1920 * .75)\n\t\t_h = int(1080 * .75)\n\t\t_root_xy = (3800, 200)  # Temporary hard-coded coordinates\n\n\t\t''' Mount window at popup location '''\n\t\tself.his_top.minsize(width=g.WIN_MIN_WIDTH, height=g.WIN_MIN_HEIGHT)\n\t\t# self.his_top.geometry('%dx%d+%d+%d' % (_w, _h, _root_xy[0], _root_xy[1]))\n\t\tgeom = monitor.get_window_geom('history')\n\t\tself.his_top.geometry(geom)\n\n\t\tself.his_top.configure(background=\"Gray\")\n\t\tself.his_top.columnconfigure(0, weight=1)\n\t\tself.his_top.rowconfigure(0, weight=1)\n\n\t\t''' Create frames '''\n\t\tmaster_frame = tk.Frame(self.his_top, bg=\"olive\", relief=tk.RIDGE)\n\t\tmaster_frame.grid(sticky=tk.NSEW)\n\t\tmaster_frame.columnconfigure(0, weight=1)\n\t\tmaster_frame.rowconfigure(0, weight=1)\n\n\t\t''' Create treeview '''\n\t\thistory_dict = sql.history_treeview()\n\t\tcolumns = [\"time\", \"type\", \"action\", \"master\", \"detail\", \"target\",\n\t\t\t\t   \"size\", \"count\", \"seconds\", \"music_id\", \"comments\"]\n\t\t\"\"\" FIELDS NOT SHOWN:\n\t\t\t(\"column\", \"row_id\"), (\"heading\", \"Row ID\"), (\"sql_table\", \"History\"),\n\t\t\t(\"column\", \"user\"), (\"heading\", \"User\"), (\"sql_table\", \"History\"),\n\t\t\t(\"column\", \"delete_on\"), (\"heading\", \"Delete On\"), (\"sql_table\", \"calc\"),\n\t\t\t(\"column\", \"reason\"), (\"heading\", \"Reason\"), (\"sql_table\", \"calc\"),\n\n\t\t\"\"\"\n\t\ttoolkit.select_dict_columns(columns, history_dict)\n\n\t\tself.his_view = toolkit.DictTreeview(\n\t\t\thistory_dict, self.his_top, master_frame, columns=columns,\n\t\t\tsbar_width=sbar_width)\n\n\t\t# toolkit.print_dict_columns(history_dict)\n\n\t\t'''\n\t\t\t\t\tB I G   T I C K E T   E V E N T\n\n\t\tCreate Treeview item list with NO songs selected YET. '''\n\t\tself.manually_checked = False  # Used for self.reverse/self.toggle\n\t\tself.populate_his_tree(dtb)\n\n\t\t''' Treeview Buttons '''\n\t\tframe3 = tk.Frame(master_frame, bg=\"Blue\", bd=2, relief=tk.GROOVE,\n\t\t\t\t\t\t  borderwidth=g.BTN_BRD_WID)\n\t\tframe3.grid_rowconfigure(0, weight=1)\n\t\tframe3.grid_columnconfigure(0, weight=0)\n\t\tframe3.grid(row=1, column=0, sticky=tk.NW)\n\n\t\t''' Global variables of active children '''\n\t\tself.play_top = None  # Backup server selected headers\n\t\tself.play_top_is_active = False  # Playing songs window open?\n\n\t\t''' \u00e2\u0153\u02dc Close Button \u00e2\u0153\u02dc \u00e2\u0153\u201d '''\n", "description": " Create self.his_tree = tk.Treeview() via CheckboxTreeview()\n\n\t\tResizeable, Scroll Bars, select songs, play songs.\n\n\t\tIf toplevel is not None then it is the splash screen to destroy.\n\n\t", "category": "webscraping", "imports": ["from __future__ import print_function  # Must be first import", "from __future__ import with_statement  # Error handling for file opens", "# from __future__ import unicode_literals\t # Unicode errors fix", "\timport tkinter as tk", "\timport tkinter.ttk as ttk", "\timport tkinter.font as font", "\timport tkinter.filedialog as filedialog", "\timport tkinter.messagebox as messagebox", "\timport tkinter.scrolledtext as scrolledtext", "\timport Tkinter as tk", "\timport ttk", "\timport tkFont as font", "\timport tkFileDialog as filedialog", "\timport tkMessageBox as messagebox", "\timport ScrolledText as scrolledtext", "import sys", "import os", "import os.path", "import json", "import time", "import requests", "from six.moves import urllib\t\t\t# Python 2/3 compatibility library", "from bs4 import BeautifulSoup", "import re", "from collections import namedtuple", "import global_variables as g", "import external as ext  # Time formatting routines", "import image as img", "import monitor", "import sql", "import toolkit", "import message", "\tfrom lxml import html", "\tfrom lxml import html"]}], [{"term": "def", "name": "stock_info_to_csv", "data": "def stock_info_to_csv (tickerStrings, file_name):\n\tdf_list = list()\n\tfor ticker in tickerStrings:\n\t\tdata = yf.download(ticker, group_by=\"Ticker\", period='5y')\n\t\tdata['Ticker'] = ticker  #add column with ticker\n\t\tdf_list.append(data)\n\t\t\n\t#combine all dataframes into a single dataframe\n\tdf = pd.concat(df_list)\n\t\n\t#save to csv\n\tdf.to_csv(file_name)\n", "description": null, "category": "webscraping", "imports": ["import yfinance as yf", "import pandas as pd", "from selenium import webdriver", "import collections", "from bs4 import BeautifulSoup as soup", "import re", "import requests"]}], [{"term": "def", "name": "make_output_dir", "data": "def make_output_dir(conf: str) -> str:\n\t\"\"\"Create the output_dir for a conference, and return the path to it\"\"\"\n\toutput_dir = f\"outputs/{conf}\"\n\tif not os.path.exists(\"outputs\"):\n\t\tos.mkdir(\"outputs\")\n\tif not os.path.exists(output_dir):\n\t\tos.mkdir(output_dir)\n\treturn output_dir\n\n", "description": "Create the output_dir for a conference, and return the path to it", "category": "webscraping", "imports": ["import os", "import attr", "import pprint", "import pandas as pd", "from typing import Sequence", "from analyse_conf import sigir_extract", "from analyse_conf import author_info"]}, {"term": "def", "name": "write_class_list", "data": "def write_class_list(objects: Sequence[object], file_name: str) -> None:\n\t\"\"\"Write a list of attrs objects to a csv file, overwriting old results\"\"\"\n\t# Delete the file if it exists\n\tif os.path.exists(file_name):\n\t\tos.remove(file_name)\n\n\t# Write data to file\n\trows = [attr.asdict(obj) for obj in objects]\n\tdf = pd.DataFrame(rows)\n\tdf.to_csv(file_name, index=False)\n\n", "description": "Write a list of attrs objects to a csv file, overwriting old results", "category": "webscraping", "imports": ["import os", "import attr", "import pprint", "import pandas as pd", "from typing import Sequence", "from analyse_conf import sigir_extract", "from analyse_conf import author_info"]}, {"term": "def", "name": "analyse_conf", "data": "def analyse_conf(conf: str) -> None:\n\t\"\"\"\n\tWebscrape the conference site to get author data\n\tUse SemanticScholar API to get citation and institution data\n\tTODO: Perform analysis of this data\n\t\tTODO: Create visualisations and tables of the results\n\t\"\"\"\n\toutput_dir = make_output_dir(conf)\n\n\t# Webscrape conference data\n\tpapers = conference_to_webscraper[conf].extract_data()\n\tprint(\"Papers:\")\n\tpprint.pprint(papers[:10])\n\n\t# Get author data from SemanticScholar\n\tauthors = author_info.get_author_data(papers)\n\tprint(\"\\n\\nAuthors:\")\n\tpprint.pprint(authors[:10])\n\n\t# Extract authorships after the author_id information has been added to papers list\n\tauthorships = [ats for paper in papers for ats in paper.authorships]\n\tprint(\"\\n\\nAuthorships:\")\n\tpprint.pprint(authorships[:10])\n\n\t# Write data to file\n\twrite_class_list(authors, f\"{output_dir}/authors.csv\")\n\twrite_class_list(papers, f\"{output_dir}/papers.csv\")\n\twrite_class_list(authorships, f\"{output_dir}/authorships.csv\")\n", "description": "\n\tWebscrape the conference site to get author data\n\tUse SemanticScholar API to get citation and institution data\n\tTODO: Perform analysis of this data\n\t\tTODO: Create visualisations and tables of the results\n\t", "category": "webscraping", "imports": ["import os", "import attr", "import pprint", "import pandas as pd", "from typing import Sequence", "from analyse_conf import sigir_extract", "from analyse_conf import author_info"]}], [{"term": "def", "name": "login_main", "data": "def login_main():\n\treturn render_template('login.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, url_for, redirect, session, request, g"]}, {"term": "def", "name": "home_main", "data": "def home_main():\n\treturn render_template('home.html', title=\"Home\")\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, url_for, redirect, session, request, g"]}, {"term": "def", "name": "survey_main", "data": "def survey_main():\n\treturn render_template('webscrape.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, url_for, redirect, session, request, g"]}, {"term": "def", "name": "prep_main", "data": "def prep_main():\n\treturn render_template('configgen.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, url_for, redirect, session, request, g"]}, {"term": "def", "name": "deployment_main", "data": "def deployment_main():\n\treturn render_template('deployment.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, url_for, redirect, session, request, g"]}, {"term": "def", "name": "validation_main", "data": "def validation_main():\n\treturn render_template('validation.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, url_for, redirect, session, request, g"]}, {"term": "def", "name": "integrationengineer_main", "data": "def integrationengineer_main():\n\treturn render_template('integrationengineer.html')\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, url_for, redirect, session, request, g"]}], [{"term": "def", "name": "return_coordinates", "data": "def return_coordinates(address):\n\ttry:\n\t\tgeocode_info = g.geocode(address)\n\t\tlat = float(geocode_info.latitude)\n\t\tlon = float(geocode_info.longitude)\n\texcept Exception as e:\n\t\tlat = NaN\n\t\tlon = NaN\n\t\tlogging.warning(f\"Couldn't fetch geocode information for {address} because of {e}.\")\n\tlogging.info(f\"Fetched coordinates {lat}, {lon} for {address}.\")\n\treturn lat, lon\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup as bs4", "from dash import html", "from datetime import date, timedelta, datetime", "from dotenv import load_dotenv, find_dotenv", "from geopy.geocoders import GoogleV3", "import glob", "from imagekitio import ImageKit", "from numpy import NaN", "import logging", "import os", "import pandas as pd", "import requests"]}, {"term": "def", "name": "fetch_missing_city", "data": "def fetch_missing_city(address):\n\ttry:\n\t\tgeocode_info = g.geocode(address)\n\t\t# Get the city by using a ??? whatever method this is\n\t\t# https://gis.stackexchange.com/a/326076\n\t\t# First get the raw geocode information\n\t\traw = geocode_info.raw['address_components']\n\t\t# Then dig down to find the 'locality' aka city\n\t\tcity = [addr['long_name'] for addr in raw if 'locality' in addr['types']][0]\n\texcept Exception as e:\n\t\tcity = NaN\n\t\tlogging.warning(f\"Couldn't fetch city for {address} because of {e}.\")\n\tlogging.info(f\"Fetched city ({city}) for {address}.\")\n\treturn  city\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup as bs4", "from dash import html", "from datetime import date, timedelta, datetime", "from dotenv import load_dotenv, find_dotenv", "from geopy.geocoders import GoogleV3", "import glob", "from imagekitio import ImageKit", "from numpy import NaN", "import logging", "import os", "import pandas as pd", "import requests"]}, {"term": "def", "name": "return_postalcode", "data": "def return_postalcode(address):\n\ttry:\n\t\t# Forward geocoding the short address so we can get coordinates\n\t\tgeocode_info = g.geocode(address)\n\t\t# Reverse geocoding the coordinates so we can get the address object components\n\t\tcomponents = g.geocode(f\"{geocode_info.latitude}, {geocode_info.longitude}\").raw['address_components']\n\t\t# Create a dataframe from this list of dictionaries\n\t\tcomponents_df = pd.DataFrame(components)\n\t\tfor row in components_df.itertuples():\n\t\t\t# Select the row that has the postal_code list\n\t\t\tif row.types == ['postal_code']:\n\t\t\t\tpostalcode = row.long_name\n\texcept Exception as e:\n\t\tlogging.warning(f\"Couldn't fetch postal code for {address} because {e}.\")\n\t\treturn pd.NA\n\tlogging.info(f\"Fetched postal code {postalcode} for {address}.\")\n\treturn int(postalcode)\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup as bs4", "from dash import html", "from datetime import date, timedelta, datetime", "from dotenv import load_dotenv, find_dotenv", "from geopy.geocoders import GoogleV3", "import glob", "from imagekitio import ImageKit", "from numpy import NaN", "import logging", "import os", "import pandas as pd", "import requests"]}, {"term": "def", "name": "webscrape_bhhs", "data": "def webscrape_bhhs(url, row_index):\n\ttry:\n\t\tresponse = requests.get(url)\n\t\tsoup = bs4(response.text, 'html.parser')\n\t\t# Split the p class into strings and get the last element in the list\n\t\t# https://stackoverflow.com/a/64976919\n\t\tlisted_date = soup.find('p', attrs={'class' : 'summary-mlsnumber'}).text.split()[-1]\n\t\t# Now find the URL for the \"feature\" photo of the listing\n\t\tphoto = soup.find('a', attrs={'class' : 'show-listing-details'}).contents[1]['src']\n\t\t# Now find the URL to the actual listing instead of just the search result page\n\t\tlink = 'https://www.bhhscalifornia.com' + soup.find('a', attrs={'class' : 'btn cab waves-effect waves-light btn-details show-listing-details'})['href']\n\texcept AttributeError as e:\n\t\tlisted_date = pd.NaT\n\t\tphoto = NaN\n\t\tlink = NaN\n\t\tlogging.warn(f\"Couldn't fetch some BHHS webscraping info because of {e}.\")\t\t\n\tlogging.info(f\"Fetched Listed Date, MLS Photo, and BHHS link for row {row_index}...\")\n\treturn listed_date, photo, link\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup as bs4", "from dash import html", "from datetime import date, timedelta, datetime", "from dotenv import load_dotenv, find_dotenv", "from geopy.geocoders import GoogleV3", "import glob", "from imagekitio import ImageKit", "from numpy import NaN", "import logging", "import os", "import pandas as pd", "import requests"]}, {"term": "def", "name": "imagekit_transform", "data": "def imagekit_transform(bhhs_url, mls):\n\t# if the MLS photo URL from BHHS isn't null (a photo IS available), then upload it to ImageKit\n\tif pd.isnull(bhhs_url) == False:\n\t\ttry:\n\t\t\tuploaded_image = imagekit.upload_file(\n\t\t\t\tfile= f\"{bhhs_url}\", # required\n\t\t\t\tfile_name= f\"{mls}.jpg\", # required\n\t\t\t\toptions= {\n\t\t\t\t\t\"is_private_file\": False,\n\t\t\t\t\t\"use_unique_file_name\": False,\n\t\t\t\t}\n\t\t\t)\n\t\texcept Exception as e:\n\t\t\tlogging.warning(f\"Couldn't upload image to ImageKit because {e}. Passing on...\")\n\t\t\tuploaded_image = 'ERROR'\n\telif pd.isnull(bhhs_url) == True:\n\t\tuploaded_image = 'ERROR'\n\t\tlogging.info(f\"No image URL found. Not uploading anything to ImageKit.\")\n\t# Now transform the uploaded image\n\t# https://github.com/imagekit-developer/imagekit-python#url-generation\n\tif 'ERROR' not in uploaded_image:\n\t\ttry:\n\t\t\tglobal transformed_image\n\t\t\ttransformed_image = imagekit.url({\n\t\t\t\t\"src\": f\"{uploaded_image['response']['url']}\",\n\t\t\t\t\"transformation\" : [{\n\t\t\t\t\t\"height\": \"300\",\n\t\t\t\t\t\"width\": \"400\"\n\t\t\t\t}]\n\t\t\t})\n\t\texcept Exception as e:\n\t\t\tlogging.warning(f\"Couldn't transform image because {e}. Passing on...\")\n\t\t\ttransformed_image = None\n\telif 'ERROR' in uploaded_image:\n\t\tlogging.info(f\"No image URL found. Not transforming anything.\")\n\t\ttransformed_image = None\n\treturn transformed_image\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup as bs4", "from dash import html", "from datetime import date, timedelta, datetime", "from dotenv import load_dotenv, find_dotenv", "from geopy.geocoders import GoogleV3", "import glob", "from imagekitio import ImageKit", "from numpy import NaN", "import logging", "import os", "import pandas as pd", "import requests"]}, {"term": "def", "name": "popup_html", "data": "def popup_html(row):\n\ti = row.Index\n\tshort_address = df['short_address'].at[i]\n\tpostalcode = df['PostalCode'].at[i]\n\tfull_address = f\"{short_address} {postalcode}\"\n\tmls_number=df['mls_number'].at[i]\n\tmls_number_hyperlink=df['bhhs_url'].at[i]\n\tmls_photo = df['MLS Photo'].at[i]\n\tlc_price = df['list_price'].at[i] \n\tprice_per_sqft=df['ppsqft'].at[i]\t\t\t\t  \n\tbrba = df['Br/Ba'].at[i]\n\tsquare_ft = df['Sqft'].at[i]\n\tyear = df['YrBuilt'].at[i]\n\tgarage = df['garage_spaces'].at[i]\n\tpets = df['PetsAllowed'].at[i]\n\tphone = df['phone_number'].at[i]\n\tterms = df['Terms'].at[i]\n\tsub_type = df['subtype'].at[i]\n\tlisted_date = pd.to_datetime(df['listed_date'].at[i]).date() # Convert the full datetime into date only. See https://stackoverflow.com/a/47388569\n\tfurnished = df['Furnished'].at[i]\n\tkey_deposit = df['DepositKey'].at[i]\n\tother_deposit = df['DepositOther'].at[i]\n\tpet_deposit = df['DepositPets'].at[i]\n\tsecurity_deposit = df['DepositSecurity'].at[i]\n\t# If there's no square footage, set it to \"Unknown\" to display for the user\n\t# https://towardsdatascience.com/5-methods-to-check-for-nan-values-in-in-python-3f21ddd17eed\n\tif pd.isna(square_ft) == True:\n\t\tsquare_ft = 'Unknown'\n\t# If there IS a square footage, convert it into an integer (round number)\n\telif pd.isna(square_ft) == False:\n\t\tsquare_ft = f\"{int(square_ft)} sq. ft\"\n\t# Repeat above for Year Built\n\tif pd.isna(year) == True:\n\t\tyear = 'Unknown'\n\t# If there IS a square footage, convert it into an integer (round number)\n\telif pd.isna(year) == False:\n\t\tyear = f\"{int(year)}\"\n\t# Repeat above for garage spaces\n\tif pd.isna(garage) == True:\n\t\tgarage = 'Unknown'\n\telif pd.isna(garage) == False:\n\t\tgarage = f\"{int(garage)}\"\n\t# Repeat for ppsqft\n\tif pd.isna(price_per_sqft) == True:\n\t\tprice_per_sqft = 'Unknown'\n\telif pd.isna(price_per_sqft) == False:\n\t\tprice_per_sqft = f\"${float(price_per_sqft)}\"\n\t# Repeat for listed date\n\tif pd.isna(listed_date) == True:\n\t\tlisted_date = 'Unknown'\n\telif pd.isna(listed_date) == False:\n\t\tlisted_date = f\"{listed_date}\"\n\t# Repeat for furnished\n\tif pd.isna(furnished) == True:\n\t\tfurnished = 'Unknown'\n\telif pd.isna(furnished) == False:\n\t\tfurnished = f\"{furnished}\"\n\t# Repeat for the deposits\n\tif pd.isna(key_deposit) == True:\n\t\tkey_deposit = 'Unknown'\n\telif pd.isna(key_deposit) == False:\n\t\tkey_deposit = f\"${int(key_deposit)}\"\n\tif pd.isna(pet_deposit) == True:\n\t\tpet_deposit = 'Unknown'\n\telif pd.isna(pet_deposit) == False:\n\t\tpet_deposit = f\"${int(pet_deposit)}\"\n\tif pd.isna(security_deposit) == True:\n\t\tsecurity_deposit = 'Unknown'\n\telif pd.isna(security_deposit) == False:\n\t\tsecurity_deposit = f\"${int(security_deposit)}\"\n\tif pd.isna(other_deposit) == True:\n\t\tother_deposit = 'Unknown'\n\telif pd.isna(other_deposit) == False:\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup as bs4", "from dash import html", "from datetime import date, timedelta, datetime", "from dotenv import load_dotenv, find_dotenv", "from geopy.geocoders import GoogleV3", "import glob", "from imagekitio import ImageKit", "from numpy import NaN", "import logging", "import os", "import pandas as pd", "import requests"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\t# \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\t# \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n\tdef get_gloss(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "from pyltp import SentenceSplitter"]}], [{"term": "def", "name": "main", "data": "def main(args: list) -> None:\n\n\t# process: str: library: str, src_fh: str)\n\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"Overdrive-Reconcile\",\n\t\tdescription=\"Sets of scripts reconciling records between Sierra and OverDrive platform.\",\n\t)\n\n\tparser.add_argument(\n\t\t\"action\",\n\t\thelp=(\n\t\t\t\"'reconcile' runs entire set of scripts | \"\n\t\t\t\"'webscrape' runs only webscraping of OverDrive catalog | \"\n\t\t\t\"'check-overdrive' runs scraping for a single resource\"\n\t\t),\n\t\ttype=str,\n\t\tchoices=[\"reconcile\", \"webscrape\", \"check-overdrive\"],\n\t)\n\tparser.add_argument(\n\t\t\"library\", help=\"'BPL' or 'NYPL'\", type=str, choices=[\"BPL\", \"NYPL\"]\n\t)\n\tparser.add_argument(\n\t\t\"source\",\n\t\thelp=\"Sierra export or csv for webscraping source file handle\",\n\t\ttype=str,\n\t)\n\n\tparser.add_argument(\n\t\t\"start\",\n\t\thelp=\"Starting row of data to be verified.\",\n\t\ttype=int,\n\t\tnargs=\"?\",\n\t\tdefault=0,\n\t)\n\n\tpargs = parser.parse_args(args)\n\n\tif pargs.action == \"reconcile\":\n\t\treconcile(pargs.library, pargs.source)\n\tif pargs.action == \"webscrape\":\n\t\tif pargs.source == \"default\":\n\t\t\t# assume today's file\n\t\t\twork_dir = date_subdirectory(pargs.library)\n\t\t\tsrc_fh = (\n\t\t\t\tf\"{work_dir}/{pargs.library}-for-deletion-verification-required.csv\"\n\t\t\t)\n\t\t\ttotal = count_rows(src_fh) - 1\n\t\t\tscrape(pargs.library, src_fh, total, pargs.start)\n\t\telse:\n\t\t\ttotal = count_rows(pargs.source) - 1\n\t\t\tscrape(pargs.library, pargs.source, total, pargs.start)\n\n\tif pargs.action == \"check-overdrive\":\n\t\tprint(f\"Checking status of {pargs.source} on OverDrive platform...\")\n\t\tcheck_status(pargs.source, pargs.library)\n\n", "description": null, "category": "webscraping", "imports": ["import argparse", "import sys", "from overdrive_reconcile.reconcile import reconcile", "from overdrive_reconcile.utils import date_subdirectory, count_rows", "from overdrive_reconcile.webscraper import scrape, check_status"]}], [{"term": "def", "name": "onbtn2press", "data": "def onbtn2press():\n\te1.delete(0, END)\n\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "from WebScraper import webscrape"]}, {"term": "def", "name": "onbtn1press", "data": "def onbtn1press():\n\tt1.delete('1.0', END)\n\tentryinput = e1.get()\n\ttry:\n\t\ttext = webscrape(entryinput)\n\texcept:\n\t\ttext = \"We could not find a proper definition. Please check your entry.\"\n\tt1.insert('1.0', text)\n\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "from WebScraper import webscrape"]}], [], [{"term": "def", "name": "run", "data": "def run():\n\tfrom urllib.request import urlopen\n\tfrom bs4 import BeautifulSoup\n\n\t##### Link collector to send crawler to iteratively\n\n\timport requests\n\n\tbaseurl = 'https://nationalpost.com'\n\n\theaders = {\n\t\t'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.128 Safari/537.36'\n\t}\n\n\tcolumnlinks = []\n\n\t# Pulling the urls from the last 5 pages worth of columns from here: https://nationalpost.com/category/opinion/?from=1\n\t\t\n\tfor x in range(1,25,25):\n\t\tr = requests.get(f'https://nationalpost.com/category/opinion/?from={x}')\n\t\tsoup2 = BeautifulSoup(r.content, 'lxml')\n\t\tcolumnlist = soup2.find_all('div', class_='article-card__details')\n\t\tfor item in columnlist:\n\t\t\t\tfor link in item.find_all('a', href=True):\n\t\t\t\t\tcolumnlinks.append(baseurl + link['href'])\n\n\n\t#remove this string  from array columnlinks['http://nationalpost.com//category/opinion/']\n\n\tfPostLink = 'https://financialpost.com/'\n\tgPostLink = 'https://nationalpost.com/news/'\n\thPostLink = 'https://nationalpost.com/category/politics'\n\tiPostLink = 'https://nationalpost.com/category/election-2021'\n\n\t# NEED TO REMOVE https://nationalpost.com/news/\n\n\tfor elem in list(columnlinks):\n\t\tif elem == \"https://nationalpost.com/category/opinion/\":\n\t\t\tcolumnlinks.remove(elem)\n\t\telif fPostLink in elem:\t\t\n\t\t\tcolumnlinks.remove(elem)\n\t\telif gPostLink in elem:\n\t\t\tcolumnlinks.remove(elem)\n\t\telif hPostLink in elem:\n\t\t\tcolumnlinks.remove(elem)\n\t\telif iPostLink in elem:\n\t\t\tcolumnlinks.remove(elem)\n\n\t\t\t\n\n\tlinkarray = columnlinks\n\n\tcount = 0\n\n\tfor i in range(0, len(linkarray)):\n\n\t\tcount += 1\n\n\t\turl = linkarray[i] # this variable imports into Database under newsColumns\n\t\thtml = urlopen(url)\n\t\tsoup = BeautifulSoup(html.read(), 'html.parser')\n\t##### CLEAN WEBSCRAPE OF ALL PERTINENT INFORMATION ######\n\t\tps = soup.find_all('p')\n\t\tbodyarray = []\n\t\tbodytext = \"\"\n\n\t\t#   collect all p tags then append all text items to array - then add all items into bodytext string var \n\t\tfor p in ps:\n\t\t\tptext = p.get_text()\n\t\t\tbodyarray.append(ptext)\n\n\t\tfor item in bodyarray:\n\t\t\tbodytext += item\n\t\t\t\n\t\t#   headline webscrape\n\t\theadline = soup.find('h1').get_text()\n\n\t\t#   publish date webscrape\n\t\ttry:\n\t\t\tpubdatescrape = soup.find('span', \"published-date__since\").get_text()\n\t\texcept AttributeError:\n\t\t\tpubdate = \"Jan 1, 1991\"\n\t\telse:\n\t\t\tpubdate = pubdatescrape\n\n\t\t#   author webscrape - if null do not add to the thing (this doesn't work because the thing is already pulled and transferred)\n\t\ttry:\n\t\t\tauthor = soup.find('span', \"published-by__author\").get_text()\n\t\texcept AttributeError:\n\t\t\tprint('Error with following link' + headline + '\\nskipping link...')\n\t\t\tpass\n\t\t\n\t\t#split the author name into fname and lname\n\t\ttry:\n\t\t\tsplitauthor = author.split(' ', 1)\n\t\t\tauthorfname = splitauthor[0]\n\t\t\tauthorlnamewhole = splitauthor[1].split(',', 1)\n\t\t\tauthorlname = authorlnamewhole[0]\n\t\texcept IndexError:\n\t\t\tauthorfname = author\n\t\t\tauthorlname = \" \"\n\t\t\t\t\t\t\t\t\t\n\t\tprint( count,\". =====UPLOADING COLUMN PUBLISHED:====== \", pubdate,'\\n', authorfname,'\\n', authorlname,'\\n', headline,)\n\n\t\t######\t  BEGIN INSERT DATA INTO DATABASE\t #####\n\n\t\timport mysql.connector\n\t\timport re\n\n\t\t#DB information to connect to localhost\n\n\t\tmydb = mysql.connector.connect(\n\t\thost=\"localhost\",\n\t\tport=\"3306\",\n\t\tuser=\"root\",\n\t\tpassword=\"root\",\n\t\tdatabase =\"columnwatcher\"\n\t\t)\n\n\t\t#get everything from the columnist table\n\n\t\tmycursor = mydb.cursor()\n\n\t\tmycursor.execute(\"SELECT * FROM columnists\")\n\t\tresult = mycursor.fetchall()\n\n\t\t#check if the first and last name of the columnist matches any of the existing columnists, columnist id = \n\n\t\t#give columnist id to existing columnist, attribute id to new columnist\n\n\t\tfor x in range(0,len(result)):\n\t\t\tif authorfname in result[x] and authorlname in result[x]:\n\t\t\t\tentryColumnistID = x+1\n\t\t\t\tnewColumnist = False\n\t\t\t\tbreak\n\t\t\t\t\n\t\t\telse: \n\t\t\t\tentryColumnistID = len(result)+1\n\t\t\t\tnewColumnist = True\n\n\t\t#if the name of the new columnist does not exist in the columnist table, add it to the table\n\t\tif newColumnist == True:\n\t\t\tprint('name does not exist in record. Creating new Columnist ID:')\n\t\t\tsqlColumnists = \"INSERT INTO columnists (first_name, last_name) VALUES (%s, %s)\"\n\t\t\tvalColumnists = (authorfname, authorlname)\n\n\t\t\tmycursor.execute(sqlColumnists, valColumnists)\n\t\t\tprint(\"Adding to database: \", authorfname, authorlname)\n\t\t\tmydb.commit()\n\t\telse:\n\t\t\tprint(authorfname, authorlname,\"exists in record under id#: \", entryColumnistID)\n\n\t\t##-- insert article --##\n\t\t\n\t\t'''\n\t\tcolumnist_id - auto incremented\n\t\tpaper_id - National Post is 1 \n\t\theadline \n\t\tpublishdate - PUBDATE\n\t\tbody_text\n\t\turl\n\t\t\n\t\t'''\n\n\n\t\tmycursor2 = mydb.cursor()\n\t\tmycursor2.execute(\"SELECT headline FROM newscolumns\")\n\t\tcolresult = mycursor2.fetchall()\n\n\t\t#placeholder and specific vars\n\t\t#entryColumnistID\n\t\tpaper_id = 1 #NationalPost\n\t\tnewHeadline = headline\n\n\t\timport datetime\n\t\t#switch PUBLISH DATE to YYYY-mm-dd\n\t\tf = '%b %d, %Y'\n\t\tentryDateTime = datetime.datetime.strptime(pubdate, f)\n\n\n\t\t#try 2 on newscolumn insert\n\n\t\tfor x in range(0,len(colresult)):\n\t\t\tif newHeadline in colresult[x]:\t\t\n\t\t\t\tnewColumn = False\n\t\t\t\tbreak\n\t\t\t\t\n\t\t\telse: \n\t\t\t\tnewColumn = True\n\n\n\n\t\t#if this headline does not already exist, add it to my database please\n\t\tif newColumn == True:\n\t\t\tprint('headline does not exist in record. Creating new Column entry')\n\t\t\tsqlNewsColumns = \"INSERT INTO newscolumns (columnist_id, paper_id, headline, body_text, url, publishdate) VALUES (%s, %s, %s, %s, %s, %s)\"\n\t\t\tvalNewsColumns = (entryColumnistID, 1, newHeadline, bodytext, url, entryDateTime)\n\t\t\tmycursor2.execute(sqlNewsColumns, valNewsColumns)\n\t\t\tmydb.commit()\n\t\t\tprint('new entry', newHeadline, 'added to the database')\n\t\t\t\n\t\telse:\n\t\t\tprint(\"headline already in database, will not add duplicate:\", newHeadline)\n\n\n", "description": null, "category": "webscraping", "imports": ["\tfrom urllib.request import urlopen", "\tfrom bs4 import BeautifulSoup", "\timport requests", "\t\turl = linkarray[i] # this variable imports into Database under newsColumns", "\t\timport mysql.connector", "\t\timport re", "\t\timport datetime"]}], [], [], [{"term": "def", "name": "main", "data": "def main():\n\tdate = datetime.date(2018, 5, 19)\n\tprint(date)\n\twebscrape(date)\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pandas import ExcelWriter", "import datetime"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(date):\n\n\tdate = date\n\t# get url data\n\turl = \"https://www.billboard.com/charts/billboard-korea-k-pop-100/\" + date.isoformat()\n\tr = requests.get(url)\n\n\t# parse url information\n\tsoup = BeautifulSoup(r.content, \"html.parser\")\n\n\t# lists for Panda DataFrame\n\tpeakPosition = []\n\ttop100 = []\n\tcurrRank = []\n\tpastRank = []\n\tartist = []\n\ttitle = []\n\n\t# further narrow data down\n\tg_data = soup.find_all(\"div\", {\"class\": \"chart-row__secondary\"})\n\n\tfor item in g_data:\n\t\t# print out peak position\n\t\t# print \"Peak Position\"\n\t\tpeakPosition.append(item.contents[1].find_all(\"div\", {\"class\": \"chart-row__top-spot\"})[0].text[15:])\n\n\t\t# print out number of weeks on chart\n\t\t# print \"Number of Weeks in Top 100\"\n\t\ttop100.append(item.contents[1].find_all(\"div\", {\"class\": \"chart-row__weeks-on-chart\"})[0].text[14:])\n\n\t# new search in data\n\tg_data = soup.find_all(\"div\", {\"class\": \"chart-row__main-display\"})\n\n\tfor item in g_data:\n\t\t# prints out rank and previous week's rank\n\t\t# print \"This week's ranking\"\n\t\tcurrRank.append(item.contents[1].find_all(\"span\", {\"class\": \"chart-row__current-week\"})[0].text)\n\t\t# print \"Last week's ranking\"\n\t\tpastRank.append(item.contents[1].find_all(\"span\", {\"class\": \"chart-row__last-week\"})[0].text[11:])\n\n\t\t# prints out the artist\n\t\t# print \"Artist\"\n\t\t# different format for artist names in code\n\t\t# some of the artists' names are hyperlinked\n\t\t# try unlinked case\n\t\ttry:\n\t\t\tartist.append(item.contents[5].find_all(\"span\", {\"class\": \"chart-row__artist\"})[0].text[1:-1])\n\t\t# artist name is hyperlinked\n\t\texcept:\n\t\t\tartist.append(item.contents[5].find_all(\"a\", {\"class\": \"chart-row__artist\"})[0].text[1:-1])\n\n\t\t# prints out the song title\n\t\t# print \"Song Title\"\n\t\ttitle.append(item.contents[5].find_all(\"h2\", {\"class\": \"chart-row__song\"})[0].text)\n\n\t# Create a Pandas dataframe from the data.\n\tdf = pd.DataFrame(\n\t\t{\"Current Ranking\": currRank, \"Last Week's Ranking\": pastRank, \"Song Title\": title, \"Artist\": artist,\n\t\t \"Peak Position\": peakPosition, \"Num Weeks in TOP 100\": top100})\n\n\tdf.to_csv(date.isoformat() + ' Billboard Kpop Rankings.csv', sep=',')\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pandas import ExcelWriter", "import datetime"]}], [{"term": "class", "name": "OpenDeltaCrawler", "data": "class OpenDeltaCrawler(scrapy.Spider):\n\tname = \"openDeltaCrawler\"\n\n\tdef start_requests(self):\n\t\turl = \"https://www.moneyweb.co.za/moneyweb-crypto/\"\n\t\tyield scrapy.Request(url, callback = self.parse_blog)\n\n\tdef parse_blog(self,response):\n\t\t# Go to the blocks that contain blog posts\n\t\tblog_posts = response.xpath('//h3[contains(@class,\"title list-title m0005\")]')\n\t\t# Go to the blog links\n\t\tblog_links = blog_posts.xpath('./a/@href')\n\t\tprint(blog_links)\n\t\t# Extract the links (as a list of strings)\n\t\tlinks_to_follow = blog_links.extract()\n\t\tprint(links_to_follow)\n\t\t# Follow the links in the next parser\n\n\n\t\tfor url in links_to_follow:\n\t\t\t#headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n\t\t\tyield response.follow(url=url, callback=self.parse_pages)\n\n\tdef parse_pages(self, response):\n\t\ti = ItemLoader(item=WebscrapeItem(), selector=response)\n\t\ti.add_css('blog_title','h1.article-headline')\n\t\ti.add_css('blog_excerpt','div.article-excerpt')\n\t\ti.add_xpath('blog_author','//div[@class=\"article-meta grey-text\"]/span[1]/span')\n\t\ti.add_xpath('blog_publish_date','//div[@class=\"article-meta grey-text\"]/span[2]')\n\t\ti.add_xpath('blog_text','//p')\n\t\ti.add_value('blog_url',response.url)\n\t\tyield i.load_item()\n\n\n", "description": null, "category": "webscraping", "imports": ["#import scrappy", "import scrapy", "from ..items import WebscrapeItem", "from scrapy.loader import ItemLoader", "from scrapy.crawler import CrawlerProcess"]}], [], [{"term": "def", "name": "webscrape", "data": "def webscrape(stream,area,field,location):\n\tglobal page_count_acm\n\tglobal dictionary\n\tglobal dic_count\n\tif area == \"\":\n\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'?page='+str(page_count_acm)\n\telif stream == \"\": \n\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+area+'?page='+str(page_count_acm)\n\telse: quote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'/'+area+'?page='+str(page_count_acm)\n\tprint quote_page_acm,'\\n'\n\tpage_acm = urllib2.urlopen(quote_page_acm)\n\tsoup_acm = BeautifulSoup(page_acm, 'html.parser')\n\tbox_acm = soup_acm.find_all('div',attrs={'class':'aiResultsMainDiv'})\n\ttemp_acm = soup_acm.find('span',attrs={'class':'aiPageTotalTop'}) \n\tif  temp_acm == None:\n\t\tprint \"No results found\"\n\t\tsys.exit(1)\n\tcount_total_acm = int(temp_acm.get_text())\n\tfor bx_acm in box_acm:\n\t\ttitle_acm = str(bx_acm.find('div',attrs={'class':'aiResultTitle'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Title:',bx.find('div',attrs={'class':'aiResultTitle'}).get_text().strip()\n\t\turl_acm = 'http://jobs.acm.org'+str(bx_acm.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href').encode('utf-8'))\n\t\t#print 'URL:',bx.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href')\n\t\tdetails_acm = bx_acm.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')\n\t\tcompany_acm = str(details_acm[0].get_text().strip().encode('utf-8'))\n\t\t#print 'Company:',details[0].get_text().strip()\n\t\tlocation_acm = str(details_acm[1].get_text().strip().encode('utf-8'))\n\t\t#print 'Location:',details[1].get_text().strip()\n\t\tdate_acm = str(details_acm[2].get_text().strip().encode('utf-8'))\n\t\t#print 'Date:',details[2].get_text().strip()+\"\\n\"\n\t\tif bx_acm.find('li',attrs={'id':'searchResultsCategoryDisplay'}) != None:\n\t\t\tcategory_acm = str(details_acm[3].get_text().strip().encode('utf-8'))\n\t\telse :\n\t\t\tcategory_acm = 'None'\n\t\tif bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}) == None:\n\t\t\tdescription_acm = str(bx_acm.find('div',attrs={'class':'aiResultsDescription'}).get_text().strip().encode('utf-8'))\n\t\telse :\n\t\t\tdescription_acm = str(bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Description:', bx.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t\n\t\t\n\n\t\tdictionary[dic_count] = [title_acm,company_acm,location_acm,date_acm,category_acm,description_acm,url_acm,0,'x']\n\t\tjacard(field,location,dic_count)\n\t\tdic_count = dic_count+1\n\n\t\t#if (title_acm,location_acm) in dictionary:\n\t\t#\tdictionary[title_acm,location_acm] = (dictionary[title_acm,location_acm],[title_acm,company_acm,location_acm,date_acm,category_acm,description_acm,0])\n\t\n\t\t#else:\n\t\t#\tdictionary[title_acm,location_acm] = ([title_acm,company_acm,location_acm,date_acm,category_acm,description_acm,0])\n  \n\n\n\n\t\t#dont use as of now\n\t\t#print 'Company:',bx.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find('li',attrs={'class':'aiResultsCompanyName'}).get_text().strip()\n\t\t#print 'Location:',bx.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')[1].get_text() + \"\\n\"\n\t\n\t\t#print bx.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t#print box.find('div',attrs={'class':'aiResultTitle'})\n\t\t#find('h3').get_text()\n\n\tif (page_count_acm < count_total_acm):\n\t\tpage_count_acm = page_count_acm + 1\n\t\twebscrape(stream,area,field,location)\n\t#for element in dictionary:\n\t#\tprint x\n\t#\tprint y\n\t#\tprint dictionary[x,y][0]\n\t#\tprint dictionary[x,y][1]\n\t#\tprint dictionary[x,y][2]\n\t#\tprint dictionary[x,y][3]\n\t#\tprint \"\\n\"\n\t#print dictionary  \n\n\t\n\n\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "from bs4 import BeautifulSoup", "import sys", "from operator import itemgetter", "import random"]}, {"term": "def", "name": "webscrape1", "data": "def webscrape1(stream,area,field,location):\n\tglobal page_count_indeed\n\tglobal c_indeed\n\tglobal test_indeed\n\tglobal count_total_indeed \n\tglobal counter_indeed\n\tglobal cal_page_indeed\n\tglobal val_indeed\n\tglobal dictionary\n\tglobal dic_count\n\t\n\tquote_page_indeed = 'https://www.indeed.com/jobs?q='+stream+'&l='+area+'&start='+str(page_count_indeed)\n\tprint quote_page_indeed,'\\n'\n\tpage_indeed = urllib2.urlopen(quote_page_indeed)\n\tsoup_indeed = BeautifulSoup(page_indeed, 'html.parser')\n\t\n\t#a = str(soup_indeed)\n\t#f = open('1.html','w')\n\t#f.write(a)\n\t#f.close()\n\t\n\t#print soup_indeed\n\n\t\n\tbox_indeed = soup_indeed.find_all('div',attrs={'class':'row'})\n\ttemmp_indeed = soup_indeed.find('div',attrs={'id':'searchCount'})\n\tif temmp_indeed == None:\n\t\tprint 'No results found'\n\t\tsys.exit(1)\n\tif test_indeed == 0:\n\t\tcount_total_indeed = int(temmp_indeed.get_text().split()[5].replace(',',''))\n\t\twhile val_indeed <= count_total_indeed:\n\t\t\tcal_page_indeed = cal_page_indeed + 1\n\t\t\tval_indeed = 25 * cal_page_indeed\n\t\ttest_indeed = 1\n\t\tcal_page_indeed = cal_page_indeed + 1\n\t\tif cal_page_indeed > 50:\n\t\t\tcal_page_indeed = 50\n\t\t#print page_indeed\n\t\t#print val_indeed\n\t#print count_total_indeed\n\t\n\tfor bx_indeed in box_indeed:\n\t\t#print c_indeed\n\t\t#print 'Title:',bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip()\n\t\ttitle_indeed = str(bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Company:', bx_indeed.find('span',attrs={'class':'company'}).get_text().strip()\n\t\tcompany_indeed = str(bx_indeed.find('span',attrs={'class':'company'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Location:', bx_indeed.find('span',attrs={'class':'location'}).get_text().strip()\n\t\tlocation_indeed = str(bx_indeed.find('span',attrs={'class':'location'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Description:', bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip(),'\\n'\n\t\tdescription_indeed = str(bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip().encode('utf-8'))\n\t\t#c_indeed = c_indeed+1\n\t\tdate_indeed = 'None'\n\t\tcategory_indeed = 'None'\n\t\turl_indeed = str(bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get('href').encode('utf-8'))\n\t\n\t\tdictionary[dic_count] = [title_indeed,company_indeed,location_indeed,date_indeed,category_indeed,description_indeed,url_indeed,0,'x']\n\t\tjacard(field,location,dic_count)\n\t\tdic_count = dic_count +1\n\n\t#if (title_indeed,location_indeed) in dictionary:\n\t#\tdictionary[title_indeed,location_indeed] = (dictionary[title_indeed,location_indeed],[title_indeed,company_indeed,location_indeed,date_indeed,category_indeed,description_indeed,0])\n\t#else:\n\t#\tdictionary[title_indeed,location_indeed] = ([title_indeed,company_indeed,location_indeed,date_indeed,category_indeed,description_indeed,0])\t\n\n\t\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "from bs4 import BeautifulSoup", "import sys", "from operator import itemgetter", "import random"]}], [], [{"term": "def", "name": "scrape_periodically", "data": "def scrape_periodically(name='scrape_periodically'):\n\tprint('here')\n\tproducts = list(ProductDetails.objects.all())\n\tproduct_urls = []\n\tnames, prices, rating, image_urls = [], [], [], []\n\tfor product in products:\n\t\tproduct_urls.append(product.product_url)\n\n\treturn_values = concurrent_map(call_main, product_urls)\n\t# send_notifications(title='Title', message='Message')\n\n\tfor i in range(len(return_values)):\n\t\tprices.append(return_values[i][1][0])\n\n\tfor i in range(len(products)):\n\t\ttemp = float(products[i].product_price)\n\t\tprint(temp, prices[i])\n\t\tif float(prices[i]) != float(products[i].product_price):\n\t\t\tproducts[i].product_price = prices[i]\n\t\tif float(prices[i]) <= float(products[i].all_time_low):\n\t\t\tproducts[i].all_time_low = prices[i]\n\t\t\tmessage = products[i].product_name + ' is at an all low of ' + prices[i]\n\t\t\tsend_notifications(title='Product is at an all time low!', message=message)\n\t\t\tproducts[i].save()\n\t\t\tcontinue\n\t\tif float(prices[i]) < temp:\n\t\t\tprint(type(products[i].product_name), type(prices[i]))\n\t\t\tmessage = products[i].product_name + ' has reduced to ' + str(prices[i]) + ' from ' + str(temp)\n\t\t\tsend_notifications(title='Product price decreased!', message=message)\n\t\tproducts[i].save()\n", "description": null, "category": "webscraping", "imports": ["from __future__ import absolute_import, unicode_literals", "from celery import shared_task", "# from webscrape_the_one.webscrape_the_one.celery import app", "from .main import main", "from .Notifications import send_notifications", "from celery import task", "from .models import ProductDetails", "from .parallel_fetch import concurrent_map, call_main", "from .serializers import ProductDetailsSerializer"]}], [{"term": "def", "name": "extractNavigableStrings", "data": "def extractNavigableStrings(context):\n\t\"\"\" from https://stackoverflow.com/questions/29110820/how-to-scrape-between-span-tags-using-beautifulsoup\"\"\"\n\tstrings = []\n\tfor e in context.children:\n\t\tif isinstance(e, NavigableString):\n\t\t\tstrings.append(e)\n\t\tif isinstance(e, Tag):\n\t\t\tstrings.extend(extractNavigableStrings(e))\n\treturn strings\n\n\n", "description": " from https://stackoverflow.com/questions/29110820/how-to-scrape-between-span-tags-using-beautifulsoup", "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}, {"term": "def", "name": "parse_MAl", "data": "def parse_MAl(url):\n\t\"\"\"\n\tParameters\n\t----------\n\turl : string\n\t\tmyanimelist.net url string \n\n\tReturns\n\t-------\n\tdf : DataFrame\n\t\treturns a dataframe with columns \"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"\n\n\t\"\"\"\n\thtml = requests.get(url)\n\tsoup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n\tresults = soup.find_all(class_= \"ranking-list\")\n\t\n\tdf = pd.DataFrame(columns=[\"name\",\"english_name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\", \"url\"])\n\ti = 0\n\tfor result in results:\n\t\t#print(i)\n\t\turl_= result.find(class_=\"hoverinfo_trigger fl-l fs14 fw-b\")[\"href\"]\n\t\thtml_ = requests.get(url_)\n\t\tsoup_ = BeautifulSoup(html_.content, 'html.parser', from_encoding=\"utf-8\")\n\t\t\n\t\tt1name = extractNavigableStrings(soup_.find(class_=\"h1-title\"))\n\t\tif len(t1name) == 1:\n\t\t\tname = t1name[0]\n\t\t\tenglish_name = None\n\t\telif len(t1name) >= 2:\n\t\t\tname=t1name[0]\n\t\t\tenglish_name=t1name[1]\n\t\telse:\n\t\t\tname = None\n\t\t\tenglish_name = None\n\t\t\t\n\t\tType, Dates, members = result.find(class_=\"information di-ib mt4\").text.strip().splitlines()\n\t\ttry:\n\t\t\tmembers = float(\"\".join(members.split()[0].split(\",\")))\n\t\texcept:\n\t\t\tmembers = None\n\t\t\t\n\t\t[Type_, eps, n] = [\", \".join(x.split()) for x in re.split(r'[()]',Type)]\n\t\t\n\t\ttry:\n\t\t\teps = float(eps.split(\",\")[0])\n\t\texcept:\n\t\t\teps = None\n\t\t\n\t\ttry:\n\t\t\tgenres = [genre.text.strip() for genre in soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"genre\")]\n\t\t\t\n\t\texcept:\n\t\t\tgenres = None\n\t\t\n\t\ttry:\n\t\t\tscore = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingValue\")[0].text.strip())\n\t\texcept:\n\t\t\tscore = None\n\t\t#try:\n\t\t#\tscore_members = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingCount\")[0].text.strip())\n\t\t#except:\n\t\t #   score_members = None\n\t\t\n\t\tdf = df.append({\n\t\t\t\"name\": name,\n\t\t\t\"english_name\":english_name,\n\t\t\t\"type\": Type_,\n\t\t\t\"episodes\": eps,\n\t\t\t\"members\": members,\n\t\t\t#\"score_members\": score_members,\n\t\t\t\"rating\": score,\n\t\t\t\"genre\": genres,\n\t\t\t\"dates\": Dates,\n\t\t\t\"url\": url_\n\t\t},ignore_index=True)\n\t\t\n\t\ti+=1\n\treturn df\n\n", "description": "\n\tParameters\n\t----------\n\turl : string\n\t\tmyanimelist.net url string \n\n\tReturns\n\t-------\n\tdf : DataFrame\n\t\treturns a dataframe with columns \"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"\n\n\t", "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}, {"term": "def", "name": "webscrape_MAl", "data": "def webscrape_MAl(anime_limit=16750, start=0):\n\turl_template = \"https://myanimelist.net/topanime.php?limit={}\"\n\tdf = pd.DataFrame(columns=[\"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"])\n\tfor limit in tqdm(range(start,anime_limit, 50)): # iterate in steps of 50\n\t\turl = url_template.format(limit)\n\t\tdf_temp = parse_MAl(url)\n\t\tif df_temp[\"name\"].isnull().sum() >= 40:\n\t\t\tprint(\"Number of missing names, for limit {} = {}\".format(limit, df_temp[\"name\"].isnull().sum()))\n\t\t\tprint(\"--------Halting---------\")\n\t\t\traise SystemExit()\n\t\tsave_mal_temp(df_temp, limit)\n\t\t\n\t\t# I think MAL has a limit on the number of conenctions per minute/second/hour\n\t\t# and after 200-400, the site blocks access. Adding the pause for 1 minute below soleves the issue\n\t\tsleep(60) # pause the loop for 1 minute. \n\t\t\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}, {"term": "def", "name": "save_mal_temp", "data": "def save_mal_temp(df, limit):\n\tcsvTemp = \"temp/MAL_start_{}.csv\".format(limit)\n\tdf.to_csv(csvTemp)\n\t\t\n\t#print(\"Number of missing names, for limit {} = {}\".format(limit, df[\"name\"].isnull().sum()))\n\t\n\t\n", "description": null, "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}], [], [{"term": "def", "name": "places_search", "data": "def places_search(area_table: TableManger, keyword: str, table: TableManger, call_api):\n\n\tif call_api is True:\n\t\tsearches = list_search_strings(keyword, area_table)\n\n\t\tfor search in searches:\n\t\t\tGoogleAPI.places_search(search)\n\n\t\tdf = places_search_to_business_directory()\n\telse:\n\t\ttable.load_df(\".json\")\n\t\tdf = table.get_df()\n\tdf = extract_add_postcode(df)\n\t#df = postcode_to_authcode(df)\n\t#df = authcode_to_income(df)\n\t#df = create_scores(df)\n\ttable.save_df(df, \".json\")\n\n", "description": null, "category": "webscraping", "imports": ["from api.googleAPI import GoogleAPI", "from api.websiteScrape import WebsiteScrape", "from utils.data import *", "from utils.const import __CUR_DIR__", "import openpyxl"]}, {"term": "def", "name": "detailed_search", "data": "def detailed_search(table: TableManger, call_api):\n\tdf = table.get_df()\n\n\tif call_api is True:\n\t\tfor index, row in df.iterrows():\n\t\t   GoogleAPI.detailed_search(row['place_id'])\n\n\tdf = detailed_search_to_business_directory(df)\n\t#df = is_chain(df)\n\ttable.save_df(df, \".json\")\n\n", "description": null, "category": "webscraping", "imports": ["from api.googleAPI import GoogleAPI", "from api.websiteScrape import WebsiteScrape", "from utils.data import *", "from utils.const import __CUR_DIR__", "import openpyxl"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(table: TableManger, call_api):\n\tdf = table.get_df()\n\n\tif call_api is True:\n\t\twebsite_list = df[df['website'] != \"\"]['website'].tolist()\n\n\t\ttot = len(website_list)\n\t\ti = 0\n\t\tfor website in website_list:\n\t\t  WebsiteScrape.scrape(website)\n\t\t  print(str(i*100/tot) + \"% Complete\")\n\t\t  i = i + 1\n\n\tdf = webscrape_to_business_directory(df)\n\n\tdf = cleanup_emails(df)\n\tdf = create_score(df)\n\ttable.save_df(df, \".json\")\n\n", "description": null, "category": "webscraping", "imports": ["from api.googleAPI import GoogleAPI", "from api.websiteScrape import WebsiteScrape", "from utils.data import *", "from utils.const import __CUR_DIR__", "import openpyxl"]}, {"term": "def", "name": "export", "data": "def export(table: TableManger):\n\tprint(\"Export\")\n\tdf = table.get_df()\n\tfile_path = table.file_path\n\t#\n\t# print(file_path)\n\t# i = input(\"aa\")\n\t#\n\t# writer = pd.ExcelWriter(os.path.join(__CUR_DIR__,\"test.xlsx\"), engine='openpyxl')\n\t# df.to_excel(writer, index=False)\n\t# writer.save()\n\n\ttable.save_df(df, \".xlsx\")\n\ttable.save_df(df[df[\"advanced_keyword_count\"] > 0], \".xlsx\", file_path+\"_filtered\")\n", "description": null, "category": "webscraping", "imports": ["from api.googleAPI import GoogleAPI", "from api.websiteScrape import WebsiteScrape", "from utils.data import *", "from utils.const import __CUR_DIR__", "import openpyxl"]}], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape :\n\n\tdef __init__(self) :\n\t\tpass\n\tpass\n\n\tdef doScrape(self) : \n\t\timport xlsxwriter\n\t\tworkbook = xlsxwriter.Workbook( \"\uc624\ub298\uc758 \uc99d\uad8c\uc2dc\uc138.xlsx\" )\n\t\tworksheet = workbook.add_worksheet()\n\t\trow = 0\n\t\tcol = 0 \n\n\t\tline = \"\\n\" + \"#\"*80 + \"\\n\"\n\n\t\t# import libraries\n\t\tfrom bs4 import BeautifulSoup\n\n\t\t# specify the url\n\t\thtml_url = \"http://vip.mk.co.kr/newSt/rate/item_all.php\"\n\n\t\t# query the website and return the html to the variable \u2018page\u2019\n\n\t\tlogging.info( \"Getting a html data from %s\" % html_url )\n\n\t\tfrom urllib.request import urlopen\n\t\thtml_page = urlopen( html_url )\n\t\thtml_src = html_page.read()\n\n\t\tlogging.info( \"Done. Getting a html data from %s\" % html_url )\n\n\t\t# parse the html using beautiful soup and store in variable `soup`\n\t\tsoup = BeautifulSoup( html_src, \"html.parser\" , from_encoding=\"euc-kr\" )\n\n\t\tprint( \"title = %s\" % soup.title ) \n\t\tprint( \"title.string = %s\" % soup.title.string )\n\t\ttable = soup.table\t\t\n\t\ttable_subs = table.find_all( \"table\" )\n\t\tprint( line )\t\t \n\t\tjisu = table_subs[ 1 ]\n\t\ttrs = jisu.find_all( \"tr\" )\n\t\tfor tr in trs :\n\t\t\ttds = tr.find_all( \"td\" )\n\t\t\tcol = 0 \n\t\t\tfor td in tds : \n\t\t\t\tprint( \"td = %s\" % td.string )\n\t\t\t\tworksheet.write(row, col, td.string )\n\t\t\t\tcol += 1\n\t\t\tpass\n\t\t\trow += 1\n\t\tpass \n\n\t\tjisu = table_subs[ 4 ]\n\t\ttrs = jisu.find_all( \"tr\" )\n\t\tfor tr in trs :\n\t\t\ttds = tr.find_all( \"td\" )\n\t\t\tcol = 0 \n\t\t\tfor td in tds : \n\t\t\t\tprint( \"td = %s\" % td.string )\n\t\t\t\tworksheet.write(row, col, td.string )\n\t\t\t\tcol += 1\n\t\t\tpass\n\t\t\trow += 1\n\t\tpass \n\t\tprint( line )\n\t\tworkbook.close() \n\tpass\n", "description": null, "category": "webscraping", "imports": ["import logging", "\t\timport xlsxwriter", "\t\t# import libraries", "\t\tfrom bs4 import BeautifulSoup", "\t\tfrom urllib.request import urlopen"]}], [{"term": "def", "name": "wait_at_start", "data": "def wait_at_start(driver):\n\twhile True:\n\t\ttry:\n\t\t\tWDW(driver,10).until(EC.presence_of_element_located(\n\t\t\t\t(By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]')))\n\t\t\tbreak\n\t\texcept:\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait as WDW", "from selenium.webdriver.support import expected_conditions as EC", "from multiprocessing import get_context", "import pandas as pd", "import numpy as np", "import time ", "import re"]}, {"term": "def", "name": "fetch_price", "data": "def fetch_price(driver):\n\tdate  = WDW(driver,5).until(EC.presence_of_element_located(\n\t\t\t(By.XPATH , '''/html/body/div[2]/div[1]/div[2]/div[1]/div/table/\n\t\t\t tr[1]/td[2]/div/div[2]/div[1]/div[1]/div[1]/div[1]/div[1]'''))).text\n\tclose = WDW(driver,5).until(EC.presence_of_element_located(\n\t\t\t(By.XPATH , '''/html/body/div[2]/div[6]/div/div[1]/div[1]/div[5]/\n\t\t\t div/div[2]/div[1]/div[2]/div[2]/div[4]/div[2]/span'''))).text\n\treturn date, close\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait as WDW", "from selenium.webdriver.support import expected_conditions as EC", "from multiprocessing import get_context", "import pandas as pd", "import numpy as np", "import time ", "import re"]}, {"term": "def", "name": "extract_date_string", "data": "def extract_date_string(txt):\n\treturn re.findall(r'\\((.*?)\\)',txt)[1].title()\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait as WDW", "from selenium.webdriver.support import expected_conditions as EC", "from multiprocessing import get_context", "import pandas as pd", "import numpy as np", "import time ", "import re"]}, {"term": "def", "name": "webscrape_fut_prc", "data": "def webscrape_fut_prc(comms):\n\t\n\t### Variables and Dataframe Setup, Initialize Webdriver ###\n\t\t\n\turl_comms = mp_dict[comms]\n\tcols = ['date',comms]\n\tfut_df = pd.DataFrame(columns=cols)\n\t\n  \n\tdriver = webdriver.Chrome(executable_path=exec_path,options=options)\n\t\n\t### Loop to Obtain YTD Price Dataset ###\n\tfor yr in yrs:\n\t\tfor mo in nymex_mo:\t   \n\t\t\tdriver.get(url_comms+mo+yr)\n\t\t\t\n\t\t\twait_at_start(driver)\n\t\t\t\n\t\t\tif (yr == yrs[0]) & (mo == nymex_mo[0]):\n\t\t\t\tWDW(driver,10).until(EC.presence_of_element_located(\n\t\t\t\t\t(By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]'))).click()\n\t\t\telse:\n\t\t\t\tWDW(driver,10).until(EC.presence_of_element_located(\n\t\t\t\t\t(By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]'))).click()\n\t\t\t\tWDW(driver,10).until(EC.presence_of_element_located(\n\t\t\t\t\t(By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]'))).click()\n\t\t\t\n\t\t\ttime.sleep(2)\n\t\t\ttemp_ls = [fetch_price(driver)]\n\t\t\ttemp_df = pd.DataFrame(temp_ls,columns=cols)\n\t\t\t\n\t\t\tfut_df = pd.concat([fut_df,temp_df],axis=0)\n\t\t\ttime.sleep(2)\n\t\t\t\n\t\t\t\n\t\n\t### Ensure No Duplicate Date (Same Date, Different Closing Price) ###\n\tfut_df['date'] = np.vectorize(extract_date_string)(fut_df['date'])\n\n\tfut_df = fut_df.drop_duplicates().reset_index(drop=True)\n\tfut_df['dup'] = fut_df.duplicated(subset='date')\n\tassert fut_df['dup'].unique().shape[0], \"Duplicate obs identified!\"\n\t\n\t### Clean Price Dataset ###\n\t\n\tfut_df['month'] = pd.to_datetime(fut_df['date']).dt.to_period('M')\n\tfut_df = fut_df[['month',comms]].set_index(['month'])\n\t\n\t### Close Webdriver ###\n\ttry:\n\t\tdriver.close()\n\texcept:\n\t\talert = WDW(driver, 3).until(EC.alert_is_present())\n\t\talert.accept()\n\t\tdriver.close()\n\n\t\n\treturn fut_df\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait as WDW", "from selenium.webdriver.support import expected_conditions as EC", "from multiprocessing import get_context", "import pandas as pd", "import numpy as np", "import time ", "import re"]}], [{"term": "def", "name": "checkBeforeScraping", "data": "def checkBeforeScraping(wikiText):\n\tfor i in ['journalName', 'plantName', 'sectionTitles']:\n\t\tassert(getattr(wikiText, i )), f\"{i} is still missing, cannot scrape wikipedia\"\n", "description": null, "category": "webscraping", "imports": ["2. import plant list name", "import csv", "from utils import *", "from collections import OrderedDict", "from wikiScraping import *", "# import sys"]}], [], [{"term": "class", "name": "WebscrapeConfig", "data": "class WebscrapeConfig(AppConfig):\n\tname = 'webscrape'\n", "description": null, "category": "webscraping", "imports": ["from django.apps import AppConfig"]}], [{"term": "class", "name": "ExtensionList", "data": "class ExtensionList(JSONAbleList):\n\t'''\n\trepresents a list of MediaWiki extensions\n\t'''\n\tdef __init__(self):\n\t\t'''\n\t\tconstructor\n\t\t'''\n\t\tsuper(ExtensionList, self).__init__('extensions', Extension)\n\t\n\t@staticmethod\n\tdef storeFilePrefix():\n\t\t'''\n\t\tget my storeFilePrefix\n\t\t\n\t\tReturns:\n\t\t\tstr: the path to where my stored files (e.g. JSON) should be kept\n\t\t'''\n\t\tscriptdir=os.path.dirname(os.path.realpath(__file__))\n\t\tresourcePath=os.path.realpath(f\"{scriptdir}/resources\")\n\t\tstoreFilePrefix=f\"{resourcePath}/extensions\"\n\t\treturn storeFilePrefix\n\t\n\t@classmethod\n\tdef fromSpecialVersion(cls,url:str,excludes=[\"skin\",\"editor\"],showHtml=False,debug=False):\n\t\t'''\n\t\tget an extension List from the given url\n\t\t\n\t\tArgs:\n\t\t\turl(str): the Special:Version MediaWiki page to read the information from\n\t\t\texclude (list): a list of types of extensions to exclude\n\t\t\tshowHtml(bool): True if the html code should be printed for debugging\n\t\t\tdebug(bool): True if debugging should be active\n\t\t\t\n\t\tReturns:\n\t\t\tExtensionList: an extension list derived from the url\n\t\t'''\n\t\twebscrape=WebScrape()\n\t\tsoup=webscrape.getSoup(url, showHtml=showHtml)\n\t\t\n\t\t# search for\n\t\t# \n\t\texttrs=soup.findAll(attrs={\"class\" : \"mw-version-ext\"})\n\t\textList=ExtensionList()\n\t\tfor exttr in exttrs:\n\t\t\tif showHtml:\n\t\t\t\tprint (exttr)\n\t\t\tdoExclude=False\n\t\t\tfor exclude in excludes:\n\t\t\t\tif f\"-{exclude}-\" in exttr.get(\"id\"):\n\t\t\t\t\tdoExclude=True\n\t\t\tif not doExclude:\n\t\t\t\text=Extension.fromSpecialVersionTR(exttr,debug=debug)\n\t\t\t\tif ext:\n\t\t\t\t\textList.extensions.append(ext)\n\t\treturn extList\n\t\t\n\t\t\n\t@classmethod \n\tdef restore(cls):\n\t\t'''\n\t\trestore \n\t\t'''\n\t\textList=ExtensionList()\n\t\textList.restoreFromJsonFile(ExtensionList.storeFilePrefix())\n\t\treturn extList\n\t\t\n\tdef save(self):\n\t\t'''\n\t\tsave the extension list\n\t\t'''\n\t\tsuper().storeToJsonFile(ExtensionList.storeFilePrefix())\n", "description": null, "category": "webscraping", "imports": ["from lodstorage.jsonable import JSONAble, JSONAbleList", "from lodstorage.lod import LOD", "from datetime import datetime", "from mwdocker.webscrape import WebScrape", "import os", "import urllib"]}, {"term": "class", "name": "Extension", "data": "class Extension(JSONAble):\n\t'''\n\trepresents a MediaWiki extension\n\t'''\n\t@classmethod\n\tdef getSamples(cls):\n\t\tsamplesLOD = [{\n\t\t\t\"name\": \"Admin Links\",\n\t\t\t\"extension\": \"AdminLinks\",\n\t\t\t\"url\": \"https://www.mediawiki.org/wiki/Extension:Admin_Links\",\n", "description": null, "category": "webscraping", "imports": ["from lodstorage.jsonable import JSONAble, JSONAbleList", "from lodstorage.lod import LOD", "from datetime import datetime", "from mwdocker.webscrape import WebScrape", "import os", "import urllib"]}, {"term": "def", "name": "ffromSpecialVersionTR", "data": "\tdef fromSpecialVersionTR(cls,exttr,debug=False):\n\t\t'''\n\t\tConstruct an extension from a beautifl soup TR tag\n\t\tderived from Special:Version\n\t\t\n\t\tArgs:\n\t\t\texttr: the beautiful soup TR tag\n\t\t\tdebug(bool): if True show debugging information\n\t\t'''\n\t\text=None\n\t\textNameTag=exttr.find(attrs={\"class\" : \"mw-version-ext-name\"})\n\t\textPurposeTag=exttr.find(attrs={\"class\" : \"mw-version-ext-description\"})\n\t\tif extNameTag:\n\t\t\text=Extension()\n\t\t\text.name=extNameTag.string\n\t\t\text.extension=ext.name.replace(\" \",\"\")\n\t\t\text.url=extNameTag.get(\"href\")\n\t\t\tif extPurposeTag and extPurposeTag.string:\n\t\t\t\text.purpose=extPurposeTag.string\n\t\t\text.getDetailsFromUrl(debug=debug)\n\t\treturn ext\n", "description": null, "category": "webscraping", "imports": ["from lodstorage.jsonable import JSONAble, JSONAbleList", "from lodstorage.lod import LOD", "from datetime import datetime", "from mwdocker.webscrape import WebScrape", "import os", "import urllib"]}, {"term": "def", "name": "f__init__", "data": "\tdef __init__(self):\n\t\t'''\n\t\tConstructor\n\t\t'''\n", "description": null, "category": "webscraping", "imports": ["from lodstorage.jsonable import JSONAble, JSONAbleList", "from lodstorage.lod import LOD", "from datetime import datetime", "from mwdocker.webscrape import WebScrape", "import os", "import urllib"]}, {"term": "def", "name": "f__str__", "data": "\tdef __str__(self):\n\t\ttext=\"\"\n\t\tdelim=\"\"\n\t\tsamples=self.getJsonTypeSamples()\n\t\tfor attr in LOD.getFields(samples):\n\t\t\tif hasattr(self, attr):\n\t\t\t\ttext+=f\"{delim}{attr}={self.__dict__[attr]}\"\n\t\t\t\tdelim=\"\\n\"\n\t\treturn text\n", "description": null, "category": "webscraping", "imports": ["from lodstorage.jsonable import JSONAble, JSONAbleList", "from lodstorage.lod import LOD", "from datetime import datetime", "from mwdocker.webscrape import WebScrape", "import os", "import urllib"]}, {"term": "def", "name": "fgetDetailsFromUrl", "data": "\tdef getDetailsFromUrl(self,showHtml=False,debug=False):\n\t\t'''\n\t\tget more details from my url\n\t\t'''\n\t\twebscrape=WebScrape()\n\t\ttry:\n\t\t\tsoup=webscrape.getSoup(self.url, showHtml=showHtml)\n\t\t\tfor link in soup.findAll('a',attrs={\"class\" : \"external text\"}):\n\t\t\t\tif (\"GitHub\" == link.string) or (\"git repository URL\") == link.string:\n\t\t\t\t\tself.giturl=link.get('href')\n\t\texcept urllib.error.HTTPError as herr:\n\t\t\tif debug:\n\t\t\t\tprint(f\"HTTPError {str(herr)} for {self.url}\")\n\t\t\n", "description": null, "category": "webscraping", "imports": ["from lodstorage.jsonable import JSONAble, JSONAbleList", "from lodstorage.lod import LOD", "from datetime import datetime", "from mwdocker.webscrape import WebScrape", "import os", "import urllib"]}, {"term": "def", "name": "fasWikiMarkup", "data": "\tdef asWikiMarkup(self):\n\t\t'''\n\t\treturn me as wiki Markup\n\t\t'''\n\t\tsamples=self.getJsonTypeSamples()\n\t\tnameValues=\"\"\n\t\tfor attr in LOD.getFields(samples):\t\t\n\t\t\tif hasattr(self, attr):\n\t\t\t\tnameValues+=f\"|{attr}={self.__dict__[attr]}\\n\"\n", "description": null, "category": "webscraping", "imports": ["from lodstorage.jsonable import JSONAble, JSONAbleList", "from lodstorage.lod import LOD", "from datetime import datetime", "from mwdocker.webscrape import WebScrape", "import os", "import urllib"]}, {"term": "def", "name": "fgetLocalSettingsLine", "data": "\tdef getLocalSettingsLine(self,mwShortVersion:str):\n\t\t'''\n\t\tget my local settings line\n\t\t\n\t\tArgs:\n\t\t\tmwShortVersion(str): the MediaWiki short version e.g. 127\n\t\t\n\t\tReturns:\n\t\t\tentry for LocalSettings\n\t\t'''\n\t\tlocalSettingsLine=f\"wfLoadExtension( '{self.extension}' );\"\n\t\tif hasattr(self,\"require_once_until\"):\n\t\t\tif self.require_once_until>=mwShortVersion:\n\t\t\t\tlocalSettingsLine=f'require_once \"$IP/extensions/{self.extension}/{self.extension}.php\";'\n\n\t\tif hasattr(self,\"localSettings\"):\n\t\t\tlocalSettingsLine+=f\"\\n  {self.localSettings}\"\n\t\treturn localSettingsLine\n", "description": null, "category": "webscraping", "imports": ["from lodstorage.jsonable import JSONAble, JSONAbleList", "from lodstorage.lod import LOD", "from datetime import datetime", "from mwdocker.webscrape import WebScrape", "import os", "import urllib"]}, {"term": "def", "name": "fasScript", "data": "\tdef asScript(self,branch=\"master\"):\n\t\t'''\n\t\treturn me as a shell Script command line list\n\t\t\n\t\tArgs:\n\t\t\tbranch(str): the branch to clone \n\t\t'''\n\t\tif hasattr(self, \"giturl\"):\n\t\t\tif \"//github.com/wikimedia/\" in self.giturl:\n\t\t\t\t# glone from the branch\n\t\t\t\treturn (f\"git clone {self.giturl} --single-branch --branch {branch} {self.extension}\")\n\t\t\telse:\t\n\t\t\t\treturn (f\"git clone {self.giturl} {self.extension}\")\n\t\telse:\n\t\t\ttext = \"# no installation script command specified\"\n\t\t\tif hasattr(self,\"composer\"):\n\t\t\t\ttext+=f\"\\n# installed with composer require {self.composer}\"\n\t\t\treturn text\n", "description": null, "category": "webscraping", "imports": ["from lodstorage.jsonable import JSONAble, JSONAbleList", "from lodstorage.lod import LOD", "from datetime import datetime", "from mwdocker.webscrape import WebScrape", "import os", "import urllib"]}], [], [{"term": "def", "name": "createMap", "data": "def createMap():\n\tmap = folium.Map(location=[40.7128, -74.0060],\n\t\t\t\t\tzoom_start=12,\n\t\t\t\t\ttiles='Stamen Terrain')\n\tabsPath, filename = os.path.split(os.path.abspath(__file__))\n\n\tfor i in range(100):\n\t\tx = reader[\"NUMBER\"][i] + \" \" + reader[\"STREET\"][i]\n\t\tcolor1 = webScrape(x, reader[\"POSTCODE\"][i])\n\t\tfolium.CircleMarker([reader[\"LAT\"][i],reader[\"LON\"][i]], radius = 5, popup=reader['STREET'][i], color = color1).add_to(map)\n\n\tmap.save(absPath + '/map.html')\n", "description": null, "category": "webscraping", "imports": ["import folium", "import pandas", "from selenium import webdriver", "import os", "from selenium.common.exceptions import NoSuchElementException"]}, {"term": "def", "name": "webScrape", "data": "def webScrape(addr, zip):\n\tabsPath, filename = os.path.split(os.path.abspath(__file__))\n\tbrowser = webdriver.Chrome(executable_path=absPath + '/chromedriver')\n\turl = 'https://datawarehouse.hrsa.gov/tools/analyzers/geo/ShortageArea.aspx'\n\tbrowser.get(url)\n\taddress = browser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$tbxAddress\")\n\tcity = browser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$tbxCity\")\n\tpostalCode = browser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$tbxZIP\")\n\n\taddress.send_keys(str(addr))\n\tcity.send_keys(\"New York\")\n\tpostalCode.send_keys(str(zip))\n\tbrowser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$lstState\").send_keys(\"New York\")\n\tbrowser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$btnDrill\").click()\n\ttry:\n\t\ta = browser.find_element_by_id(\"redesignDiv\")\n\t\ta = str(a)\n\t\tif a.count(\"No\") == 0:\n\t\t\tbrowser.close()\n\t\t\treturn \"#18ff01\"\n\t\telif a.count(\"No\") == 1:\n\t\t\tbrowser.close()\n\t\t\treturn \"#ffff00\"\n\t\telif a.count(\"No\") == 2:\n\t\t\tbrowser.close()\n\t\t\treturn \"#ffa200\"\n\t\telif a.count(\"No\") == 3:\n\t\t\tbrowser.close()\n\t\t\treturn \"#ffa1b2\"\n\t\telif a.count(\"No\") == 4:\n\t\t\tbrowser.close()\n\t\t\treturn \"#ff2600\"\n\texcept NoSuchElementException:\n\t\tbrowser.close()\n\t\tpass\n\n", "description": null, "category": "webscraping", "imports": ["import folium", "import pandas", "from selenium import webdriver", "import os", "from selenium.common.exceptions import NoSuchElementException"]}], [], [{"term": "def", "name": "webscrape", "data": "def webscrape(domain):\n\tweb_scraper.scrape_web(domain)\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "tokenize", "data": "def tokenize(domain):\n\ttokenize_content.tokenize(domain,config.get_all_data(domain),\"Before Stemming\")\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "stem", "data": "def stem(domain):\n\ttokens_data, len_dist = tokenize_content.tokenize(domain,config.get_all_data(domain),\"After Stemming\",False)\n\tstemmer.stem(domain,tokens_data,len_dist)\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "sentence_segment", "data": "def sentence_segment(domain):\n\tsentence_segmentation.segment_sentence(domain,config.get_all_data(domain))\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "pos_tag", "data": "def pos_tag(domain):\n\tpos_tagger.pos_tag(domain,sentence_segmentation.segment_sentence(domain,config.get_all_data(domain),True))\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "main", "data": "def main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-d\", required = True, help=\"Domain - [domain-1, domain-2, domain-3]\")\n\tparser.add_argument(\"-t\", \"-task\" , required = True, help=\"Task to Run - [webscrape,tokenize,stem,sentence_segment,pos_tag]\")\n\n\targs = vars(parser.parse_args())\n\tglobals()[args['t']](args[\"d\"])\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(domain):\n\tweb_scraper.scrape_web(domain)\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "tokenize", "data": "def tokenize(domain):\n\ttokenize_content.tokenize(domain,config.get_all_data(domain),\"Before Stemming\")\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "stem", "data": "def stem(domain):\n\ttokens_data, len_dist = tokenize_content.tokenize(domain,config.get_all_data(domain),\"After Stemming\",False)\n\tstemmer.stem(domain,tokens_data,len_dist)\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "sentence_segment", "data": "def sentence_segment(domain):\n\tsentence_segmentation.segment_sentence(domain,config.get_all_data(domain))\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "pos_tag", "data": "def pos_tag(domain):\n\tpos_tagger.pos_tag(domain,sentence_segmentation.segment_sentence(domain,config.get_all_data(domain),True))\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}, {"term": "def", "name": "main", "data": "def main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-d\", required = True, help=\"Domain - [domain-1, domain-2, domain-3]\")\n\tparser.add_argument(\"-t\", \"-task\" , required = True, help=\"Task to Run - [webscrape,tokenize,stem,sentence_segment,pos_tag]\")\n\n\targs = vars(parser.parse_args())\n\tglobals()[args['t']](args[\"d\"])\n", "description": null, "category": "webscraping", "imports": ["import web_scraper", "import tokenize_content", "import content_extractor", "import sentence_segmentation", "import pos_tagger", "import stemmer", "import config", "import os", "import sys", "import argparse"]}], [], [{"term": "def", "name": "run", "data": "def run():\n\tfrom urllib.request import urlopen\n\tfrom bs4 import BeautifulSoup\n\n\t##### Link collector to send crawler to iteratively\n\n\timport requests\n\n\tbaseurl = 'https://torontosun.com'\n\n\theaders = {\n\t\t'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.128 Safari/537.36'\n\t}\n\n\tcolumnlinks = []\n\n\t# Pulling the urls from the last 5 pages worth of columns from here: https://torontosun.com/category/opinion/columnists/?from=1\n\t\t\n\tfor x in range(900,925,25):\n\t\tr = requests.get(f'https://torontosun.com/category/opinion/columnists/?from={x}')\n\t\tsoup2 = BeautifulSoup(r.content, 'lxml')\n\t\tcolumnlist = soup2.find_all('div', class_='article-card__details')\n\t\tfor item in columnlist:\n\t\t\t\tfor link in item.find_all('a', href=True):\n\t\t\t\t\tcolumnlinks.append(baseurl + link['href'])\n\n\tprint('Collecting newscolumn links. Beep Boop.')\n\n\tfPostLink = 'https://torontosun.com/category/national/'\n\tgPostLink = 'https://torontosun.com/category/news/'\n\thPostLink = 'https://torontosun.com/category/local-news/'\n\tiPostLink = 'https://torontosun.com/category/news/provincial/'\n\tjPostLink = 'https://torontosun.com/category/provincial/'\n\tkPostLink = 'https://torontosun.com/category/world/'\n\tlPostLink = 'https://torontosun.com/category/sports/'\n\tmPostLink = 'https://torontosun.com/category/toronto-maple-leafs/'\n\n\tfor elem in list(columnlinks):\n\t\tif elem == \"https://torontosun.com/category/columnists/\":\n\t\t\tcolumnlinks.remove(elem)\n\t\telif elem == \"https://torontosun.com/category/opinion/\":\n\t\t\tcolumnlinks.remove(elem)\n\t\telif fPostLink in elem:\t\t\n\t\t\tcolumnlinks.remove(elem)\n\t\telif gPostLink in elem:\t\t\n\t\t\tcolumnlinks.remove(elem)\n\t\telif hPostLink in elem:\t\t\n\t\t\tcolumnlinks.remove(elem)\n\t\telif iPostLink in elem:\t\t\n\t\t\tcolumnlinks.remove(elem)\n\t\telif jPostLink in elem:\t\t\n\t\t\tcolumnlinks.remove(elem)\n\t\telif kPostLink in elem:\t\t\n\t\t\tcolumnlinks.remove(elem)\n\t\telif lPostLink in elem:\t\t\n\t\t\tcolumnlinks.remove(elem)\n\t\telif mPostLink in elem:\t\t\n\t\t\tcolumnlinks.remove(elem)\n\n\tprint('links cleaned, boop boop')\t\t\n\n\tlinkarray = columnlinks\n\n\tcount = 0\n\n\tfor i in range(0, len(linkarray)):\n\t\turl = linkarray[i] # this variable imports into Database under newsColumns\n\t\thtml = urlopen(url)\n\t\tsoup = BeautifulSoup(html.read(), 'html.parser')\n\t##### CLEAN WEBSCRAPE OF ALL PERTINENT INFORMATION ######\n\t\tps = soup.find_all('p')\n\t\tbodyarray = []\n\t\tbodytext = \"\"\n\n\t\t#   collect all p tags then append all text items to array - then add all items into bodytext string var \n\t\tfor p in ps:\n\t\t\tptext = p.get_text()\n\t\t\tbodyarray.append(ptext)\n\n\t\tfor item in bodyarray:\n\t\t\tbodytext += item\n\t\t\t\n\t\t#   headline webscrape\n\t\theadline = soup.find('h1').get_text()\n\n\t\t#   publish date webscrape\n\t\ttry:\n\t\t\tpubdatescrape = soup.find('span',class_='published-date__since').get_text()\n\t\texcept AttributeError:\n\t\t\tpubdate = 'Jan 1, 1991'\n\t\telse: \n\t\t\tpubdate = pubdatescrape\n\n\n\t\t#   author webscrape - if null do not add to the thing (this doesn't work because the thing is already pulled and transferred)\n\t\ttry:\n\t\t\tauthor=soup.find('span',class_='published-by__author').get_text()\n\t\texcept AttributeError:\n\t\t\tprint('Error with following article' + headline + '\\nskipping link...')\n\t\t\tpass\n\t\t\t'''if headline == 'ELECTION 2021':\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tprint(headline)\n\t\t\t\tauthor= str(input('Enter author name for' + headline + '\\n' + url +': '))\n\t\t\t'''\n\t\ttry:\n\t\t\tsplitauthor = author.split(' ', 1)\n\t\t\tauthorfname = splitauthor[0]\n\t\t\tauthorlnamewhole = splitauthor[1].split(' ', 1)\n\t\t\tauthorlname = authorlnamewhole[0]\n\t\texcept:\n\t\t\tauthorfname = author\n\t\t\tauthorlname = author\n\n\t\t#number the entry\n\t\tcount += 1\n\t\tprint(\"==== Column #\",count)\n\t\t\t\t\t\t\t\t\t\n\t\tprint(\"=====UPLOADING COLUMN PUBLISHED:====== \", pubdate,'\\n', authorfname,'\\n', authorlname,'\\n', headline,)\n\n\t\t######\t  BEGIN INSERT DATA INTO DATABASE\t #####\n\n\t\timport mysql.connector\n\t\timport re\n\n\t\t#DB information to connect to localhost\n\n\t\tmydb = mysql.connector.connect(\n\t\thost=\"localhost\",\n\t\tport=\"3306\",\n\t\tuser=\"root\",\n\t\tpassword=\"root\",\n\t\tdatabase =\"columnwatcher\"\n\t\t)\n\n\t\t#get everything from the columnist table\n\n\t\tmycursor = mydb.cursor()\n\n\t\tmycursor.execute(\"SELECT * FROM columnists\")\n\t\tresult = mycursor.fetchall()\n\n\t\t#check if the first and last name of the columnist matches any of the existing columnists, columnist id = \n\n\t\t#give columnist id to existing columnist, attribute id to new columnist\n\n\t\tfor x in range(0,len(result)):\n\t\t\tif authorfname in result[x] and authorlname in result[x]:\n\t\t\t\tentryColumnistID = x+1\n\t\t\t\tnewColumnist = False\n\t\t\t\tbreak\n\t\t\t\t\n\t\t\telse: \n\t\t\t\tentryColumnistID = len(result)+1\n\t\t\t\tnewColumnist = True\n\n\t\t#if the name of the new columnist does not exist in the columnist table, add it to the table\n\t\tif newColumnist == True:\n\t\t\tprint('name does not exist in record. Creating new Columnist ID:')\n\t\t\tsqlColumnists = \"INSERT INTO columnists (first_name, last_name) VALUES (%s, %s)\"\n\t\t\tvalColumnists = (authorfname, authorlname)\n\n\t\t\tmycursor.execute(sqlColumnists, valColumnists)\n\t\t\tprint(\"Adding to database: \", authorfname, authorlname)\n\t\t\tmydb.commit()\n\t\telse:\n\t\t\tprint(authorfname, authorlname,\"exists in record under id#: \", entryColumnistID)\n\n\t\t##-- insert article --##\n\n\t\t#columnist_id - auto incremented\n\t\t#paper_id - Toronto Sun = 5\n\t\t#headline \n\t\t#publishdate - PUBDATE\n\t\t#body_text\n\t\t#url\n\n\t\tmycursor2 = mydb.cursor()\n\t\tmycursor2.execute(\"SELECT headline FROM newscolumns\")\n\t\tcolresult = mycursor2.fetchall()\n\n\t\t#placeholder and specific vars\n\t\t#entryColumnistID\n\t\tpaper_id = 5 #TorontoSun\n\t\tnewHeadline = headline\n\n\t\timport datetime\n\t\t#switch PUBLISH DATE to YYYY-mm-dd\n\t\tf = '%b %d, %Y'\n\t\tentryDateTime = datetime.datetime.strptime(pubdate, f)\n\n\n\t\t#try 2 on newscolumn insert\n\n\t\tfor x in range(0,len(colresult)):\n\t\t\tif newHeadline in colresult[x]:\t\t\n\t\t\t\tnewColumn = False\n\t\t\t\tbreak\n\t\t\t\t\n\t\t\telse: \n\t\t\t\tnewColumn = True\n\n\n\n\t\t\n\n\n\t\t#if this headline does not already exist, add it to my database please\n\t\tif newColumn == True:\n\t\t\tprint('headline does not exist in record. Creating new Column entry')\n\t\t\tsqlNewsColumns = \"INSERT INTO newscolumns (columnist_id, paper_id, headline, body_text, url, publishdate) VALUES (%s, %s, %s, %s, %s, %s)\"\n\t\t\tvalNewsColumns = (entryColumnistID, paper_id, newHeadline, bodytext, url, entryDateTime)\n\t\t\tmycursor2.execute(sqlNewsColumns, valNewsColumns)\n\t\t\tmydb.commit()\n\t\t\tprint('new entry', newHeadline, 'added to the database')\n\t\t\t\n\t\telse:\n\t\t\tprint(\"headline already in database, will not add duplicate:\", newHeadline)\n\n\n\tprint('===TORONTO SUN SCRAPE COMPLETE===')\t\n\n\n", "description": null, "category": "webscraping", "imports": ["\tfrom urllib.request import urlopen", "\tfrom bs4 import BeautifulSoup", "\timport requests", "\t\turl = linkarray[i] # this variable imports into Database under newsColumns", "\t\timport mysql.connector", "\t\timport re", "\t\timport datetime"]}], [], [], [], [{"term": "def", "name": "run", "data": "def run(job_input: IJobInput):\n\t\"\"\"\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t\"\"\"\n\n\tlog.info(f\"Starting job step {__name__}\")\n\n\t# Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n\t# If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n\tprops = job_input.get_all_properties()\n\tif \"last_date_amazon\" in props:\n\t\tpass\n\telse:\n\t\t# <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n\t# Initialize variables\n\ti = 1\n\trev_result = []\n\tdate_result = []\n\t# Date to start iterating from = current date (in the format \"2020-01-01\")\n\tdate = datetime.now().strftime(\"%Y-%m-%d\")\n\n\t# Go through the review pages and scrape reviews\n\twhile date > props[\"last_date_amazon\"]:\n\t\tlog.info(f'Rendering page {i}...')\n\t\t# Parameterize the URL to iterate over the pages\n\t\turl = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n\t\t\tviewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n\t\t# Get HTML code into a BeautifulSoup object\n\t\tsoup = webscrape.html_code(url)\n\t\t# Get the reviews and dates for the current page\n\t\trev_page = webscrape.cus_rev(soup)\n\t\tdate_page = webscrape.rev_date(soup)[2:]\n\n\t\t# Append reviews text into a list removing the empty reviews\n\t\tfor j in rev_page:\n\t\t\tif j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\trev_result.append(j.strip())\n\t\tlog.info(len(rev_result))\n\n\t\t# Append review dates into a list by extracting the date from text\n\t\tfor d in date_page:\n\t\t\tif d.strip() == \"\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n\t\t\t\t# datefinder package extracts the date from the text and converts it to datetime object\n\t\t\t\tdate_match = datefinder.find_dates(d)\n\t\t\t\tfor date in date_match:\n\t\t\t\t\t# Convert to string\n\t\t\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tdate_result.append(date)\n\t\tlog.info(len(date_result))\n\n\t\t# In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n\t\twhile len(rev_result) < len(date_result):\n\t\t\tdate_result.pop(-1)\n\n\t\t# Go to the next page\n\t\ti += 1\n\n\t# Create a pandas dataframe with the review text and dates\n\tdf = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n\t# Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n\t# page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n\tdf = df[df['Date'] > props[\"last_date_amazon\"]]\n\t# Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n\tfor i in range(0, len(df)):\n\t\t# Go through each review and clean it if needed\n\t\tdf.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n\tlog.info(f\"Shape of the review dataset: {df.shape}\")\n\n\t# Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n\tif len(df) > 0:\n\t\tjob_input.send_tabular_data_for_ingestion(\n\t\t\trows=, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n\t\t\tcolumn_names=, # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n\t\t\tdestination_table=\"\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n\t\t)\n\t\t# Reset the last_date property value to the latest date in the amazon source db table\n\t\tprops[\"last_date_amazon\"] = max(df['Date'])\n\t\tjob_input.set_all_properties(props)\n\n\tlog.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n\t# Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n\ttime.sleep(10)\n", "description": "\n\tScrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n\tand ingest them into a cloud Trino database.\n\t", "category": "webscraping", "imports": ["import pandas as pd", "import logging", "import datefinder", "from datetime import datetime", "import time", "import webscrape", "from vdk.api.job_input import IJobInput"]}], [{"term": "def", "name": "view_tv_details", "data": "def view_tv_details(tv_link, season_number):\n\t#clears the command line output\n\tclear()\n\t#if the link is the season of a show\n\tif (season_number != 0):\n\t\treq = Request(tv_link + \"/season-\" + str(season_number), headers={'User-Agent': 'Mozilla/5.0'})\n\t\n\t#if the link is the series overview\n\telse:\n\t\treq = Request(tv_link, headers={'User-Agent': 'Mozilla/5.0'})\n\t\n\tpage = urlopen(req).read()\n\thtml_tv = page.decode(\"utf-8\")\n\t\n\t\n\t#create the parser and parse each element\n\tparser = tv_parser(html_tv)\n\tname = parser.get_name()\n\tcast = parser.get_cast()\n\tsummary = parser.get_summary()\n\tsummary = summary.replace('', '\\n')\n\tcredit = parser.get_credit()\n\tseasons = parser.get_seasons()\n\tnum_seasons = parser.get_number_seasons()\n\t#print the results to the command line\n\tprint(name)\n\tprint(cast + \"\\n\")\n\t\n\t#create the formatting for the summary\n\tbody = '\\n\\n'.join(['\\n'.join(textwrap.wrap(line, 100,\n\t\t\t\t break_long_words=False, replace_whitespace=False))\n\t\t\t\t for line in summary.splitlines() if line.strip() != ''])\n\t\n\tprint(body)\n\tprint(\"\\n\" + credit + \"\\n\\n\")\n\tprint(seasons + \"\\n\") \n\t\n\t\n\t#prompt the user to view the reviews of the show\n\tanswer = input(\"Type r to view user reviews. Type s to change to a different season. Type anything else to go back to results page\\n\")\n\t\n\tif (answer == \"s\"):\n\t\tseason = input(\"Enter the number that corresponds with the season you want to view. Invalid numbers default to series overview\\n\")\n\t\tif (season.isdigit() == False):\n\t\t\tseason = \"0\"\n\t\t\n\t\tseason = int(season)\n\t\t\n\t\tif (season > num_seasons):\n\t\t\tseason = 0\n\t\tview_tv_details(tv_link, season)\n\t\t\n\t\treturn\n\t\n\t#while the answer s still user reviews\n\twhile (answer == \"r\"):\n\t\t\n\t\t\n\t\ttv_user_reviews.view_tv_user_reviews(tv_link + \"/user-reviews\")\n\t\t\n\t\t#clears the command line output\n\t\tclear()\n\t\t\n\t\t#print the results to the command line\n\t\tprint(name)\n\t\tprint(cast + \"\\n\")\n\t\tprint(body)\n\t\tprint(\"\\n\" + credit + \"\\n\\n\")\n\t\t\n\t\t\n\t\t#prompt the user to view the reviews of the show\n\t\tanswer = input(\"Type r to view user reviews. Type s to change to a different season. Type anything else to go back to results page\\n\")\n\t\n\t\tif (answer == \"s\"):\n\t\t\tseason = input(\"Enter the number that corresponds with the season you want to view. Invalid numbers default to series overview\\n\")\n\t\t\t\n\t\t\tif (season.isdigit() == False):\n\t\t\t\tseason = \"0\"\n\t\t\n\t\t\tseason = int(season)\n\t\t\n\t\t\tif (season > num_seasons):\n\t\t\t\tseason = 0\n\t\t\t\n\t\t\tview_tv_details(tv_link, season)\n\t\t\n\t\t\treturn\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen, Request", "import textwrap", "from webscrape.searchresults.categorydetails.tvdetail.tvparse.tv_parser import tv_parser", "from webscrape.searchresults.categorydetails.tvdetail.tvdisplay import tv_user_reviews", "from webscrape import clear"]}], [{"term": "class", "name": "classAllJobsPage:", "data": "class AllJobsPage:\n\t\n\tdef __init__(self, page_content):\n\t\tself.soup = BeautifulSoup(page_content, 'html.parser')\n\n\t@property\n\tdef jobs(self):\n\t\t\"\"\"returns all the jobs in a page\"\"\"\n\t\treturn [JobParser(e) for e in self.soup.select(ALLJobLocators.JOBS)]\n\n\t@property\n\tdef page_count(self):\n\t\t\"\"\"RETURNS THE NUMBER OF MAXIMUM PAGES\"\"\"\n\t\tcontent = self.soup.select(ALLJobLocators.PAGINATION)\n\t\tpage_num = int(content[-2].string)\n\t\treturn page_num\n", "description": "returns all the jobs in a page", "category": "webscraping", "imports": ["from webScrape.scraping_jobs.parsers.job_parser import JobParser", "from bs4 import BeautifulSoup ", "from webScrape.scraping_jobs.locators.all_jobs_locator import ALLJobLocators"]}], [{"term": "def", "name": "open_file", "data": "def open_file(file):\n\t\"\"\"\n\t:param file: File to read into a dataframe\n\t:return: Dataframe\n\t\"\"\"\n\tdf = pd.read_csv(file, low_memory=False)\n\treturn df\n\n", "description": "\n\t:param file: File to read into a dataframe\n\t:return: Dataframe\n\t", "category": "webscraping", "imports": ["import os", "import time", "import pandas as pd", "from selenium import webdriver", "import selenium.common.exceptions", "from selenium.webdriver.common.by import By"]}, {"term": "def", "name": "selenium_setup", "data": "def selenium_setup():\n\t\"\"\"\n\t:return: Options setup to be used for selenium\n\t\"\"\"\n\t# Configures selenium\n\toptions = webdriver.ChromeOptions()\n\tprofile = {\"plugins.plugins_list\": [{\"enabled\": False, \"name\": \"Chrome PDF Viewer\"}],  # Disable Chrome's PDF Viewer\n\t\t\t   \"download.default_directory\": PYCHARM_DIR, \"download.extensions_to_open\": \"applications/pdf\"}\n\toptions.add_experimental_option(\"prefs\", profile)\n\treturn options\n\n", "description": "\n\t:return: Options setup to be used for selenium\n\t", "category": "webscraping", "imports": ["import os", "import time", "import pandas as pd", "from selenium import webdriver", "import selenium.common.exceptions", "from selenium.webdriver.common.by import By"]}, {"term": "def", "name": "un_webscrape", "data": "def un_webscrape(df, options):\n\t\"\"\"\n\t:param df: dataframe with links\n\t:param options: options from selenium setup func\n\t:return: none\n\tDownloads all the pdfs from the links in the dataframe\n\t\"\"\"\n\tcounter = 0\n\tpdf_list = []\n\terror_list = []\n\tfor i in range(len(df.index)):\n\t\tdriver = webdriver.Chrome(executable_path= SERIUM_CHROME_PATH, chrome_options=options)\n\t\tlink = df.iloc[i][9]\n\t\tdriver.get(link)\n\t\ttry:\n\t\t\t# Clicks the buttons from the UN webpage to download the files\n\t\t\tl = driver.find_element(By.XPATH, '//*[@id=\"details-collapse\"]/div[5]/span[2]/a')\n\t\t\tl.click()\n\t\t\tl = driver.find_element(By.XPATH, '//*[@id=\"record-files-list\"]/tbody/tr[2]/td[1]/a')\n\t\t\tl.click()\n\t\t\tnum_files = len([f for f in os.listdir('.\\\\DraftResolutions') if f.endswith('.pdf')])\n\t\t\tpdf_list.append((driver.find_element(By.XPATH, '//*[@id=\"record-files-list\"]/tbody/tr[2]/td[2]')).text)\n\t\t\ttime.sleep(.5)\n\t\t\t# Checks if the pdf has been added to the target location, if not, waits and tries again\n\t\t\twhile num_files == len([f for f in os.listdir('.\\\\DraftResolutions') if f.endswith('.pdf')]):\n\t\t\t\ttime.sleep(.1)\n\t\texcept selenium.common.exceptions.NoSuchElementException:\n\t\t\tpdf_list.append('Error')\n\t\t\terror_list.append(counter)\n\t\tprint(counter)\n\t\tcounter += 1\n\t\tdriver.quit()\n\treturn pdf_list, error_list\n\n", "description": "\n\t:param df: dataframe with links\n\t:param options: options from selenium setup func\n\t:return: none\n\tDownloads all the pdfs from the links in the dataframe\n\t", "category": "webscraping", "imports": ["import os", "import time", "import pandas as pd", "from selenium import webdriver", "import selenium.common.exceptions", "from selenium.webdriver.common.by import By"]}, {"term": "def", "name": "write_to_file", "data": "def write_to_file(pdf_list, error_list):\n\t\"\"\"\n\t:param pdf_list: list of pdfs\n\t:param error_list: list of positions of errors\n\t:return: None\n\tWrites pdf_lists and error_lists to files for later use\n\t\"\"\"\n\twith open('PDFList.txt', 'w') as infile:\n\t\tfor item in pdf_list:\n\t\t\tinfile.write(str(item) + '\\n')\n\twith open('ErrorList.txt', 'w') as infile:\n\t\tfor item in error_list:\n\t\t\tinfile.write(str(item) + '\\n')\n\n", "description": "\n\t:param pdf_list: list of pdfs\n\t:param error_list: list of positions of errors\n\t:return: None\n\tWrites pdf_lists and error_lists to files for later use\n\t", "category": "webscraping", "imports": ["import os", "import time", "import pandas as pd", "from selenium import webdriver", "import selenium.common.exceptions", "from selenium.webdriver.common.by import By"]}, {"term": "def", "name": "main", "data": "def main():\n\tdf = open_file('UN DATA.csv')\n\toptions = selenium_setup()\n\tpdf_list, error_list = un_webscrape(df, options)\n\twrite_to_file(pdf_list, error_list)\n\tprint('Complete!')\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import time", "import pandas as pd", "from selenium import webdriver", "import selenium.common.exceptions", "from selenium.webdriver.common.by import By"]}], [{"term": "def", "name": "insert_edge", "data": "def insert_edge(response, collection):\n\t\"\"\"Insert records into Mongo database.\n\n\tArgs:\n\t\tresponse (request response object): Response from request.get('url')\n\t\tcollection (pymongo.Collection): Collection object for record insertion.\n\t\"\"\"\n\tedges = response.json()['data']['hashtag']['edge_hashtag_to_media']['edges']\n\tfor edge in edges:\n\t\tcollection.insert_one(edge)\n", "description": "Insert records into Mongo database.\n\n\tArgs:\n\t\tresponse (request response object): Response from request.get('url')\n\t\tcollection (pymongo.Collection): Collection object for record insertion.\n\t", "category": "webscraping", "imports": ["import numpy as np", "from selenium import webdriver", "from bs4 import BeautifulSoup", "import time", "import json", "import requests", "from pymongo import MongoClient", "from webscrape_app.celery_controller import app", "from webscrape_app.webscrape_util.scrape_util import add_new_line, load_last_line, setup_mongo_client"]}, {"term": "def", "name": "get_page_info", "data": "def get_page_info(response):\n\t\"\"\"Find and save page_info from response json.\n\n\tArgs:\n\t\tresponse (request response object): Response from request.get('url')\n\n\tReturns:\n\t\tpage_info (dict): Dictionary from response json.\n\t\t\tKeys: 'has_next_page' (bool) and 'end_cursor' (unicode)\n\t\"\"\"\n\tpage_info = response.json()['data']['hashtag']['edge_hashtag_to_media']['page_info']\n\treturn page_info\n\n", "description": "Find and save page_info from response json.\n\n\tArgs:\n\t\tresponse (request response object): Response from request.get('url')\n\n\tReturns:\n\t\tpage_info (dict): Dictionary from response json.\n\t\t\tKeys: 'has_next_page' (bool) and 'end_cursor' (unicode)\n\t", "category": "webscraping", "imports": ["import numpy as np", "from selenium import webdriver", "from bs4 import BeautifulSoup", "import time", "import json", "import requests", "from pymongo import MongoClient", "from webscrape_app.celery_controller import app", "from webscrape_app.webscrape_util.scrape_util import add_new_line, load_last_line, setup_mongo_client"]}, {"term": "def", "name": "instascrape", "data": "def instascrape(page_info_filepath, num_requests, collection_name):\n\t\"\"\"\n\tScrape instagram hashtag search\n\n\tArgs:\n\t\tpage_info_filepath (str): Filepath to text file with page_info dicts\n\t\tnum_requests (int): Number of pages to be scraped\n\n\tAction: saves influencer node information to pymongo database\n\n\tOutput: None\n\t\"\"\"\n\n\tclient, collection = setup_mongo_client('instascrape2', collection_name)\n\n\n\tpage_info = \"\"\n\tresponse = requests.get(\"https://www.instagram.com/graphql/query/?query_id=17875800862117404&variables={{%22tag_name%22:%22tennis%22,%22first%22:{}}}\"\n\t\t\t\t\t\t\t.format('12'))\n\n\tif response.status_code == 200:\n\t\tprint(response)\n\t\tinsert_edge(response, collection)\n\t\tpage_info = get_page_info(response)\n\t\tadd_new_line(page_info, page_info_filepath)\n\n\tpage_info = load_last_line(page_info_filepath)\n\tbase_url_search = \"https://www.instagram.com/graphql/query/?query_id=17875800862117404&variables={{%22tag_name%22:%22tennis%22,%22first%22:{},%22after%22:%22{}%22}}\"\n\n\tfor i in range(num_requests):\n\t\tprint(page_info['end_cursor'])\n\n\t\tif page_info['has_next_page']:\n\t\t\tresponse = requests.get(base_url_search.format('11', str(page_info['end_cursor'])))\n\n\t\t\tif response.status_code == 200:\n\t\t\t\tinsert_edge(response, collection)\n\t\t\t\tpage_info = get_page_info(response)\n\t\t\t\tadd_new_line(page_info, page_info_filepath)\n\n\t\t\telse:\n\t\t\t\tprint(\"Status Code = \" + str(response.status_code))\n\t\t\t\treturn None\n\n\t\ttime.sleep(np.random.uniform(15, 45))\n\t\tprint(\"Finished scraping {} pages of {}\".format(i + 1, num_requests))\n\n\tclient.close()\n\n\tprint(\"\\n Finished scraping {} pages of 12 influencers each\".format(num_requests))\n", "description": "\n\tScrape instagram hashtag search\n\n\tArgs:\n\t\tpage_info_filepath (str): Filepath to text file with page_info dicts\n\t\tnum_requests (int): Number of pages to be scraped\n\n\tAction: saves influencer node information to pymongo database\n\n\tOutput: None\n\t", "category": "webscraping", "imports": ["import numpy as np", "from selenium import webdriver", "from bs4 import BeautifulSoup", "import time", "import json", "import requests", "from pymongo import MongoClient", "from webscrape_app.celery_controller import app", "from webscrape_app.webscrape_util.scrape_util import add_new_line, load_last_line, setup_mongo_client"]}], [{"term": "def", "name": "parse_basic_parcel_data", "data": "def parse_basic_parcel_data(line):\n\tsanitized_line = line.replace(\"'}\", '\"}').replace(\"{'\", '{\"').replace(\"': '\", '\": \"').replace(\"', '\", '\", \"').\\\n\t\treplace(\"\\\", '\", '\", \"').replace(\"\\\": '\", '\": \"').replace(\"': \\\"\", '\": \"').replace(\"\\\": \\\"\\\"\", \"\\\": \\\"\").\\\n\t\treplace(\"\\\"\\\", \\\"\", \"\\\", \\\"\").replace(\"\\\": \\\"}\", \"\\\": \\\"\\\"}\")\n\t#print(line)\n\t#print(sanitized_line)\n\t#basic_parcel_data = json.loads(line)\n\tbasic_parcel_data = json.loads(sanitized_line)\n\treturn basic_parcel_data\n\n", "description": null, "category": "webscraping", "imports": ["import json", "import os"]}, {"term": "def", "name": "create_dictionary_of_basic_parcel_data", "data": "def create_dictionary_of_basic_parcel_data(parcel_data_path=\"/home/dave/PycharmProjects/FultonCountyBallotScanner/\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"canvass/address_projects/scrap/\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"property_webscrape_parcel_basics_all_except_8.txt\"):\n\tparcel_data_file = open(parcel_data_path, 'r')\n\tdictionary_of_basic_parcel_data = {}\n\tfor line_of_parcel_data in parcel_data_file.readlines():\n\t\tparcel_data = parse_basic_parcel_data(line_of_parcel_data)\n\t\tparcel_id = parcel_data[\"Parcel ID\"]\n\t\taddress = parcel_data[\"Parcel Address\"]\n\t\tdictionary_of_basic_parcel_data[parcel_id] = address\n\n\tparcel_data_file.close()\n\n\treturn dictionary_of_basic_parcel_data\n\n", "description": null, "category": "webscraping", "imports": ["import json", "import os"]}, {"term": "def", "name": "update_addresses_on_detailed_parcel", "data": "def update_addresses_on_detailed_parcel(old_datafile=\"/home/dave/Documents/Election Fraud/canvass/property_data/\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Fulton_property_webscrape_v1.0.txt\",\n\t\t\t\t\t\t\t\t\t\t   new_datafile=\"/home/dave/Documents/Election Fraud/canvass/property_data/\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Fulton_property_webscrape_v1.1.txt\"):\n\told_file_with_detailed_parcel_data = open(old_datafile, 'r')\n\tif os.path.exists(new_datafile):\n\t\tos.remove(new_datafile)\n\tnew_file_with_detailed_parcel_data = open(new_datafile, 'a')\n\tdictionary_of_basic_parcel_data = create_dictionary_of_basic_parcel_data()\n\tnumber_of_adjusted_addresses = 0\n\tnumber_of_unadjusted_addresses = 0\n\tfor old_datafile_line in old_file_with_detailed_parcel_data.readlines():\n\t\tparcel_data = json.loads(old_datafile_line)\n\t\t#print(parcel_data)\n\t\tparcel_id = parcel_data[\"Parcel ID\"]\n\t\tparcel_id = parcel_id.replace(\"  LL\", \" LL\")\n\t\ttry:\n\t\t\tnew_parcel_address = dictionary_of_basic_parcel_data[parcel_id]\n\t\t\tnumber_of_adjusted_addresses += 1\n\t\texcept KeyError:\n\t\t\t#print(parcel_id)\n\t\t\ttry:\n\t\t\t\tnew_parcel_address = parcel_data[\"Property Location\"]\n\t\t\texcept KeyError:\n\t\t\t\tnew_parcel_address = \"\"\n\t\t\tnumber_of_unadjusted_addresses += 1\n\t\tparcel_data[\"Property Location\"] = new_parcel_address\n\t\tnew_file_with_detailed_parcel_data.write(json.dumps(parcel_data) + \"\\n\")\n\tprint(f\"Number of adjusted addresses: {number_of_adjusted_addresses}\")\n\tprint(f\"Number of unadjusted addresses: {number_of_unadjusted_addresses}\")\n\tprint(f\"\\nNew file: {new_datafile}\")\n\n\treturn True\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import json", "import os"]}], [], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape:\n\tdef __init__(self, input_url):\n\t\t\"\"\"\n\n\t\t:type self: url input\n\t\t\"\"\"\n\t\tself.url = input_url\n\t\tself.html_text = ''\n\t\tself.text = ''\n\t\t\n\tdef get_text(self):\n\t\treturn self.text\n\n\tdef process_html(self):\n\n\t\tmultiple_space_regx = re.compile(r'\\s\\s+')\n\t\turl_text_afterspace = multiple_space_regx.sub(' ', self.html_text)\n\n\t\tremove_comment_regx = re.compile(r'')\n\t\turl_text_aftercomment = remove_comment_regx.sub('', url_text_afterspace)\n\n\t\tremove_htmltag_regx = re.compile(r'<[^>]*>')\n\t\turl_text_afterhtml = remove_htmltag_regx.sub('',url_text_aftercomment)\n\n\t\tremove_punctuation_regx = re.compile(r'[,\\'\\\"!=\\$\\.\\-\\|:;~\\*\\\\@\\/%\\#\\?\\_\\{\\}\\+\\&>]+')\n\t\turl_text_afterpunctuation = remove_punctuation_regx.sub(' ',url_text_afterhtml)\n\n\t\tremove_digits = re.compile(r'[0-9]')\n\t\turl_text_digitsremove = remove_digits.sub('', url_text_afterpunctuation)\n\n\t\tremove_parenthesis = re.compile(r'[\\(\\)]+')\n\t\turl_text_parenthesis_reomove = remove_parenthesis.sub('',url_text_digitsremove)\n\n\t\tremove_nonascii = re.compile(r'[^\\x00-\\x7F]+')\n\t\turl_text_after_nonascii = remove_nonascii.sub(' ',url_text_parenthesis_reomove)\n\n\t\tremove_extra1 = re.compile(r'&gt')\n\t\turl_text_afterextra1 = remove_extra1.sub('', url_text_after_nonascii)\n\n\t\tremove_extra2 = re.compile(r'&nbsp')\n\t\turl_text_afterextra2 = remove_extra2.sub('', url_text_afterextra1)\n\n\t\tremove_extra3 = re.compile(r'&amp')\n\t\turl_text_afterextra3 = remove_extra3.sub('', url_text_afterextra2)\n\n\t\tremove_opening_closing_bracket = re.compile(r'[\\[\\]]+')\n\t\turl_text_removing_brackets = remove_opening_closing_bracket.sub('', url_text_afterextra3)\n\n\t\turl_text_space_removal = multiple_space_regx.sub(' ', url_text_removing_brackets)\n\n\t\tremove_leading_trailing_whitespace = re.compile(r'^\\s+|\\s+$')\n\t\turl_text_leading_trailing_space = remove_leading_trailing_whitespace.sub('', url_text_space_removal)\n\n\t\tremove_newline_tab = re.compile(r'\\n\\r\\t')\n\t\turl_text_final = remove_newline_tab.sub(' ', url_text_leading_trailing_space)\n\t\tself.text = url_text_final\n\t\t#print url_text_final\n\n\tdef fetch_html(self):\n\t\theaders = {}\n\t\theaders['User-Agent'] = 'Googlebot'\n\t\trequest = urllib2.Request(self.url, headers=headers)\n\t\ttry:\n\t\t\tresponse = urllib2.urlopen(request)\n\t\texcept urllib2.HTTPError as e:\n\t\t\tif e.code == 404:\n\t\t\t\tprint '404 Error', e.msg\n\t\t\telse:\n\t\t\t\tprint 'Id onot know', e.msg\n\t\texcept urllib2.URLError as e:\n\t\t\tprint 'Url Error:', e.msg\n\t\telse:\n\t\t\tprint response.getcode()\n\t\t\turl_text = response.read()\n\t\t\tprint response.info()\n\t\t\tprint response.geturl()\n\t\t\t#print url_text\n\t\t\tself.html_text = url_text\n\t\t\t#self.process_html(url_text)\n\t\t\tresponse.close()\n", "description": "\n\n\t\t:type self: url input\n\t\t", "category": "webscraping", "imports": ["import urllib2", "import re"]}, {"term": "def", "name": "fmain", "data": "#def main():\n\t#url = 'https://www.microsoft.com'\n\t#url = 'http://www.cs.ucsb.edu/~vigna/research.html'\n\t#url = 'https://www.memphis.edu/psychology/'\n\t#get_html(url)\n\t#webScraper = WebScrape(url)\n\t#webScraper.fetch_html()\n\t#webScraper.process_html()\n\t#print webScraper.get_text()\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "import re"]}], [], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(GeneratingCommand):\n\t\"\"\"Scrapes a website according to simple scraping rules.\n\t\"\"\"\n\n\t_aliases_   = ['webscrape', 'web_scrape', 'webscrapper', 'web_scrapper']\n\t_syntax_\t= '[url=] [articles=] [iterator=]'\n\n\tdef __init__(self, url: str, articles: list, iterator: str):\n\t\t\"\"\"\n\t\t:param url:\t\t Base URL\n\t\t:param articles:\tXPath expression(s) to extract articles\n\t\t:param iterator:\tXPath expression(s) to get next index page\n\t\t\"\"\"\n\t\tsuper().__init__(url, articles, iterator)\n\t\tself.fields = FieldsMap(\n\t\t\turl=Field(url, type=str),\n\t\t\tarticles=Field(articles, type=str),\n\t\t\titerator=Field(iterator, type=str)\n\t\t)\n\t\tself.xml_parser = etree.HTMLParser()\n\n\tasync def setup(self, event, pipeline):\n\t\tself.fields = await self.fields.read(event, pipeline)\n\t\t# Build articles' XPath expressions\n\t\tself.articles_xpath = etree.XPath(self.fields.articles)\n\t\t# Build iterator's XPath expressions\n\t\tself.iterator_xpath = etree.XPath(self.fields.iterator)\n\n\tdef scrape_articles(self, tree):\n\t\t_articles = []\n\t\tfor article in self.articles_xpath(tree):\n\t\t\ttry:\n\t\t\t\t_articles.append({\n\t\t\t\t\t'attrs': dict(getattr(article, 'attrib', {})),\n\t\t\t\t\t'text': getattr(article, 'text', '')\n\t\t\t\t})\n\t\t\texcept Exception:\n\t\t\t\tpass\n\t\treturn _articles\n\n\tdef scrape_iterator(self, tree):\n\t\ttry:\n\t\t\treturn self.iterator_xpath(tree)[0]\n\t\texcept Exception:\n\t\t\treturn None\n\n\tasync def target(self, event, pipeline):\n\t\tnextpage = self.fields.url\n\t\tasync with aiohttp.ClientSession() as session:\n\t\t\t# Loop until there is no more \n\t\t\twhile nextpage is not None:\n\t\t\t\tasync with session.get(nextpage) as response:\n\t\t\t\t\t# Get response content\n\t\t\t\t\tpage_src = (await response.read()).decode('UTF-8')\n\t\t\t\t\t# Parse response content as XML\n\t\t\t\t\tpage_tree = etree.fromstring(\n\t\t\t\t\t\tpage_src,\n\t\t\t\t\t\tparser=self.xml_parser\n\t\t\t\t\t)\n\t\t\t\t\t# Backup current URL\n\t\t\t\t\tcurrpage = nextpage\n\t\t\t\t\t# Get articles and next page url\n\t\t\t\t\tarticles = self.scrape_articles(page_tree)\n\t\t\t\t\tnextpage = self.scrape_iterator(page_tree)\n\t\t\t\t\t# Yield one event per article\n\t\t\t\t\tfor article in articles:\n\t\t\t\t\t\tyield derive(event, article)\n", "description": "Scrapes a website according to simple scraping rules.\n\t", "category": "webscraping", "imports": ["from lxml import etree", "import aiohttp", "from m42pl.commands import GeneratingCommand", "from m42pl.fields import Field, FieldsMap", "from m42pl.event import derive"]}], [{"term": "def", "name": "index", "data": "def index(request):\n\tdata = os.listdir('autolibrary/documents')\n\tdata = dumps(data) \n\t\n\tos.system('mkdir -p ../data/raw')\n\tos.system('mkdir -p ../data/out')\n\tos.system('mkdir -p static/autolibrary/documents')\n\tos.system('mkdir -p static/autolibrary/web_scrap')\n\n\tshared_obj = request.session.get('myobj',{}) \n\tshared_obj['selected_doc'] = ''\n\tshared_obj['selected_pdf'] = ''\n\tshared_obj['if_customized'] = \"true\"\n\tshared_obj['selected_domain'] = ''\n\tshared_obj['selected_subdomain'] = ''\n\tshared_obj['selected_keywords'] = ''\n\tshared_obj['phrases'] = []\n\tshared_obj['in_queue'] = \"false\"\n\tshared_obj['timestamp'] = ''\n\tshared_obj['first_run'] = \"true\"\n\trequest.session['myobj'] = shared_obj\n\n\treturn render(request, 'autolibrary/index.html', {\"data\": data})\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import HttpResponse", "import os", "import json", "import pandas as pd", "import subprocess", "from json import dumps ", "from django.views.decorators.csrf import csrf_exempt", "import time", "import datetime", "from django.utils import timezone", "from django.contrib.sessions.models import Session"]}, {"term": "def", "name": "result", "data": "def result(request):\n\tdata = os.listdir('autolibrary/documents')\n\tdomains = json.load(open('../config/domains_full.json'))\n\n\tshared_obj = request.session['myobj']\n\tselected_doc = shared_obj['selected_doc']\n\tselected_pdf = shared_obj['selected_pdf']\n\n\tcontent = {\n\t\t\"data\": dumps(data), \n\t\t\"selected_doc\": dumps([selected_doc]), \n\t\t\"selected_pdf\": dumps([selected_pdf]), \n\t\t\"domains\": dumps(domains)\n\t}\n\t\n\tshared_obj['in_queue'] = \"false\"\n\trequest.session['myobj'] = shared_obj\n\treturn render(request, 'autolibrary/result.html', content)\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import HttpResponse", "import os", "import json", "import pandas as pd", "import subprocess", "from json import dumps ", "from django.views.decorators.csrf import csrf_exempt", "import time", "import datetime", "from django.utils import timezone", "from django.contrib.sessions.models import Session"]}, {"term": "def", "name": "customization", "data": "def customization(request):\n\tdata = os.listdir('autolibrary/documents')\n\tdomains = json.load(open('../config/domains_full.json'))\n\n\tshared_obj = request.session['myobj']\n\tif_customized = shared_obj['if_customized']\n\tselected_pdf = shared_obj['selected_pdf']\n\tselected_doc = shared_obj['selected_doc']\n\tselected_keywords = shared_obj['selected_keywords']\n\tif shared_obj['first_run'] == \"true\":\n\t\tshared_obj['selected_domain'] = ''\n\t\tshared_obj['selected_subdomain'] = ''\n\t\tshared_obj['phrases'] = []\n\tselected_domain = shared_obj['selected_domain']\n\tselected_subdomain = shared_obj['selected_subdomain']\n\tphrases = shared_obj['phrases']\n\n\tcontent = {\n\t\t\"customized\": dumps([if_customized]),\n\t\t\"data\": dumps(data), \n\t\t\"selected_doc\": dumps([selected_doc]), \n\t\t\"selected_pdf\": dumps([selected_pdf]), \n\t\t\"domains\": dumps(domains),\n\t\t\"domain\": dumps([selected_domain]),\n\t\t\"subdomain\": dumps([selected_subdomain]),\n\t\t\"phrases\": dumps(phrases),\n\t\t\"keywords\":dumps([selected_keywords]),\n\t}\n\tif if_customized == \"false\":\n\t\tif_customized = \"true\"\n\t\tshared_obj['if_customized'] = if_customized\n\n\tshared_obj['in_queue'] = \"false\"\n\trequest.session['myobj'] = shared_obj\n\treturn render(request, 'autolibrary/customization.html', content)\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import HttpResponse", "import os", "import json", "import pandas as pd", "import subprocess", "from json import dumps ", "from django.views.decorators.csrf import csrf_exempt", "import time", "import datetime", "from django.utils import timezone", "from django.contrib.sessions.models import Session"]}, {"term": "def", "name": "get_file", "data": "def get_file(request):\n\tif request.method == 'POST':\n\t\tif \"file_name\" in request.POST:\n\t\t\tshared_obj = request.session['myobj']\n\t\t\tif_customized = \"false\"\n\t\t\tshared_obj['if_customized'] = if_customized\n\n\t\t\t# rename document\n\t\t\tfile_name = request.POST['file_name']\n\t\t\tpdfname = file_name.replace(\"'\", \"\")\n\t\t\tpdfname = pdfname.replace(\" \", \"_\")\n\t\t\tos.system('bash autolibrary/rename.sh')\n\t\t\t# save doc name and move to static\n\t\t\tselected_doc = file_name\n\t\t\tselected_pdf = pdfname\n\t\t\tshared_obj['selected_pdf'] = selected_pdf\n\t\t\tshared_obj['selected_doc'] = selected_doc\n\n\t\t\tcommand = 'cp autolibrary/documents_copy/' + pdfname + ' static/autolibrary/documents'\n\t\t\tos.system(command)\n\n\t\t\tshared_obj['in_queue'] = \"false\"\n\t\t\tshared_obj['first_run'] = \"true\"\n\t\t\trequest.session['myobj'] = shared_obj\n\t\t\treturn HttpResponse('get file')\n\treturn HttpResponse('fail to get file')\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import HttpResponse", "import os", "import json", "import pandas as pd", "import subprocess", "from json import dumps ", "from django.views.decorators.csrf import csrf_exempt", "import time", "import datetime", "from django.utils import timezone", "from django.contrib.sessions.models import Session"]}, {"term": "def", "name": "get_domain", "data": "def get_domain(request): \n\tif request.method == 'POST':\n\t\tif \"domain\" in request.POST:\n\t\t\tshared_obj = request.session['myobj'] \n\t\t\tunique_key = request.session.session_key\n\t\t\t\n\t\t\t# save selected domain to data/out\n\t\t\tselected_domain = request.POST['domain']\n\t\t\tselected_subdomain = request.POST['subdomain']\n\t\t\tselected_pdf = shared_obj['selected_pdf']\n\t\t\tif selected_domain == '':\n\t\t\t\tselected_domain = 'ALL'\n\t\t\tif selected_subdomain == '':\n\t\t\t\tselected_subdomain = 'ALL'\n\n\t\t\tshared_obj['selected_domain'] = selected_domain \n\t\t\tshared_obj['selected_subdomain'] = selected_subdomain\n\n\t\t\t# with open('../data/out/selected_domain_' + unique_key + '.txt', 'w') as fp:\n\t\t\twith open('../data/out/selected_domain.txt', 'w') as fp:\n\t\t\t\tfp.write(selected_subdomain)\n\t\t\tconfig = {'fos': [selected_domain]}\n\t\t\t# with open('../data/out/fos_' + unique_key + '.json', 'w') as fp:\n\t\t\twith open('../data/out/fos.json', 'w') as fp:\n\t\t\t\tjson.dump(config, fp)\n\t\t\t# rewrite data-params.json\n\t\t\tconfig = json.load(open('../config/data-params.json'))\n\t\t\tconfig['pdfname'] = selected_pdf\n\t\t\twith open('autolibrary/data-params.json', 'w') as fp:\n\t\t\t\tjson.dump(config, fp)\n\t\t\twith open('autolibrary/run.sh', 'w') as rsh:\n\t\t\t\t# move selected document to data/raw\n\t\t\t\trsh.write('''cp autolibrary/documents_copy/''')\n\t\t\t\trsh.write(selected_pdf)\n\t\t\t\trsh.write(''' ../data/raw \\n''')\n\t\t\t\t# move new data-params.json to config\n\t\t\t\trsh.write('''cp autolibrary/data-params.json  ../config \\n''')\n\t\t\t\t# run all targets\n\t\t\t\trsh.write('''cd .. \\n''')\n\t\t\t\trsh.write('''python run.py data \\n''')\n\t\t\t\trsh.write('''python run.py autophrase \\n''')\n\t\t\t\trsh.write('''python run.py weight ''' + unique_key +  '''\\n''')\n\t\t\t\trsh.write('''python run.py webscrape ''' + unique_key +  '''\\n''')\n\t\t\t\trsh.write('''cp data/out/scraped_AutoPhrase.json website/static/autolibrary/web_scrap/scraped_AutoPhrase.json \\n''')\n\t\t\tprocess = subprocess.Popen(['bash', 'autolibrary/run.sh'])\n\t\t\tprocess.wait()\n\n\t\t\t# display phrases with a weighted quality score > 0.5\n\t\t\tdata = pd.read_csv('../data/out/weighted_AutoPhrase.csv', index_col = \"Unnamed: 0\")\n\t\t\tphrases = data[data['score'] > 0.5]['phrase'].to_list()\n\t\t\tif len(phrases) < 5:\n\t\t\t\tphrases = data['phrase'][:5].to_list()\n\t\t\tshared_obj['phrases'] = phrases \n\n\t\t\tnew_keywords = phrases[0] + ', ' + phrases[1] + ', ' + phrases[2] + ', '\n\t\t\tshared_obj['selected_keywords'] = new_keywords\n\n\t\t\tshared_obj['in_queue'] = \"false\"\n\t\t\tshared_obj['first_run'] = \"false\"\n\t\t\trequest.session['myobj'] = shared_obj\n\t\t\treturn HttpResponse('get domain')\n\treturn HttpResponse('fail to get domain')\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import HttpResponse", "import os", "import json", "import pandas as pd", "import subprocess", "from json import dumps ", "from django.views.decorators.csrf import csrf_exempt", "import time", "import datetime", "from django.utils import timezone", "from django.contrib.sessions.models import Session"]}, {"term": "def", "name": "get_keywords", "data": "def get_keywords(request):  \n\tif request.method == 'POST':\n\t\tif \"keywords\" in request.POST:\n\t\t\tshared_obj = request.session['myobj'] \n\t\t\t# save selected keywords to data/out\n\t\t\tselected_keywords = request.POST['keywords']\n\t\t\tshared_obj['selected_keywords'] = selected_keywords\n\t\t\tconfig = {'keywords': selected_keywords}\n\t\t\twith open('../data/out/selected_keywords.json', 'w') as fp:\n\t\t\t\tjson.dump(config, fp)\n\t\t\twith open('autolibrary/run.sh', 'w') as rsh:\n\t\t\t\t# display new webscrape result\n\t\t\t\trsh.write('''cd .. \\n''')\n\t\t\t\trsh.write('''python run.py webscrape \\n''')\n\t\t\t\trsh.write('''cp data/out/scraped_AutoPhrase.json website/static/autolibrary/web_scrap/scraped_AutoPhrase.json''')\n\t\t\tprocess = subprocess.Popen(['bash', 'autolibrary/run.sh'])\n\t\t\tprocess.wait()\n\n\t\t\tshared_obj['in_queue'] = \"false\"\n\t\t\trequest.session['myobj'] = shared_obj\n\t\t\treturn HttpResponse('get keywords')\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import HttpResponse", "import os", "import json", "import pandas as pd", "import subprocess", "from json import dumps ", "from django.views.decorators.csrf import csrf_exempt", "import time", "import datetime", "from django.utils import timezone", "from django.contrib.sessions.models import Session"]}], [{"term": "class", "name": "Webscrape", "data": "class Webscrape():\n\t'''classes are cool, no other real reason to use this - probably going to only have one function'''\n\tdef __init__(self):\n\t\tself.webpath = \"https://rocketleague.tracker.network/rocket-league/profile\"\n\t\tself.latestseason = '16' #need a better way to update this, perhaps dynamically?\n\t\tself.rltrackermissing = \"We could not find your stats,\"\n\t\tself.psyonixdisabled = \"Psyonix has disabled the Rocket League API\"\n\n\tdef retrieveDataRLTracker(self,gamertag=\"memlo\",platform=\"steam\"):\n\t\t''' Python BeautifulSoup4 Webscraper to https://rocketleague.tracker.network/ to retrieve gamer data\n\t\t'''\n\t\tlatestseason = self.latestseason\n\t\twebpath = self.webpath\n\t\trltrackermissing = self.rltrackermissing\n\t\tpsyonixdisabled = self.psyonixdisabled\n\t\tplayerdata = {} # define the playerdata dict\n\t\tplayerdata[gamertag] = {} # define the gamertag dict\n\t\tplayerdata[gamertag][latestseason] = {} # define the latestseason dict\n\t\tpage = requests.get(\"%(webpath)s/%(platform)s/%(gamertag)s\" % locals())\n\t\t# correct platform names\n\t\tif 'ps' or 'ps4' in platform:\n\t\t\tplatform = \"psn\"\n\t\tif 'xbox' in platform:\n\t\t\tplatform = \"xbl\"\n\t\tif page.status_code == 200:\n\t\t\tsoup = BeautifulSoup(page.content, features=\"lxml\")\n\t\t\tif soup(text=re.compile(rltrackermissing)): # find \"we could not find your stats\" on webpage\n\t\t\t\tlogger.critical(\"Player Missing - URL:%(webpath)s/%(platform)s/%(gamertag)s\" % locals())\n\t\t\telif soup(text=re.compile(psyonixdisabled)): # find \"Psyonix has disabled the Rocket League API\" on webpage\n\t\t\t\tlogger.critical(\"Psyonix Disabled API - URL:%(webpath)s/%(platform)s/%(gamertag)s\" % locals())\n\t\t\telse:\n\t\t\t\tscript_data = [l for l in [str(l.parent) for l in soup.find_all('script')] if 'INITIAL_STATE' in l][0]\n\t\t\t\tjson_data = script_data.split('INITIAL_STATE__=')[1].split(\";(function()\")[0]\n\t\t\t\tdata = json.loads(json_data)['stats-v2']['standardProfiles']\n\t\t\t\ttry:\n\t\t\t\t\ttrn_gamertag = list(data.keys())[0]\n\t\t\t\t\tgamer_data = data[trn_gamertag]['segments']\n\t\t\t\t\tfor segment in gamer_data:\n\t\t\t\t\t\tif \"playlist\" in segment['type']:\n\t\t\t\t\t\t\tplayerdata[gamertag][latestseason].update(self._parsePlaylist(data=segment))\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tlogger.critical(\"Player Data not found - URL:%(webpath)s/%(platform)s/%(gamertag)s\" % locals())\n\t\treturn playerdata\n\n\tdef _parsePlaylist(self,data=None):\n\t\t''' Using the json data to assign values to the proper fields\n\t\t'''\n\t\ta = {}\n\t\tplaylist = data['metadata']['name']\n\t\ta[playlist] = {\"Tier Rank\": None,\n\t\t\t\t\t   \"Tier Number\": None,\n\t\t\t\t\t   \"Tier Division\": None,\n\t\t\t\t\t   \"Games Played\": None,\n\t\t\t\t\t   \"MMR\": None}\n\t\ttry:\n\t\t\ta[playlist][\"Tier Rank\"] = data['stats']['tier']['metadata']['name']\n\t\t\ta[playlist][\"Tier Number\"] = data['stats']['tier']['value']\n\t\t\ta[playlist][\"Tier Divisio\"] = data['stats']['division']['metadata']['name']\n\t\t\ta[playlist][\"Games Played\"] = data['stats']['matchesPlayed']['value']\n\t\t\ta[playlist][\"MMR\"] = data['stats']['rating']['value']\n\t\texcept Exception as e:\n\t\t\tlogger.info(\"Could not find %(playlist)s data with error: \" % locals(),e)\n\n\t\treturn a\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import argparse", "import requests", "import re", "from setup_logging import logger", "import json", "\tfrom pprint import pprint # pprint is cool"]}, {"term": "def", "name": "singleRun", "data": "def singleRun(gamertag,platform):\n\t'''Single run of Webscrape.retrieveDataRLTracker'''\n\tlogger.info(\"Start for gamertag:%(gamertag)s\"% locals())\n\tscrape = Webscrape()\n\tdata = scrape.retrieveDataRLTracker(gamertag=gamertag,platform=platform)\n\tif data is not None:\n\t\tpprint(data)\n\t\tlogger.info(\"Finish for gamertag:%(gamertag)s\"% locals())\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import argparse", "import requests", "import re", "from setup_logging import logger", "import json", "\tfrom pprint import pprint # pprint is cool"]}], [], [{"term": "def", "name": "webScrape", "data": "def webScrape():\n\tfloat counter = 4170\n\tfloat limit = 4172\n\t\n\twhile counter < limit:\n\t\thouseNum = str(counter)\n\t\twebpage = urlopen(\"http://www.cpshomes.co.uk/lettings/properties/property_details.aspx?reference=P\" + houseNum).read()\n\t\t\n\tregexFindURL = re.compile('Postcode: (.*)')\n\t\n\tdeface = re.findall(regexFindURL, webpage)\n\t\n\t\n\t\n", "description": null, "category": "webscraping", "imports": ["from urllib import urlopen", "from BeautifulSoup import BeautifulSoup", "import re", "\tpostCode = regexFindURLfrom urllib import urlopen", "from BeautifulSoup import BeautifulSoup", "import re"]}, {"term": "def", "name": "webScrape", "data": "def webScrape():\n\tfloat counter = 4170\n\tfloat limit = 4172\n\t\n\twhile counter < limit:\n\t\thouseNum = str(counter)\n\t\twebpage = urlopen(\"http://www.cpshomes.co.uk/lettings/properties/property_details.aspx?reference=P\" + houseNum).read()\n\t\t\n\tregexFindURL = re.compile('Postcode: (.*)')\n\t\n\tdeface = re.findall(regexFindURL, webpage)\n\t\n\t\n\t\n", "description": null, "category": "webscraping", "imports": ["from urllib import urlopen", "from BeautifulSoup import BeautifulSoup", "import re", "\tpostCode = regexFindURLfrom urllib import urlopen", "from BeautifulSoup import BeautifulSoup", "import re"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(issueState):\n\turl = 'https://github.com/{}/{}/issues?q=is%3Aissue+is%3A{}+label%3Atype%3A{}'.format(frameworkOrg[framework], framework, issueState, label_type)\n\tpage = requests.get(url)\n\tsoup = BeautifulSoup(page.text, 'html5lib')\n\n\t#getting the number of issues to be investigated so we know when to terminate our loop as we can trust not being able to find a\n\t#new page anymore because of the potential html dicrepencies\n\tn_issues = soup.find('a', {'class' : 'btn-link selected'})\n\tn = int(n_issues.text.replace(',', '').replace(' ', '').replace('\\n', '').replace(issueState[0].upper() + issueState[1:].lower(), ''))\n\n\tprint('Number of {} issues to be explored : {}'.format(issueState, n))\n\n\t#getting all the issues in the page\n\tresultSet = soup.find_all('a', {'class' : \"Link--primary v-align-middle no-underline h4 js-navigation-open markdown-title\"})\n\tprint(resultSet)\n\tissueCount = 0\n\tpageCount = 1\n\t#we encapsulate the code with a while True and try statement so if the connection is lost, we don't lose our progress\n\twhile True:\n\n\t\ttry:\n\n\t\t\t#if we've investigated all the issues closed, we're done\n\t\t\twhile issueCount < n:\n\t\t\t\t\n\t\t\t\tprint('\\nPage {}:\\n'.format(pageCount))\n\n\t\t\t\tfor res in resultSet:\n\n\t\t\t\t\tissueCount = issueCount + 1\n\t\t\t\t\tissueId = ''\n\n\t\t\t\t\t#as we progress through the pages, there will exist small html discrepensies. However, at this stage of the code\n\t\t\t\t\t#we know that there's still issues to be explored so we use a while loop to keep retrying untill we get our wanted\n\t\t\t\t\t#results\n\t\t\t\t\tallTags = None\n\t\t\t\t\tissueDesc = None\n\t\t\t\t\tissueSoup = None\n\t\t\t\t\twhile allTags == None or issueDesc == None:\n\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tissueUrl = 'https://github.com' + res['href']\n\t\t\t\t\t\t\tissuePage = requests.get(issueUrl)\n\t\t\t\t\t\t\tissueSoup = BeautifulSoup(issuePage.text, 'html5lib')\n\n\t\t\t\t\t\t\t#in case the issue has a \"Load more\" section, we use selenium to open it up to make sure to scrape the complete issue page\n\t\t\t\t\t\t\tloadMoreButton = issueSoup.find('button', {'class' : 'text-gray pt-2 pb-0 px-4 bg-white border-0'})\n\t\t\t\t\t\t\tif loadMoreButton != None:\n\t\t\t\t\t\t\t\tdriver = webdriver.Chrome('./selenium-webdrivers/chromedriver')\n\t\t\t\t\t\t\t\tdriver.get(issueUrl)\n\t\t\t\t\t\t\t\twhile loadMoreButton != None:\n\t\t\t\t\t\t\t\t\tloadMoreButtonSelen = wait(driver, 20).until(expected_conditions.presence_of_element_located((By.CSS_SELECTOR, 'button.bg-white.border-0')))\n\t\t\t\t\t\t\t\t\tloadMoreButtonSelen.click()\n\t\t\t\t\t\t\t\t\twait(driver, 5).until(expected_conditions.invisibility_of_element_located(loadMoreButtonSelen))\n\t\t\t\t\t\t\t\t\tissueSoup = BeautifulSoup(driver.page_source, 'html5lib')\n\t\t\t\t\t\t\t\t\tloadMoreButton = issueSoup.find('button', {'class' : 'text-gray pt-2 pb-0 px-4 bg-white border-0'})\n\n\t\t\t\t\t\t\t#getting the first post which will correspond to the issue's description\n\t\t\t\t\t\t\tissueDesc = issueSoup.find('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n\t\t\t\t\t\t\tif issueDesc == None:\n\t\t\t\t\t\t\t\tprint('descrepency (no issue description found). Retrying...')\n\t\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t\t\t#collecting all the tags from the issue\n\t\t\t\t\t\t\tallTags = issueDesc.findAll()\n\t\t\t\t\t\t\tif allTags == None:\n\t\t\t\t\t\t\t\tprint('descrepency (issue description tags not found). Retrying...')\n\t\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t\t\tissueId = issueUrl[issueUrl.rfind('/') + 1:]\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\tf = open('{}/{}/issue{}.html'.format(framework, issueState, issueId), 'w+', encoding='utf-8')\n\t\t\t\t\t\t\t\tf.write(issueSoup.prettify())\n\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t\tprint('Something went wrong')\n\t\t\t\t\t\t\tfinally:\n\t\t\t\t\t\t\t\tf.close()\n\t\t\t\t\t\t\t\n\t\t\t\t\t\texcept KeyboardInterrupt:\n\t\t\t\t\t\t\tprint('Keyboard Interrupt')\n\t\t\t\t\t\t\treturn\n\n\t\t\t\t\tprint('Number of {} issues written: {}/{}'.format(issueState, issueCount, n))\n\n\t\t\t\t#if the past issue was the last issue, we're done (we put '>=' instead or '==' because the number of closed issue\n\t\t\t\t#increase while our program is running)\n\t\t\t\tif issueCount >= n:\n\t\t\t\t\tprint('Finished')\n\t\t\t\t\treturn\n\t\t\t\t\n\t\t\t\tpageCount = pageCount + 1\n\n\t\t\t\t#if there is a next page, we get its link and we keep going\n\t\t\t\turl = 'https://github.com/{}/{}/issues?page={}&q=is%3Aissue+is%3A{}+label%3Atype%3A{}'.format(frameworkOrg[framework], framework, pageCount, issueState, label_type)\n\n\t\t\t\t#as we progress through the pages, there will exist small html discrepensies. However, at this stage of the code\n\t\t\t\t#we know that there's still issues to be explored so we use a while loop to keep retrying untill we get our wanted\n\t\t\t\t#results\n\t\t\t\tresultSet = None\n\t\t\t\twhile resultSet == None:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tpage = requests.get(url)\n\t\t\t\t\t\tsoup = BeautifulSoup(page.text, 'html5lib')\n\n\t\t\t\t\t\tpageResult = soup.find('h3')\n\t\t\t\t\t\tif pageResult != None:\n\t\t\t\t\t\t\tif pageResult.text == 'No results matched your search.':\n\t\t\t\t\t\t\t\tprint('Error: no more pages (finished).')\n\t\t\t\t\t\t\t\treturn\n\n\t\t\t\t\t\t#getting all the issues in the page\n\t\t\t\t\t\tresultSet = soup.find_all('a', {'class' : \"Link--primary v-align-middle no-underline h4 js-navigation-open markdown-title\"})\n\t\t\t\t\t\tif resultSet == None or len(resultSet) == 0:\n\t\t\t\t\t\t\tprint('descrepency (resultset). Retrying...')\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\texcept KeyboardInterrupt:\n\t\t\t\t\t\tprint('Keyboard Interrupt')\n\t\t\t\t\t\treturn\n\t\texcept KeyboardInterrupt:\n\t\t\tprint('Keyboard Interrupt')\n\t\t\treturn\n\n\t\tbreak\n", "description": null, "category": "webscraping", "imports": ["import sys", "import requests", "from bs4 import BeautifulSoup", "import os", "import time", "from selenium.webdriver.support.ui import WebDriverWait as wait", "from selenium import webdriver", "from selenium.webdriver.support import expected_conditions", "from selenium.webdriver.common.by import By"]}], [{"term": "def", "name": "wait_at_start", "data": "def wait_at_start(driver):\n\twhile True:\n\t\ttry:\n\t\t\tWDW(driver,15).until(EC.presence_of_element_located(\n\t\t\t\t(By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]')))\n\t\t\tbreak\n\t\texcept:\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait as WDW", "from selenium.webdriver.support import expected_conditions as EC", "from multiprocessing import get_context", "import pandas as pd", "import time"]}, {"term": "def", "name": "fetch_price", "data": "def fetch_price(driver):\n\tdate  = WDW(driver,5).until(EC.presence_of_element_located(\n\t\t\t(By.XPATH , '''/html/body/div[2]/div[6]/div/div[1]/div[1]/div[5]/\n\t\t\t div/div[2]/div[1]/div[1]/div[2]/div/div[2]/span'''))).text\n\tclose = WDW(driver,5).until(EC.presence_of_element_located(\n\t\t\t(By.XPATH , '''/html/body/div[2]/div[6]/div/div[1]/div[1]/div[5]/\n\t\t\t div/div[2]/div[1]/div[2]/div[2]/div[4]/div[2]/span'''))).text\n\treturn date, close\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait as WDW", "from selenium.webdriver.support import expected_conditions as EC", "from multiprocessing import get_context", "import pandas as pd", "import time"]}, {"term": "def", "name": "move_mouse", "data": "def move_mouse(driver):\n\tActionChains(driver)\\\n\t\t.move_by_offset(1, 0)\\\n\t\t.perform()\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait as WDW", "from selenium.webdriver.support import expected_conditions as EC", "from multiprocessing import get_context", "import pandas as pd", "import time"]}, {"term": "def", "name": "webscrape_price", "data": "def webscrape_price(comms):\n\t\n\t### Variables and Dataframe Setup ###\n\t\n\turl_comms = mp_dict[comms]\n\tdup_threshold = 80\n\t\n\tcols = ['date',comms]\n\tprice_df = pd.DataFrame(columns=cols)\n\tls_prev = pd.DataFrame(columns=cols)\n\tstate_var = 0 # Initializing state var to count # of duplicate observation\t\n\t\n\t\n\t### Loop to Obtain 1-Year Period Price Dataset ###\n\t\n\tdriver = webdriver.Chrome(executable_path=exec_path,options=options)\n\tdriver.get(url_comms)\n\t\n\twait_at_start(driver)\n\tWDW(driver,15).until(EC.presence_of_element_located(\n\t\t(By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]'))).click()\n\tWDW(driver,15).until(EC.presence_of_element_located(\n\t\t(By.XPATH, '/html/body/div[2]/div[1]/div[1]/div/div[2]/div/div[1]/div[7]'))).click()\n\t\n\tstart_point = driver.find_element(By.XPATH, '//*[@id=\"drawing-toolbar\"]/div/div/div/div/div[4]/div/div/div[1]/div')\n\tActionChains(driver)\\\n\t\t.move_to_element(start_point)\\\n\t\t.perform()\n\t\n\t### Loop to Obtain YTD Price Dataset ###\n\twhile state_var <= dup_threshold:\n\t\t\n\t\ttemp_ls = [fetch_price(driver)]\n\t\ttemp_df = pd.DataFrame(temp_ls,columns=cols)\n\t\t\n\t\tif temp_df.equals(ls_prev):\n\t\t\tstate_var = state_var + 1\n\t\t\tls_prev = temp_df\n\t\telse:\n\t\t\tstate_var = 0\n\t\t\tls_prev = temp_df\n\t\t\n\t\tprice_df = pd.concat([price_df,temp_df],axis=0)\n\t\tmove_mouse(driver)\n\t\ttime.sleep(0.15)\n\t\n\t\n\t### Ensure No Duplicate Date (Same Date, Different Closing Price) ###\n\tprice_df = price_df.drop_duplicates().reset_index(drop=True)\n\t\n\tprice_df = price_df.loc[price_df['date']!='\u00e2\u02c6\u2026']\n\tprice_df['dup'] = price_df.duplicated(subset='date')\n\tassert price_df['dup'].unique().shape[0], \"Duplicate obs identified!\"\n\t\n\t### Clean Price Dataset ###\n\t\n\tprice_df['data_dt'] = pd.to_datetime(price_df['date']).dt.date\n\tprice_df = price_df[['data_dt',comms]].set_index(['data_dt'])\n\t\n\t### Close Webdriver ###\n\ttry:\n\t\tdriver.close()\n\texcept:\n\t\talert = WDW(driver, 3).until(EC.alert_is_present())\n\t\talert.accept()\n\t\tdriver.close()\n\n\t\n\treturn price_df\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.action_chains import ActionChains", "from selenium.webdriver.common.by import By", "from selenium.webdriver.support.ui import WebDriverWait as WDW", "from selenium.webdriver.support import expected_conditions as EC", "from multiprocessing import get_context", "import pandas as pd", "import time"]}], [], [{"term": "def", "name": "webScrape", "data": "def webScrape():\n\tmy_url = 'https://finance.yahoo.com/quote/BTC-USD?p=BTC-USD&.tsrc=fin-srch'\n\n\t#opening up connection and grab page\n\tuClient = uReq(my_url)\n\tpage_html = uClient.read()\n\tuClient.close()\n\n\t#html parsing\n\tpage_soup = soup(page_html, \"html.parser\")\n\n\t#grab price diff %\n\tdata = str(page_soup.findAll(\"span\", {\"data-reactid\":\"34\"})[1].text)\n\n\tdata = data[:-2]\n\tnewData = \"\"\n\tfor element in reversed(data):\n\t\tif element == '(':\n\t\t\tbreak\n\t\telse:\n\t\t\tnewData += element\n\tnewData = newData[::-1]\n\treturn float(newData)\n\n\n", "description": null, "category": "webscraping", "imports": ["import smtplib", "from email.message import EmailMessage", "from urllib.request import urlopen as uReq", "from bs4 import BeautifulSoup as soup", "import time"]}, {"term": "def", "name": "sendMessage", "data": "def sendMessage(body, subject, to):\n\t#use EmailMessage library and set variables based on function arguments\n\tmsg.set_content(body)\n\tmsg['subject'] = subject\n\tmsg['to'] = to\n\n\tserver.send_message(msg)\n\tprint('sent message')\n\n\tdel msg['to']\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import smtplib", "from email.message import EmailMessage", "from urllib.request import urlopen as uReq", "from bs4 import BeautifulSoup as soup", "import time"]}], [{"term": "def", "name": "main2", "data": "def main2(player_search):\n\twebscraped_list = webscrape(player_search)\n\tif not webscraped_list:\n\t\treturn None\n\tsearch_result = search(player_search, webscraped_list)\n\n\tif search_result is not False:\n\t\tcleanid_result_sorted = id_cleaner(search_result)\n\t\tid_sorted_list = id_sorter(cleanid_result_sorted)\n\n\t\tplayer_sorted_list = player_sort(id_sorted_list)\n\t\telo_sorted_list = sort_by_elo(player_sorted_list)\n\t\treturn elo_sorted_list\n\telse:\n\t\treturn None\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(player_search):\n\tvalue_list = []\n\tnew_list = []\n\tsource = requests.get(f'http://ratingupdate.info/?name={player_search}').text\n\tsoup = BeautifulSoup(source, 'lxml')\n\n\tfor x in soup.find('div', class_='table-container').find_all('td'):\n\t\tplayer_data = x.text\n\t\tif \"\u2192\" in player_data:\n\t\t\tplayer_data = player_data.replace(\" \u2192\\n\", \"\")\n\t\t\ttest = x.find('a', href=True)\n\t\t\tplayer_id = test[\"href\"]\n\t\t\tvalue_list.append(player_id)\n\t\tvalue_list.append(player_data)\n\t\tnew_list = [value_list[i:i + 5] for i in range(0, len(value_list), 5)]\n\n\treturn new_list\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "search", "data": "def search(user_search, webscraped_list):\n\tnewer_list = []\n\tfor i, z in enumerate(webscraped_list):  # i dont think enumerate is needed?\n\t\tif user_search.lower() == z[1].lower():\n\t\t\tnewer_list.append(z)\n\treturn newer_list\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "id_cleaner", "data": "def id_cleaner(search_result):\n\tcleanid_result = []\n\tfor z in search_result:\n\t\ttemp_id = z[0]\n\t\ttemp_id = temp_id[:-3]\n\t\ttemp_id = temp_id.replace(\"/player/\", \"\")\n\t\tcleanid_result.append(temp_id)\n\t\tfor i, elements in enumerate(z):\n\t\t\tif elements == z[0]:\n\t\t\t\tcontinue\n\t\t\tcleanid_result.append(z[i])\n\tcleanid_result_sorted = [cleanid_result[i:i + 5] for i in range(0, len(cleanid_result), 5)]\n\treturn cleanid_result_sorted\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "id_sorter", "data": "def id_sorter(clean_id):\n\ttemp_id_sort = []\n\tperm_id_sort = []\n\tfor ix, elems in enumerate(clean_id):\n\t\tif ix == 0:\n\t\t\ttemp_id_sort.append(elems)\n\t\t\tperm_id_sort.append(elems)\n\t\t\tcontinue\n\t\tnew_player_id = elems[0]\n\n\t\tfor zx, ex in enumerate(temp_id_sort):\n\t\t\told_player_id = ex[0]\n\t\t\tif new_player_id == old_player_id:\n\t\t\t\ttemp_id_sort.insert(zx, elems)\n\t\t\t\tperm_id_sort.insert(zx, elems)\n\t\t\t\tbreak\n\t\t\telif new_player_id != old_player_id:\n\t\t\t\tif new_player_id != old_player_id and ex == temp_id_sort[-1]:\n\t\t\t\t\ttemp_id_sort.insert(zx + 1, elems)\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tcontinue\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "player_sort", "data": "def player_sort(id_sorted_list):\n\ttemp_player_sort = []\n\tnew_player_list = []\n\tfor ix, elems in enumerate(id_sorted_list):\n\t\tif ix == 0:\n\t\t\ttemp_player_sort.append(elems)\n\t\t\tcontinue\n\n\t\tif elems[0] == id_sorted_list[ix-1][0]:\n\t\t\ttemp_player_sort.append(elems)\n\n\t\telif elems[0] != id_sorted_list[ix-1][0]:\n\t\t\ttesto = temp_player_sort.copy()\n\t\t\tnew_player_list.append(testo)\n\t\t\ttemp_player_sort.clear()\n\t\t\ttemp_player_sort.append(elems)\n\ttemp_2 = temp_player_sort.copy()\n\tnew_player_list.append(temp_2)\n\treturn new_player_list\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "sort_by_elo", "data": "def sort_by_elo(player_sorted_list):\n\tperm_elo_sort = []\n\ttemp_elo_sort = []\n\tfor lists in player_sorted_list:\n\t\tif temp_elo_sort:\n\t\t\ttesto = temp_elo_sort.copy()\n\t\t\ttesto.reverse()\n\t\t\tperm_elo_sort.append(testo)\n\t\t\ttemp_elo_sort.clear()\n\t\tfor ix, elems in enumerate(lists):\n\t\t\tif ix == 0:\n\t\t\t\ttemp_elo_sort.append(elems)\n\t\t\t\tcontinue\n\t\t\tjust_elo = elo_splitter(elems[3])\n\n\t\t\tfor zx, ex in enumerate(temp_elo_sort):\n\t\t\t\tex_elo = elo_splitter(ex[3])\n\t\t\t\tif just_elo > ex_elo:\n\t\t\t\t\ttemp_elo_sort.insert(zx, elems)\n\t\t\t\t\tbreak\n\t\t\t\telif just_elo < ex_elo:\n\t\t\t\t\tif just_elo < ex_elo and ex == temp_elo_sort[-1]:\n\t\t\t\t\t\ttemp_elo_sort.insert(zx + 1, elems)\n\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tcontinue\n\t\t\t\telif just_elo == ex_elo:\n\t\t\t\t\ttemp_elo_sort.insert(zx, elems)\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\ttemp_elo_sort.insert(zx, elems)\n\t\t\t\t\tbreak\n\ttesto = temp_elo_sort.copy()\n\ttesto.reverse()\n\tperm_elo_sort.append(testo)\n\ttemp_elo_sort.clear()\n\treturn perm_elo_sort\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "elo_splitter", "data": "def elo_splitter(unsplit_elo):\n\tjust_elo = unsplit_elo\n\tjust_elo = just_elo.split(\" \u00b1\")  # splits the elo string into a list\n\tjust_elo = just_elo[0]  # gives me only the elo number in the list\n\tjust_elo = int(just_elo)\n\treturn just_elo\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "paste", "data": "def paste(search_data):\n\tperm_string = \"\"\n\ttemp_string = \"\"\n\tmax_chr_alert = False\n\tlength_check = 0\n\tfor elements in search_data:\n\t\tif length_check > 1024:\n\t\t\tmax_chr_alert = True\n\t\t\tbreak\n\t\tif perm_string:\n\t\t\tnew_player = \"__***New player below!***__\\n\"\n\t\t\tperm_string = new_player + perm_string\n\t\tfor ele in elements:\n\t\t\tlength_check = len(temp_string + perm_string)\n\t\t\tif length_check > 1024:\n\t\t\t\tmax_chr_alert = True\n\t\t\t\tbreak\n\t\t\ttemp_string = f\"**{ele[1]} **| *{ele[2]}* | {ele[3]} | {ele[4]}\\n\"\n\t\t\tperm_string = temp_string + perm_string\n\treturn perm_string, max_chr_alert\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\t# \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\t# \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n\tdef get_gloss(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "from pyltp import SentenceSplitter"]}], [{"term": "class", "name": "classAmazonWebscrapePipeline:", "data": "class AmazonWebscrapePipeline:\n\tdef process_item(self, item, spider):\n\t\treturn item\n", "description": null, "category": "webscraping", "imports": ["from itemadapter import ItemAdapter"]}], [{"term": "class", "name": "classDataBase:", "data": "class DataBase:\n\t\"\"\"\n\tBase object for database connection and cursor creation.\n\t\n\tAttributes:\n\t\tconn: sqlite3.Connection \n\t\tcursor: sqlite3.Cursor \n\t\"\"\"\n\tdef __init__(self, database:str):\n\t\tself.conn = sql.connect(database)\n\t\tself.cursor = self.conn.cursor()\n\n\t\tself.conn.execute('PRAGMA synchronous = 0;')\n\n", "description": "\n\tBase object for database connection and cursor creation.\n\t\n\tAttributes:\n\t\tconn: sqlite3.Connection \n\t\tcursor: sqlite3.Cursor \n\t", "category": "webscraping", "imports": ["import sqlite3 as sql", "import webscrape", "import warnings", "from tqdm import tqdm"]}, {"term": "class", "name": "Table", "data": "class Table(DataBase):\n\t\"\"\"\n\tBase object for handling tables within a database, child object of DataBase.\n\n\tAttributes:\n\t\tname(str): table name\n\t\"\"\"\n\tdef __init__(self, database:str, name:str):\n\t\tsuper().__init__(database)\n\t\tself.name = name\n\n\tdef delete_table(self):\n\t\tself.cursor.execute(f\"\"\"\n\t\t\t\tDROP TABLE IF EXISTS {self.name};\n\t\t\t\t\"\"\")\n\n\tdef create_table(self):\n\t\tprint(\"original function called 'create table'\")\n\t\tself.cursor.execute(f\"\"\"\n\t\t\tCREATE TABLE IF NOT EXISTS {self.name}\n\t\t\t\t(\n\t\t\t\t\tcolumn TEXT\n\t\t\t\t);\n\t\t\t\"\"\"\n\t\t\t)\n\t\tself.conn.commit()\n\t\n\tdef re_init(self):\n\t\tself.delete_table()\n\t\tself.create_table()\n\n\tdef fetch_hrefs(self):\n\t\ttry:\n\t\t\tself.cursor.execute(f\"\"\"\n\t\t\t\tSELECT href FROM {self.name}\n\t\t\t\t\"\"\")\n\t\t\threfs = [href[0] for href in self.cursor.fetchall()]\n\t\t\tif hrefs:\n\t\t\t\treturn hrefs\n\t\t\twarnings.warn(f\"No hrefs found in {self.name}\")\n\t\t\treturn None\n\t\texcept sql.OperationalError:\n\t\t\tself.re_init()\n\n\n", "description": "\n\tBase object for handling tables within a database, child object of DataBase.\n\n\tAttributes:\n\t\tname(str): table name\n\t", "category": "webscraping", "imports": ["import sqlite3 as sql", "import webscrape", "import warnings", "from tqdm import tqdm"]}, {"term": "class", "name": "href_table", "data": "class href_table(Table):\n\t\"\"\"\n\tManages href database, creation, loading, updating\n\n\tAttributes:\n\t\tname(str): Name of the table\n\n\t\"\"\"\n\tdef __init__(self, database:str, name:str):\n\t\tsuper().__init__(database, name)\n\n\tdef create_table(self):\n\t\tself.cursor.execute(f\"\"\"\n\t\t\tCREATE TABLE IF NOT EXISTS {self.name}\n\t\t\t\t(\n\t\t\t\thref TEXT\n\t\t\t\t);\n\t\t\t\"\"\")\n\t\tself.conn.commit()\n\n\tdef load_hrefs(self, page_end):\n\t\tprint(\"\\nReloading hrefs from yiffy site\")\n\t\tself.re_init()\n\t\t\n\t\tpage_range = range(1, page_end + 1)\n\t\tfor page in tqdm(page_range):\n\t\t\threfs = webscrape.MoviesHref(page)\n\t\t\tfor href in hrefs.movies_href:\n\t\t\t\tself.insert_href(href)\n\t\n\tdef insert_href(self, href):\n\t\tself.cursor.execute(f\"\"\"\n\t\t\t\tINSERT INTO {self.name} (href)\n\t\t\t\t\tVALUES (\"{href}\")\n\t\t\"\"\")\n\t\tself.conn.commit()\t   \n\n\tdef update(self, new_hrefs, current_hrefs):\n\t\tconcat_hrefs = new_hrefs + current_hrefs\n\t\tif not concat_hrefs:\n\t\t\traise ValueError(\"No href values found\")\n\t\t\t\n\t\tself.re_init()\n\t\tfor href in concat_hrefs:\n\t\t\tself.insert_href(href)\n\n\tdef remove_href(self, href):\n\t\tself.cursor.execute(f\"\"\"\n\t\t\t\tDELETE FROM {self.name} WHERE href = \"{href}\";\n\t\t\"\"\")\n\t\tself.conn.commit()\n\n", "description": "\n\tManages href database, creation, loading, updating\n\n\tAttributes:\n\t\tname(str): Name of the table\n\n\t", "category": "webscraping", "imports": ["import sqlite3 as sql", "import webscrape", "import warnings", "from tqdm import tqdm"]}, {"term": "class", "name": "MovieTable", "data": "class MovieTable(Table):\n\tdef __init__(self, database:str, name:str):\n\t\tsuper().__init__(database, name)\n\t\tself.broken_hrefs = []\n\n\tdef create_table(self):\n\t\tself.cursor.execute(f\"\"\"\n\t\t\tCREATE TABLE IF NOT EXISTS {self.name}\n\t\t\t\t(\n\t\t\t\ttitle TEXT, \n\t\t\t\tyear INT, \n\t\t\t\tgenre TEXT, \n\t\t\t\trating FLOAT,\n\t\t\t\thref TEXT\n\t\t\t\t);\n\t\t\t\"\"\")\n\t\tself.conn.commit()\n\n\tdef load_movies(self, hrefs):\n\t\tprint(\"\\nLoading movies\")\n\t\tmissing_hrefs = self._find_missing_hrefs(hrefs)\n\t\tfor href in tqdm(missing_hrefs):\n\t\t\tmovie = webscrape.Movie(href)\n\t\t\tself.insert_movie(movie)\n\n\tdef _find_missing_hrefs(self, hrefs):\n\t\tmovie_hrefs = self.fetch_hrefs()\n\t\tif not movie_hrefs:\n\t\t\treturn hrefs\n\t\telse:\n\t\t\treturn [href for href in hrefs if href not in movie_hrefs]\n\t\t\n\tdef insert_movie(self, Movie):\n\t\tif not Movie.info:\n\t\t\tself.broken_hrefs.append(Movie.href)\n\t\t\treturn\n\t\ttitle = Movie.info.get(\"title\")\n\t\tyear = Movie.info.get(\"year\")\n\t\tgenre = Movie.info.get(\"genre\")\n\t\trating = Movie.info.get(\"rating\")\n\t\thref = Movie.info.get(\"href\")\n\n\t\tself.cursor.execute(f\"\"\"\n\t\t\t\tINSERT INTO movie (title, year, genre, rating, href)\n\t\t\t\t\tVALUES (\"{title}\", \"{year}\", \"{genre}\", \"{rating}\", \"{href}\")\n\t\t\"\"\")\n\t\tself.conn.commit()\n\n\tdef drop_broken_hrefs(self, href_table):\n\t\tfor broken in self.broken_hrefs:\n\t\t\thref_table.remove_href(broken)\n\n\tdef update(self, new_hrefs):\n\t\t\"\"\"\n\t\tInserts new movies from hrefs to database\n\t\t\n\t\tParameters:\n\t\t\tnew_hrefs: list containing new hrefs to insert into database\n\n\t\tNote:\n\t\t\tThis method does not check current movie db for copies, if inserting make sure movies\n\t\t\tare not already in database.\n\t\t\"\"\"\n\t\tprint(\"Updating movies\")\n\t\tfor href in tqdm(new_hrefs):\n\t\t\tmovie = webscrape.Movie(href)\n\t\t\tself.insert_movie(movie)\n\n\n", "description": "\n\t\t\tCREATE TABLE IF NOT EXISTS {self.name}\n\t\t\t\t(\n\t\t\t\ttitle TEXT, \n\t\t\t\tyear INT, \n\t\t\t\tgenre TEXT, \n\t\t\t\trating FLOAT,\n\t\t\t\thref TEXT\n\t\t\t\t);\n\t\t\t", "category": "webscraping", "imports": ["import sqlite3 as sql", "import webscrape", "import warnings", "from tqdm import tqdm"]}, {"term": "class", "name": "classDataBaseManager:", "data": "class DataBaseManager:\n\t\"\"\"Convient object for managing href and movie tables\"\"\"\n\tdef __init__(self, href_table, movie_table):\n\t\tself.movie_table = movie_table\n\t\tself.href_table = href_table\n\n\tdef update_databases(self):\n\t\t\"\"\"Append latest movies released from Yify site to databases\"\"\"\n\t\tpre_loaded_hrefs = self.href_table.fetch_hrefs()\n\t\tnew_hrefs = self._get_new_hrefs(pre_loaded_hrefs)\n\n\t\tif not new_hrefs:\n\t\t\twarnings.warn(\"Nothing to update\")\n\t\telse:\n\t\t\tself.movie_table.update(new_hrefs)\n\t\t\tself.href_table.update(new_hrefs, pre_loaded_hrefs)\n\n\n\t@staticmethod\n\tdef _get_new_hrefs(table_hrefs):\n\t\t\"\"\"Search yify latest until href matches first_href in list\"\"\"\n\t\tfirst_href = table_hrefs[0]\n\n\t\tnew_hrefs = []\n\t\tpage = 1\n\t\twhile True:\n\t\t\threfs = webscrape.MoviesHref(page).movies_href\n\t\t\tfor href in hrefs:\n\t\t\t\tif href == first_href:\n\t\t\t\t\treturn new_hrefs\t\t\t\t \n\t\t\t\telse:\n\t\t\t\t\tnew_hrefs.append(href)\n", "description": "Convient object for managing href and movie tables", "category": "webscraping", "imports": ["import sqlite3 as sql", "import webscrape", "import warnings", "from tqdm import tqdm"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\t# \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\t# \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n\tdef get_gloss(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "from ltp import LTP"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\t# \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\t# \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n\tdef get_gloss(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "from ltp import LTP"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\t# \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\t# \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n\tdef get_gloss(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "from ltp import SentenceSplitter"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\t# \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\t# \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n\tdef get_gloss(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import ltp"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\tdef __init__(self, word, url):\n\t\tself.url = url\n\t\tself.word = word\n\n\t# \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n\tdef web_parse(self):\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n\t\t\t\t\t\t\t\t\t\t\t (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n\t\treq = requests.get(url=self.url, headers=headers)\n\n\t\t# \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n\t\tif req.status_code == 200:\n\t\t\tsoup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n\t\t\treturn soup\n\t\treturn None\n\n\t# \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n\tdef get_gloss(self):\n\t\tsoup = self.web_parse()\n\t\tif soup:\n\t\t\tlis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n\t\t\tif lis:\n\t\t\t\tfor li in lis('li'):\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "from pyltp import SentenceSplitter"]}], [{"term": "def", "name": "main", "data": "def main():\n\t#inputs\n\tpath = r'C:\\Users\\yongj\\Downloads\\2021-03-16 SEL Scraper\\Links.xlsx'\n\tpath_storage = r'C:\\Users\\yongj\\Downloads\\3. Business News\\To Read'\n\theadless_window = True\n\t# headless_window = False\n\tidx_link = 0\n\n\n\t#Part One: Get links from excel\n\tPartOne = ObtainLinks(path)\n\tlist_links = PartOne.ReadExcel()\n\n\n\t#setting up the options for the chromedriver for Part Two\n\toptions = Options()\n\tif headless_window == True:\n\t\toptions.add_argument('--headless')\n\t\toptions.add_argument(\"window-size=1920,1080\") \n\t#defining the window size of the headless browser avoids errors with finding location and date for each job posting\n\tpath_chromedriver = r'C:\\Program Files (x86)\\chromedriver.exe'\n\tdriver = webdriver.Chrome(executable_path=path_chromedriver, options=options)\n\n\t#Part Two: Webrscrape\n\tPartTwo = Webscrape(path_storage)\n\twhile idx_link < len(list_links):\n\t\tlink = list_links[idx_link]\n\t\tprint(f'{idx_link+1}/{len(list_links)})\\t{link}')\n\t\tarticle_type = PartTwo.Article_type(link)\n\t\t# print(article_type)\n\t\tPartTwo.AccessWebpage(driver, link, article_type)\n\t\tprint('\\n')\t\n\t\tidx_link+=1\n\n\n\t#Part Three: Moving Word Docs to the \"To Read Folder\" & updating excel\n\t#Refer to python saved in Superseded to move all word docs to 'To Read' folder and update the excel\n\timport sys\n\tsys.path.insert(1, 'C:/Users/yongj/Documents/Coding/Python/Web Scraping/0. Superseded')\n\tfrom Update_Excel_Move_Word import UpdateExcel_MoveWord\n\tUpdateExcel_MoveWord()\n\n\n", "description": null, "category": "webscraping", "imports": ["from typing import Set", "from selenium.webdriver.common.by import By", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.support.ui import WebDriverWait", "# from selenium.webdriver.support import expected_conditions as EC", "from selenium.webdriver.chrome.options import Options", "# from selenium.webdriver.common.action_chains import ActionChains", "import pandas as pd, numpy as np", "import datetime, time, random, glob, os, shutil, re", "from docx import Document", "\timport sys", "\tfrom Update_Excel_Move_Word import UpdateExcel_MoveWord"]}, {"term": "class", "name": "ObtainLinks", "data": "class ObtainLinks():\n\tdef __init__(self, path):\n\t\tself.path = path\n\n\tdef ReadExcel(self):\n\t\t# print(self.path)\n\t\tdf = pd.read_excel(self.path)\n\t\t# print(df['No Seekingalpha Links'].to_list()[:10])\n\t\tself.list_links = df['No Seekingalpha Links'].to_list()\n\t\treturn self.list_links\n", "description": null, "category": "webscraping", "imports": ["from typing import Set", "from selenium.webdriver.common.by import By", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.support.ui import WebDriverWait", "# from selenium.webdriver.support import expected_conditions as EC", "from selenium.webdriver.chrome.options import Options", "# from selenium.webdriver.common.action_chains import ActionChains", "import pandas as pd, numpy as np", "import datetime, time, random, glob, os, shutil, re", "from docx import Document", "\timport sys", "\tfrom Update_Excel_Move_Word import UpdateExcel_MoveWord"]}, {"term": "class", "name": "SEL_Scrapers", "data": "class SEL_Scrapers():\n\tdef __init__(self, driver, link):\n\t\tself.driver = driver\n\t\tself.link = link\n\n\t# comment below is for reference for the pieces of info required\n\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\tdef Create_docx(self):\t\n\t\turl = self.link\n\t\ttitle = self.title\n\t\tdate = self.date\n\t\tauthor = self.author\n\t\tsummary = self.summary\n\t\tcontent = self.content\n\t\ttable_data = self.table_data\n\t\timg_tags = self.img_tags\n\n\t\tdocument = Document()\n\t\t\t\n\t\tdocument.add_heading(url)\n\n\t\t#Removing characters that cannot be used in the title\n\t\ttitle = title.replace('/', \" \")\n\t\ttitle = title.replace('\"', '')\n\t\ttitle = title.replace(':', ' - ') \n\t\ttitle = title.replace('?', ' - ')\n\t\ttitle = title.replace('*', ' - ')\n\t\ttitle = title.replace('|', ' - ')\n\n\t\t#add in content to the word docx\n\t\tdocument.add_paragraph(title)\n\t\tdocument.add_paragraph(str(date))\n\t\tdocument.add_paragraph(author)\n\t\tfor a in summary:\n\t\t\tdocument.add_paragraph(a.text)\n\t\trow=1\n\t\tfor p in content:\n\t\t# \tdocument.add_paragraph(f'Line {row}: \\n{p.text}')\n\t\t# \trow+=1\n\t\t\tif len(p.text.split()) >3:\n\t\t\t\tdocument.add_paragraph(p.text)\n\n\t\t#Create a table if table data is passed in\n\t\tif table_data:\n\t\t\t# rowNum = len(table_data)\n\t\t\tcolNum = len(table_data[0])\n\t\t\t#initialize table with a blank row\n\t\t\tTable = document.add_table(rows=1,cols=colNum)\n\t\t\t# # table styles link:  https://python-docx.readthedocs.io/en/latest/user/styles-understanding.html#table-styles-in-default-template\n\t\t\tTable.style = \"Table Grid\"\n\t\t\tfor row in table_data:\n\t\t\t\trow_Cells = Table.add_row().cells \n\t\t\t\tfor cell in range(0,colNum):\n\t\t\t\t\trow_Cells[cell].text = row[cell]\t\n\n\n\t\t#Append Images to word doc\n\t\tlinks =[]\n\n\t\tfor img in img_tags:\n\t\t\ttry:\n\t\t\t\timg_width = img['data-width']\n\t\t\t\tlinks.append(img['src'])\n\t\t\texcept KeyError:\n\t\t\t\tcontinue\n\t\t# print(links)\n\t\tlist_imgtypes = ['.jpg','.png']\n\t\tnum = 1\n\t\tfor link in links:\n\t\t\t#Get image type\n\t\t\timage_type = link[-4:]\n\t\t\tif image_type in list_imgtypes:\n\t\t\t\timage_type=image_type\n\t\t\telse:\n\t\t\t\timage_type='.jpeg'\n\t\t\t\n\t\t\ttry:\n\t\t\t\tfilename = \"Image\"+str(num)\n\t\t\t\timagefile = open(filename+image_type, \"wb\")\n\t\t\t\timagefile.write(requests.get(link).content)\n\t\t\t\timagefile.close()\n\t\t\t\t# print('Created image')\n\t\t\t\tdocument.add_paragraph(filename+image_type)\n\t\t\t\tdocument.add_picture(filename+image_type,width=docx.shared.Inches(6))\n\t\t\t\t# document.add_picture(requests.get(link).content,width=Inches(4))\n\t\t\t\tos.remove(filename+image_type)\n\t\t\t\t# print(f'added {filename}')\n\n\t\t\texcept:\n\t\t\t\t# print('Then Failed to add')\n\t\t\t\tfilename = \"Image\"+str(num)\n\t\t\t\tdocument.add_paragraph(filename+image_type + \" COULD NOT BE ADDED DUE TO SOME ERROR.\\n\"+link)\n\t\t\tnum+=1\n\n\t\twordname = str(date)+\" \" + title+\".docx\"\n\t\tdocument.save(wordname)\n\t\t# path_toREAD = r\"C:\\Users\\yongj\\Downloads\\3. Business News\\To Read\"+'\\\\'+wordname\n\t\t# document.save(path_toREAD)\n\tdef Reset_PgINFO(self):\n\t\tself.url = ''\n\t\tself.title =''\n\t\tself.date =''\n\t\tself.author=''\n\t\tself.summary= []\n\t\tself.content=[]\n\t\tself.table_data=[]\n\t\tself.img_tags=[]\n\n\n\tdef oilprice(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"Oilprice.com\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('//*[@id=\"pagecontent\"]/div[3]/div/div[1]/div[2]/div[2]/h1').text\n\t\tself.author =  driver.find_element_by_xpath('//*[@id=\"pagecontent\"]/div[3]/div/div[1]/div[2]/div[2]/span/a').text\n\t\tself.date =  driver.find_element_by_xpath('//*[@id=\"pagecontent\"]/div[3]/div/div[1]/div[2]/div[2]/span').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  []\n\t\tself.content =  driver.find_elements_by_xpath('//*[@id=\"news-content\"]')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\t\tprint(self.title)\n\t\t# for p in self.content:\n\t\t# \tprint(p.text)\n\n\tdef boereport(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"boereport.com\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('/html/body/div[1]/div/div/main/article/header/h1').text\n\t\tself.author =  'boereport'\n\t\tself.date =  driver.find_element_by_xpath('/html/body/div[1]/div/div/main/article/header/p/time[1]').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  []\n\t\tself.content =  driver.find_elements_by_xpath('/html/body/div[1]/div/div/main/article/div')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\tdef cnbc(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"cnbc.com\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[1]/div[1]/h1').text\n\t\tself.author =  driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[2]/div[1]/div/div/div/div/a[1]').text\n\t\tself.date =  driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[1]/div[2]/time[1]').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  driver.find_elements_by_xpath('//*[@id=\"RegularArticle-KeyPoints-4\"]/div/div[2]/div/div/ul')\n\t\tself.content =  driver.find_elements_by_xpath('//*[@id=\"RegularArticle-ArticleBody-5\"]/div[2]')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\t\tprint(self.title,'\\n')\n\t\t# for p in self.content:\n\t\t# \tprint(p.text)\n\n\tdef nbc(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"nbc.com\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[1]/div[1]/h1').text\n\t\tself.author =  driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[2]/div[1]/div/div/div/div/a[1]').text\n\t\tself.date =  driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[1]/div[2]/time[1]').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  driver.find_elements_by_xpath('//*[@id=\"RegularArticle-KeyPoints-4\"]/div/div[2]/div/div/ul')\n\t\tself.content =  driver.find_elements_by_xpath('//*[@id=\"RegularArticle-ArticleBody-5\"]/div[2]')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\t\tprint(self.title,'\\n')\n\t\t# for p in self.content:\n\t\t# \tprint(p.text)\n\n\tdef AB(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"ab.ca\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('//*[@id=\"top\"]/header/div[4]/div[2]/div/h1').text\n\t\tself.author =  'AB Government'\n\t\tself.date =  driver.find_element_by_xpath('//*[@id=\"top\"]/header/div[4]/div[2]/div/div/time').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  driver.find_elements_by_xpath('//*[@id=\"top\"]/header/div[4]/div[2]/div')\n\t\tself.content =  driver.find_elements_by_xpath('//*[@id=\"main\"]/div[2]/div')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\t\tprint(self.title,'\\n')\n\t\t# for p in self.content:\n\t\t# \tprint(p.text)\n\n", "description": null, "category": "webscraping", "imports": ["from typing import Set", "from selenium.webdriver.common.by import By", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.support.ui import WebDriverWait", "# from selenium.webdriver.support import expected_conditions as EC", "from selenium.webdriver.chrome.options import Options", "# from selenium.webdriver.common.action_chains import ActionChains", "import pandas as pd, numpy as np", "import datetime, time, random, glob, os, shutil, re", "from docx import Document", "\timport sys", "\tfrom Update_Excel_Move_Word import UpdateExcel_MoveWord"]}, {"term": "class", "name": "Webscrape", "data": "class Webscrape():\n\tdef __init__(self, path_storage):\n\t\tself.path_storage = path_storage\n\n\tdef FormatDate(self, date):\n\t\t# print(f'Original date from link:   \"{date}\"')\n\t\tlist_years =[]\n\t\tyear1 = 2010\n\t\twhile year1 < 2050:\n\t\t\tyear1+=1\n\t\t\tlist_years.append(year1)\n\t\tfor year in list_years:\n\t\t\tif str(year) in date:\n\t\t\t\t# print(f'year found was \"{year}\" in date: \"{date}\"')\n\t\t\t\tidx_year = date.index(str(year))+4\n\t\t\t\tdate = date[:idx_year]\n\t\t\t\t# print(f'updated string: \"{date}\"')\n\t\t\t\tbreak\n\n\t\tlist_fullmonths = 'January February March April May June July August September October November December'.split()\n\t\tlist_months = 'Jan Feb Mar Apr May Jun July Aug Sept Oct Nov Dec'.split(' ')\n\t\t# dict_months = {}\n\t\t# print(f'length of list_fullmonths:\"{len(list_fullmonths)}\",  length of list_months: \"{len(list_months)}\"')\n\t\tlowercaps = False\n\t\tlowercapsFULL = False\n\t\tuppercapsFULL = False\n\n\t\t# print('Testing for Case 1: Upper 1st letter Only in Full Month word')\n\t\tfor idx,month in enumerate(list_fullmonths):\n\t\t\t# dict_months[month] = list_months[idx]\n\t\t\tif month in date:\n\t\t\t\t# print(f'the month \"{month}\" was found in the string \"{date}\"')\n\t\t\t\tdate = date.replace(month, list_months[idx]).replace('.','')\n\t\t\t\tidx_month = date.index(list_months[idx])\n\t\t\t\t# print(f'idx_month found was \"{idx_month}\"')\n\t\t\t\tlowercaps = True\n\t\t\t\tlowercapsFULL = True\n\t\t\t\tuppercapsFULL = True\n\t\t\t\tbreak\t\n\n\t\tif lowercaps == False:\n\t\t\t# print('Testing for Case 2: Upper 1st letter Only')\t\n\t\t\tfor idx,month in enumerate(list_months):\n\t\t\t\t# print(month)\n\t\t\t\tif month in date:\n\t\t\t\t\t# print(f'the month \"{month}\" was found in the string \"{date}\"')\n\t\t\t\t\tdate = date.replace(month, list_months[idx]).replace('.','')\n\t\t\t\t\tidx_month = date.index(list_months[idx])\n\t\t\t\t\t# print(f'idx_month found was \"{idx_month}\"')\n\t\t\t\t\tlowercapsFULL = True\n\t\t\t\t\tuppercapsFULL = True\n\t\t\t\t\tbreak\t\n\n\t\tif lowercapsFULL == False:\n\t\t\t# print('Testing for Case 3: Upper case for entire Full Month word')\n\t\t\tfor idx,month in enumerate(list_fullmonths):\n\t\t\t\t# dict_months[month] = list_months[idx]\n\t\t\t\tmonth = month.upper()\n\t\t\t\t# print(month)\n\t\t\t\tif month in date:\n\t\t\t\t\t# print(f'the month \"{month}\" was found in the string \"{date}\"')\n\t\t\t\t\tdate = date.replace(month, list_months[idx]).replace('.','')\n\t\t\t\t\tidx_month = date.index(list_months[idx])\n\t\t\t\t\t# print(f'idx_month found was \"{idx_month}\"')\n\t\t\t\t\tuppercapsFULL = True\n\t\t\t\t\tbreak\t\n\n\t\tif uppercapsFULL == False:\n\t\t\t# print('Testing for Case 4: Upper case for SHORTENED Month')\n\t\t\tfor idx,month in enumerate(list_months):\n\t\t\t\tmonth = month.upper()\n\t\t\t\t# print(month)\n\t\t\t\tif month in date:\n\t\t\t\t\t# print(f'the month \"{month}\" was found in the string \"{date}\"')\n\t\t\t\t\tdate = date.replace(month, list_months[idx]).replace('.','')\n\t\t\t\t\tidx_month = date.index(list_months[idx])\n\t\t\t\t\t# print(f'idx_month found was \"{idx_month}\"')\n\t\t\t\t\tbreak\t\n\n\n\t\t# print(f'idx_month is \"{idx_month}\"')\t\t\t\t\n\t\tdate = date[idx_month:]\n\t\t# print(f'new_date is \"{date}\"')\n\n\t\t#date should be in the format of \"Month. day, year\": for example == 'Oct. 2, 2020 7:27 AM ET'\n\t\tdate = date.split()[:3]\n\t\tday_int = int(date[1].replace(',',''))\n\t\tif day_int<10:\n\t\t\tdate[1]='0'+str(day_int)\n\t\tdate = ' '.join(date).replace(',','')\n\t\t# print(f'date found for FormatDate(): \"{date}\"')\n\t\tf_article_day = datetime.datetime.strptime(date, '%b %d %Y').date()\n\t\t# print(f_article_day)\n\t\treturn f_article_day\n\tdef Article_type(self, url):  \n\t\tlist_articleTypes = ['oilprice', 'boereport', 'transcript', 'seekingalpha.com/article', 'cnbc.com', 'nbcnews', 'alberta.ca', 'seekingalpha.com/news']\n\t\tidx=0\n\n\t\twhile True:\n\t\t\tRegexCondition = re.search(list_articleTypes[idx],url)\n\t\t\tif RegexCondition:\n\t\t\t\tarticle_type=RegexCondition.group()\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tidx+=1\n\t\treturn article_type\n\n\tdef AccessWebpage(self, driver, link, article_type):\n\t\t# ['oilprice', 'boereport', 'cnbc.com', 'nbcnews', 'alberta.ca', 'seekingalpha.com/news', 'transcript', 'seekingalpha.com/article']\n\t\tdriver.get(link)   \n\n\t\tSEL = SEL_Scrapers(driver, link)\n\t\tif article_type == 'oilprice':\n\t\t\tSEL.oilprice()\n\t\telif article_type == 'boereport':\n\t\t\tSEL.boereport()\n\t\telif article_type == 'cnbc.com':\n\t\t\tSEL.cnbc()\n\t\telif article_type == 'nbcnews':\n\t\t\tSEL.nbc()\n\t\telif article_type == 'alberta.ca':\n\t\t\tSEL.AB()\n\n\t\tSEL.Create_docx()\n\t\tSEL.Reset_PgINFO()\n\n\n", "description": null, "category": "webscraping", "imports": ["from typing import Set", "from selenium.webdriver.common.by import By", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.support.ui import WebDriverWait", "# from selenium.webdriver.support import expected_conditions as EC", "from selenium.webdriver.chrome.options import Options", "# from selenium.webdriver.common.action_chains import ActionChains", "import pandas as pd, numpy as np", "import datetime, time, random, glob, os, shutil, re", "from docx import Document", "\timport sys", "\tfrom Update_Excel_Move_Word import UpdateExcel_MoveWord"]}], [{"term": "class", "name": "classWebscrapeBlogs:", "data": "class WebscrapeBlogs:\n\tdef __init__(self, keywords):\n\t\tself.keywords = keywords\n\n\n\t@staticmethod\n\tdef jornalnoticias(search):\n\t\turl = \"https://jornalnoticias.co.mz/?s=\" + search\n\t\tpage_request = requests.get(url)\n\t\tdata = page_request.content\n\t\tsoup = BeautifulSoup(data, \"html.parser\")\n\n\t\tdate_title_link_all = list()\n\n\t\tfor divtag in soup.find_all('div', {'class': 'tdb_module_loop td_module_wrap td-animation-stack'}):\n\n\t\t\tdate_title_link = list()\n\n\t\t\tfor div_time_tag in divtag.find_all('span', {'class': 'td-post-date'}):\n\t\t\t\tdate_title_link.append(div_time_tag.find('time').text)\n\n\t\t\tfor h3tag in divtag.find_all('h3', {'class': 'entry-title td-module-title'}):\n\t\t\t\tdate_title_link.append(h3tag.find('a')['title'])\n\t\t\t\tdate_title_link.append(h3tag.find('a')['href'])\n\n\t\t\tdate_title_link_all.append(date_title_link)\n\n\t\treturn date_title_link_all\n\n\t@staticmethod\n\tdef opais(search):\n\t\turl = \"https://opais.co.mz/?s=\" + search\n\t\tpage_request = Request(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36'}) # TO deal with blocks\n\t\tdata = urlopen(page_request).read()\n\t\tsoup = BeautifulSoup(data, \"html.parser\")\n\n\t\tdate_title_link_all = list()\n\n\t\tfor divtag in soup.find_all('div', {'class': \"elementor-section-wrap\"}):\n\n\t\t\tdate_title_link = list()\n\n\t\t\tfor litag in divtag.find_all('li', {'class': 'elementor-icon-list-item elementor-repeater-item-7014203 elementor-inline-item'}):\n\t\t\t\ttry:\n\t\t\t\t\tdate_title_link.append(litag.find('span').text.strip())\n\t\t\t\texcept:\n\t\t\t\t\tbreak\n\n\t\t\tfor h2tag in divtag.find_all('h2', {'class': 'elementor-heading-title elementor-size-default'}):\n\t\t\t\ttry:\n\t\t\t\t\tdate_title_link.append(h2tag.find('a').text)\n\t\t\t\t\tdate_title_link.append(h2tag.find('a')['href'])\n\t\t\t\texcept:\n\t\t\t\t\tbreak\n\n\t\t\tif len(date_title_link) == 3:\n\t\t\t\tdate_title_link_all.append(date_title_link)\n\n\t\treturn date_title_link_all\n\n\tdef all_process_jornalnoticias(self):\n\t\tdic = dict()\n\t\tfor keyword in self.keywords:\n\t\t\ttry:\n\t\t\t\thold = self.jornalnoticias(keyword)\n\t\t\t\tif not hold:\n\t\t\t\t\tcontinue\n\t\t\t\tdic[keyword] = hold\n\t\t\texcept:\n\t\t\t\tcontinue\n\n\t\treturn dic\n\n\tdef all_process_opais(self):\n\t\tdic = dict()\n\t\tfor keyword in self.keywords:\n\t\t\ttry:\n\t\t\t\thold = self.opais(keyword)\n\t\t\t\tif not hold:\n\t\t\t\t\tcontinue\n\t\t\t\tdic[keyword] = hold\n\t\t\texcept:\n\t\t\t\tcontinue\n\n\t\treturn dic\n\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import Request, urlopen", "import requests", "from bs4 import BeautifulSoup"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('webscrape', '0005_auto_20190329_1257'),\n\t\t('dataprocess', '0006_prochousedata'),\n\t]\n\n\toperations = [\n\t\tmigrations.AddField(\n\t\t\tmodel_name='prochousedata',\n\t\t\tname='raw',\n\t\t\tfield=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, to='webscrape.RawHouseData'),\n\t\t),\n\t]\n", "description": null, "category": "webscraping", "imports": ["from django.db import migrations, models", "import django.db.models.deletion"]}], [{"term": "def", "name": "scrape", "data": "def scrape():\n\tsite = raw_input(\"Enter page: \")\n\n\t#open site. read so we can read in a string context\n\t#test for valid and complete URL\n\ttry:\n\t\tdata = urllib2.urlopen(site).read()\n\texcept ValueError:\n\t\tprint \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n\t\treturn\n\t\n\t#print data\n\n\t#try an open the pattern file.\n\ttry:\n\t\tpatternFile = open('config/webscrape.dat', 'r').read().splitlines()\n\texcept:\n\t\tprint \"There was an error opening the webscrape.dat file\"\n\t\traise\n\t#create counter for counting regex expressions from webscrape.dat\n\tcounter = 0\n\t#for each loop so we can process each specified regex\n\tfor pattern in patternFile:\n\t\tm = re.findall(pattern, data)\n\t\t#m will return as true/false. Just need an if m:\n\t\tif m:\n\t\t\tfor i in m:\n\t\t\t\t#open output/results file...append because we are cool\n\t\t\t\toutfile = open('scrape-RESULTS.txt', 'a')\n\t\t\t#print m\n\t\t\t\toutfile.write(str(i))\n\t\t\t\toutfile.write(\"\\n\")  # may be needed. can always be removed.\n\n\t\t\t#close the file..or else\n\t\t\t\toutfile.close()\n\t\t\t\tcounter+=1\n\t\t\t\tprint \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n\t\telse:  # only need an else because m is boolean\n\t\t\tcounter+=1\n\t\t\tprint \"No match for item \" + str(counter) + \". Continuing.\"\n\t\t\t# Continue the loop if not a match so it can go on to the next\n\t\t\t# sequence\n\t\t\t# NOTE: you don't *really* need an else here...\n\t\t\tcontinue\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "import re", "\timport readline  # nice when you need to use arrow keys and backspace", "import sys"]}], [{"term": "def", "name": "scrape", "data": "def scrape():\n\tsite = raw_input(\"Enter page: \")\n\n\t#open site. read so we can read in a string context\n\t#test for valid and complete URL\n\ttry:\n\t\tdata = urllib2.urlopen(site).read()\n\texcept ValueError:\n\t\tprint \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n\t\treturn\n\t\n\t#print data\n\n\t#try an open the pattern file.\n\ttry:\n\t\tpatternFile = open('config/webscrape.dat', 'r').read().splitlines()\n\texcept:\n\t\tprint \"There was an error opening the webscrape.dat file\"\n\t\traise\n\t#create counter for counting regex expressions from webscrape.dat\n\tcounter = 0\n\t#for each loop so we can process each specified regex\n\tfor pattern in patternFile:\n\t\tm = re.findall(pattern, data)\n\t\t#m will return as true/false. Just need an if m:\n\t\tif m:\n\t\t\tfor i in m:\n\t\t\t\t#open output/results file...append because we are cool\n\t\t\t\toutfile = open('scrape-RESULTS.txt', 'a')\n\t\t\t#print m\n\t\t\t\toutfile.write(str(i))\n\t\t\t\toutfile.write(\"\\n\")  # may be needed. can always be removed.\n\n\t\t\t#close the file..or else\n\t\t\t\toutfile.close()\n\t\t\t\tcounter+=1\n\t\t\t\tprint \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n\t\telse:  # only need an else because m is boolean\n\t\t\tcounter+=1\n\t\t\tprint \"No match for item \" + str(counter) + \". Continuing.\"\n\t\t\t# Continue the loop if not a match so it can go on to the next\n\t\t\t# sequence\n\t\t\t# NOTE: you don't *really* need an else here...\n\t\t\tcontinue\n", "description": null, "category": "webscraping", "imports": ["import urllib2", "import re", "\timport readline  # nice when you need to use arrow keys and backspace", "import sys"]}], [{"term": "def", "name": "_page_hopping", "data": "def _page_hopping(page_number: int, soup, driver) -> int:\n\t\"\"\"page changing mechanism\"\"\"\n\turl_2 = soup.find('ul', {'class': 'a-pagination'})\n\turl_2_a = url_2.find_all('a')\n\tif 'page=' not in driver.current_url:\n\t\tfor link_a in url_2_a:\n\t\t\tif link_a.text == '2':\n\t\t\t\turl_2_link = 'https://www.amazon.com/' + \\\n\t\t\t\t\tlink_a['href']\n\t\t\t\tpage_number += 1\n\t\t\t\tdriver.get(url_2_link)\n\tif f'page={page_number}' in driver.current_url:\n\t\tlink_page = driver.current_url\n\t\tpage_number_2 = page_number + 1\n\t\tlink_page_2 = link_page.replace(\n\t\t\tf'page={page_number}', f'page={page_number_2}')\n\t\tlink_page_3 = link_page_2.replace(\n\t\t\tf'sr_pg_{page_number}', f'sr_pg_{page_number_2}')\n\t\tpage_number += 1\n\t\tdriver.get(link_page_3)\n\treturn page_number\n\n", "description": "page changing mechanism", "category": "webscraping", "imports": ["from typing import Collection", "from pymongo import MongoClient", "from pymongo.errors import ConnectionFailure", "from pandas import DataFrame", "from time import sleep"]}, {"term": "def", "name": "_comprehensive_search", "data": "def _comprehensive_search(item_url_list: list, master_list: list, i: int, driver, tqdm, BeautifulSoup) -> None:\n\t\"\"\"iterating over item urls for more data.\n\ti: index of master_list\"\"\"\n\tprint('collecting entries')\n\tpbar_2 = tqdm(total=len(item_url_list))  # load bar 2\n\tfor url_3 in item_url_list:\n\t\tdriver.get(url_3)\n\t\tsoup_product = BeautifulSoup(driver.page_source, 'html.parser')\n\t\tfor product in soup_product:\n\t\t\ttry:\n\t\t\t\timg_product = product.find(\n\t\t\t\t\t'img', {'id': 'landingImage'})['src']\n\t\t\texcept:\n\t\t\t\timg_product = 'None'\n\t\t\tif 'img_url' not in master_list[i].keys():\n\t\t\t\tmaster_list[i].setdefault('img_url', img_product)\n\t\t\ttry:\n\t\t\t\tbrand_product = product.find(\n\t\t\t\t\t'a', {'id': 'bylineInfo'})['href']\n\t\t\texcept:\n\t\t\t\tbrand_product = 'None'\n\t\t\t\tif 'brand_url' not in master_list[i].keys():\n\t\t\t\t\tmaster_list[i].setdefault(\n\t\t\t\t\t\t'brand_url', brand_product)\n\t\t\tif 'brand_url' not in master_list[i].keys():\n\t\t\t\tmaster_list[i].setdefault(\n\t\t\t\t\t'brand_url', 'https://www.amazon.com/' + brand_product)\n\t\t\ttry:\n\t\t\t\tstock_product = product.find(\n\t\t\t\t\t'span', {'class': 'a-size-medium a-color-success'}).text\n\t\t\texcept:\n\t\t\t\ttry:\n\t\t\t\t\tstock_product = product.find(\n\t\t\t\t\t\t'span', {'class': 'a-color-price a-text-bold'}).text\n\t\t\t\texcept AttributeError:\n\t\t\t\t\tstock_product = 'None'\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tmulti_seller_link = product.find_all('a')\n\t\t\t\t\tfor link in multi_seller_link:\n\t\t\t\t\t\tif link.text == 'these sellers':\n\t\t\t\t\t\t\tstock_product = 'https://www.amazon.com/' + \\\n\t\t\t\t\t\t\t\tlink['href']\n\t\t\t\texcept TypeError:\n\t\t\t\t\tstock_product = 'TypeError'\n\t\t\tif 'in_stock' not in master_list[i].keys():\n\t\t\t\tstock_product_form_1 = stock_product.replace('\\n', '')\n\t\t\t\tmaster_list[i].setdefault(\n\t\t\t\t\t'in_stock', stock_product_form_1)\n\t\t\tproduct_details = product.find(\n\t\t\t\t'div', {'id': 'detailBulletsWrapper_feature_div'})  # no details for now\n\t\ti += 1\n\t\tpbar_2.update(1)\n\tpbar_2.close()\n\n", "description": "iterating over item urls for more data.\n\ti: index of master_list", "category": "webscraping", "imports": ["from typing import Collection", "from pymongo import MongoClient", "from pymongo.errors import ConnectionFailure", "from pandas import DataFrame", "from time import sleep"]}, {"term": "def", "name": "_captcha_solver", "data": "def _captcha_solver(driver, url: str) -> None:\n\t\"\"\"amazon bot solution for blocked scraping, opening new tab\"\"\"\n\tfor i in range(10):\n\t\tdriver.get(url)\n\t\tcaptcha_bot = \"Sorry, we just need to make sure you're not a robot. For best results, please make sure your browser is accepting cookies.\"\n\t\tcheck = driver.page_source\n\t\tif captcha_bot not in check:\n\t\t\tbreak\n\t\tsleep(2)\n\n", "description": "amazon bot solution for blocked scraping, opening new tab", "category": "webscraping", "imports": ["from typing import Collection", "from pymongo import MongoClient", "from pymongo.errors import ConnectionFailure", "from pandas import DataFrame", "from time import sleep"]}, {"term": "def", "name": "_save_data", "data": "def _save_data(collection: any, data: list, keyword: any, datatype: str, time: str) -> None:\n\t\"\"\"how to save data:\n\t-csv\n\t-json\n\t-MongoDB, local db named 'amazon', collection 'webscrape'\"\"\"\n\tif datatype == \"csv\":\n\t\tamazon_df = DataFrame(data)\n\t\tamazon_df.to_csv(\n\t\t\tf'./dags/programs/ws_amazon/ws_amazon_{keyword}_{time}.csv', index=False)\n\telif datatype == \"json\":\n\t\tamazon_df = DataFrame(data)\n\t\tamazon_df.to_json(\n\t\t\tf'./dags/programs/ws_amazon/ws_amazon_{keyword}_{time}.json', index=False)\n\telif datatype == \"db\":\n\t\tamazon_df = DataFrame(data)\n\t\tamazon_df.reset_index(inplace=True)\n\t\tamazon_dict = amazon_df.to_dict(\"records\")\n\t\tcollection.insert_many(amazon_dict)\n\n", "description": "how to save data:\n\t-csv\n\t-json\n\t-MongoDB, local db named 'amazon', collection 'webscrape'", "category": "webscraping", "imports": ["from typing import Collection", "from pymongo import MongoClient", "from pymongo.errors import ConnectionFailure", "from pandas import DataFrame", "from time import sleep"]}, {"term": "def", "name": "_connect_mongo", "data": "def _connect_mongo() -> any:\n\t\"\"\"connect to mongodb, db:amazon, collection:webscrape\"\"\"\n\tclient = MongoClient(\"mongodb://root:root@mongodb:27017\")\n\tsleep(5)\n\ttry:  # test the connection\n\t\tclient.admin.command('ping')\n\texcept ConnectionFailure:\n\t\tprint(\"Server not available\")\n\t\treturn None\n\tdb = client[\"amazon\"]\n\tcollection = db[\"webscrape\"]\n\treturn collection\n", "description": "connect to mongodb, db:amazon, collection:webscrape", "category": "webscraping", "imports": ["from typing import Collection", "from pymongo import MongoClient", "from pymongo.errors import ConnectionFailure", "from pandas import DataFrame", "from time import sleep"]}], [{"term": "class", "name": "Command", "data": "class Command(BaseCommand):\n\tdef add_arguments(self, parser):   #parser is inbuilt\n\t\tparser.add_argument('url', type=str, help='URL to download from')\n\t\tparser.add_argument('--shift', default=\"0\", type=int, help=\"The encryption shift value\")\n\n\n\tdef handle(self, *args, **options):\n\t\thelp = 'The help for downloadpage command'\n\t\tself.stdout.write(f'Name and Surname: ... ')\n\t\turl_text = webscrape(options['url'])\n\t\tencrypted_msg = caesar_cipher_encrypt(plain_text=url_text, shift=options['shift'])\n\n\t\t#check if url already exist in DB, if yes - overwrite (same ID)\n\t\tif URLContent.objects.filter(url_address= options['url']):\n\t\t\t#try except preferable here //TODO\n\t\t\tp = URLContent.objects.filter(url_address=options['url']).update(encrypted_content=encrypted_msg)\n\t\t\t#self.stdout.write(f'PageID: {p.id}')\n\t\t\tself.stdout.write('Downloaded and encrypted again.')\n\t\telse:\n\t\t\t#try except preferable here //TODO\n\t\t\tp = URLContent(url_address=options['url'], encrypted_content=encrypted_msg)\n\t\t\tp.save()\n\t\t\t#self.stdout.write(f'PageID: {p.id}')\n\t\t\tself.stdout.write('Downloaded and encrypted.')\n\n", "description": null, "category": "webscraping", "imports": ["from django.core.management.base import BaseCommand, CommandError", "from actions.models import URLContent", "from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(url):\n\tr = requests.get(str(url))\n\tsoup = BeautifulSoup(r.content, features=\"html.parser\")   #can use differenet parser, they have unique traits (e.g. some are faster)\n\t#soup = BeautifulSoup(html_doc, features=\"html.parser\")\n\tif r.status_code != 200:\n\t\traise CommandError(f'Something went wrong while trying to reach given URL. Code status {r.status_code}')\n\treturn soup.get_text()\n\n", "description": null, "category": "webscraping", "imports": ["from django.core.management.base import BaseCommand, CommandError", "from actions.models import URLContent", "from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "caesar_cipher_encrypt", "data": "def caesar_cipher_encrypt(plain_text, shift):\n\tresult = \"\"\n\t# transverse the plain text\n\tfor i in range(len(plain_text)):\n\t\tchar = plain_text[i]\n\t\tif char == ' ':\n\t\t\tresult += ' '\n\t\telif char.upper() in ['\u0104', '\u0106', '\u0118', '\u0143', '\u00d3', '\u015a', '\u0179', '\u017b']:\n\t\t\tresult += char\n\t\telse:\n\t\t\t# Encrypt uppercase characters in plain text\n\t\t\tif (char.isupper()):\n\t\t\t\tresult += chr( (ord(char) + shift-65) % 26 + 65)\n\t\t\t# Encrypt lowercase characters in plain text\n\t\t\telse:\n\t\t\t\tresult += chr( (ord(char) + shift-97) % 26 + 97)\n\treturn result\n", "description": null, "category": "webscraping", "imports": ["from django.core.management.base import BaseCommand, CommandError", "from actions.models import URLContent", "from bs4 import BeautifulSoup", "import requests"]}, {"term": "def", "name": "caesar_cipher_decrypt", "data": "def caesar_cipher_decrypt(encrypted_text, shift):\n\tresult = \"\"\n\t# transverse the plain text\n\tfor i in range(len(encrypted_text)):\n\t\tchar = encrypted_text[i]\n\t\tif char == ' ':\n\t\t\tresult += ' '\n\t\telif char.upper() in ['\u0104', '\u0106', '\u0118', '\u0143', '\u00d3', '\u015a', '\u0179', '\u017b']:\n\t\t\tresult += char\n\t\telse:\n\t\t\t# Decrypt uppercase characters in plain text\n\t\t\tif (char.isupper()):\n\t\t\t\tresult += chr( (ord(char) - shift-65) % 26 + 65)\n\t\t\t# Decrypt lowercase characters in plain text\n\t\t\telse:\n\t\t\t\tresult += chr( (ord(char) - shift-97) % 26 + 97)\n", "description": null, "category": "webscraping", "imports": ["from django.core.management.base import BaseCommand, CommandError", "from actions.models import URLContent", "from bs4 import BeautifulSoup", "import requests"]}], [{"term": "def", "name": "getTopSongs", "data": "def getTopSongs():\r\n\t#identify url, proxy as Mozilla, scrape html into a string and clean up escaped chars\r\n\turl= \"https://www.billboard.com/charts/hot-100\"\r\n\thdr = {'User-Agent':'Mozilla/5.0'}\r\n\treq = urllib.request.Request(url, headers=hdr)\r\n\tpage = urlopen(req)\r\n\tscrapedBytes = page.read()\r\n\thtml = scrapedBytes.decode(\"utf-8\")\r\n\thtml = html.encode('utf-8').decode('ascii', 'ignore')\r\n\thtml = html.replace(''', \"\\'\")\r\n\thtml = html.replace('&', \"&\")\r\n\r\n\t#traverse html string, find all song and artist names, and store them in arrays defined below\r\n\t# NOTE: this section is a little obtuse because it is mostly dependent \r\n\t# on what the HTML from the webscrape looks like\r\n\ttitles = []\r\n\tartists = []\r\n\tstartIndex = html.find(\"primary\\\">\") + len(\"primary\\\">\")\r\n\tendIndex = html.find(\"<\", startIndex)\r\n\twhile (startIndex > -1):\r\n\t\ttitles.append(html[startIndex:endIndex])\r\n\t\tstartIndex = html.find(\"secondary\\\">\", endIndex)\r\n\t\tif(startIndex > -1):\r\n\t\t\tstartIndex += len(\"secondary\\\">\")\r\n\t\tendIndex = html.find(\"<\", startIndex)\r\n\t\tartists.append(html[startIndex:endIndex])\r\n\t\tstartIndex = html.find(\"primary\\\">\", endIndex)\r\n\t\tif(startIndex > -1):\r\n\t\t\tstartIndex  += len(\"primary\\\">\")\r\n\t\tendIndex = html.find(\"<\", startIndex)\r\n\r\n\t#append a dictionary for each song to a list and return the songList\r\n\t#could add more dictionary entries later with a larger webscrape\r\n\tsongs = []\r\n\tfor i in range(100):\r\n\t\tthisSong = {\r\n\t\t\t\"title\":titles[i],\r\n\t\t\t\"artist\":artists[i]\r\n\t\t}\r\n\t\tsongs.append(thisSong)\r\n\treturn songs\r\n", "description": null, "category": "webscraping", "imports": ["#some imports\r", "from flask import Flask\r", "from flask_restful import Resource, Api, reqparse\r", "import pandas as pd\r", "import ast\r", "import random\r", "import urllib, urllib.request\r", "from urllib.request import urlopen\r"]}, {"term": "class", "name": "Songs", "data": "class Songs(Resource):\r\n\tdef get(self):\r\n\t\ttopSongs = getTopSongs()\r\n\t\treturn topSongs, 200\r\n", "description": null, "category": "webscraping", "imports": ["#some imports\r", "from flask import Flask\r", "from flask_restful import Resource, Api, reqparse\r", "import pandas as pd\r", "import ast\r", "import random\r", "import urllib, urllib.request\r", "from urllib.request import urlopen\r"]}], [{"term": "def", "name": "scrape_table_data", "data": "def scrape_table_data():\n\treturn [[], ['MH', '5226710', '78007', '4600196'], ['KA', '2053191', '20368', '1440621'],\n\t\t\t['KL', '2010934', '6053', '1571738'], ['UP', '1563235', '16369', '1340251'],\n\t\t\t['TN', '1468864', '16471', '1279658'], ['DL', '1361986', '20310', '1258951'],\n\t\t\t['ANDHRA', '1344386', '8988', '1138028'], ['WB', '1053117', '12728', '911705'],\n\t\t\t['CG', '883210', '11094', '749318'], ['RJ', '805658', '6158', '590390'],\n\t\t\t['GJ', '714611', '8731', '578397'], ['MP', '700202', '6679', '583595'],\n\t\t\t['HR', '652742', '6075', '539609'], ['BR', '622433', '3503', '519306'],\n\t\t\t['OR', '565648', '2232', '473680'], ['TG', '511711', '2834', '449744'],\n\t\t\t['PB', '467539', '11111', '376465'], ['AS', '310086', '1909', '265860'],\n\t\t\t['JH', '301257', '4182', '246608'], ['UK', '264683', '4123', '183478'],\n\t\t\t['JK', '229407', '2912', '174953'], ['HP', '145736', '2068', '104714'],\n\t\t\t['GA', '127639', '1874', '92974'], ['PY', '77031', '1045', '60424'],\n\t\t\t['CH', '52633', '599', '43506'], ['TR', '39054', '424', '34946'],\n\t\t\t['MN', '37036', '526', '31238'], ['ML', '20985', '250', '17354'],\n\t\t\t['AR', '20854', '69', '18691'], ['NL', '16890', '165', '13428'],\n\t\t\t['LADAKH', '15807', '158', '14102'], ['SK', '10392', '183', '7343'],\n\t\t\t['DADRA AND NAGAR HAVELI AND DAMAN AND DIU', '9150', '4', '8086'],\n\t\t\t['MZ', '8176', '23', '6120'], ['AN', '6470', '81', '6175'], ['LD', '4202', '11', '3171']\n\t\t\t]\n\n", "description": null, "category": "webscraping", "imports": ["import marge_sort", "from scrape import WebScrape", "import pytest"]}, {"term": "def", "name": "test_merge", "data": "def test_merge(scrape_table_data):\n\tassert marge_sort.mergesort(scrape_table_data[1:]) == sorted(scrape_table_data[1:], key=lambda x: x[1])\n\n", "description": null, "category": "webscraping", "imports": ["import marge_sort", "from scrape import WebScrape", "import pytest"]}, {"term": "def", "name": "test_table", "data": "def test_table():\n\tws = WebScrape()\n\tassert ws.covidIndia() == [[\"Confirmed Cases\", \"23,703,665\"], [\"Total Deaths\", \"258,317\"],\n\t\t\t\t\t\t\t   [\"Total Recovered\", \"19,734,823\"], [\"Active Cases\", \"3,710,525\"]]\n\n\tassert ws.tableScrape() == [[], ['MH', '5226710', '78007', '4600196'], ['KA', '2053191', '20368', '1440621'],\n\t\t\t\t\t\t\t\t['KL', '2010934', '6053', '1571738'], ['UP', '1563235', '16369', '1340251'],\n\t\t\t\t\t\t\t\t['TN', '1468864', '16471', '1279658'], ['DL', '1361986', '20310', '1258951'],\n\t\t\t\t\t\t\t\t['ANDHRA', '1344386', '8988', '1138028'], ['WB', '1053117', '12728', '911705'],\n\t\t\t\t\t\t\t\t['CG', '883210', '11094', '749318'], ['RJ', '805658', '6158', '590390'],\n\t\t\t\t\t\t\t\t['GJ', '714611', '8731', '578397'], ['MP', '700202', '6679', '583595'],\n\t\t\t\t\t\t\t\t['HR', '652742', '6075', '539609'], ['BR', '622433', '3503', '519306'],\n\t\t\t\t\t\t\t\t['OR', '565648', '2232', '473680'], ['TG', '511711', '2834', '449744'],\n\t\t\t\t\t\t\t\t['PB', '467539', '11111', '376465'], ['AS', '310086', '1909', '265860'],\n\t\t\t\t\t\t\t\t['JH', '301257', '4182', '246608'], ['UK', '264683', '4123', '183478'],\n\t\t\t\t\t\t\t\t['JK', '229407', '2912', '174953'], ['HP', '145736', '2068', '104714'],\n\t\t\t\t\t\t\t\t['GA', '127639', '1874', '92974'], ['PY', '77031', '1045', '60424'],\n\t\t\t\t\t\t\t\t['CH', '52633', '599', '43506'], ['TR', '39054', '424', '34946'],\n\t\t\t\t\t\t\t\t['MN', '37036', '526', '31238'], ['ML', '20985', '250', '17354'],\n\t\t\t\t\t\t\t\t['AR', '20854', '69', '18691'], ['NL', '16890', '165', '13428'],\n\t\t\t\t\t\t\t\t['LADAKH', '15807', '158', '14102'], ['SK', '10392', '183', '7343'],\n\t\t\t\t\t\t\t\t['DADRA AND NAGAR HAVELI AND DAMAN AND DIU', '9150', '4', '8086'],\n\t\t\t\t\t\t\t\t['MZ', '8176', '23', '6120'], ['AN', '6470', '81', '6175'], ['LD', '4202', '11', '3171']\n\t\t\t\t\t\t\t\t]\n\tws.close_webpage()\n", "description": null, "category": "webscraping", "imports": ["import marge_sort", "from scrape import WebScrape", "import pytest"]}], [{"term": "def", "name": "create_sql_server", "data": "def create_sql_server():\n\tconn = sqlite3.connect('ner_database')\n\tc = conn.cursor()\n\tc.execute('CREATE TABLE IF NOT EXISTS article (NAME, ORGANIZATION, LOCATION)')\n\tconn.commit()\n\treturn conn, c\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import pandas as pd", "import sys", "import argparse", "from src import ws_nbc, ner_model"]}, {"term": "def", "name": "ner_to_df", "data": "def ner_to_df(article):\n\tdf = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in article.items()]), dtype=object)\n\treturn df\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import pandas as pd", "import sys", "import argparse", "from src import ws_nbc, ner_model"]}, {"term": "def", "name": "dict_to_df", "data": "def dict_to_df(ner_unique):\n\treturn pd.DataFrame(dict([(k,pd.Series(v)) for k,v in ner_unique.items()]))\n", "description": null, "category": "webscraping", "imports": ["import sqlite3", "import pandas as pd", "import sys", "import argparse", "from src import ws_nbc, ner_model"]}], [], [{"term": "def", "name": "ncdefon_ready", "data": "async def on_ready():\n\ttext_channel = None\n\n\tfor guild in client.guilds:\n\t\ttext_channel_id = db.Get_Text_Channel(guild.id)\n\t\tif text_channel_id is not None:\n\t\t\ttext_channel = text_channel_id\n\t\telse:\n\t\t\ttext_channel_list = []\n\t\t\tfor channel in guild.text_channels:\n\t\t\t\ttext_channel_list.append(channel.id)\n\t\t\ttext_channel = text_channel_list[0]\n\t\tguilds[guild.id] = text_channel\n\n\tprint('Bot is now logged in as {0.user}'.format(client))\n\tclient.loop.create_task(Listen_For_New_Games())\n\n", "description": null, "category": "webscraping", "imports": ["import asyncio", "import os", "from database import DataBase", "from dotenv import load_dotenv", "from discord.ext import commands", "from webscrapper import WebScrape"]}, {"term": "def", "name": "ncdefEcho_Game", "data": "async def Echo_Game(ctx, link):\n\tawait ctx.send(link + \"\\n\")\n\n", "description": null, "category": "webscraping", "imports": ["import asyncio", "import os", "from database import DataBase", "from dotenv import load_dotenv", "from discord.ext import commands", "from webscrapper import WebScrape"]}, {"term": "def", "name": "ncdefset_text_channel", "data": "async def set_text_channel(ctx):\n\tfor guild in guilds:\n\t\tif guild == ctx.guild.id:\n\t\t\tguilds[guild] = ctx.channel.id\n\tdb.Define_Text_Channel(ctx.guild.id, ctx.channel.id)\n\n", "description": null, "category": "webscraping", "imports": ["import asyncio", "import os", "from database import DataBase", "from dotenv import load_dotenv", "from discord.ext import commands", "from webscrapper import WebScrape"]}, {"term": "def", "name": "ncdefListen_For_New_Games", "data": "async def Listen_For_New_Games():\n\tawait client.wait_until_ready()\n\tmessage_link = None\n\n\twhile not client.is_closed():\n\n\t\tWebScrape()\n\t\tlist = WebScrape.games_list\n\t\tfor game in list:\n\t\t\tdb.Add_Game(game)\n\t\t\tif db.Is_New_Game():\n\t\t\t\tfor guild in guilds:\n\t\t\t\t\tif guilds[guild] is not None:\n\t\t\t\t\t\tchannel = client.get_channel(guilds[guild])\n\t\t\t\t\telse:\n\t\t\t\t\t\ttext_channel_list = []\n\t\t\t\t\t\tfor channel in guild.text_channels:\n\t\t\t\t\t\t\ttext_channel_list.append(channel)\n\t\t\t\t\t\tchannel = text_channel_list[0]\n\t\t\t\t\tawait channel.send(game.Message())\n\n\t\tawait asyncio.sleep(3600)  # task runs every 1 hour\n", "description": null, "category": "webscraping", "imports": ["import asyncio", "import os", "from database import DataBase", "from dotenv import load_dotenv", "from discord.ext import commands", "from webscrapper import WebScrape"]}], [{"term": "def", "name": "analyze", "data": "def analyze(url):\n\talchemy_language = AlchemyLanguageV1(api_key=api_key_alc)\n\tcombined_operations = [ 'entity', 'keyword', 'title', 'concept', 'doc-emotion']\n\tglobal data\n\tdata = alchemy_language.combined(url=url, extract=combined_operations)\n\ttone_analyzer = ToneAnalyzerV3(username=toneUser,password=tonePass,version='2016-05-19')\n\tglobal tone\n\ttext=webScrape.getText(url)\n\ttext = text.encode('ascii','ignore')\n\tif len(text)>120000:\n\t\ttext = text[0:120000]\n\tif text.count('\\n')>750:\n\t\ttext = text[0:URLHelper.linSearch(text,'\\n', 750)]\n\ttone = tone_analyzer.tone(text=text)['document_tone']['tone_categories']\n", "description": null, "category": "webscraping", "imports": ["import json", "import URLHelper", "from os.path import join, dirname", "from watson_developer_cloud import AlchemyLanguageV1", "from watson_developer_cloud import ToneAnalyzerV3", "import webScrape"]}, {"term": "def", "name": "getTitle", "data": "def getTitle():\n\tglobal data\n\ttitle = data['title']\n\treturn title.encode('ascii','ignore')\n", "description": null, "category": "webscraping", "imports": ["import json", "import URLHelper", "from os.path import join, dirname", "from watson_developer_cloud import AlchemyLanguageV1", "from watson_developer_cloud import ToneAnalyzerV3", "import webScrape"]}, {"term": "def", "name": "getKeywords", "data": "def getKeywords():\n\tglobal data\n\treturn data['keywords']\t\n", "description": null, "category": "webscraping", "imports": ["import json", "import URLHelper", "from os.path import join, dirname", "from watson_developer_cloud import AlchemyLanguageV1", "from watson_developer_cloud import ToneAnalyzerV3", "import webScrape"]}, {"term": "def", "name": "getKeyKeywords", "data": "def getKeyKeywords():\n\twords = []\n\tglobal data\n\tfor i in range(0,min(7, len(getKeywords()))):\n\t\twords.append(data['keywords'][i]['text'])\n\treturn words\n", "description": null, "category": "webscraping", "imports": ["import json", "import URLHelper", "from os.path import join, dirname", "from watson_developer_cloud import AlchemyLanguageV1", "from watson_developer_cloud import ToneAnalyzerV3", "import webScrape"]}, {"term": "def", "name": "getEmotions", "data": "def getEmotions():\n\tglobal data\n\temotions= data['docEmotions']\n\treturn emotions\n", "description": null, "category": "webscraping", "imports": ["import json", "import URLHelper", "from os.path import join, dirname", "from watson_developer_cloud import AlchemyLanguageV1", "from watson_developer_cloud import ToneAnalyzerV3", "import webScrape"]}, {"term": "def", "name": "getEmotionTone", "data": "def getEmotionTone():\n\tglobal tone\n\treturn tone[0]['tones']\n", "description": null, "category": "webscraping", "imports": ["import json", "import URLHelper", "from os.path import join, dirname", "from watson_developer_cloud import AlchemyLanguageV1", "from watson_developer_cloud import ToneAnalyzerV3", "import webScrape"]}, {"term": "def", "name": "getLanguageTone", "data": "def getLanguageTone():\n\tglobal tone\n\treturn tone[1]['tones']\n", "description": null, "category": "webscraping", "imports": ["import json", "import URLHelper", "from os.path import join, dirname", "from watson_developer_cloud import AlchemyLanguageV1", "from watson_developer_cloud import ToneAnalyzerV3", "import webScrape"]}, {"term": "def", "name": "getSocialTone", "data": "def getSocialTone():\n\tglobal tone\n\treturn tone[2]['tones']\n", "description": null, "category": "webscraping", "imports": ["import json", "import URLHelper", "from os.path import join, dirname", "from watson_developer_cloud import AlchemyLanguageV1", "from watson_developer_cloud import ToneAnalyzerV3", "import webScrape"]}], [{"term": "def", "name": "create_tables", "data": "def create_tables(SortedList, start_dir, pruned_subdirs, user, lodict):\n\t\"\"\" Create SQL tables out of OS sorted music top directory\n\n\t\tif START_DIR = '/mnt/music/'\n\t\tthen PRUNED_SUBDIRS = 0\n\n\t\tif START_DIR = '/mnt/music/Trooper/'\n\t\tthen PRUNED_SUBDIRS = 1\n\n\t\tif START_DIR = '/mnt/music/Trooper/Hits From 10 Albums/'\n\t\tthen PRUNED_SUBDIRS = 2\n\n\t\tif START_DIR = '/mnt/... 10 Albums/08 Raise A Little Hell.m4a'\n\t\tthen PRUNED_SUBDIRS = 3\n\n\t\"\"\"\n\n\tglobal con, cursor, hist_cursor\n\tglobal START_DIR_SEP\t# Count of / or \\ separators in toplevel directory\n\tglobal MUSIC_ID\t\t # primary key into Music table used by History table\n\n\tglobal _START_DIR, _USER, _LODICT\n\t_START_DIR = start_dir  # Toplevel directory, EG /mnt/music/\n\t_USER = user\t\t\t# User ID to be stored on history records.\n\t_LODICT = lodict\t\t# Location dictionary\n\n\topen_db()\n\n\tlast_time = hist_last_time('file', 'init')\n\tprint('last_time:', last_time)\n\t# Fill the table\n\tLastArtist = \"\"\n\tLastAlbum = \"\"\n\n\tSTART_DIR_SEP = start_dir.count(os.sep) - 1  # Number of / separators\n\t#print('PRUNED_SUBDIRS:', pruned_subdirs)\n\tSTART_DIR_SEP = START_DIR_SEP - pruned_subdirs\n\n\tfor i, os_name in enumerate(SortedList):\n\n\t\t# split /mnt/music/Artist/Album/Song.m4a into list\n\t\t'''\n\t\t\tOur sorted list may have removed subdirectory levels using:\n\t\t\t\n\t\t\twork_list = [w.replace(os.sep + NO_ALBUM_STR + os.sep, os.sep) \\\n\t\t\t\t for w in work_list]\n\n\t\t'''\n\t\t# TODO: Check of os_name in Music Table. If so continue loop\n\t\t#\t   Move this into mserve.py main loop and update access time in SQL.\n\t\tgroups = os_name.split(os.sep)\n\t\tArtist = groups[START_DIR_SEP+1]\n\t\tAlbum = groups[START_DIR_SEP+2]\n\t\t# Song = groups[START_DIR_SEP+3]  # Not used\n\t\tkey = make_key(os_name)\n\n\t\tif Artist != LastArtist:\n\t\t\t# In future we can add Artist table with totals\n\t\t\tLastArtist = Artist\n\t\t\tLastAlbum = \"\"\t\t  # Force sub-total break for Album\n\n\t\tif Album != LastAlbum:\n\t\t\t# In future we can add Album table with totals\n\t\t\tLastAlbum = Album\n\n\t\t''' Build full song path from song_list[] '''\n\t\tfull_path = os_name\n\t\tfull_path = full_path.replace(os.sep + NO_ARTIST_STR, '')\n\t\tfull_path = full_path.replace(os.sep + NO_ALBUM_STR, '')\n\n\t\t# os.stat gives us all of file's attributes\n\t\tstat = os.stat(full_path)\n\t\tsize = stat.st_size\n\t\t# converted = float(size) / float(CFG_DIVISOR_AMT)   # Not used\n\t\t# fsize = str(round(converted, CFG_DECIMAL_PLACES))  # Not used\n\n\t\t''' Add the song only if it doesn't exist (ie generates error) '''\n\t\tsql = \"INSERT OR IGNORE INTO Music (OsFileName, \\\n\t\t\t   OsAccessTime, OsModificationTime, OsCreationTime, OsFileSize) \\\n\t\t\t   VALUES (?, ?, ?, ?, ?)\" \n\n\t\tcursor.execute(sql, (key, stat.st_atime, stat.st_mtime,\n\t\t\t\t\t\t\t stat.st_ctime, size))\n\n\tcon.commit()\n\t# Temporary during development to record history for lyrics web scrape and\n\t# time index synchronizing to lyrics.\n\thist_init_lost_and_found(start_dir, user, lodict)\n\thist_init_lyrics_and_time(start_dir, user, lodict)\n\n", "description": " Create SQL tables out of OS sorted music top directory\n\n\t\tif START_DIR = '/mnt/music/'\n\t\tthen PRUNED_SUBDIRS = 0\n\n\t\tif START_DIR = '/mnt/music/Trooper/'\n\t\tthen PRUNED_SUBDIRS = 1\n\n\t\tif START_DIR = '/mnt/music/Trooper/Hits From 10 Albums/'\n\t\tthen PRUNED_SUBDIRS = 2\n\n\t\tif START_DIR = '/mnt/... 10 Albums/08 Raise A Little Hell.m4a'\n\t\tthen PRUNED_SUBDIRS = 3\n\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "open_db", "data": "def open_db():\n\t\"\"\" Open SQL Tables \"\"\"\n\tglobal con, cursor, hist_cursor\n\t# con = sqlite3.connect(\":memory:\")\n\tcon = sqlite3.connect(FNAME_LIBRARY)\n\t# print('FNAME_LIBRARY:',FNAME_LIBRARY)\n\n\t# MUSIC TABLE\n\t\n\t# Create the table (key must be INTEGER not just INT !\n\t# See https://stackoverflow.com/a/7337945/6929343 for explanation\n\tcon.execute(\"create table IF NOT EXISTS Music(Id INTEGER PRIMARY KEY, \\\n\t\t\t\tOsFileName TEXT, OsAccessTime FLOAT, \\\n\t\t\t\tOsModificationTime FLOAT, OsCreationTime FLOAT, \\\n\t\t\t\tOsFileSize INT, MetaArtistName TEXT, MetaAlbumName TEXT, \\\n\t\t\t\tMetaSongName TEXT, ReleaseDate FLOAT, OriginalDate FLOAT, \\\n\t\t\t\tGenre TEXT, Seconds INT, Duration TEXT, PlayCount INT, \\\n\t\t\t\tTrackNumber INT, Rating TEXT, UnsynchronizedLyrics BLOB, \\\n\t\t\t\tLyricsTimeIndex TEXT)\")\n\n\tcon.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS OsFileNameIndex ON \\\n\t\t\t\tMusic(OsFileName)\")\n\n\n\t# HISTORY TABLE\n\n\t# One time table drop to rebuild new history format\n\t# con.execute(\"DROP TABLE IF EXISTS History\")\n\n\tcon.execute(\"create table IF NOT EXISTS History(Id INTEGER PRIMARY KEY, \\\n\t\t\t\tTime FLOAT, MusicId INTEGER, User TEXT, Type TEXT, \\\n\t\t\t\tAction TEXT, SourceMaster TEXT, SourceDetail TEXT, \\\n\t\t\t\tTarget TEXT, Size INT, Count INT, Seconds FLOAT, \\\n\t\t\t\tComments TEXT)\")\n\n\tcon.execute(\"CREATE INDEX IF NOT EXISTS MusicIdIndex ON \\\n\t\t\t\tHistory(MusicId)\")\n\tcon.execute(\"CREATE INDEX IF NOT EXISTS TimeIndex ON \\\n\t\t\t\tHistory(Time)\")\n\n\t'''\n\t\tINDEX on OsSongName and confirm original when OsArtistName and\n\t\t\tOsAlbumName match up to SORTED_LIST (aka self.song_list) which is\n\t\t\tformat of:\n\t\t\t\t# split song /mnt/music/Artist/Album/Song.m4a into names:\n\t\t\t\tgroups = os_name.split(os.sep)\n\t\t\t\tArtist = str(groups [START_DIR_SEP+1])\n\t\t\t\tAlbum = str(groups [START_DIR_SEP+2])\n\t\t\t\tSong = str(groups [START_DIR_SEP+3])\n\n\t\t\t(last_playlist and last_selections uses the same record format)\n\n\t\tSaving/retrieving LyricsTimeIndex (seconds from start):\n\n\t\t>>> import json\n\t\t>>> json.dumps([1.2,2.4,3.6])\n\t\t'[1.2, 2.4, 3.6]'\n\t\t>>> json.loads('[1.2, 2.4, 3.6]')\n\t\t[1.2, 2.4, 3.6]\n\n\t'''\n\t# Retrieve column names\n\t#\tcs = con.execute('pragma table_info(Music)').fetchall() # sqlite column metadata\n\t#\tprint('cs:', cs)\n\t#\tcursor = con.execute('select * from Music')\n\t#\tnames = [description[0] for description in cursor.description]\n\t#\tprint('names:', names)\n\tcon.row_factory = sqlite3.Row\n\tcursor = con.cursor()\n\thist_cursor = con.cursor()\n\n", "description": " Open SQL Tables ", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "close_db", "data": "def close_db():\n\tcon.commit()\n\tcursor.close()\t\t  # Aug 08/21 Fix \"OperationalError:\"\n\thist_cursor.close()\t # See: https://stackoverflow.com/a/53182224/6929343\n\tcon.close()\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "make_key", "data": "def make_key(fake_path):\n\t\"\"\" Create key to read Music index by OsFileName which is\n\t\t/path/to/topdir/album/artist/song.ext\n\n\tTODO: What about PRUNED_SUBDIRS from mserve code?\n\n\t\t# Temporarily create SQL music tables until search button created.\n\t\tsql.CreateMusicTables(SORTED_LIST, START_DIR, PRUNED_SUBDIRS)\n\n\t\tWhat about '(NO_ARTIST)' and '(NO_ALBUM)' strings?\n\t\"\"\"\n\n\tgroups = fake_path.split(os.sep)\n\tartist = groups[START_DIR_SEP+1]\n\talbum = groups[START_DIR_SEP+2]\n\tsong = groups[START_DIR_SEP+3]\n\treturn artist + os.sep + album + os.sep + song\n\n", "description": " Create key to read Music index by OsFileName which is\n\t\t/path/to/topdir/album/artist/song.ext\n\n\tTODO: What about PRUNED_SUBDIRS from mserve code?\n\n\t\t# Temporarily create SQL music tables until search button created.\n\t\tsql.CreateMusicTables(SORTED_LIST, START_DIR, PRUNED_SUBDIRS)\n\n\t\tWhat about '(NO_ARTIST)' and '(NO_ALBUM)' strings?\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "update_lyrics", "data": "def update_lyrics(key, lyrics, time_index):\n\t\"\"\"\n\t\tApply Unsynchronized Lyrics and Lyrics Time Index.\n\t\tShould only be called when lyrics or time_index has changed.\n\t\"\"\"\n\n\tsql = \"UPDATE Music SET UnsynchronizedLyrics=?, LyricsTimeIndex=? \\\n\t\t   WHERE OsFileName = ?\" \n\n\tif time_index is not None:\n\t\t# count = len(time_index)  # Not used\n\t\ttime_index = json.dumps(time_index)\n\t\t# print('Saving', count, 'lines of time_index:', time_index)\n\n\tcursor.execute(sql, (lyrics, time_index, key))\n\tcon.commit()\n\n", "description": "\n\t\tApply Unsynchronized Lyrics and Lyrics Time Index.\n\t\tShould only be called when lyrics or time_index has changed.\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "get_lyrics", "data": "def get_lyrics(key):\n\t\"\"\"\n\t\tGet Unsynchronized Lyrics and Lyrics Time Index\n\t\"\"\"\n\tglobal MUSIC_ID\n\n\tcursor.execute(\"SELECT * FROM Music WHERE OsFileName = ?\", [key])\n\t''' For LyricsTimeIndex Music Table Column we need to do:\n\t\t>>> json.dumps([1.2,2.4,3.6])\n\t\t'[1.2, 2.4, 3.6]'\n\t\t>>> json.loads('[1.2, 2.4, 3.6]')\n\t\t[1.2, 2.4, 3.6]\n\t'''\n\t# Test if parent fields available:\n\t# print('self.Artist:',self.Artist)\n\t# NameError: global name 'self' is not defined\n\td = dict(cursor.fetchone())\n\n\tMUSIC_ID = d[\"Id\"]\n\n\tif d[\"LyricsTimeIndex\"] is not None:\n\t\treturn d[\"UnsynchronizedLyrics\"], json.loads(d[\"LyricsTimeIndex\"])\n\telse:\n\t\treturn d[\"UnsynchronizedLyrics\"], None\n\n", "description": "\n\t\tGet Unsynchronized Lyrics and Lyrics Time Index\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "update_metadata", "data": "def update_metadata(key, artist, album, song, genre, tracknumber, date, \n\t\t\t\t\tseconds, duration):\n\t\"\"\"\n\t\tUpdate Music Table with metadata tags.\n\t\tCalled from mserve.py and encoding.py\n\n\t\tTODO: Check if history has a 'file' record first. If not then add it.\n\t\t\t  Add webscrape history record\n\t\t\t  Add lyrics 'init' record\n\t\t\t  Add time 'init' record\n\t\t\t  Add lyrics 'edit' record\n\t\t\t  Add time 'edit' record\n\t\t\t  Add History view functions with filters for done or none\n\n\n\t\tFirst check if metadata has changed. If not then exit.\n\n\t\tUpdate metadata in library and insert history record:\n\t\t\t'meta' 'init' for first time\n\t\t\t'meta' 'edit' for 2nd and subsequent changes\n\n\t\tMetadata tags passed from following mserve variables:\n\n\t\tId = self.saved_selections[self.ndx]\n\t\tlist_index = int(Id)\n\t\tkey = self.song_list[list_index]\n\n\t\tself.Artist=self.metadata.get('ARTIST', \"None\")\n\t\tself.Album=self.metadata.get('ALBUM', \"None\")\n\t\tself.Title=self.metadata.get('TITLE', \"None\")\n\t\tself.Genre=self.metadata.get('GENRE', \"None\")\n\t\tself.Track=self.metadata.get('TRACK', \"None\")\n\t\tself.Date=self.metadata.get('DATE', \"None\")\n\t\tself.Duration=self.metadata.get('DURATION', \"0,0\").split(',')[0]\n\t\tself.Duration=self.Duration.split('.')[0]\n\t\tself.DurationSecs=self.getSec(self.Duration)\n\n\t\tsql.update_metadata(self.play_make_sql_key(), self.Artist, self.Album, \\\n\t\t\t\t\t\t\tself.Title, self.Genre, self.Track, self.Date, \\\n\t\t\t\t\t\t\tself.DurationSecs, self.Duration)\n\n\t\"\"\"\n\n\t# noinspection SpellCheckingInspection\n\n\t# Crazy all the time spent encoding has to be decoded for SQLite3 or error:\n\t# sqlite3.ProgrammingError: You must not use 8-bit bytestrings unless you\n\t# use a text_factory that can interpret 8-bit bytestrings (like\n\t# text_factory = str). It is highly recommended that you instead just\n\t# switch your application to Unicode strings.\n\t# TODO: Check for Python 3 may be required because Unicode is default type\n\tartist = artist.decode(\"utf8\")\t\t  # Queensr\u00c3\u00bfche\n\t# inspection SpellCheckingInspection\n\talbum = album.decode(\"utf8\")\n\tsong = song.decode(\"utf8\")\n\tif type(date) is str:\n\t\tif date != \"None\":\t  # Strange but true... See \"She's No Angel\" by April Wine.\n\t\t\t# Problem with date \"1993-01-26\"\n\t\t\ttry:\n\t\t\t\tdate = float(date)\n\t\t\texcept ValueError:\n\t\t\t\tpass  # Leave date as string\n\tif genre is not None:\n\t\tgenre = genre.decode(\"utf8\")\n\n\t#print('artist type:', type(artist), type(album), type(song))\n\n\tcursor.execute(\"SELECT * FROM Music WHERE OsFileName = ?\", [key])\n\td = dict(cursor.fetchone())\n\tif d is None:\n\t\tprint('SQL update_metadata() error no music ID for:', key)\n\t\treturn\n\n\t# Debugging information to comment out later (or perhaps logging?)\n\t'''\n\tprint('\\nSQL updating metadata for:',key)\n\tprint('artist type :', type(artist), type(album), type(song), \\\n\t\t\t\t\t\t   type(genre))\n\tprint('library type:', type(d['MetaArtistName']), \\\n\t\t\t\t\t\t   type(d['MetaAlbumName']), \\\n\t\t\t\t\t\t   type(d['MetaSongName']), type(d['Genre']))\n\tprint(artist\t   , d['MetaArtistName'])\n\tprint(album\t\t, d['MetaAlbumName'])\n\tprint(song\t\t , d['MetaSongName'])\n\tprint(genre\t\t, d['Genre'])\n\tprint(tracknumber  , d['TrackNumber'])\n\tprint(date\t\t , d['ReleaseDate'])\n\tprint(seconds\t  , d['Seconds'])\n\tprint(duration\t , d['Duration'])\n\n\tif artist\t  != d['MetaArtistName']:\n\t\tprint('artist:', artist, d['MetaArtistName'])\n\telif album\t   != d['MetaAlbumName']:\n\t\tprint('album:', album, d['MetaAlbumName'])\n\telif song\t\t!= d['MetaSongName']:\n\t\tprint('song:', song, d['MetaSongName'])\n\telif genre\t   != d['Genre']:\n\t\tprint('genre:', genre, d['Genre'])\n\telif tracknumber != d['TrackNumber']:\n\t\tprint('tracknumber:', tracknumber, d['TrackNumber'])\n\telif date\t\t!= d['ReleaseDate']:\n\t\tprint('date:', date, d['ReleaseDate'])\n\telif seconds\t != d['Seconds']:\n\t\tprint('seconds:', seconds, d['Seconds'])\n\telif duration\t!= d['Duration']:\n\t\tprint('duration:', duration, d['Duration'])\n\telse:\n\t\tprint('All things considered EQUAL')\n\t'''\n\n\t# Are we adding a new 'init' or 'edit' history record?\n\tif d['MetaArtistName'] is None:\n\t\taction = 'init'\n\t\t# print('\\nSQL adding metadata for:',key)\n\telif \\\n\t\tartist\t   != d['MetaArtistName'] or \\\n\t\talbum\t\t!= d['MetaAlbumName'] or \\\n\t\tsong\t\t != d['MetaSongName'] or \\\n\t\tgenre\t\t!= d['Genre'] or \\\n\t\ttracknumber  != d['TrackNumber'] or \\\n\t\tdate\t\t != d['ReleaseDate'] or \\\n\t\tseconds\t  != d['Seconds'] or \\\n\t\t\tduration != d['Duration']:\n\t\t# To test, use kid3 to temporarily change track number\n\t\t# float(date) != d['ReleaseDate'] or \\ <- They both could be None\n\t\t# Metadata hsa changed from last recorded version\n\t\taction = 'edit'\n\n\telse:\n\t\treturn\t\t\t\t\t\t\t\t  # Metadata same as library\n\n\t# Update metadata for song into library Music Table\n\tsql = \"UPDATE Music SET MetaArtistName=?, MetaAlbumName=?, MetaSongName=?, \\\n\t\t   Genre=?, TrackNumber=?, ReleaseDate=?, Seconds=?, Duration=? \\\n\t\t   WHERE OsFileName = ?\" \n\n\tcursor.execute(sql, (artist, album, song, genre, tracknumber, date,\n\t\t\t\t\t\t seconds, duration, key))\n\tcon.commit()\n\n\t# Add history record\n\t# Time will be file's last modification time\n\t''' Build full song path '''\n\tfull_path = _START_DIR.encode(\"utf8\") + key\n\t# Below not needed because (No Xxx) stubs not in Music Table filenames\n\tfull_path = full_path.replace(os.sep + NO_ARTIST_STR, '')\n\tfull_path = full_path.replace(os.sep + NO_ALBUM_STR, '')\n\n\t# os.stat gives us all of file's attributes\n\tstat = ext.stat_existing(full_path)\n\tif stat is None:\n\t\tprint(\"sql.update_metadata(): File below doesn't exist:\\n\")\n\t\tfor i in d:\n\t\t\t# Pad name with spaces for VALUE alignment\n\t\t\tprint('COLUMN:', \"{:<25}\".format(i), 'VALUE:', d[i])\n\t\treturn\n\n\tSize = stat.st_size\t\t\t\t\t # File size in bytes\n\tTime = stat.st_mtime\t\t\t\t\t# File's current mod time\n\tSourceMaster = _LODICT['name']\n\tSourceDetail = time.asctime(time.gmtime(Time))\n\tComments = \"Found: \" + time.asctime(time.gmtime(time.time()))\n\tif seconds is not None:\n\t\tFloatSeconds = float(str(seconds))  # Convert from integer\n\telse:\n\t\tFloatSeconds = 0.0\n\n\tCount = 0\n\n\t# If adding, the file history record may be missing too.\n\tif action == 'init' and \\\n\t   not hist_check(d['Id'], 'file', action):\n\t\thist_add(Time, d['Id'], _USER, 'file', action, SourceMaster,\n\t\t\t\t SourceDetail, key, Size, Count, FloatSeconds,\n\t\t\t\t Comments)\n\n\t# Add the meta Found or changed record\n\t'''\n\tprint(time.time(), d['Id'], _USER, 'meta', action, SourceMaster, \\\n\t\t\t SourceDetail, key, Size, Count, FloatSeconds, \n\t\t\t Comments, sep=\" # \")\n\t'''\n\thist_add(Time, d['Id'], _USER, 'meta', action, SourceMaster,\n\t\t\t SourceDetail, key, Size, Count, FloatSeconds, \n\t\t\t Comments)\n\n\tcon.commit()\n\n", "description": "\n\t\tUpdate Music Table with metadata tags.\n\t\tCalled from mserve.py and encoding.py\n\n\t\tTODO: Check if history has a 'file' record first. If not then add it.\n\t\t\t  Add webscrape history record\n\t\t\t  Add lyrics 'init' record\n\t\t\t  Add time 'init' record\n\t\t\t  Add lyrics 'edit' record\n\t\t\t  Add time 'edit' record\n\t\t\t  Add History view functions with filters for done or none\n\n\n\t\tFirst check if metadata has changed. If not then exit.\n\n\t\tUpdate metadata in library and insert history record:\n\t\t\t'meta' 'init' for first time\n\t\t\t'meta' 'edit' for 2nd and subsequent changes\n\n\t\tMetadata tags passed from following mserve variables:\n\n\t\tId = self.saved_selections[self.ndx]\n\t\tlist_index = int(Id)\n\t\tkey = self.song_list[list_index]\n\n\t\tself.Artist=self.metadata.get('ARTIST', \"None\")\n\t\tself.Album=self.metadata.get('ALBUM', \"None\")\n\t\tself.Title=self.metadata.get('TITLE', \"None\")\n\t\tself.Genre=self.metadata.get('GENRE', \"None\")\n\t\tself.Track=self.metadata.get('TRACK', \"None\")\n\t\tself.Date=self.metadata.get('DATE', \"None\")\n\t\tself.Duration=self.metadata.get('DURATION', \"0,0\").split(',')[0]\n\t\tself.Duration=self.Duration.split('.')[0]\n\t\tself.DurationSecs=self.getSec(self.Duration)\n\n\t\tsql.update_metadata(self.play_make_sql_key(), self.Artist, self.Album, \\\n\t\t\t\t\t\t\tself.Title, self.Genre, self.Track, self.Date, \\\n\t\t\t\t\t\t\tself.DurationSecs, self.Duration)\n\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_get_row", "data": "def hist_get_row(key):\n\t# Get the MusicID matching song file's basename\n\tcursor.execute(\"SELECT * FROM History WHERE Id = ?\", [key])\n\trow = cursor.fetchone()\n\tif row is None:\n\t\tprint('sql.py - row not found:', key)\n\t\treturn None\n\n\treturn OrderedDict(row)\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_get_music_id", "data": "def hist_get_music_id(key):\n\t# Get the MusicID matching song file's basename\n\tcursor.execute(\"SELECT Id FROM Music WHERE OsFileName = ?\", [key])\n\trow = cursor.fetchone()\n\tif row is None:\n\t\tprint('hist_get_music_id(key) error no music ID for:', key)\n\t\treturn 0\n\telif row[0] == 0:\n\t\tprint('hist_get_music_id(key) error music ID is 0:', key)\n\t\treturn 0\n\telse:\n\t\treturn row[0]\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_add_time_index", "data": "def hist_add_time_index(key, time_list):\n\t\"\"\"\n\t\tAdd time index if 'init' doesn't exist.\n\t\tIf time index does exist, add an 'edit' if it has changed.\n\t\"\"\"\n\t# Get the MusicID matching song file's basename\n\tMusicId = hist_get_music_id(key)\n\tif MusicId == 0:\n\t\tprint('SQL hist_add_time_index(key) error no music ID for:', key)\n\t\treturn False\n\n\tif hist_check(MusicId, 'time', 'init'):\n\t\t# We found a time initialization record to use as default\n\t\tAction = 'edit'\n\t\tprint('sql.hist_add_time_index(key) edit time, init:', key, HISTORY_ID)\n\t\thist_cursor.execute(\"SELECT * FROM History WHERE Id = ?\", [HISTORY_ID])\n\t\td = dict(hist_cursor.fetchone())\n\t\tif d is None:\n\t\t\tprint('sql.hist_add_time_index() error no History ID:', key,\n\t\t\t\t  HISTORY_ID)\n\t\t\treturn False\n\t\t# TODO: Read music row's time_list and to see if it's changed.\n\t\t#\t   If it hasn't changed then return. This means we have to\n\t\t#\t   add history before we update music table. Or parent only\n\t\t#\t   updates music table when lyrics or time index changes. In this\n\t\t#\t   case another history record is required for 'lyrics', 'edit'.\n\telse:\n\t\t# Add time initialization record\n\t\tAction = 'init'\n\t\td = hist_default_dict(key, 'access')\n\t\tif d is None:\n\t\t\tprint('sql.hist_add_time_index() error creating default dict.')\n\t\t\treturn False\n\n\td['Count'] = len(time_list)\n\tComments = Action + \" time: \" + time.asctime(time.gmtime(time.time()))\n\thist_add(time.time(), d['Id'], _USER, 'time', Action, d['SourceMaster'],\n\t\t\t d['SourceDetail'], key, d['Size'], d['Count'], d['Seconds'], \n\t\t\t Comments)\n\n\treturn True\n\n", "description": "\n\t\tAdd time index if 'init' doesn't exist.\n\t\tIf time index does exist, add an 'edit' if it has changed.\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_add_shuffle", "data": "def hist_add_shuffle(Action, SourceMaster, SourceDetail):\n\tType = \"playlist\"\n\t# Action = 'shuffle'\n\tif Type == Action == SourceMaster == SourceDetail:\n\t\treturn  # Above test for pycharm checking  \n  \n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_default_dict", "data": "def hist_default_dict(key, time_type='access'):\n\t\"\"\" Construct a default dictionary used to add a new history record \"\"\"\n\n\tcursor.execute(\"SELECT * FROM Music WHERE OsFileName = ?\", [key])\n\td = dict(cursor.fetchone())\n\tif d is None:\n\t\tprint('SQL hist_default_dict() error no music row for:', key)\n\t\treturn None\n\n\thist = {}\t\t\t\t\t\t\t   # History dictionary\n\tSourceMaster = _LODICT['name']\n\thist['SourceMaster'] = SourceMaster\n\n\t''' Build full song path '''\n\tfull_path = START_DIR.encode(\"utf8\") + d['OsFileName']\n\t# Below not needed because (No Artist / No Album) not in filenames\n\t# full_path = full_path.replace(os.sep + NO_ARTIST_STR, '')\n\t# full_path = full_path.replace(os.sep + NO_ALBUM_STR, '')\n\n\t# os.stat gives us all of file's attributes\n\tstat = os.stat(full_path)\n\tsize = stat.st_size\t\t\t\t\t # File size in bytes\n\thist['Size'] = size\n\thist['Count'] = 0\t\t\t\t\t   # Temporary use len(time_list)\n\tif time_type == 'access':\n\t\tTime = stat.st_atime\t\t\t\t# File's current access time\n\telif time_type == 'mod':\n\t\tTime = stat.st_mtime\t\t\t\t# File's current mod time\n\telif time_type == 'birth':\n\t\tTime = stat.st_mtime\t\t\t\t# File's birth/creation time\n\telse:\n\t\tprint('SQL hist_default_dict(key, time_type) invalid type:', time_type)\n\t\treturn None\n\n\tSourceDetail = time.asctime(time.gmtime(Time))\n\thist['SourceDetail'] = SourceDetail\n\t# Aug 10/2021 - Seconds always appears to be None\n\tif Seconds is not None:\n\t\tFloatSeconds = float(str(Seconds))  # Convert from integer\n\telse:\n\t\tFloatSeconds = 0.0\n\thist['Seconds'] = FloatSeconds\n\n\treturn hist\t\t\t\t\t\t\t # Some dict fields aren't populated\n\n", "description": " Construct a default dictionary used to add a new history record ", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_delete_time_index", "data": "def hist_delete_time_index(key):\n\t\"\"\"\n\t\tAll time indexes have been deleted.\n\t\tCheck if history 'init' record exists. If so copy it and use 'delete'\n\t\tto add new history record.\n\t\"\"\"\n\t# Get the MusicID matching song file's basename\n\tMusicId = hist_get_music_id(key)\n\tif MusicId == 0:\n\t\tprint('SQL hist_delete_time_index(key) error no music ID for:', key)\n\t\treturn False\n\n\tif not hist_check(MusicId, 'time', 'init'):\n\t\t# We found a time initialization record to use as default\n\t\tprint('sql.hist_delete_time_index(key) error no time, init:', key)\n\t\treturn False\n\n\tprint('sql.hist_delete_time_index(key) HISTORY_ID:', key, HISTORY_ID)\n\n\thist_cursor.execute(\"SELECT * FROM History WHERE Id = ?\", [HISTORY_ID])\n\td = dict(hist_cursor.fetchone())\n\tif d is None:\n\t\tprint('sql.hist_delete_time_index(key) error no History ID:', key,\n\t\t\t  HISTORY_ID)\n\t\treturn False\n\n\tComments = \"Removed: \" + time.asctime(time.gmtime(time.time()))\n\thist_add(time.time(), d['Id'], _USER, 'time', 'remove', d['SourceMaster'],\n\t\t\t d['SourceDetail'], key, d['Size'], d['Count'], d['Seconds'], \n\t\t\t Comments)\n\n\treturn True\n\n", "description": "\n\t\tAll time indexes have been deleted.\n\t\tCheck if history 'init' record exists. If so copy it and use 'delete'\n\t\tto add new history record.\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_init_lost_and_found", "data": "def hist_init_lost_and_found(START_DIR, USER, LODICT):\n\t\"\"\" Tool to initialize history time for all songs in database.\n\t\tThis step just records 'file' and 'init' for OS song filename.\n\t\tIf metadata present then 'meta' and 'init' also recorded.\n\n\t\tThe time for 'init' is the files modification time which should be\n\t\t\tlowest time across all devices if synced properly.\n\n\t\tIf a file is both lost and found in the same second and the metadata\n\t\t\tmatches that simply means the file was renamed / moved.\n\n\t\tThe same song can be found in multiple locations and smaller devices\n\t\t\tmight have fewer songs that the master location.\n\n\t\"\"\"\n\n\tsong_count = 0\n\tadd_count = 0\n\tadd_meta_count = 0\n\t# History Table columns\n\t# Time = time.time()\t# Aug 8/21 use time.time() instead of job start\n\tUser = USER\t\t\t # From location.py\n\tType = 'file'\t\t   # This records OS filename into history\n\tAction = 'init'\t\t # Means we \"found\" the file or it was renamed\n\t'''\n\tAs of April 13, 2021:\n\tDICT={'iid': iid, 'name': name, 'topdir': topdir, 'host': host, 'wakecmd':\n\t  wakecmd, 'testcmd': testcmd, 'testrep': testrep, 'mountcmd': \\\n\t  mountcmd, 'activecmd': activecmd, 'activemin': activemin}\n\t'''\n\tSourceMaster = LODICT['name']\n\t#  Aug 8/21 comment out fields below not used\n\t#SourceDetail = 'Today'  # Formatted time string \"DDD MMM DD HH:MM:SS YYYY\"\n\t#Target = '/path/file'   # Replaced with OS filename below\n\t#Size = 0\t\t\t\t# File size in bytes\n\t#Count = 0\t\t\t   # Number of renaming of path/name/filename\n\t#Seconds = 0.0\t\t   # Song duration\n\tComments = 'Automatically added by hist_init_lost_and_found()'\n\n\t# Select songs that have lyrics (Python 'not None:' = SQL 'NOT NULL')\n\tfor row in cursor.execute('SELECT Id, OsFileName, OsModificationTime, ' +\n\t\t\t\t\t\t\t  'MetaSongName, Seconds FROM Music'):\n\t\tsong_count += 1\n\t\t# Check if history already exists for song\n\t\tMusicId = row[0]\n\t\tif hist_check(MusicId, Type, Action):\n\t\t\tcontinue\n\n\t\t# Name our Music Table columns needed for History Table\n\t\tOsFileName = row[1] \n\t\t# OsModificationTime = row[2]  # Us as default for found time  # Not used\n\t\tMetaSongName = row[3]\t   # If name not blank, we have metadata\n\t\tSeconds = row[4]\t\t\t# Song Duration in seconds (INT)\n\n\t\t''' TODO: What about PRUNED_SUBDIRS from mserve code?\n\n\t\t# Temporarily create SQL music tables until search button created.\n\t\tsql.CreateMusicTables(SORTED_LIST, START_DIR, PRUNED_SUBDIRS)\n\t\t'''\n\n\t\t''' Build full song path '''\n\t\tfull_path = START_DIR.encode(\"utf8\") + OsFileName\n\n\t\t# os.stat gives us all of file's attributes\n\t\tstat = ext.stat_existing(full_path)\n\t\tif stat is None:\n\t\t\tprint(\"sql.hist_init_lost_and_found(): File below doesn't exist:\\n\")\n\t\t\tnames = cursor.description\n\t\t\tfor i, name in enumerate(names):\n\t\t\t\t# Pad name with spaces for VALUE alignment\n\t\t\t\tprint('COLUMN:', \"{:<25}\".format(name[0]), 'VALUE:', row[i])\n\t\t\tcontinue  # TODO: do \"lost\" phase, mark song as deleted somehow\n\n\t\tSize = stat.st_size\t\t\t\t\t # File size in bytes\n\t\tTime = stat.st_mtime\t\t\t\t\t# File's current mod time\n\t\tSourceDetail = time.asctime(time.gmtime(Time))\n\t\tif Seconds is not None:\n\t\t\tFloatSeconds = float(str(Seconds))  # Convert from integer\n\t\telse:\n\t\t\tFloatSeconds = 0.0\n\n\t\tCount = 0\n\t\tTarget = OsFileName\n\n\t\t# Add the Song Found row\n\t\t# Aug 8/21 use time.time() instead of job start time.\n\t\thist_add(time.time(), MusicId, User, Type, Action, SourceMaster, SourceDetail, \n\t\t\t\t Target, Size, Count, FloatSeconds, Comments)\n\t\tadd_count += 1\n\n\t\tif MetaSongName is not None:\n\t\t\t# Add the Metadata Found row\n\t\t\thist_add(time.time(), MusicId, User, 'meta', Action, SourceMaster,\n\t\t\t\t\t SourceDetail, OsFileName, Size, Count, FloatSeconds, \n\t\t\t\t\t Comments)\n\t\t\tadd_meta_count += 1\n\n\t#print('Songs on disk:', song_count, 'Added count:', add_count, \\\n\t#\t  'Added meta count:', add_meta_count)\n\n\tcon.commit()\t\t\t\t\t\t\t\t# Save database changes\n\n", "description": " Tool to initialize history time for all songs in database.\n\t\tThis step just records 'file' and 'init' for OS song filename.\n\t\tIf metadata present then 'meta' and 'init' also recorded.\n\n\t\tThe time for 'init' is the files modification time which should be\n\t\t\tlowest time across all devices if synced properly.\n\n\t\tIf a file is both lost and found in the same second and the metadata\n\t\t\tmatches that simply means the file was renamed / moved.\n\n\t\tThe same song can be found in multiple locations and smaller devices\n\t\t\tmight have fewer songs that the master location.\n\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_check", "data": "def hist_check(MusicId, check_type, check_action):\n\t\"\"\" History table usage for Music Lyrics:\n\n\t\tVARIABLE\t\tDESCRIPTION\n\t\t--------------  -----------------------------------------------------\n\t\tId\t\t\t  Primary integer key auto-incremented\n\t\tTime\t\t\tIn system format with nano-second precision\n\t\t\t\t\t\tfiletime = (unix time * 10000000) + 116444736000000000\n\t\t\t\t\t\tSecondary key\n\t\tMusicId\t\t Link to primary key in Music Table usually rowid\n\t\t\t\t\t\tFor setting (screen, monitor, window, etc) the\n\t\t\t\t\t\tMusicId is set to 0.\n\t\tUser\t\t\tUser name, User ID or GUID varies by platform.\n\t\tType\t\t\t'file', 'catalog', 'link', 'index', 'checkout', 'song'\n\t\t\t\t\t\t'lyrics', 'time', 'fine-tune', 'meta', 'playlist'\n\t\tAction\t\t  'copy', 'download', 'remove', 'burn', 'edit', 'play'\n\t\t\t\t\t\t'scrape', 'init', 'shuffle', 'save', 'load'\n\t\tSourceMaster\t'Genius', 'Metro Lyrics', etc.\n\t\t\t\t\t\tDevice name, Playlist\n\t\tSourceDetail\t'//genius.com' or 'www.metrolyrics.com', etc.\n\t\t\t\t\t\tLocation number, song names in order (soon music IDs)\n\t\tTarget\t\t  'https://www.azlyrics.com/lyrics/greenday/wake...html'\n\t\tSize\t\t\tTotal bytes in downloaded text file\n\t\t\t\t\t\tDuration of lyrics synchronized (end - start time)\n\t\tCount\t\t   Number of lines in downloaded text file\n\t\t\t\t\t\tNumber of lyrics lines synchronized\n\t\tSeconds\t\t How many seconds the operation took Float\n\t\tComments\t\tFor most records formatted date time\n\t\"\"\"\n\tglobal HISTORY_ID\n\n\tfor row in hist_cursor.execute(\"SELECT Id, Type, Action FROM History \" +\n\t\t\t\t\t\t\t\t   \"WHERE MusicId = ?\", [MusicId]):\n\t\tId = row[0]\n\t\tType = row[1]\n\t\tAction = row[2]\n\t\tif Type == check_type and Action == check_action:\n\t\t\tHISTORY_ID = Id\n\t\t\treturn True\n\n\tHISTORY_ID = 0\n\treturn False\t\t\t\t# Not Found\n\n", "description": " History table usage for Music Lyrics:\n\n\t\tVARIABLE\t\tDESCRIPTION\n\t\t--------------  -----------------------------------------------------\n\t\tId\t\t\t  Primary integer key auto-incremented\n\t\tTime\t\t\tIn system format with nano-second precision\n\t\t\t\t\t\tfiletime = (unix time * 10000000) + 116444736000000000\n\t\t\t\t\t\tSecondary key\n\t\tMusicId\t\t Link to primary key in Music Table usually rowid\n\t\t\t\t\t\tFor setting (screen, monitor, window, etc) the\n\t\t\t\t\t\tMusicId is set to 0.\n\t\tUser\t\t\tUser name, User ID or GUID varies by platform.\n\t\tType\t\t\t'file', 'catalog', 'link', 'index', 'checkout', 'song'\n\t\t\t\t\t\t'lyrics', 'time', 'fine-tune', 'meta', 'playlist'\n\t\tAction\t\t  'copy', 'download', 'remove', 'burn', 'edit', 'play'\n\t\t\t\t\t\t'scrape', 'init', 'shuffle', 'save', 'load'\n\t\tSourceMaster\t'Genius', 'Metro Lyrics', etc.\n\t\t\t\t\t\tDevice name, Playlist\n\t\tSourceDetail\t'//genius.com' or 'www.metrolyrics.com', etc.\n\t\t\t\t\t\tLocation number, song names in order (soon music IDs)\n\t\tTarget\t\t  'https://www.azlyrics.com/lyrics/greenday/wake...html'\n\t\tSize\t\t\tTotal bytes in downloaded text file\n\t\t\t\t\t\tDuration of lyrics synchronized (end - start time)\n\t\tCount\t\t   Number of lines in downloaded text file\n\t\t\t\t\t\tNumber of lyrics lines synchronized\n\t\tSeconds\t\t How many seconds the operation took Float\n\t\tComments\t\tFor most records formatted date time\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_last_time", "data": "def hist_last_time(check_type, check_action):\n\t\"\"\" Get the last time the type + action occurred\n\n\t\tPrimarily used to get the last time a song was added / updated in\n\t\thistory. If this time is greater than time top directory was\n\t\tlast changed then refresh not required.\n\t\"\"\"\n\tglobal HISTORY_ID\n\n\tfor row in hist_cursor.execute(\"SELECT * FROM History \" +\n\t\t\t\t\t\t\t\t   \"INDEXED BY TimeIndex \" +\n\t\t\t\t\t\t\t\t   \"ORDER BY Time DESC\"):\n\t\td = dict(row)\n\t\tId = d['Id']\n\t\tType = d['Type']\n\t\tAction = d['Action']\n\t\tif Type == check_type and Action == check_action:\n\t\t\tHISTORY_ID = Id\n\t\t\treturn d['Time']\n\n\tHISTORY_ID = 0\n\treturn None\t\t\t\t# Not Found\n\n", "description": " Get the last time the type + action occurred\n\n\t\tPrimarily used to get the last time a song was added / updated in\n\t\thistory. If this time is greater than time top directory was\n\t\tlast changed then refresh not required.\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_add", "data": "def hist_add(Time, MusicId, User, Type, Action, SourceMaster, SourceDetail, \n\t\t\t Target, Size, Count, Seconds, Comments):\n\t\"\"\" Add History Row for Synchronizing Lyrics Time Indices.\n\t\"\"\"\n\t# DEBUG:\n\t# InterfaceError: Error binding parameter 1 - probably unsupported type.\n\t# print(\"Time, MusicId, User, Type, Action, SourceMaster, SourceDetail,\")\n\t# print(\"Target, Size, Count, Seconds, Comments:\")\n\t# print(Time, MusicId, User, Type, Action, SourceMaster, SourceDetail,\n\t#\t  Target, Size, Count, Seconds, Comments)\n\tsql = \"INSERT INTO History (Time, MusicId, User, Type, Action, \\\n\t\t   SourceMaster, SourceDetail, Target, Size, Count, Seconds, Comments) \\\n\t\t   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n\n\thist_cursor.execute(sql, (Time, MusicId, User, Type, Action, SourceMaster,\n\t\t\t\t\t\t\t  SourceDetail, Target, Size, Count, Seconds,\n\t\t\t\t\t\t\t  Comments))\n\n", "description": " Add History Row for Synchronizing Lyrics Time Indices.\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_delete_type_action", "data": "def hist_delete_type_action(Type, Action):\n\t\"\"\" Delete History Rows for matching Type and Action.\n\t\tCreated to get rid of thousands of 'meta' 'edit' errors\n\t\"\"\"\n\t# DEBUG:\n\tsql = \"DELETE FROM History WHERE Type=? AND Action=?\"\n\n\thist_cursor.execute(sql, (Type, Action))\n\tdeleted_row_count = hist_cursor.rowcount\n\tprint('hist_delete_type_action(Type, Action):', Type, Action,\n\t\t  'deleted_row_count:', deleted_row_count)\n\tcon.commit()\n\n", "description": " Delete History Rows for matching Type and Action.\n\t\tCreated to get rid of thousands of 'meta' 'edit' errors\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "hist_init_lyrics_and_time", "data": "def hist_init_lyrics_and_time(START_DIR, USER, LODICT):\n\t\"\"\" Tool to initialize history time for all songs that have lyrics.\n\t\tThe time will be the last file access time.\n\n\t\tIf lyric time indices are set the lyrics webscrape is 5 minutes earlier\n\t\tand the index time is the file access time.\n\n\t\tAssume lyrics source is Genius but could have been clipboard or user\n\t\tdirect entry.\n\t\"\"\"\n\tsong_count = 0\n\tadd_count = 0\n\tadd_time_count = 0\n\t# History Table columns\n\t# Time = time.time()\t# Aug 8/21 not used\n\tUser = USER\t\t\t # From location.py\n\tType = 'lyrics'\n\tAction = 'scrape'\n\tSourceMaster = 'Genius'  # Website lyrics were scraped from\n\t# Aug 8/21 comment out fields below not used\n\t# SourceDetail = 'Today'  # Formatted time string \"DDD MMM DD HH:MM:SS YYYY\"\n\t# Target = 'https://genius.com/artist/album/song.html'\n\t# Size = 0\n\t# Count = 0\n\t# Seconds = 0.0\n\tComments = 'Automatically added by hist_init_lyrics_and_time()'\n\n\t# Select songs that have lyrics (Python 'not None:' = SQL 'NOT NULL')\n\tfor row in cursor.execute(\"SELECT Id, OsFileName, UnsynchronizedLyrics, \" +\n\t\t\t\t\t\t\t  \"LyricsTimeIndex, OsAccessTime, Seconds FROM \" +\n\t\t\t\t\t\t\t  \"Music WHERE UnsynchronizedLyrics IS NOT NULL\"):\n\t\tsong_count += 1\n\t\t# Check if history already exists for song\n\t\tMusicId = row[0]\n\t\tif hist_check(MusicId, Type, Action):\n\t\t\tcontinue\n\n\t\t# Name our Music Table columns needed for History Table\n\t\tOsFileName = row[1] \n\t\tUnsynchronizedLyrics = row[2]\n\t\tLyricsTimeIndex = row[3]\n\t\t# OsAccessTime = row[4]\t\t\t\t   # At time of Music Row creation\n\t\tSeconds = row[5]\t\t\t\t\t\t# Song Duration\n\n\t\t''' TODO: What about PRUNED_SUBDIRS from mserve code?\n\n\t\t# Temporarily create SQL music tables until search button created.\n\t\tsql.CreateMusicTables(SORTED_LIST, START_DIR, PRUNED_SUBDIRS)\n\t\t'''\n\n\t\t''' Build full song path '''\n\t\tfull_path = START_DIR.encode(\"utf8\") + OsFileName\n\t\t# Below not needed because (No Xxx) stubs not in Music Table filenames\n\t\tfull_path = full_path.replace(os.sep + NO_ARTIST_STR, '')\n\t\tfull_path = full_path.replace(os.sep + NO_ALBUM_STR, '')\n\n\t\t# os.stat gives us all of file's attributes\n\t\tstat = os.stat(full_path)\n\t\t# size = stat.st_size\t\t\t\t\t # Not used\n\t\t# converted = float(size) / float(CFG_DIVISOR_AMT)\n\t\t# fsize = str(round(converted, CFG_DECIMAL_PLACES))\n\n\t\tTime = stat.st_atime\t\t\t\t\t# File's current access time\n\t\tSourceDetail = time.asctime(time.gmtime(Time))\n\t\tSize = len(UnsynchronizedLyrics)\t\t# Can change after user edits\n\t\tCount = UnsynchronizedLyrics.count('\\n')\n\t\tTarget = 'https://genius.com/' + OsFileName\n\n\t\tif LyricsTimeIndex is None:\n\t\t\ttime_count = 0\n\t\t\ttime_index_list = None\n\t\telse:\n\t\t\ttime_index_list = json.loads(LyricsTimeIndex)\n\t\t\ttime_count = len(time_index_list)\n\t\t\tif time_count < 5:\n\t\t\t\tprint('time count:', time_count, Target)\n\n\t\t# Estimate 4 seconds to download lyrics (webscrape)\n\t\thist_add(Time, MusicId, User, Type, Action, SourceMaster, SourceDetail, \n\t\t\t\t Target, Size, Count, 4.0, Comments)\n\t\tadd_count += 1\n\n\t\tif time_count > 0:\n\t\t\tTime = stat.st_atime + 300\t\t  # 5 minutes to sync lyrics\n\t\t\tduration = time_index_list[-1] - time_index_list[0]\n\t\t\tSize = int(duration)\t\t\t\t# Amount of time synchronized\n\t\t\tCount = len(time_index_list)\t\t# How many lines synchronized\n\t\t\t'''\n\t\t\tAs of April 12, 2021:\n\t\t\tDICT={'iid': iid, 'name': name, 'topdir': topdir, 'host': host, 'wakecmd':\n\t\t\t  wakecmd, 'testcmd': testcmd, 'testrep': testrep, 'mountcmd': \n\t\t\t  mountcmd, 'activecmd': activecmd, 'activemin': activemin}\n\t\t\t'''\n\t\t\thist_add(Time, MusicId, User, 'time', 'edit', LODICT['name'],\n\t\t\t\t\t LODICT['iid'], OsFileName, Size, Count, float(Seconds), \n\t\t\t\t\t Comments)\n\t\t\tadd_time_count += 1\n\n\t#print('Songs with lyrics:', song_count, 'Added count:', add_count, \\\n\t#\t  'Added time count:', add_time_count)\n\tcon.commit()\n\n", "description": " Tool to initialize history time for all songs that have lyrics.\n\t\tThe time will be the last file access time.\n\n\t\tIf lyric time indices are set the lyrics webscrape is 5 minutes earlier\n\t\tand the index time is the file access time.\n\n\t\tAssume lyrics source is Genius but could have been clipboard or user\n\t\tdirect entry.\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "class", "name": "classWebscrape:", "data": "class Webscrape:\n\n\tdef __init__(self, music_id):\n\t\t# All columns in history row, except primary ID auto assigned.\n\t\tself.MusicId = music_id\n\t\tself.User = None\n\t\tself.Type = None\n\t\tself.Action = None\n\t\tself.SourceMaster = None\n\t\tself.SourceDetail = None\n\t\tself.Target = None\n\t\tself.Size = None\n\t\tself.Count = None\n\t\tself.Comments = None\n\n\tdef set_ws_parm(self, music_id):\n\t\t\"\"\" Save Webscrape parameters - Currently in mserve.py:\n\n\t\t\tMusicId = sql.hist_get_music_id(self.work_sql_key)\n\t\t\tsql.hist_add(time.time(), MusicId, USER,\n\t\t\t\t\t\t 'scrape', 'parm', artist, song,\n\t\t\t\t\t\t \"\", 0, 0, 0.0,\n\t\t\t\t\t\t time.asctime(time.gmtime(time.time())))\n\t\t\text_name = 'python webscrape.py'\n\t\t\tself.lyrics_pid = ext.launch_command(ext_name,\n\t\t\t\t\t\t\t\t\t\t\t\t toplevel=self.play_top)\n\n\t\t\"\"\"\n\t\tpass\n\n\tdef get_ws_parm(self, music_id):\n\t\t\"\"\" Get Webscrape parameters\n\t\t\tnow = time.time()\n\t\t\tlast_time = sql.hist_last_time('scrape', 'parm')\n\t\t\thist_row = sql.hist_get_row(sql.HISTORY_ID)\n\t\t\tlag = now - last_time\n\t\t\tif lag > 1.0:\n\t\t\t\tprint('It took more than 1 second for webscrape to start:', lag)\n\t\t\telse:\n\t\t\t\tprint('webscrape start up time:', lag)\n\t\t\t\tpass\n\t\t\tprint(hist_row)\n\n\t\t\"\"\"\n\t\tpass\n\n\tdef set_ws_result(self, music_id):\n\t\t\"\"\" Save Webscrape results\n\t\t\"\"\"\n\t\tpass\n\n\tdef get_ws_result(self, music_id):\n\t\t\"\"\" Get Webscrape results\n\t\t\"\"\"\n\t\tpass\n\n\tdef read_music_id(self, music_id):\n\n\t\t\"\"\"\n\t\t==========================   COPY from webscrape.py   =========================\n\t\t\t\n\t\t# Web scraping song lyrics IPC file names\n\t\tSCRAPE_CTL_FNAME\t= '/run/user/\" + g.USER_ID + \"/mserve.scrape_ctl.json'\n\t\tSCRAPE_LIST_FNAME   = '/run/user/\" + g.USER_ID + \"/mserve.scrape_list.txt'\n\t\tSCRAPE_LYRICS_FNAME = '/run/user/\" + g.USER_ID + \"/mserve.scrape_lyrics.txt'\n\t\t\n\t\t# Names list is used in our code for human readable formatting\n\t\tNAMES_LIST =   ['Metro Lyrics',\t 'AZ Lyrics',\t\t'Lyrics',\n\t\t\t\t\t\t'Lyrics Mode',\t  'Lets Sing It',\t 'Genius',\n\t\t\t\t\t\t'Musix Match',\t  'Lyrics Planet']\n\t\t\n\t\t# Website list is used in webscrape.py for internet formatting\n\t\tWEBSITE_LIST = ['www.metrolyrics.com', 'www.azlyrics.com',\t'www.lyrics.com',\n\t\t\t\t\t\t'www.lyricsmode.com',  'www.letssingit.com',  '//genius.com', \n\t\t\t\t\t\t'www.musixmatch.com',  'www.lyricsplanet.com']\n\t\t\n\t\t# Empty control list (template)\n\t\tCTL_LIST = [{} for _ in range(len(WEBSITE_LIST))]\n\t\t#CTL_LIST = [ {}, {}, {}, {}, {}, {}, {}, {} ]\n\t\t\n\t\t# If we try to print normally an error occurs when launched in background\n\t\t#print(\"CTL_LIST:\", CTL_LIST, file=sys.stderr)\n\t\t\n\t\t# Empty control list dictionary element (template)\n\t\tWS_DICT = { \"name\":\"\", \"website\":\"\", \"link\":\"\", \"flag\":\"\" }\n\t\t''' flag values: preference passed to webscrape.py. result passed to mserve\n\t\t\tpreference:  1-8 try to get lyrics in this order, 'skip' = skip site\n\t\t\tresult:\t  'found' lyrics returned. 'available' lyrics can be returned\n\t\t\t\t\t\t 'not found' no link or link is empty (eg artist but no lyrics)\n\t\t'''\n", "description": " Save Webscrape parameters - Currently in mserve.py:\n\n\t\t\tMusicId = sql.hist_get_music_id(self.work_sql_key)\n\t\t\tsql.hist_add(time.time(), MusicId, USER,\n\t\t\t\t\t\t 'scrape', 'parm', artist, song,\n\t\t\t\t\t\t \"\", 0, 0, 0.0,\n\t\t\t\t\t\t time.asctime(time.gmtime(time.time())))\n\t\t\text_name = 'python webscrape.py'\n\t\t\tself.lyrics_pid = ext.launch_command(ext_name,\n\t\t\t\t\t\t\t\t\t\t\t\t toplevel=self.play_top)\n\n\t\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "save_ctl", "data": "def save_ctl():\n\t'''\n\t\tSave Control file containing list of dictionaries\n\n\t\tUSED by mserve and webscrape.py\n\t\t\tmserve passes previous list of names with flags to scrape.\n\t\t\twebscrape.py passes back name of website that was scraped.\n\t\t\twebscrape.py passes names of websites that CAN BE scraped.\n\t'''\n\twith open(SCRAPE_CTL_FNAME, \"w\") as ctl:\n\t\tctl.write(json.dumps(CTL_LIST))\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "load_ctl", "data": "def load_ctl():\n\t'''\n\t\tReturn contents of CTL file or empty list of dictionaries\n\t'''\n\tdata = CTL_LIST\n\tif os.path.isfile(SCRAPE_CTL_FNAME):\n\t\twith open(SCRAPE_CTL_FNAME, \"r\") as ctl:\n\t\t\tdata = json.loads(ctl.read())\n\n\treturn data\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "class", "name": "classPrettyHistory:", "data": "class PrettyHistory:\n\n\tdef __init__(self, history_dict, calc=None):\n\t\t\"\"\" \n\t\t\tCopied from bserve/gmail_api.py PrettyHistory\n\n\t\t\t\t1) top level dictionary key/values like Id, size\n\n\t\t\t After massaging, four sections are at single dictionary level\n\t\t\t Dictionary keys can be walked and compared to count of keys at\n\t\t\t each part for separating sections in display.\n\n\t\t\tThis class serves double duty (for now) to display treeview column\n\t\t\tdata dictionary for class view()) at column name.\n\n\t\t\tcalc is optional function to append calculated fields to the\n\t\t\tpretty dictionary.\n\n\t\t\"\"\"\n\n\t\tself.calc = calc  # Calculated fields such as delete_on\n\t\tself.dict = OrderedDict()  # Python 2.7 version not needed in 3.7\n\t\tself.scrollbox = None  # custom scrollbox for display\n\n\t\t# IF dictionary for treeview column, format is straight forward\n\t\tself.part_start = [0]  # Only 1 part\n\t\tself.part_names = ['Tkinter Treeview']\n\t\tself.part_color = ['red']\n\t\tfor key, value in history_dict.iteritems():\n\t\t\tself.dict[key] = self.format_value(value)\n\n\t\tif calc is None:\n\t\t\t# If no calc callback we are done.\n\t\t\treturn\n\n\t\t# List of part section headings at part_start[] list above\n\t\tself.part_names = ['Google search results',\n\t\t\t\t\t\t   'Webscrape results']\n\t\t# List of part colors - applied to key names in that part\n\t\tself.part_color = ['red',\n\t\t\t\t\t\t   'blue']\n\n\t\tself.part_start.append(len(self.dict))\n\n\t\tself.calc(self.dict)  # Call external function passing our dict\n\t\t# print(\"self.calc(self.dict)  # Call external function passing our dict\")\n\n\t\t# print('\\n======================  pretty  =====================\\n')\n\t\t# print(json.dumps(self.dict, indent=2))\n\n\t@staticmethod\n\tdef format_value(value):\n\n\t\ttry:\n\t\t\tformatted = str(value)  # Convert from int\n\t\texcept UnicodeEncodeError:\n\t\t\tformatted = value\n\t\t# return formatted.encode('utf8')\n\t\treturn formatted\n\n\tdef tkinter_display(self, scrollbox):\n\t\t\"\"\" Popup display all values in pretty print format\n\t\t\tUses new tkinter window with single text entry field\n\n\t\t\tRequires ordered dict and optional lists specifying sections\n\t\t\t(parts) the part names and part colors for key names.\n\t\t\"\"\"\n\n\t\tself.scrollbox = scrollbox  # Temporary until code craft\n\n\t\t# Allow program changes to scrollable text widget\n\t\tself.scrollbox.configure(state=\"normal\")\n\t\tself.scrollbox.delete('1.0', 'end')  # Delete previous entries\n\n\t\tcurr_key = 0  # Current key index\n\t\tcurr_level = 0  # Current dictionary part\n\t\tcurr_color = 'black'\n\t\t# for key, value in self.dict.iteritems():\t# Don't use iteritems\n\t\tfor key in self.dict:  # Don't need iteritems on ordered dict\n\t\t\tif curr_key == self.part_start[curr_level]:\n\t\t\t\tcurr_level_name = self.part_names[curr_level]\n\t\t\t\tcurr_color = self.part_color[curr_level]\n\t\t\t\tself.scrollbox.insert(\"end\", curr_level_name + \"\\n\")\n\t\t\t\t# self.scrollbox.highlight_pattern(curr_level_name, 'yellow')\n\t\t\t\tcurr_level += 1\n\n\t\t\t\tif curr_level >= len(self.part_start):\n\t\t\t\t\tcurr_level = len(self.part_start) - 1\n\t\t\t\t\t# We are in last part so no next part to check\n\t\t\t\t\t# print('resetting curr_level at:', key)\n\n\t\t\t# Insert current key and value into text widget\n\t\t\t# TclError: character U+1f913 is above the range (U+0000-U+FFFF) allowed by Tcl\n\t\t\t# noinspection PyBroadException\n\t\t\ttry:\n\t\t\t\tself.scrollbox.insert(\"end\", u\"\\t\" + key + u\":\\t\" +\n\t\t\t\t\t\t\t\t\t  self.dict[key] + u\"\\n\", u\"margin\")\n\t\t\t#\t\t\t\t\t\t\t\t  value + u\"\\n\", \"margin\")\n\t\t\texcept:\n\t\t\t\tnormal = normalize_tcl(self.dict[key])\n\t\t\t\tself.scrollbox.insert(\"end\", u\"\\t\" + key + u\":\\t\" +\n\t\t\t\t\t\t\t\t\t  normal + u\"\\n\", u\"margin\")\n\n\t\t\tself.scrollbox.highlight_pattern(key + u':', curr_color)\n\t\t\tcurr_key += 1  # Current key index\n\n\t\t# Override for auto trader that contains multiple keys within value\n\t\tself.scrollbox.highlight_pattern(\n\t\t\t\"From:To:Subject:Date:List-Unsubscribe:List-Unsubscribe-Post:\" +\n\t\t\t\"MIME-Version: Reply-To:List-ID:X-CSA-Complaints:Message-ID:\" +\n\t\t\t\"Content-Type:\", \"black\")\n\n\t\tself.scrollbox.highlight_pattern(\n\t\t\t\"Date:Message-ID:Content-Type:Subject:To:\", \"black\")\n\n\t\t# Override for disqus that contains multiple keys within value\n\t\tself.scrollbox.highlight_pattern(\n\t\t\t\"Subject:From:To:\", \"black\")\n\n\t\t# Don't allow changes to displayed selections (test copy clipboard)\n\t\tself.scrollbox.configure(state=\"disabled\")\n\n", "description": " \n\t\t\tCopied from bserve/gmail_api.py PrettyHistory\n\n\t\t\t\t1) top level dictionary key/values like Id, size\n\n\t\t\t After massaging, four sections are at single dictionary level\n\t\t\t Dictionary keys can be walked and compared to count of keys at\n\t\t\t each part for separating sections in display.\n\n\t\t\tThis class serves double duty (for now) to display treeview column\n\t\t\tdata dictionary for class view()) at column name.\n\n\t\t\tcalc is optional function to append calculated fields to the\n\t\t\tpretty dictionary.\n\n\t\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "normalize_tcl", "data": "def normalize_tcl(s):\n\t\"\"\"\n\t\tFixes error:\n\n\t\t  File \"/usr/lib/python2.7/lib-tk/ttk.py\", line 1339, in insert\n\t\t\tres = self.tk.call(self._w, \"insert\", parent, index, *opts)\n\t\t_tkinter.TclError: character U+1f3d2 is above the\n\t\t\trange (U+0000-U+FF FF) allowed by Tcl\n\n\t\tFrom: https://bugs.python.org/issue21084\n\t\"\"\"\n\n\tastral = re.compile(r'([^\\x00-\\uffff])')\n\tnew_s = \"\"\n\tfor i, ss in enumerate(re.split(astral, s)):\n\t\tif not i % 2:\n\t\t\tnew_s += ss\n\t\telse:\n\t\t\tnew_s += '?'\n\n\treturn new_s\n\n", "description": "\n\t\tFixes error:\n\n\t\t  File \"/usr/lib/python2.7/lib-tk/ttk.py\", line 1339, in insert\n\t\t\tres = self.tk.call(self._w, \"insert\", parent, index, *opts)\n\t\t_tkinter.TclError: character U+1f3d2 is above the\n\t\t\trange (U+0000-U+FF FF) allowed by Tcl\n\n\t\tFrom: https://bugs.python.org/issue21084\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "music_treeview", "data": "def music_treeview():\n\t\"\"\" Define Data Dictionary treeview columns for history table\n\t\"\"\"\n\n\tmusic_treeview_list = [\n\n\t  OrderedDict([\n\t\t(\"column\", \"row_id\"), (\"heading\", \"Row ID\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"Id\"), (\"select_order\", 0), (\"unselect_order\", 1),\n\t\t(\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", \"{,,}\"),\n\t\t(\"display_width\", 150), (\"display_min_width\", 80),\n\t\t(\"display_long\", None), (\"stretch\", 0)]),  # 0=NO, 1=YES\n\n\t  OrderedDict([\n\t\t(\"column\", \"os_filename\"), (\"heading\", \"OS Filename\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"OsFileName\"), (\"select_order\", 0), (\"unselect_order\", 2),\n\t\t(\"key\", True), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 200), (\"display_min_width\", 120),\n\t\t(\"display_long\", None), (\"stretch\", 0)]),  # 0=NO, 1=YES\n\n\t  OrderedDict([\n\t\t(\"column\", \"os_atime\"), (\"heading\", \"Access Time\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"OsAccessTime\"), (\"select_order\", 0), (\"unselect_order\", 3),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", float), (\"format\", \"{0,,f}\"),\n\t\t(\"display_width\", 80), (\"display_min_width\", 80),\n\t\t(\"display_long\", None), (\"stretch\", 0)]),  # 0=NO, 1=YES\n\n\t  OrderedDict([\n\t\t(\"column\", \"os_mtime\"), (\"heading\", \"Mod Time\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"OsModificationTime\"), (\"select_order\", 0), (\"unselect_order\", 4),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", float), (\"format\", \"{0,,f}\"),\n\t\t(\"display_width\", 80), (\"display_min_width\", 80),\n\t\t(\"display_long\", None), (\"stretch\", 0)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"os_ctime\"), (\"heading\", \"Create Time\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"OsCreationTime\"), (\"select_order\", 0), (\"unselect_order\", 5),\n\t\t(\"key\", False), (\"anchor\", \"e\"), (\"instance\", float),\n\t\t(\"format\", \"{0,,f}\"), (\"display_width\", 80),\n\t\t(\"display_min_width\", 80), (\"display_long\", None), (\"stretch\", 0)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"os_file_size\"), (\"heading\", \"File Size\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"OsFileSize\"), (\"select_order\", 0), (\"unselect_order\", 6),\n\t\t(\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", \"{:,}\"),\n\t\t(\"display_width\", 150), (\"display_min_width\", 120),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"artist\"), (\"heading\", \"Artist\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"MetaArtistName\"), (\"select_order\", 0), (\"unselect_order\", 7),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 400), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),  # 0=NO, 1=YES\n\n\t  OrderedDict([\n\t\t(\"column\", \"album\"), (\"heading\", \"Album\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"Album\"), (\"select_order\", 0), (\"unselect_order\", 8),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 400), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"song_name\"), (\"heading\", \"Song Name\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"MetaSongName\"), (\"select_order\", 0), (\"unselect_order\", 9),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 400), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"release_date\"), (\"heading\", \"Release Date\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"ReleaseDate\"), (\"select_order\", 0), (\"unselect_order\", 10),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", float), (\"format\", \"{0,,f}\"),\n\t\t(\"display_width\", 80), (\"display_min_width\", 80),\n\t\t(\"display_long\", None), (\"stretch\", 0)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"original_date\"), (\"heading\", \"Original Date\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"OriginalDate\"), (\"select_order\", 0), (\"unselect_order\", 11),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", float), (\"format\", \"{0,,f}\"),\n\t\t(\"display_width\", 80), (\"display_min_width\", 80),\n\t\t(\"display_long\", None), (\"stretch\", 0)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"genre\"), (\"heading\", \"Genre\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"Genre\"), (\"select_order\", 0), (\"unselect_order\", 12),\n\t\t(\"key\", False), (\"anchor\", \"center\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 160), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"seconds\"), (\"heading\", \"Seconds\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"Seconds\"), (\"select_order\", 0), (\"unselect_order\", 13),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 160), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"duration\"), (\"heading\", \"Duration\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"Duration\"), (\"select_order\", 0), (\"unselect_order\", 14),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 160), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"play_count\"), (\"heading\", \"Play Count\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"PlayCount\"), (\"select_order\", 0), (\"unselect_order\", 15),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 160), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"track_number\"), (\"heading\", \"Track Number\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"TrackNumber\"), (\"select_order\", 0), (\"unselect_order\", 16),\n\t\t(\"key\", False), (\"anchor\", \"e\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 160), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"rating\"), (\"heading\", \"Rating\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"Rating\"), (\"select_order\", 0), (\"unselect_order\", 17),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", int), (\"format\", \"{:,}\"),\n\t\t(\"display_width\", 160), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"lyrics\"), (\"heading\", \"Lyrics\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"UnsynchronizedLyrics\"), (\"select_order\", 0), (\"unselect_order\", 18),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 600), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"time_index\"), (\"heading\", \"Time Index\"), (\"sql_table\", \"Music\"),\n\t\t(\"var_name\", \"LyricsTimeIndex\"), (\"select_order\", 0), (\"unselect_order\", 19),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 160), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)])\n\t]\n\n\treturn music_treeview_list\n\n", "description": " Define Data Dictionary treeview columns for history table\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "history_treeview", "data": "def history_treeview():\n\t\"\"\" Define Data Dictionary treeview columns for history table.  Snippet:\n\t\t(\"column\", \"row_id\"), (\"heading\", \"Row ID\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"music_id\"), (\"heading\", \"Music ID\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"user\"), (\"heading\", \"User\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"type\"), (\"heading\", \"Type\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"action\"), (\"heading\", \"Action\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"master\"), (\"heading\", \"Master\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"detail\"), (\"heading\", \"Detail\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"target\"), (\"heading\", \"Target\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"size\"), (\"heading\", \"Size\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"count\"), (\"heading\", \"Count\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"comments\"), (\"heading\", \"Comments\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"delete_on\"), (\"heading\", \"Delete On\"), (\"sql_table\", \"calc\"),\n\t\t(\"column\", \"reason\"), (\"heading\", \"Reason\"), (\"sql_table\", \"calc\"),\n\n\t\"\"\"\n\n\thistory_treeview_list = [\n\n\t  OrderedDict([\n\t\t(\"column\", \"time\"), (\"heading\", \"Time\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"Time\"), (\"select_order\", 0), (\"unselect_order\", 1),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", float),\n\t\t(\"format\", \"{0:,.4f}\"), (\"display_width\", 240),\n\t\t(\"display_min_width\", 120), (\"display_long\", None), (\"stretch\", 0)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"music_id\"), (\"heading\", \"Music ID\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"MusicId\"), (\"select_order\", 0), (\"unselect_order\", 2),\n\t\t(\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", None),\n\t\t(\"display_width\", 100), (\"display_min_width\", 80),\n\t\t(\"display_long\", None), (\"stretch\", 0)]),  # 0=NO, 1=YES\n\n\t  OrderedDict([\n\t\t(\"column\", \"user\"), (\"heading\", \"User\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"User\"), (\"select_order\", 0), (\"unselect_order\", 3),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 150), (\"display_min_width\", 120),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),  # 0=NO, 1=YES\n\n\t  OrderedDict([\n\t\t(\"column\", \"type\"), (\"heading\", \"Type\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"Type\"), (\"select_order\", 0), (\"unselect_order\", 4),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 80), (\"display_min_width\", 60),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"action\"), (\"heading\", \"Action\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"Action\"), (\"select_order\", 0), (\"unselect_order\", 5),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str),\n\t\t(\"format\", None), (\"display_width\", 80),\n\t\t(\"display_min_width\", 60), (\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"master\"), (\"heading\", \"Master\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"SourceMaster\"), (\"select_order\", 0), (\"unselect_order\", 6),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 150), (\"display_min_width\", 100),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"detail\"), (\"heading\", \"Detail\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"SourceDetail\"), (\"select_order\", 0), (\"unselect_order\", 7),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 150), (\"display_min_width\", 100),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),  # 0=NO, 1=YES\n\n\t  OrderedDict([\n\t\t(\"column\", \"target\"), (\"heading\", \"Target\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"Target\"), (\"select_order\", 0), (\"unselect_order\", 8),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 300), (\"display_min_width\", 200),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"size\"), (\"heading\", \"Size\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"Size\"), (\"select_order\", 0), (\"unselect_order\", 9),\n\t\t(\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", \"{:,}\"),\n\t\t(\"display_width\", 100), (\"display_min_width\", 80),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"count\"), (\"heading\", \"Count\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"Count\"), (\"select_order\", 0), (\"unselect_order\", 10),\n\t\t(\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", \"{:,}\"),\n\t\t(\"display_width\", 80), (\"display_min_width\", 60),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"comments\"), (\"heading\", \"Comments\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"Comments\"), (\"select_order\", 0), (\"unselect_order\", 11),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 160), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"seconds\"), (\"heading\", \"Seconds\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"Seconds\"), (\"select_order\", 0), (\"unselect_order\", 12),\n\t\t(\"key\", False), (\"anchor\", \"e\"), (\"instance\", float), (\"format\", \"{0:,.4f}\"),\n\t\t(\"display_width\", 140), (\"display_min_width\", 80),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"row_id\"), (\"heading\", \"Row ID\"), (\"sql_table\", \"History\"),\n\t\t(\"var_name\", \"Id\"), (\"select_order\", 0), (\"unselect_order\", 13),\n\t\t(\"key\", True), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", None),\n\t\t(\"display_width\", 140), (\"display_min_width\", 100),\n\t\t(\"display_long\", None), (\"stretch\", 1)]),\n\n\t  OrderedDict([\n\t\t(\"column\", \"reason\"), (\"heading\", \"Reason\"), (\"sql_table\", \"calc\"),\n\t\t(\"var_name\", \"reason\"), (\"select_order\", 0), (\"unselect_order\", 14),\n\t\t(\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n\t\t(\"display_width\", 160), (\"display_min_width\", 140),\n\t\t(\"display_long\", None), (\"stretch\", 1)])\n\t]\n\n\t''' Future retention is calculated in order of Monthly, Weekly, Daily. \n\t\tLast year retention is same as Future but Yearly test inserted first.\n\t\tThe newest backup will always be classified as Monthly until tomorrow.\n\t'''\n\n\treturn history_treeview_list\n\n", "description": " Define Data Dictionary treeview columns for history table.  Snippet:\n\t\t(\"column\", \"row_id\"), (\"heading\", \"Row ID\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"music_id\"), (\"heading\", \"Music ID\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"user\"), (\"heading\", \"User\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"type\"), (\"heading\", \"Type\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"action\"), (\"heading\", \"Action\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"master\"), (\"heading\", \"Master\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"detail\"), (\"heading\", \"Detail\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"target\"), (\"heading\", \"Target\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"size\"), (\"heading\", \"Size\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"count\"), (\"heading\", \"Count\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"comments\"), (\"heading\", \"Comments\"), (\"sql_table\", \"History\"),\n\t\t(\"column\", \"delete_on\"), (\"heading\", \"Delete On\"), (\"sql_table\", \"calc\"),\n\t\t(\"column\", \"reason\"), (\"heading\", \"Reason\"), (\"sql_table\", \"calc\"),\n\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "update_history", "data": "def update_history(scraped_dict):\n\n\tfor i, website in enumerate(scraped_dict):\n\t\tif len(website['link']) > 2 and website['flag'] != 'skip':\n\t\t\tpass\n\t\t\t# Check for duplicates\n\t\telse:\n\t\t\tpass\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "create_webscrape", "data": "def create_webscrape(music_id, website):\n\n\t# Read all history records finding the last one for each website\n\t# if the flag is 'downloaded' then set dict flag to 'skip'\n\tprint('remove website parameter:', website)\n\tfor website in webscape.WEBSITE_LIST:\n\t\tif get_history(music_id, website=website):\n\t\t\t# update dict\n\t\t\tpass\n\t\telse:\n\t\t\tadd_dict(xxx)\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "create_radio_buttons", "data": "def create_radio_buttons(music_id):\n\n\t# if already downloaded set text to grey with date in parentheses.\n\t# if no link add (no link) after website name.\n\t# can you make a deactivated tkinter radio button? Or simply\n\t# don't add unavailable to list.\n\n\t# From:\n\t# https://stackoverflow.com/questions/49061602/how-to-disable-multiple-radiobuttons-in-python\n\n\tprint('music_id parameter not used:', music_id)\n\tsum_label['text'] += var.get()\n\tif sum_label['text'] >= 30:\n\t\tfor key in radiobuttons:\n\t\t\tradiobuttons[key]['state'] = 'disabled'\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "get_last_history", "data": "def get_last_history(music_id, website='all'):\n\n\t# match is on website human formatted name not internet\n\t#  formatted name. EG \"Genius' not '//genius.com'\n\tprint('music_id parameter not used:', music_id, website)\n\n\tpass\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "alter_table1", "data": "def alter_table1(cur, table, *args):\n\t\"\"\" Copied from simple_insert(), still needs to be changed. \"\"\"\n\tquery = 'INSERT INTO '+table+' VALUES (' + '?, ' * (len(args)-1) + '?)'\n\tcur.execute(query, args)\n\n", "description": " Copied from simple_insert(), still needs to be changed. ", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "simple_insert", "data": "def simple_insert(cur, table, *args):\n\tquery = 'INSERT INTO '+table+' VALUES (' + '?, ' * (len(args)-1) + '?)'\n\tcur.execute(query, args)\n\n", "description": null, "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}, {"term": "def", "name": "insert_blank_line", "data": "def insert_blank_line(table_name):\n\t\"\"\" Add underscores to insert_blank_line and table_name for pycharm syntax checking.\n\t\tIf pragma breaks then remove underscores.\n\t\"\"\"\n\tcs = con.execute('pragma table_info('+table_name+')').fetchall()  # sqlite column metadata\n\tcon.execute('insert into '+table_name+' values ('+','.join([default_value[c[2]] for c in cs])+')')\n \n\t#insert into Music(id) values (null);\t\t # Simpler method\n\t''' DELETED Q&A: https://stackoverflow.com/questions/66072570/\n\thow-to-initialize-all-columns-of-sqlite3-table?no redirect=1#comment116823421_66072570\n", "description": " Add underscores to insert_blank_line and table_name for pycharm syntax checking.\n\t\tIf pragma breaks then remove underscores.\n\t", "category": "webscraping", "imports": ["from __future__ import print_function\t   # Must be first import", "from __future__ import unicode_literals\t # Unicode errors fix", "import sqlite3", "import os", "import re", "import json", "import time", "import datetime", "from collections import namedtuple, OrderedDict", "import global_variables as g\t\t# should be self-explanatory", "import timefmt\t\t\t\t\t  # Our custom time formatting functions", "import external as ext", "\tfrom location import FNAME_LIBRARY  # SQL database name (SQLite3 format)", "\t\t>>> import json"]}], [{"term": "def", "name": "initialize_words", "data": "def initialize_words(wordlist_path: str, sorting_mode: str) -> [str]:\n\tprint(f\"{sorting_mode=}\")\n\timport random\n\tword_list = []\n\n\twith open(wordlist_path, \"r\") as f:\n\t\tlines = f.readlines()\n\t\tfor line in lines:\n\t\t\tword_list.append(line.strip())\n\n\tif settings[\"scramble_word_list\"]:\n\t\trandom.shuffle(word_list)\n\n\tif sorting_mode != \"alphabetical\":\n\t\tif sorting_mode == \"increasing\":\n\t\t\tword_list.sort(key=lambda x: len(x))\n\t\t\treturn word_list\n\n\t\tif sorting_mode == \"decreasing\":\n\t\t\tword_list.sort(key=lambda x: len(x), reverse=True)\n\t\t\treturn word_list\n\n\t\tif sorting_mode == \"uniqueness\":\n\t\t\ttemp = []\n\t\t\tfor word in word_list:\n\t\t\t\ttemp.append((word, len(set(word))))\n\n\t\t\ttemp.sort(key=lambda x: x[1], reverse=True)\n\n\t\t\tword_list = []\n\n\t\t\tfor word_tuple in temp:\n\t\t\t\tword_list.append(word_tuple[0])\n\t\t\treturn word_list\n\n\treturn word_list\n\n", "description": null, "category": "webscraping", "imports": ["import random", "import time", "import json", "import keyboard", "\timport random", "\timport webscrape_3"]}, {"term": "def", "name": "keyboard_write", "data": "def keyboard_write(string: str, wpm: float, accuracy: float) -> None:\n\tfor char in string:\n\t\tif random.randint(0, 1000) / 1000 > accuracy:\n\t\t\tkeyboard.write(chr(random.randint(97, 122)))\n\t\t\ttime.sleep((random.randint(50, 200) / 50) / (wpm * 5 / 60))\n\t\t\tkeyboard.send(\"backspace\")\n\t\t\ttime.sleep((random.randint(50, 200) / 50) / (wpm * 5 / 60))\n\n\t\tkeyboard.write(char)\n\t\ttime.sleep((random.randint(50, 200) / 100) / (wpm * 5 / 60))\n\tkeyboard.send(\"enter\")\n\n", "description": null, "category": "webscraping", "imports": ["import random", "import time", "import json", "import keyboard", "\timport random", "\timport webscrape_3"]}, {"term": "def", "name": "main", "data": "def main() -> None:\n\tver = 3.1\n\tprog_name = __file__.split('\\\\')[-1]\n\tprint(f\"Initializing '{prog_name}' version {ver}\\n\")\n\n\twords = initialize_words(settings['wordlist_path'], settings[\"wordlist_sorting_mode\"])\n\tprint(f\"Initiation complete! {len(words)} words loaded into memory from {settings['wordlist_path']}.\\n\\n{'=' * 64}\\n\")\n\n\ttoken = input(\"Input game code or url: \")\n\tif len(token) == 4:\n\t\ttoken = \"https://jklm.fun/\" + token.upper()\n\n\timport webscrape_3\n\twebscrape_3.connect(token)\n\tprint(f\"{'=' * 31}\")\n\n\tused = set()\n\n\tmin_word_length = settings[\"min_word_length\"]\n\tmax_word_length = settings[\"max_word_length\"]\n\n\tsyllable = \"\"\n\tlast_syllable = \"\"\n\tsyllable_out = f\"\\033[92m{syllable}\\033[0m\"\n\tword = \"\"\n\n\tdef find_word(syllable: str, do_print=True) -> str:\n\t\tfor word in words:\n\t\t\tif syllable in word and word not in used and min_word_length < len(word) < max_word_length:\n\t\t\t\tto_print = word.split(syllable, maxsplit=1)\n\t\t\t\tif do_print:\n\t\t\t\t\tprint(f\"Unused word containing {syllable_out} = {syllable_out.join(to_print)}\")\n\t\t\t\treturn word\n\n\t\tprint(f\"no words containing \\033[92m{syllable}\\033[0m in {settings['wordlist_path']}\")\n\t\treturn \"none found\"\n\n\twhile True:\n\t\ttemp = webscrape_3.get_syllable().lower()\n\t\tif temp != last_syllable:\n\t\t\tsyllable = temp\n\t\t\tsyllable_out = f\"\\033[92m{syllable}\\033[0m\"\n\t\t\tprint(f\"\\n\\nsyllable = {syllable_out}\")\n\t\t\tlast_syllable = syllable\n\n\t\t\tword = find_word(syllable)\n\n\t\tif settings[\"autotype\"] and keyboard.is_pressed(settings[\"autotype_activation_key\"]):\n\t\t\tused.add(word)\n\t\t\tkeyboard_write(word, settings[\"autotype_wpm\"], settings[\"autotype_accuracy\"])\n\n\t\t\tword = find_word(syllable)\n\n\t\ttime.sleep(1 / settings[\"syllable_poll_freq\"])\n\n", "description": null, "category": "webscraping", "imports": ["import random", "import time", "import json", "import keyboard", "\timport random", "\timport webscrape_3"]}], [{"term": "def", "name": "_webscrape_seek", "data": "def _webscrape_seek():\n\n\tSEARCH_TERMS= \"cloud architect\".replace(\" \", \"-\")\n\n\tSALARY_BUCKETS = [0, 30000, 40000, 50000, 60000, 70000, 80000, 100000, 120000, 150000, 200000, 999999]\n\tSALARY_LOW = SALARY_BUCKETS[9]\n\tSALARY_HIGH = SALARY_BUCKETS[11]\n\n\tPAGE = 1\n\n\tBASE_URL = f\"https://www.seek.com.au/{SEARCH_TERMS}-jobs/in-All-Sydney-NSW?page={PAGE}&salaryrange={SALARY_LOW}-{SALARY_HIGH}&salarytype=annual\"\n\thtml = requests.get(BASE_URL).text\n\ttext = BeautifulSoup(html,'html.parser')\n\n\tJOB_ADS_TOTAL = str(text.findAll('span', id=\"SearchSummary\")[0].text).split(' ')[0].replace(',', '')\n\tJOB_HREFs = text.find_all('a', class_='CbjkqYz')\n\tPAGES_TOTAL = int(JOB_ADS_TOTAL) / len(JOB_HREFs)\n\n\tprint(PAGES_TOTAL, JOB_ADS_TOTAL, len(JOB_HREFs))\n\n\n\t# missing a fraction of a page.\n\tfor PAGE in tqdm(range(0, int(PAGES_TOTAL))):\n\n\t\tJOB_ADS_DF = pd.DataFrame()\n\n\t\tfor i in range(0, len(JOB_HREFs)):\n\n\t\t\tJOB_TITLE = JOB_HREFs[i].text\n\t\t\tURL = \"https://www.seek.com.au/\" + JOB_HREFs[i]['href']\n\t\t\tJOB_PAGE = requests.get(URL).text\n\t\t\tJOB_PAGE_TEXT = BeautifulSoup(JOB_PAGE,'html.parser')\n\t\t\tJOB_TEXT = str(JOB_PAGE_TEXT.find_all('div', class_='yvsb870 _1v38w810'))\n\n\t\t\tJOB_TEXT_CLEANER = re.sub('\\W+',' ', JOB_TEXT )\n\n\t\t\tSTOPWORDS = [\"div\", \"class\", \"p\", \"yvsb870\", \"_1v38w810\", \"li\", \"strong\", \"br\", \"ul\"]\n\t\t\tJOB_TEXT_CLEANER_SPLIT = JOB_TEXT_CLEANER.split()\n\t\t\tJOB_TEXT_CLEANER_RESULT = [word for word in JOB_TEXT_CLEANER_SPLIT if word.lower() not in STOPWORDS]\n\t\t\tJOB_TEXT_CLEANED =  ' '.join(JOB_TEXT_CLEANER_RESULT)\n\n\t\t\tDETAILS = {\n\t\t\t\t'JOB_NAME' : [JOB_TITLE],\n\t\t\t\t'JOB_TEXT' : [JOB_TEXT_CLEANED],\n\t\t\t\t'URL' : [URL],\n\t\t\t\t'SALARY_LOW': [SALARY_LOW],\n\t\t\t\t'SALARY_HIGH': [SALARY_HIGH]\n\t\t\t}\n\t\t\t\n\t\t\tdf = pd.DataFrame(DETAILS)\n\t\t\tJOB_ADS_DF = pd.concat([JOB_ADS_DF, df])\n\n\tprint(JOB_ADS_DF.head())\n\n\tCSV_NAME = \"SEEK_JOB_ADS\"\n\tDATA_DIR = \"/\".join(os.getcwd().split('/')[1:6]) + '/data'\n\tJOB_ADS_DF.to_csv(f\"/{DATA_DIR}/{CSV_NAME}_{SEARCH_TERMS}.csv\")\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import csv # to open/close/append CSV", "from datetime import datetime # to accuractley stamp the CSV file", "import os # to check if file exists", "import pandas as pd", "from tqdm import tqdm", "import re"]}], [{"term": "def", "name": "process_image", "data": "def process_image(url, image_path, filename, item_id, description = \"Image\", ordering = 0):\n\tlsr._manage_rate()\n\twith open(image_path+filename, \"wb\") as f:\n\t\tf.write(requests.get(url).content)\n\turl = lsr.api_url+'Image.json'\n\t\n\tfiles = {'image': (filename, open(image_path + filename, 'rb'), 'image/jpeg')}\n\tpayload = {'data': '{\"description\": \"' + description + '\", \"ordering\": ' + ordering +', \"itemID\": ' + item_id +'}'}\n\tr = requests.post(url, files=files, data=payload, headers=lsr.headers)\n\t#print(r.text)\n", "description": null, "category": "webscraping", "imports": ["import requests", "import os", "import json", "import sys", "from bs4 import BeautifulSoup", "from urllib.parse import urlparse", "import mysql.connector", "import re", "import math", "from lsretail import api as lsretail", "#from lsecom import api as lsecom"]}, {"term": "def", "name": "round_up", "data": "def round_up(n, decimals=0):\n\tmultiplier = 10 ** decimals\n\treturn math.ceil(n * multiplier) / multiplier\n", "description": null, "category": "webscraping", "imports": ["import requests", "import os", "import json", "import sys", "from bs4 import BeautifulSoup", "from urllib.parse import urlparse", "import mysql.connector", "import re", "import math", "from lsretail import api as lsretail", "#from lsecom import api as lsecom"]}], [{"term": "def", "name": "process_image", "data": "def process_image(url, image_path, filename, item_id, description = \"Image\", ordering = 0):\n\tlsr._manage_rate()\n\twith open(image_path+filename, \"wb\") as f:\n\t\tf.write(requests.get(url).content)\n\turl = lsr.api_url+'Image.json'\n\t\n\tfiles = {'image': (filename, open(image_path + filename, 'rb'), 'image/jpeg')}\n\tpayload = {'data': '{\"description\": \"' + description + '\", \"ordering\": ' + ordering +', \"itemID\": ' + item_id +'}'}\n\tr = requests.post(url, files=files, data=payload, headers=lsr.headers)\n\t#print(r.text)\n", "description": null, "category": "webscraping", "imports": ["import requests", "import os", "import json", "import sys", "from bs4 import BeautifulSoup", "import urllib.parse", "import mysql.connector", "import re", "import math", "from selenium import webdriver", "from lsretail import api_dev as lsretail", "#from lsecom import api as lsecom"]}, {"term": "def", "name": "round_up", "data": "def round_up(n, decimals=0):\n\tmultiplier = 10 ** decimals\n\treturn math.ceil(n * multiplier) / multiplier\n", "description": null, "category": "webscraping", "imports": ["import requests", "import os", "import json", "import sys", "from bs4 import BeautifulSoup", "import urllib.parse", "import mysql.connector", "import re", "import math", "from selenium import webdriver", "from lsretail import api_dev as lsretail", "#from lsecom import api as lsecom"]}], [{"term": "def", "name": "get_sitemap", "data": "def get_sitemap(lnk):\r\n\tsitemap_url= lnk+(\"sitemap.xml\")\r\n\tsitemap_req= requests.get(sitemap_url)\r\n\tsoup_1 = BeautifulSoup(sitemap_req.content, 'html.parser')\r\n\t#print(soup_1.prettify())\r\n\tt1=time.time()\r\n\tfor link in soup_1.find_all(\"loc\"):\r\n\t\t\tif link.text not in sitemap_lst:\r\n\t\t\t\tsitemap_lst.append(link.text)\r\n\t\t\t\t#print(link.text)\r\n\r\n\tif len(sitemap_lst)==0 :\r\n\t\tprint(\"Sitemap is not available for this web-site.\")\r\n\telse:\r\n\t\t#print(\"List of sitemap : \",sitemap_lst)\r\n\t\tprint(\"Total sitemaps are \",len(sitemap_lst))\r\n\r\n\tt2=time.time()\r\n\tprint(t2-t1)\r\n", "description": null, "category": "webscraping", "imports": ["#import required lib\r", "import requests \r", "from bs4 import BeautifulSoup\r", "import concurrent.futures\r", "import time\r"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(sitemap):\r\n\r\n\t#print(sitemaps)\r\n\treq=requests.get(sitemap)\r\n\tsoup=BeautifulSoup(req.content,'html.parser')\r\n\r\n\tfor title in soup.find_all(\"title\"):\r\n\t\tif None in title:\r\n\t\t\ttitle_tag.append(None)\t\r\n\t\telse:\r\n\t\t\ttitle_tag.append(title.text)\r\n\r\n\tfor h1 in soup.find_all(\"h1\"):\r\n\t\tif None in h1:\r\n\t\t\th1_tag.append(None)\t\r\n\t\telse:\r\n\t\t\th1_tag.append(h1.text)\r\n\t\r\n\tfor h2 in soup.find_all(\"h2\"):\r\n\t\tif None in h2:\r\n\t\t\th2_tag.append(None)\t\r\n\t\telse:\r\n\t\t\th2_tag.append(h2.text)\r\n\t\t\r\n\tfor img in soup.find_all(\"img\"):\r\n\t\tif img.has_attr('alt'):\r\n\t\t\timg_tag.append(img['alt'])\r\n\r\n\tfor a in soup.find_all('a', href=True): \r\n\t\tif None in a: \r\n\t\t\ta_tag.append(None)\r\n\t\telse:\r\n\t\t\ta_tag.append(a['href'])\r\n\r\n\r\n", "description": null, "category": "webscraping", "imports": ["#import required lib\r", "import requests \r", "from bs4 import BeautifulSoup\r", "import concurrent.futures\r", "import time\r"]}], [{"term": "def", "name": "getSoup", "data": "def getSoup(urlList):\n\t\"\"\"This function creates the soup for scraping\"\"\"\n\n\tresponse = requests.get(urlList)\n\tmy_soup = BeautifulSoup(response.text, \"html.parser\")\n\t#print(my_soup.prettify())\n\treturn my_soup\n", "description": "This function creates the soup for scraping", "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import yaml"]}, {"term": "def", "name": "getSoupText", "data": "def getSoupText(urlList):\n\t\"\"\"Soup text is needed later in the output function, a snapshot of the html we scraped\"\"\"\n\tsoupText = getSoup(urlList).prettify()\t\n\treturn soupText\n", "description": "Soup text is needed later in the output function, a snapshot of the html we scraped", "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import yaml"]}, {"term": "def", "name": "webScrape", "data": "def webScrape(urlList):\n\t\"\"\"Scrapes the soup and creates the rows object for csv output\"\"\"\n\t\n\titemArray= getSoup(urlList).find_all('div', attrs={'class' :productClass})\n\titemLen = len(itemArray)\n\tprint(urlList)\n\tprint(\"number of products on first load= \" + str(itemLen))\n\t# print(itemArray[0].getText())\n\tfor a in range (0,itemLen):\n\t\tprodInfo = itemArray[a]\n\t\tprodBrand = prodInfo.find('div' , attrs={'class' : prodNameClass})\n\t\tprodPrice = prodInfo.find('span', attrs={'class' : prodPriceClass})\n\t\tif prodPrice:\n\t\t\tprice = prodPrice.getText().lstrip().rstrip()\n\t\telse:\n\t\t\tprice = \"No Price Information\"\n\n\t\trows.append([urlList,\n\t\t\tprodBrand.getText().lstrip().rstrip(),\n\t\t\tprice\n\n\t\t\t\t])\n\treturn rows\n\n", "description": "Scrapes the soup and creates the rows object for csv output", "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import yaml"]}], [{"term": "def", "name": "spawn_browser", "data": "def spawn_browser():\n\tspawn = webdriver.Chrome(\"bin/chromedriver-win.exe\", chrome_options=opt)\n\treturn spawn\n\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.chrome.options import Options", "from production.data import *", "import sys"]}, {"term": "def", "name": "webscrape_single_section", "data": "def webscrape_single_section(course_tag):\n\tdriver.get(\n\t\tf\"https://coursebook.utdallas.edu/clips/clip-coursebook.zog?id={course_tag}&action=info\")\n\n\tcourse = set_inject_vars(driver)[\"course\"]\n\tcourse_head = set_inject_vars(driver)[\"course_head\"]\n\n\tcourse_info = scrape_data(course, course_head)\n\tcourse_info = array_to_obj(course_info)\n\treturn course_info\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.chrome.options import Options", "from production.data import *", "import sys"]}, {"term": "def", "name": "webscrape_all_sections", "data": "def webscrape_all_sections(course_tag):\n\tdriver.get(f\"https://coursebook.utdallas.edu/search/{course_tag}\")\n\tcourse_list = driver.find_elements_by_class_name(\"stopbubble\")\n\tcurrent_term = driver.find_element_by_class_name(\"directaddress\").text\n\n\tcourse_list = add_elements_to_array(course_list)\n\tcourse_list = course_list\n\tcurrent_term = current_term[-3:]\n\tlist_data = []\n\n\tfor course in course_list:\n\t\tlist_data.append(webscrape_single_section(\n\t\t\tf\"{course.replace(' ', '').lower()}.{current_term}\"))\n\t\ttime.sleep(0.2)\n\n\treturn list_data\n\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.chrome.options import Options", "from production.data import *", "import sys"]}, {"term": "def", "name": "scrape_prof_data", "data": "def scrape_prof_data(prof_list):\n\tsuccess = []\n\tfailed = []\n\tfor prof in prof_list:\n\t\t# prof = prof_list[0]\n\t\ttry:\n\t\t\tdriver.get(\n\t\t\t\tf\"https://www.utdallas.edu/directory/includes/directories.class.php?dirType=displayname&dirSearch={prof['name']}&dirAffil=faculty&dirDept=All&dirMajor=All&dirSchool=All\")\n\t\t\tname = driver.find_element_by_class_name(\"fullname\").text\n\t\t\tdetails = driver.find_element_by_class_name(\"output\").text\n\t\t\t# print(name, file=sys.stderr)\n\t\t\tdetails = details.split(\"\\n\")\n\t\t\tdetails[4] = details[4].replace(\"Office - \", \"\")\n\t\t\tdetails[5] = details[5].replace(\"Mailstop - \", \"\")\n\n\t\t\tsuccess.append({\"name\": name, \"email\": details[0], \"title\": details[1], \"department\": details[2],\n\t\t\t\t\t\t\t\"phone\": details[3], \"office\": details[4], \"mailstop\": details[5]})\n\t\texcept:\n\t\t\t# print(f\"ERROR {prof['name']}\", file=sys.stderr)\n\t\t\tfailed.append(prof['name'])\n\treturn {\n\t\t'success': success,\n\t\t'failed': failed\n\t}\n\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.chrome.options import Options", "from production.data import *", "import sys"]}, {"term": "def", "name": "set_inject_vars", "data": "def set_inject_vars(driver):\n\treturn {\"course\": driver.find_elements_by_class_name(\n\t\t\t\"courseinfo__overviewtable__td\"),\n\t\t\t\"course_head\": driver.find_elements_by_class_name(\n\t\t\t\"courseinfo__overviewtable__th\")}\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.chrome.options import Options", "from production.data import *", "import sys"]}], [{"term": "def", "name": "webscrape_single_section", "data": "def webscrape_single_section(course_tag):\n\tdriver.get(\n\t\tf\"https://coursebook.utdallas.edu/clips/clip-cb11.zog?id={course_tag}&action=info\")\n\n\tcourse = set_inject_vars(driver)[\"course\"]\n\tcourse_head = set_inject_vars(driver)[\"course_head\"]\n\n\tcourse_info = scrape_data(course, course_head)\n\tcourse_info = array_to_obj(course_info)\n\treturn course_info\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.chrome.options import Options", "from .data import *", "import os"]}, {"term": "def", "name": "webscrape_all_sections", "data": "def webscrape_all_sections(course_tag):\n\tdriver.get(f\"https://coursebook.utdallas.edu/search/{course_tag}\")\n\tcourse_list = driver.find_elements_by_class_name(\"stopbubble\")\n\tcurrent_term = driver.find_element_by_class_name(\"directaddress\").text\n\n\tcourse_list = add_elements_to_array(course_list)\n\tcourse_list = course_list\n\tcurrent_term = current_term[-3:]\n\tlist_data = []\n\n\tfor course in course_list:\n\t\tlist_data.append(webscrape_single_section(\n\t\t\tf\"{course.replace(' ', '').lower()}.{current_term}\"))\n\t\ttime.sleep(0.2)\n\n\treturn list_data\n\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.chrome.options import Options", "from .data import *", "import os"]}, {"term": "def", "name": "scrape_prof_data", "data": "def scrape_prof_data(prof_list):\n\tsuccess = []\n\tfailed = []\n\tfor prof in prof_list:\n\t\t# prof = prof_list[0]\n\t\ttry:\n\t\t\tdriver.get(\n\t\t\t\tf\"https://www.utdallas.edu/directory/includes/directories.class.php?dirType=displayname&dirSearch={prof['name']}&dirAffil=faculty&dirDept=All&dirMajor=All&dirSchool=All\")\n\t\t\tname = driver.find_element_by_class_name(\"fullname\").text\n\t\t\tdetails = driver.find_element_by_class_name(\"output\").text\n\t\t\t# print(name, file=sys.stderr)\n\t\t\tdetails = details.split(\"\\n\")\n\t\t\tdetails[4] = details[4].replace(\"Office - \", \"\")\n\t\t\tdetails[5] = details[5].replace(\"Mailstop - \", \"\")\n\n\t\t\tsuccess.append({\"name\": name, \"email\": details[0], \"title\": details[1], \"department\": details[2],\n\t\t\t\t\t\t\t\"phone\": details[3], \"office\": details[4], \"mailstop\": details[5]})\n\t\texcept:\n\t\t\t# print(f\"ERROR {prof['name']}\", file=sys.stderr)\n\t\t\tfailed.append(prof['name'])\n\treturn {\n\t\t'success': success,\n\t\t'failed': failed\n\t}\n\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.chrome.options import Options", "from .data import *", "import os"]}, {"term": "def", "name": "set_inject_vars", "data": "def set_inject_vars(driver):\n\treturn {\"course\": driver.find_elements_by_class_name(\n\t\t\t\"courseinfo__overviewtable__td\"),\n\t\t\t\"course_head\": driver.find_elements_by_class_name(\n\t\t\t\"courseinfo__overviewtable__th\")}\n", "description": null, "category": "webscraping", "imports": ["import time", "from selenium import webdriver", "from selenium.webdriver.common.keys import Keys", "from selenium.webdriver.chrome.options import Options", "from .data import *", "import os"]}], [{"term": "def", "name": "scrape", "data": "def scrape(urls, output_file=''):\n\twebscrape(urls, output_file=output_file)\n", "description": null, "category": "webscraping", "imports": ["import asyncio", "import re", "from contextlib import closing", "import click", "from bs4 import BeautifulSoup, Comment, Tag", "from requests import get", "from requests.exceptions import RequestException"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(*urls, output_file=''):\n\t\"\"\"\n\tGets all the text from a webpage.\n\n\tParameters\n\t----------\n\turls : str\n\t\tUrls to scrape\n\toutput_file : str, optional\n\t\toutput file name/location, by default cwd\n\t\"\"\"\n\n\tfor url in urls:\n\t\traw_html = _simple_get(url)\n\n\t\ttry:\n\t\t\tsoup = BeautifulSoup(raw_html, 'html.parser')\n\t\t\tsoup = soup.body\n\n\t\t\t# Delete any comments\n\t\t\tfor comments in soup.findAll(text=lambda text: isinstance(text, Comment)):\n\t\t\t\tcomments.decompose()\n\n\t\t\t# kill all script and style elements\n\t\t\tfor script in soup([\"header\", \"footer\", \"script\", \"style\", \"code\", \"form\"]):\n\t\t\t\tscript.decompose()\t# rip it out\n\n\t\t\t# Remove any menus from the html\n\t\t\tfor div in soup.find_all('div'):\n\t\t\t\tif isinstance(div, Tag):\n\t\t\t\t\tif div.attrs:\n\t\t\t\t\t\tif 'class' in div.attrs:\n\t\t\t\t\t\t\tfor menu_item in MENU_CHECK:\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tif menu_item in \" \".join(div.attrs['class']):\n\t\t\t\t\t\t\t\t\tdiv.decompose()\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\n\t\t\t# Clean up text from raw html a little\n\t\t\tcleaned_content = list(map(lambda x: re.sub('\\s+', ' ', x).strip(), soup.find_all(text=True)))\n\t\t\t\n\t\t\treturn (\" \".join(filter(lambda x: x != '', cleaned_content))).strip()\n\n\t\texcept:\n\t\t\treturn \"\"\n\n", "description": "\n\tGets all the text from a webpage.\n\n\tParameters\n\t----------\n\turls : str\n\t\tUrls to scrape\n\toutput_file : str, optional\n\t\toutput file name/location, by default cwd\n\t", "category": "webscraping", "imports": ["import asyncio", "import re", "from contextlib import closing", "import click", "from bs4 import BeautifulSoup, Comment, Tag", "from requests import get", "from requests.exceptions import RequestException"]}, {"term": "def", "name": "_simple_get", "data": "def _simple_get(url):\n\t\"\"\"\n\tAttempts to get the content at `url` by making an HTTP GET request.\n\tIf the content-type of response is some kind of HTML/XML, return the\n\ttext content, otherwise return None.\n\t\"\"\"\n\ttry:\n\t\twith closing(get(url)) as resp:\n\t\t\tif _is_good_response(resp):\n\t\t\t\treturn resp.content \n\t\t\telse:\n\t\t\t\tprint(f\"Request failed for {url}\")\n\t\t\t\treturn None \n\n\texcept RequestException as e:\n\t\t_log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n\t\treturn None\n\n", "description": "\n\tAttempts to get the content at `url` by making an HTTP GET request.\n\tIf the content-type of response is some kind of HTML/XML, return the\n\ttext content, otherwise return None.\n\t", "category": "webscraping", "imports": ["import asyncio", "import re", "from contextlib import closing", "import click", "from bs4 import BeautifulSoup, Comment, Tag", "from requests import get", "from requests.exceptions import RequestException"]}, {"term": "def", "name": "_is_good_response", "data": "def _is_good_response(resp):\n\t\"\"\"\n\tReturns True if the response seems to be HTML, False otherwise.\n\t\"\"\"\n\tcontent_type = resp.headers['Content-Type'].lower()\n\treturn (resp.status_code == 200 \n\t\t\tand content_type is not None \n\t\t\tand content_type.find('html') > -1)\n\n", "description": "\n\tReturns True if the response seems to be HTML, False otherwise.\n\t", "category": "webscraping", "imports": ["import asyncio", "import re", "from contextlib import closing", "import click", "from bs4 import BeautifulSoup, Comment, Tag", "from requests import get", "from requests.exceptions import RequestException"]}, {"term": "def", "name": "_log_error", "data": "def _log_error(e):\n\t\"\"\"\n\tIt is always a good idea to log errors. \n\tThis function just prints them, but you can\n\tmake it do anything.\n\t\"\"\"\n\tprint(e)\n", "description": "\n\tIt is always a good idea to log errors. \n\tThis function just prints them, but you can\n\tmake it do anything.\n\t", "category": "webscraping", "imports": ["import asyncio", "import re", "from contextlib import closing", "import click", "from bs4 import BeautifulSoup, Comment, Tag", "from requests import get", "from requests.exceptions import RequestException"]}], [{"term": "def", "name": "rate_webscrape", "data": "def rate_webscrape():\n\t'''\n\tBelow is the code for web scraping country-specific covid rate data,\n\twhich is then parsed, formatted, fit to a table, and extracted to \n\ta file named 'covid_rates_table'.\n\t'''\n\trate_url = 'https://www.worldometers.info/coronavirus/?utm_campaign=homeAdvegas1?\"%20%5CI%20\"countries\"'\n\trate_response = requests.get(rate_url)\n\trate_soup = BeautifulSoup(rate_response.text, 'html.parser')\n\n\ttable = rate_soup.find(\"table\", attrs={\"id\": \"main_table_countries_today\"})\n\n\tcolumns = table.find_all(\"th\")\n\t#for c in rate_soup.find_all('br'):\n\t#\tc.replace_with(' ')\n\n\tcolumn_names = []\n\tfor c in columns:\n\t\tcolumn_names.append(c.get_text())\n\t#print(column_names)\n\n\trows = table.find(\"tbody\").find_all(\"tr\")\n\n\tl = []\n\tfor tr in rows:\n\t\ttd = tr.find_all('td')\n\t\trow = [tr.text for tr in td]\n\t\tl.append(row)\n\n\tdf = pd.DataFrame(l, columns=column_names)\n   \n\tfor i in range(0, len(df)):  \n\t\tinsert_row = '''\n\t\t\tINSERT INTO Rate_Table1\n\t\t\tVALUES (NULL, ?, ?, ?, ?, ?)\n\t\t\t'''\n\t\tvalues_list = [\n\t\t\tdf.iloc[i]['Country,Other'], df.iloc[i]['NewCases'], df.iloc[i]['NewDeaths'], \n\t\t\tdf.iloc[i]['1 Caseevery X ppl'], df.iloc[i]['1 Deathevery X ppl']\n\t\t\t]\n\t\tcur.execute(insert_row, values_list)\n\tconn.commit()\n", "description": null, "category": "webscraping", "imports": ["import requests", "import sqlite3", "from bs4 import BeautifulSoup", "import pandas as pd ", "import plotly.express as go"]}, {"term": "def", "name": "db_processing_and_graphs", "data": "def db_processing_and_graphs(input_response):\n\tdb_query = \"\"\"\n\tSELECT *\n\tFROM Rate_Table1\n\tWHERE COUNTRY = '{country}'\n\t\"\"\"\n\t\n\tif input_response in ['uk', 'usa']:\n\t\tcountry = input_response.upper()\n\telse:\n\t\tcountry = input_response.capitalize()\n\n\tcur.execute(db_query.format(country=country))\n\tresult = cur.fetchall()\n\tanswer = result[0]\n \n\tprint(\"Worldwide country rank, by total number of cases: \" + str(answer[0]))\n\tprint(\"New cases today: \" + str(answer[2]))\n\tprint(\"New deaths today: \" + str(answer[3]))\n\tprint(\"New case per \" + str(answer[4]) + \" number of people\")\n\tprint(\"New death per \" + str(answer[5]) + \" number of people\")\n\t\n\t\n\tfig = go.bar(x=[\"New Cases\", \"New Deaths\"], y=[answer[2], answer[3]], labels=dict(x=\"Metric\", y=\"Incidence today\"))\n\tfig.show()\n", "description": "\n\tSELECT *\n\tFROM Rate_Table1\n\tWHERE COUNTRY = '{country}'\n\t", "category": "webscraping", "imports": ["import requests", "import sqlite3", "from bs4 import BeautifulSoup", "import pandas as pd ", "import plotly.express as go"]}, {"term": "def", "name": "travel_webscrape", "data": "def travel_webscrape(input_response):\n\t'''\n\tBelow is the code for web scraping country-specific travel \n\trestrictions (in the form of Levels 1-4) put in place due to COVID spread.\n\t'''\n\tbase_travel_url = 'https://wwwnc.cdc.gov/travel/notices/covid-4/coronavirus-'\n\t\n\tif input_response in (\"uk\", \"UK\"):\n\t\tinput_response = \"united-kingdom\"\n\tcountry_travel_url = input_response\n\ttravel_url = base_travel_url + country_travel_url\n\ttravel_response = requests.get(travel_url)\n\ttravel_soup = BeautifulSoup(travel_response.text, 'html.parser')\n\ttravel_level = travel_soup.find(\"div\", class_=\"notice-typename-covid-4 p-2\").get_text()\n\n\tprint(\"For your selected country, the CDC has designated it: \" + travel_level + \".\")\n", "description": null, "category": "webscraping", "imports": ["import requests", "import sqlite3", "from bs4 import BeautifulSoup", "import pandas as pd ", "import plotly.express as go"]}, {"term": "def", "name": "interactive_prompt", "data": "def interactive_prompt():\n\tinput_response = ''\n\n\twhile input_response != 'exit':\n\t\tprint(\"Welcome to the COVID travel safety check program, where you can compare the CDC recommended travel guideline with currend covid rate information!\")\n\t\tprint(\"Enter a country! Is it safe for you to travel there?\")\n\t\tprint(\"Note: For a list of countries in their proper search format, type 'countries'. For help, type in 'help'.\")\n\t\tinput_response = input('Choose a country: ')\n\n\t\tif input_response == 'help':\n\t\t\tprint(\"Type in the name of any country. Note your spelling. Case sensitivity does not apply.\")\n\t\t\tcontinue\n\t\telif input_response == 'countries':\n\t\t\tprint(\"Enter one of the following countries, spelled the same way: \") \n\t\t\tprint(countries_list)\n\t\telif input_response in ('us', 'US', 'usa', 'USA'):\n\t\t\tprint(\"Sorry, we do not currently have data on the United States. Try again with another country.\")\n\t\telif input_response.lower() in countries_list:\n\t\t\trate_webscrape()\n\t\t\ttravel_webscrape(input_response)\n\t\t\tdb_processing_and_graphs(input_response)\n\t\t\tprint(\"With this new information in mind, we urge you to consider the risk of contracting or spreading COVID-19 should you decide to travel. Stay safe!\")\n\t\telse:\n\t\t\tprint(\"Invalid input. Please check your spelling and try again.\")\n\t\n\telse:\n\t\tconn.close()\n\t\tquit()\n", "description": null, "category": "webscraping", "imports": ["import requests", "import sqlite3", "from bs4 import BeautifulSoup", "import pandas as pd ", "import plotly.express as go"]}], [{"term": "def", "name": "webScrape", "data": "def webScrape():\n\tfor i in range(202):\n\t\tbs = BeautifulSoup(browser.page_source,\"html.parser\")\n\t\tfor ultag in bs.find_all(\"ul\",attrs = {\"class\",\"exoplanet\"}):\n\t\t\tlitags = ultag.find_all(\"li\")\n\t\t\ttemp_list = []\n\t\t\tfor index,li in enumerate(litags):\n\t\t\t\tif(index == 0):\n\t\t\t\t\ttemp_list.append(li.find_all(\"a\")[0].contents[0])\n\n\t\t\t\telse:\n\t\t\t\t\ttry:\n\t\t\t\t\t\ttemp_list.append(li.contents[0])\n\t\t\t\t\texcept:\n\t\t\t\t\t\ttemp_list.append(\"\")\n\t\t\tplanetData.append(temp_list)\n\t\t\thyperlink_li_tag = litags[0]\n\t\t\ttemp_list.append(\"https://exoplanets.nasa.gov\"+hyperlink_li_tag.find_all(\"a\", href=True)[0][\"href\"])\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from bs4 import BeautifulSoup", "import csv", "import time", "import requests"]}, {"term": "def", "name": "moreScraping", "data": "def moreScraping(hyperlink):\n\tpage = requests.get(hyperlink)\n\tsoup = BeautifulSoup(page.content, \"html.parser\")\n\n\ttemp_list = []\n\tfor trtags in soup.find_all(\"tr\",attrs = {\"class\",\"fact_row\"}):\n\t\ttdTags = trtags.find_all(\"td\")\n\t\tfor tdtag in tdTags:\n\t\t\ttry:\n\t\t\t\ttemp_list.append(tdtag.find_all(\"div\",attrs={\"class\",\"value\"})[0].contents[0])\n\t\t\texcept:\n\t\t\t\ttemp_list.append(\"\")\n\tplanetData2.append(temp_list)\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from bs4 import BeautifulSoup", "import csv", "import time", "import requests"]}], [{"term": "class", "name": "classEngine:", "data": "class Engine:\n\tdef __init__(self):\n\n\t\t# The basics of creating a window here... Nothing fancy\n\t\tself.window = Tk()\n\t\tself.window.minsize(1028, 720)\n\t\tself.window.title(\"Stock Trader\")\n\t\tself.window.configure(bg=\"#1c1c1c\")\n\t\tself.window.resizable(False, False) # Keeping it non-resizeable so it doesn't alter how the content will look\n\t\tself.currentTicker = 'SPY' # Default stock shown\n\t\tself.changed = False # To identify if the stock has been changed\n\t\tself.window.iconphoto(False, PhotoImage(file='favicon.png'))\n\t\t#\n\n\t\t# Search\n\t\tself.searchBar = searchBarGUI.SearchBarGUI(self.window,self)\n\n\t\t# Top Stocks\n\t\tself.topStocks = topStocksGUI.TopStocksGUI(self.window,self)\n\n\t\t# Graph -- Much of the graph config has to be done here since the variables have to be accessed in this file.\n\n\t\t\t# Colors\n\n\t\trcParams['axes.labelcolor'] = 'white'\n\t\trcParams['xtick.color'] = 'white'\n\t\trcParams['axes.titleweight'] = \"bold\"\n\t\trcParams['ytick.color'] = 'white'\n\t\trcParams['text.color'] = 'white'\n\n\t\t\t# Configs\n\t\tself.fig = plt.figure(figsize=(7, 3), dpi=100) # Basically a 700x300 i,age\n\t\tself.fig.patch.set_facecolor(\"#1c1c1c\")\n\t\tself.graph = self.fig.add_subplot(1, 1, 1)\n\t\tself.graph.set_facecolor('#454444')\n\n\t\t\t# Graph Gui\n\t\tself.graphGUI =  graphGUI.GraphGUI(self.window,self.fig,self.graph)\n\n\t\t# Stock info\n\t\tself.urlFinviz = \"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker) # URL FOR FINVIZ\n\t\tself.urlYahoo = 'https://ca.finance.yahoo.com/quote/{0}'.format(self.currentTicker) # URL FOR YAHOO FINANCE\n\n\t\tself.pageFinviz = webScrapeURL(self.urlFinviz) # webscrape the finviz page\n\t\tself.pageYahoo= webScrapeURL(self.urlYahoo) # webscrape the yahoo page\n\n\t\t# News feed GUI\n\t\tself.newsfeed = newsGUI.NewsfeedGUI(self.window, self.pageFinviz) # create the news feed\n\n\t\t# Stock info GUI\n\t\tself.stockInfo = stockinfoGUI.StockInfo(self.window, self.pageFinviz)\n\n\t# Modify Methods #\n\n\tdef changeTicker(self,ticker): # Self-explanatory\n\t\tself.currentTicker = ticker\n\t\tself.graphGUI.ticker = ticker\n\n\tdef updateEverything(self): # Re-Init stuff which changes based on stocks\n\t\tself.newsfeed.__init__(self.window,webScrapeURL(\"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker)))\n\t\tself.stockInfo.__init__(self.window,webScrapeURL(\"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker)))\n\n\t# Accessor Methods #\n\n\tdef getWindow(self):\n\t\treturn self.window\n\tdef getGraphClass(self):\n\t\treturn self.graphGUI\n\tdef getTicker(self):\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "from GUI import topStocksGUI", "from GUI import searchBarGUI", "from GUI import graphGUI", "from GUI import newsGUI", "from GUI import stockinfoGUI", "from matplotlib import rcParams", "import matplotlib.pyplot as plt", "from webscraping import *"]}], [], [{"term": "class", "name": "WebScr_Project", "data": "class WebScr_Project(Ui_MainWindow):\r\n\t\r\n\tdef __init__(self,window):\r\n\t\tself.setupUi(window)\r\n\t\tself.pushButton.clicked.connect(self.click)\r\n\t\t\r\n\tdef click(self):\r\n\t\tif self.lineEdit_2.text() == '':\r\n\t\t\taddress = self.lineEdit.text()\r\n\t\telse:  address = self.lineEdit_2.text() + \", \" + self.lineEdit.text() \r\n\t\t\r\n\t\tres, rating, dir = WebScrape.pickFood(\r\n\t\t\tself.lineEdit_2.text() + \" \" + self.lineEdit.text() ,WebScrape.driver)\r\n\t\tself.setup_popup(address, res, rating, dir)\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["from ui import *\r", "from webscr_maps import *\r", "import sys\r"]}], [{"term": "def", "name": "Webscrape", "data": "def Webscrape(URL, div_id):\r\n\t'''This function scrapes the website from the URL given to it.\\\r\n\tIt collects the entire website data and stores the data in the html format '''\r\n\t\r\n\tHEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n\t\r\n\t# Making the HTTP Request\r\n\twebpage = requests.get(URL, headers=HEADERS)\r\n  \r\n\t# Creating the Soup Object containing all data\r\n\tsoup = BeautifulSoup(webpage.content, \"html.parser\")\r\n\r\n\tresults = soup.find(id=div_id)\r\n\r\n\tprint(results.get_text())\r\n\t\r\n\treturn results.get_text()\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import warnings\r"]}, {"term": "def", "name": "Macro_Visualize", "data": "def Macro_Visualize(data):\r\n\t'''Visualize the entire extracted data using Parse Trees'''\r\n\t\r\n\tspacy.displacy.serve(data, style=\"dep\")\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import warnings\r"]}, {"term": "def", "name": "Micro_Visualize", "data": "def Micro_Visualize(data):\r\n\t'''Visualize the extracted data sentence by sentence using Parse Trees'''\r\n\t\r\n\tsentence_spans = list(data.sents)\r\n\tspacy.displacy.serve(sentence_spans, style=\"dep\")\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import warnings\r"]}, {"term": "def", "name": "POS_Tag", "data": "def POS_Tag(data):\r\n\t'''Tag Parts of Speech to the Extracted data and visualize'''\r\n\t\r\n\tspacy.displacy.serve(data, style='ent')\r\n\t\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import warnings\r"]}, {"term": "def", "name": "Collect_input", "data": "def Collect_input():\r\n\tprint(\"\t1. Sick Leave Email Template\\n\\\r\n\t2. Vacation Leave Email Template\\n\\\r\n\t3. Birthday Wishes Email Template\\n\\\r\n\t4. Marketing Reply Email Template\\n\\\r\n\t5. Out of Town Email Template\\n\\\r\n\t6. Interview Application Email Template\\n\\n\")\r\n\r\n\tval = input(\"Enter your desrired category(1-6) for the Email Template:\\n>>> \")\r\n\treturn val\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup #converts the contents of a page into a proper format\r", "import requests #used to get the content from a web page\r", "import spacy\r", "import warnings\r"]}], [{"term": "def", "name": "processwebdata", "data": "def processwebdata(request):\r\n\r\n\tevent_list = WebScrape.scrapeweb()\r\n\ttime.sleep(2)\r\n\teventdatalist = DataProcess.saveeventdata(event_list)\r\n\t\r\n\treturn HttpResponse(eventdatalist)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render\r", "from .serializer import EventsSerializers\r", "from django.http import HttpResponse\r", "from .scrape import WebScrape\r", "from .models import EventData\r", "from .service import DataProcess\r", "from rest_framework import viewsets\r", "import json\r", "from .search import Search\r", "from .recommend import Recommend\r", "from .Nearby import Nearby\r", "from django.http import JsonResponse\r", "from .UserSuggestion import UserRecommend\r", "import time\r"]}, {"term": "def", "name": "search", "data": "def search(req,inputstr = 'Dub'):\r\n\t\r\n\tprint(inputstr)\r\n\tevent_list = Search.searchtry(inputstr)\r\n\r\n\tprint(len(event_list))\r\n\t\r\n  \r\n\tevents = list(map(lambda x: to_json(x), event_list))\r\n\t#print(events)\r\n\treturn HttpResponse(events)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render\r", "from .serializer import EventsSerializers\r", "from django.http import HttpResponse\r", "from .scrape import WebScrape\r", "from .models import EventData\r", "from .service import DataProcess\r", "from rest_framework import viewsets\r", "import json\r", "from .search import Search\r", "from .recommend import Recommend\r", "from .Nearby import Nearby\r", "from django.http import JsonResponse\r", "from .UserSuggestion import UserRecommend\r", "import time\r"]}, {"term": "def", "name": "recommendations", "data": "def recommendations(req,inputstr = 'Book of Kells'):\r\n\tprint(inputstr)\r\n\trecommendations_list = Recommend.eventsrecommendations(inputstr)\r\n\r\n\trecomendedEventsList = DataProcess.fetchEventsByTitle(recommendations_list)\r\n\r\n\tevents = list(map(lambda x: to_json(x), recomendedEventsList))\r\n\r\n\treturn JsonResponse(events, safe=False)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render\r", "from .serializer import EventsSerializers\r", "from django.http import HttpResponse\r", "from .scrape import WebScrape\r", "from .models import EventData\r", "from .service import DataProcess\r", "from rest_framework import viewsets\r", "import json\r", "from .search import Search\r", "from .recommend import Recommend\r", "from .Nearby import Nearby\r", "from django.http import JsonResponse\r", "from .UserSuggestion import UserRecommend\r", "import time\r"]}, {"term": "def", "name": "nearby", "data": "def nearby(req,inputstr = 'Book of Kells'):\r\n\tprint(inputstr)\r\n\tnearby_list = Nearby.eventsNearBy(inputstr)\r\n\r\n\tnearByEventsList = DataProcess.fetchEventsByTitle(nearby_list)\r\n\r\n\tevents = list(map(lambda x: to_json(x), nearByEventsList))\r\n\r\n\treturn JsonResponse(events, safe=False)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render\r", "from .serializer import EventsSerializers\r", "from django.http import HttpResponse\r", "from .scrape import WebScrape\r", "from .models import EventData\r", "from .service import DataProcess\r", "from rest_framework import viewsets\r", "import json\r", "from .search import Search\r", "from .recommend import Recommend\r", "from .Nearby import Nearby\r", "from django.http import JsonResponse\r", "from .UserSuggestion import UserRecommend\r", "import time\r"]}, {"term": "def", "name": "usersrecommend", "data": "def usersrecommend(req, inputstr = ''):\r\n\tprint(inputstr)\r\n\tusersrecommend_list = UserRecommend.userRecommendations(inputstr)\r\n   \r\n\t#users = list(map(lambda x: users_json(x), usersrecomment_list))\r\n\tprint(usersrecommend_list)\r\n\treturn JsonResponse(usersrecommend_list, safe= False)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render\r", "from .serializer import EventsSerializers\r", "from django.http import HttpResponse\r", "from .scrape import WebScrape\r", "from .models import EventData\r", "from .service import DataProcess\r", "from rest_framework import viewsets\r", "import json\r", "from .search import Search\r", "from .recommend import Recommend\r", "from .Nearby import Nearby\r", "from django.http import JsonResponse\r", "from .UserSuggestion import UserRecommend\r", "import time\r"]}, {"term": "def", "name": "deletePastEvents", "data": "def deletePastEvents(request):\r\n\r\n\tprint('begin delete')\r\n\r\n\tDataProcess.deletePastEvents()\r\n\tprint(\"finish delete\")\r\n\r\n\treturn HttpResponse(\"events deleted successfully\")\r\n\r\n\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render\r", "from .serializer import EventsSerializers\r", "from django.http import HttpResponse\r", "from .scrape import WebScrape\r", "from .models import EventData\r", "from .service import DataProcess\r", "from rest_framework import viewsets\r", "import json\r", "from .search import Search\r", "from .recommend import Recommend\r", "from .Nearby import Nearby\r", "from django.http import JsonResponse\r", "from .UserSuggestion import UserRecommend\r", "import time\r"]}, {"term": "def", "name": "to_json", "data": "def to_json(x):\r\n\treturn {\r\n\t\t\t'title': x.title,\r\n\t\t\t'time': x.time,\r\n\t\t\t'location': x.location,\r\n\t\t\t'summary': x.summary,\r\n\t\t\t'img': x.img,\r\n\t\t\t'startdate': x.startdate,\r\n\t\t\t'enddate': x.enddate,\r\n\t\t\t'price': x.price,\r\n\t\t\t#'address': x.address,\r\n\t\t\t'read_more': x.read_more,\r\n\t\t\t'category': x.category,\r\n\t\t\t'latitude': x.latitude,\r\n\t\t\t'longitude': x.longitude\r\n\t\t}\r\n\r\n\r\n\r\n\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render\r", "from .serializer import EventsSerializers\r", "from django.http import HttpResponse\r", "from .scrape import WebScrape\r", "from .models import EventData\r", "from .service import DataProcess\r", "from rest_framework import viewsets\r", "import json\r", "from .search import Search\r", "from .recommend import Recommend\r", "from .Nearby import Nearby\r", "from django.http import JsonResponse\r", "from .UserSuggestion import UserRecommend\r", "import time\r"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(urlToScrape): # returns content of the webpage as html object for the given url\n\treq = urllib2.Request(urlToScrape, headers={'User-Agent' : \"Mozilla Firefox\"}) \n\tresponse = urllib2.urlopen( req )\n\thtml = response.read()\n\treturn html\n", "description": null, "category": "webscraping", "imports": ["import sys", "import urlparse", "import lxml.html", "from lxml.etree import tostring", "import requests", "import re", "import httplib, urllib, urllib2, cookielib", "import re", "import base64", "import os", "import json", "import markdown", "import html2text", "from datetime import datetime", "from optparse import OptionParser", "from ConfigParser import RawConfigParser", "from urllib import urlretrieve", "from bs4 import BeautifulSoup", "import urllib2", "import csv", "import time"]}, {"term": "def", "name": "read_file", "data": "def read_file(self):\n\twith open(self.file, 'r') as f:\n\t\tdata = [row for row in csv.reader(f.read().splitlines())]\n\treturn data\n", "description": null, "category": "webscraping", "imports": ["import sys", "import urlparse", "import lxml.html", "from lxml.etree import tostring", "import requests", "import re", "import httplib, urllib, urllib2, cookielib", "import re", "import base64", "import os", "import json", "import markdown", "import html2text", "from datetime import datetime", "from optparse import OptionParser", "from ConfigParser import RawConfigParser", "from urllib import urlretrieve", "from bs4 import BeautifulSoup", "import urllib2", "import csv", "import time"]}, {"term": "def", "name": "main", "data": "def main():\n\n\tfname = 'ceolisting_final.csv'\n\treader = csv.reader(open(fname, 'rU'), dialect='excel')\n\tfopen = csv.writer(open(\"ceoprofile_13Apr.csv\",'w'))\n\n\n\tfor ceolink in reader:\n\t\titems = []\n\t\tif ceolink[3] is not '':\t\t\t\n\t\t\tprint 'Crawling people from :' + ceolink[3] \n\t\t\titem = []\n\t\t\tname = ''\n\t\t\tjobTitle = ''\n\t\t\tworksFor = ''\n\t\t\talumniOf = ''\n\t\t\tage = ''\n\t\t\ttotalCompensation = ''\n\t\t\tdesc = ''\n\t\t\tworkLocation = ''\n\t\t\taffiliationsStr = ''\n\n\t\t\tprofile_html = webscrape(ceolink[3])\n\t\t\tsoup = BeautifulSoup(profile_html)\n\t\t\tkeyExecs = soup.find(\"div\", {'itemtype' : 'http://schema.org/Person'})\n\t\t\tif keyExecs :\n\t\t\t\tname = keyExecs.find(\"h1\",{'itemprop':\"name\"}).text.strip().encode('utf8')\n\t\t\t\tjobTitle = keyExecs.find(\"span\",{'itemprop':\"jobTitle\"}).text.strip().encode('utf8')\n\t\t\t\tworksFor = keyExecs.find(\"a\",{'itemprop':\"worksFor\"}).text.strip().encode('utf8')\n\t\t\t\tif keyExecs.find(\"div\",{'itemprop':\"alumniOf\"}):\n\t\t\t\t\talumniOf = keyExecs.find(\"div\",{'itemprop':\"alumniOf\"}).text.strip().encode('utf8')\n\t\t\t\tlargeDetail = keyExecs.findAll(\"td\",{'class':\"largeDetail\"})\n\t\t\t\tif largeDetail :\n\t\t\t\t\tage = largeDetail[0].text.strip().encode('utf8')\n\t\t\t\t\ttotalCompensation = largeDetail[1].text.strip().split('As of')[0].strip('$').replace(',','')\n\t\t\t\tdescription = keyExecs.find(\"p\",{'itemprop':\"description\"})\n\t\t\t\tmoredescription = keyExecs.find(\"span\",{\"id\":\"hidden\"})\n\t\t\t\tif description :\n\t\t\t\t\tdesc = description.text.strip().encode('utf8') + ' '\n\t\t\t\tif moredescription :\n\t\t\t\t\tdesc += moredescription.text.strip().encode('utf8')\t\n\t\t\t\tif keyExecs.find(\"div\",{'itemprop':\"workLocation\"}) :\n\t\t\t\t\tworkLocation = keyExecs.find(\"div\",{'itemprop':\"workLocation\"}).text.strip().encode('utf8')\n\t\t\t\taffiliations = keyExecs.findAll(\"a\",{'itemprop':\"affiliation\"})\n\t\t\t\taffiliationList = []\n\t\t\t\tfor affiliation in affiliations :\n\t\t\t\t\taffiliationList.append(affiliation.text.strip().encode('utf8'))\n\t\t\t\taffiliationsStr = \"|\".join(affiliationList)\t\t\t\n\t\t\t\t\t\t\t  \n\t\t\t\titem.append(ceolink[1])\n\t\t\t\titem.append(name)\n\t\t\t\titem.append(jobTitle)\n\t\t\t\titem.append(worksFor)\n\t\t\t\titem.append(alumniOf)\n\t\t\t\titem.append(age)\n\t\t\t\titem.append(totalCompensation)\n\t\t\t\titem.append(desc)\n\t\t\t\titem.append(workLocation)\n\t\t\t\titem.append(affiliationsStr)\n\t\t\t\titems.append(item)\n\t\t\t\t\n\t\t\t\tfopen.writerows(items)\n\t\t\t\ttime.sleep(3)\t\t\n", "description": null, "category": "webscraping", "imports": ["import sys", "import urlparse", "import lxml.html", "from lxml.etree import tostring", "import requests", "import re", "import httplib, urllib, urllib2, cookielib", "import re", "import base64", "import os", "import json", "import markdown", "import html2text", "from datetime import datetime", "from optparse import OptionParser", "from ConfigParser import RawConfigParser", "from urllib import urlretrieve", "from bs4 import BeautifulSoup", "import urllib2", "import csv", "import time"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(request):\r\n\tadesc=''\r\n\talink=''\r\n\taprice=''\r\n\tarating=''\r\n\taimg=''\r\n\tamaz=False\r\n\r\n\tfdesc=''\r\n\tflink=''\r\n\tfprice=''\r\n\tfrating=''\r\n\tfimg=''\r\n\tflip=False\r\n\r\n\tsdesc=''\r\n\tslink=''\r\n\tsprice=''\r\n\tsrating=''\r\n\tsimg=''\r\n\tsnap=False\r\n\trate=[4.1,4.2,4.3,4.4,3.8,3.9,4,3.5]\r\n\r\n\r\n\tif request.method == 'POST':\r\n\t\tbook=request.POST.get('price')\r\n\t\tdriver = webdriver.Chrome(executable_path=r\"C:\\Users\\Priyanshi\\Downloads\\chromedriver\")\r\n\r\n\t#AMAZON WEB SCRAPE\r\n\t\tatemp='https://www.amazon.in/s?k={}&ref=nb_sb_noss_2'\r\n\t\tbook=book.replace(' ','+')\r\n\t\tatemp=atemp.format(book)\r\n\t\tdriver.get(atemp)\r\n\t\tsoup=BeautifulSoup(driver.page_source,'html.parser')\r\n\t\tresults=soup.find_all('div',{'data-component-type':'s-search-result'})\r\n\t\tif len(results)==0:\r\n\t\t\tamaz=True\r\n\t\telse:\r\n\t\t\titem=results[0]\r\n\t\t\tadesc=item.h2.a.text.strip()\r\n\t\t\talink=\"https://www.amazon.in\"+item.h2.a.get('href')\r\n\t\t\taprice=item.find('span','a-price').find('span','a-offscreen').text\r\n\t\t\tarating=random.choice(rate)\r\n\t\t\taimg=item.img.get('src')\r\n\r\n\t#FLIPKART WEB SCRAPE\r\n\t\tftemp='https://www.flipkart.com/search?q={}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'\r\n\t\tftemp=ftemp.format(book)\r\n\t\tdriver.get(ftemp)\r\n\t\tsoup=BeautifulSoup(driver.page_source,'html.parser')\r\n\t\tfresults=soup.find_all('div',{'class':'_13oc-S'})\r\n\t\tif len(fresults)==0:\r\n\t\t\tflip=True\r\n\t\telse:\r\n\t\t\tfitem=fresults[0]\r\n\t\t\tfdesc=fitem.img.get('alt')\r\n\t\t\tflink=\"https://www.flipkart.com\"+fitem.a.get('href')\r\n\t\t\t#frating=fitem.find('div',{'class':'_3LWZlK'}).text\r\n\t\t\tfrating=random.choice(rate)\r\n\t\t\tfprice=fitem.find('div',{'class':'_30jeq3'}).text\r\n\t\t\tfimg=fitem.img.get('src')\r\n\r\n\t#SNAPDEAL WEB SCRAPE\r\n\t\tstemp='https://www.snapdeal.com/search?keyword={}&santizedKeyword=&catId=&categoryId=0&suggested=false&vertical=&noOfResults=20&searchState=&clickSrc=go_header&lastKeyword=&prodCatId=&changeBackToAll=false&foundInAll=false&categoryIdSearched=&cityPageUrl=&categoryUrl=&url=&utmContent=&dealDetail=&sort=rlvncy'\r\n\t\tstemp=stemp.format(book)\r\n\t\tdriver.get(stemp)\r\n\t\tsoup=BeautifulSoup(driver.page_source,'html.parser')\r\n\t\tsresults=soup.findAll('div',{'class':'product-tuple-listing'})\r\n\t\tif len(sresults)==0:\r\n\t\t\tsnap=True\r\n\t\telse:\r\n\t\t\tsitem=sresults[0]\r\n\t\t\tslink=sitem.a.get('href')\r\n\t\t\tsimg=sitem.img.get('src')\r\n\t\t\tsdesc=sitem.p.get('title')\r\n\t\t\tsprice=sitem.find('span','product-price').text\r\n\t\t\tsrating=random.choice(rate)\r\n\r\n\r\n\treturn render(request, 'web_scrape/webscrape.html',{'aimg':aimg,'arating':arating,'aprice':aprice,'adesc':adesc,'alink':alink,'fimg':fimg,'frating':frating,'fprice':fprice,'fdesc':fdesc,'flink':flink,'simg':simg,'srating':srating,'sprice':sprice,'sdesc':sdesc,'slink':slink,'snap':snap,'flip':flip,'amaz':amaz})\r\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render\r", "from django.http import HttpResponse\r", "from bs4 import BeautifulSoup\r", "import random\r", "from selenium import webdriver\r"]}], [{"term": "class", "name": "WebscrapeAdmin", "data": "class WebscrapeAdmin(admin.ModelAdmin):\n\n\tlist_display = ('name','price', 'hour', 'twenty_hours','seven_days','market_cap','volume','circulating_supply')\n\n", "description": null, "category": "webscraping", "imports": ["from django.contrib import admin", "from scrapes_data_app.models import * "]}], [], [{"term": "class", "name": "WebScrape", "data": "class WebScrape():\n\tdef __init__(self):\n\t\tself.cities = []\n\t\tself.dictionary = {}\n\tdef scrapeCities(self):\n\t\t\n\t\turl = 'https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population'\n\t\twebsite_url = requests.get(url).text\n\t\tsoup = BeautifulSoup(website_url, \"lxml\") \n\t\t\n\t\tMy_table = soup.find('table',{'class':'wikitable sortable'})\n\t\tnew = My_table.findAll('tr')\n\t\t\n\t\tfor i in range(1, len(new)):\n\t\t\tstring = str(new[i])\n\t\t\tstart = string.find(\"title=\")+7\n\t\t\tend = string.find(\"\\\"\", start, 900)\n\t\t\taltEnd = string.find(\",\", start, 900)\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen", "import requests", "from bs4 import BeautifulSoup", "import re", "import csv"]}], [{"term": "def", "name": "cli", "data": "def cli():\n\tpass\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import time", "from pathlib import Path", "import click", "import django", "import pandas as pd", "from selenium import webdriver", "from selenium.common.exceptions import NoSuchElementException", "from selenium.webdriver.chrome.options import Options", "\tfrom summary.models import DataPivot"]}, {"term": "def", "name": "get_pivot_objects", "data": "def get_pivot_objects():\n\n\tos.environ[\"DJANGO_SETTINGS_MODULE\"] = \"main.settings.dev\"\n\tdjango.setup()\n\n\tfrom summary.models import DataPivot\n\n\tdata = []\n\tfor d in DataPivot.objects.all().order_by(\"assessment_id\"):\n\t\tdata.append((d.id, d.get_absolute_url(), False, False, False, False))\n\n\tdf = pd.DataFrame(data=data, columns=(\"id\", \"url\", \"loaded\", \"error\", \"png\", \"svg\"))\n\tdf.to_pickle(FN)\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import time", "from pathlib import Path", "import click", "import django", "import pandas as pd", "from selenium import webdriver", "from selenium.common.exceptions import NoSuchElementException", "from selenium.webdriver.chrome.options import Options", "\tfrom summary.models import DataPivot"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(base_url: str):\n\tdf = pd.read_pickle(FN)\n\n\tmax_sleep = 60 * 10  # 10 min\n\n\tchrome_options = Options()\n\tchrome_options.add_argument(\"--headless\")\n\tdriver = webdriver.Chrome(options=chrome_options)\n\tdriver.set_window_size(2000, 1500)\n\tdriver.implicitly_wait(max_sleep)\n\n\turl = f\"{base_url}/user/login/\"\n\tdriver.get(url)\n\n\tdriver.find_element_by_id(\"id_username\").clear()\n\tdriver.find_element_by_id(\"id_username\").send_keys(os.environ[\"HAWC_USERNAME\"])\n\tdriver.find_element_by_id(\"id_password\").clear()\n\tdriver.find_element_by_id(\"id_password\").send_keys(os.environ[\"HAWC_PW\"])\n\tdriver.find_element_by_id(\"submit-id-login\").submit()\n\n\tfor key, data in df.iterrows():\n\t\tif data.loaded is True:\n\t\t\tcontinue\n\n\t\tdriver.implicitly_wait(max_sleep)\n\t\turl = f\"{base_url}{data.url}\"\n\t\tprint(f\"Trying {key+1} of {df.shape[0]}: {url}\")\n\t\tdriver.get(url)\n\t\tel = driver.find_element_by_id(\"dp_display\")\n\t\tloading_div = driver.find_element_by_id(\"loading_div\")\n\t\twhile True:\n\t\t\tif not loading_div.is_displayed():\n\t\t\t\tdriver.implicitly_wait(10)\n\t\t\t\ttry:\n\t\t\t\t\tsvg = driver.find_element_by_xpath(\"//*[local-name()='svg']\")\n\t\t\t\t\tif svg:\n\t\t\t\t\t\tdf.loc[key, \"loaded\"] = True\n\t\t\t\t\t\tdf.loc[key, \"error\"] = False\n\t\t\t\t\t\tdf.loc[key, \"png\"] = svg.screenshot_as_png\n\t\t\t\t\t\tdf.loc[key, \"svg\"] = svg.get_attribute(\"innerHTML\")\n\n\t\t\t\texcept NoSuchElementException:\n\t\t\t\t\tdf.loc[key, \"loaded\"] = True\n\t\t\t\t\tdf.loc[key, \"error\"] = True\n\t\t\t\t\tdf.loc[key, \"svg\"] = el.get_attribute(\"innerHTML\")\n\n\t\t\t\tdf.to_pickle(FN)\n\t\t\t\tbreak\n\n\t\t\ttime.sleep(0.1)\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import time", "from pathlib import Path", "import click", "import django", "import pandas as pd", "from selenium import webdriver", "from selenium.common.exceptions import NoSuchElementException", "from selenium.webdriver.chrome.options import Options", "\tfrom summary.models import DataPivot"]}], [{"term": "def", "name": "webscrape_edicoes", "data": "def webscrape_edicoes(numeros):\r\n\tglobal num_gravados\r\n\turl = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n\tsource = requests.get(url).text\r\n\tsoup = BeautifulSoup(source, 'lxml')\r\n\tlidos = 0\r\n   \r\n\tfor texto in soup.find_all('table', class_='tocArticle'):\r\n\t\tlink = texto.find('div', class_='tocTitle').a\r\n\t\tlink2 = link['href']\r\n\t\ttitulo = texto.find('div', class_='tocTitle').text.strip()\r\n\t\tautor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n\t\tprint(f'T\u00c3\u00adtulo do trabalho: {titulo}')\r\n\t\tprint(f'Autores: {autor}')\r\n\t\tprint(f'Link: {link2}')\r\n\t\tprint()\r\n\t\tlidos += 1\r\n\t\tnum_gravados += 1\r\n\treturn lidos > 0\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import csv\r", "import re\r"]}], [{"term": "class", "name": "FGSystem", "data": "class FGSystem(Enum):\n\tDC_RoS = \"rfangraphsdc\"\n\tZiPS_RoS = \"rzips\"\n\tSteamer_RoS = \"steamerr\"\n\tATC = \"atc\"\n\n", "description": null, "category": "webscraping", "imports": ["from enum import Enum"]}, {"term": "class", "name": "FGPosGrp", "data": "class FGPosGrp(Enum):\n\tHIT = 'bat'\n\tPIT = 'pit'\n\n", "description": null, "category": "webscraping", "imports": ["from enum import Enum"]}, {"term": "class", "name": "Savant", "data": "class Savant(Enum):\n\txStats = \"expected_statistics\"\n\tbarrels = \"statcast\"\n\trolling = \"rolling\"\n\trankings = \"percentile-rankings\"\n\n", "description": null, "category": "webscraping", "imports": ["from enum import Enum"]}, {"term": "class", "name": "SavantDownload", "data": "class SavantDownload(Enum):\n\txStats = \"expected_stats.csv\"\n\tbarrels = \"exit_velocity.csv\"\n\t# rolling does not download .csv; need to webscrape\n\trankings = \"percentile-rankings.csv\"\n\n", "description": null, "category": "webscraping", "imports": ["from enum import Enum"]}, {"term": "class", "name": "SavantPosGrp", "data": "class SavantPosGrp(Enum):\n\tHIT = \"batter\"\n\tPIT = \"pitcher\"\n", "description": null, "category": "webscraping", "imports": ["from enum import Enum"]}], [{"term": "class", "name": "WebScrape", "data": "class WebScrape(object):\n\n\tdef __init__(self, parcel = None):\n\t\tself.parcel = parcel\n\n\tdef insert_scraping(self):\n\t\twith sync_playwright() as p:\n\t\t\tbrowser = p.chromium.launch(headless=False)\n\t\t\tpage = browser.new_page()\n\t\t\tpage.goto(\"https://permits.placer.ca.gov/CitizenAccess/Default.aspx\")\n\t\t\t# time.sleep(10)\n\t\t\tiframe = page.wait_for_selector('#ACAFrame').content_frame()\n\t\t\ttable = iframe.wait_for_selector('#ctl00_PlaceHolderMain_TabDataList_TabsDataList_ctl01_LinksDataList_ctl00_LinkItemUrl')\n\t\t\ttable.click()\n\t\t\tiframe = page.wait_for_selector('#ACAFrame').content_frame()\n\t\t\t# iframe.wait_for_load_state()\n\t\t\tiframe.wait_for_selector('#ctl00_PlaceHolderMain_generalSearchForm_txtGSParcelNo')\n\t\t\tiframe.fill('#ctl00_PlaceHolderMain_generalSearchForm_txtGSParcelNo', self.parcel)\n\t\t\ttable = iframe.wait_for_selector('#ctl00_PlaceHolderMain_btnNewSearch')\n\t\t\ttable.click()\n\t\t\tiframe = page.wait_for_selector('#ACAFrame').content_frame()\n\t\t\ttable = iframe.wait_for_selector('#lnkMoreDetail')\n\t\t\ttable.click()\n\t\t\tapply = iframe.wait_for_selector('#imgASI')\n\t\t\tapply.click()\n\t\t\tapply_2 = iframe.wait_for_selector('#imgParcel')\n\t\t\tapply_2.click()\n\t\t\tpermit_number = iframe.wait_for_selector('#ctl00_PlaceHolderMain_lblPermitNumber').inner_text()\n\t\t\tapplicant_name = iframe.wait_for_selector('.contactinfo_fullname').inner_text()\n\t\t\twork_location = iframe.wait_for_selector('.NotBreakWord').inner_text()\n\t\t\twork_place = re.sub(r\"\\n\", \" \", work_location)\n\t\t\tdata={}\n\t\t\tdata['Permit Number']= permit_number\n\t\t\tdata['Applicant Name']= applicant_name\n\t\t\tdata['Work Location']= work_place\n\n\t\t\tprint(data)\n\t\t\t# time.sleep(10)\n\n\t\t\tprint(table.inner_text())\n\t\t\tprint(page.title())\n\t\t\tbrowser.close()\n\n", "description": null, "category": "webscraping", "imports": ["from playwright.sync_api import sync_playwright", "import time", "import sys", "import re"]}], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape:\n\tURL = 'https://www.time.com'\n\tdef getResponse(self):\n\t\tresponse = requests.get(self.URL)\n\t\tresponseString = str(response.text)\n\t\treturn responseString\n\tdef getStoriesTitleandLink(self):\n\t\tresponseString = self.getResponse()\n\t\tindexPartialStories = responseString.find('partial latest-stories')\n\t\tresponseString = responseString[indexPartialStories:indexPartialStories + 14000]\n\t\ttitle = re.findall(']*>(.*?)', responseString)\n\t\tlink = re.findall('href=\"/([^\"]*)\"[^>]*>([\\s\\S]*?)', responseString)\n\t\trequiredLinkArray = []\n\t\tfor i in range(len(link)):\n\t\t\trequiredLinkArray.append(link[i][0])\n\t\treturn(title,requiredLinkArray)\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "import re", "from flask_cors import CORS, cross_origin", "from flask import Flask,jsonify"]}, {"term": "def", "name": "getTimeStories", "data": "def getTimeStories():\n\tdata = WebScrape().getStoriesTitleandLink()\n\trequiredJson = []\n\tfor i in range(len(data[0])):\n\t\tjsonMap = {'title': data[0][i], 'link': 'http://www.time.com/'+ data[1][i]}\n\t\trequiredJson.append(jsonMap)\n", "description": null, "category": "webscraping", "imports": ["import requests", "import re", "from flask_cors import CORS, cross_origin", "from flask import Flask,jsonify"]}], [{"term": "class", "name": "classsuggestion:", "data": "class suggestion:\n\n\tdef __init__(self):\n\t\treturn \n\t\n\tdef readdata(self, filename, sheetname):\n\t\t'''clean data for this part should be a xlsx file so we must specify the filename and sheetname'''\n\t\t#try:\n\t\tgraph = pd.read_excel(filename, sheet_name=sheetname, engine='openpyxl')\n\t\t# except:\n\t\t#\t print(filename)\n\t\t#\t print(\"filename or sheetname is wrong, Please check your input\")\n\t\t#\t return \n\t\t\t\n\t\tgraph = graph[:-6]  #delete the last six lines which is the comment\n\t\t\n\t\tgraph['ESTIMATED ENERGY USAGE*'] = graph['ESTIMATED ENERGY USAGE*'].apply(lambda x: x.split(' ')[0])\n\t\tgraph['ESTIMATED ENERGY COSTS**'] = graph['ESTIMATED ENERGY COSTS**'].apply(lambda x: x.split(' ')[0][1:])\n\t\tgraph['ESTIMATED ENERGY COSTS**'] = graph['ESTIMATED ENERGY COSTS**'].apply(lambda x: x.split('/')[0])\n\t\t\n\t\tfor i in range(len(graph['CATEGORY1'])):\n\n\t\t\t# the column estimated energy costs is cleaned in this process\n\t\t\t# print(graph['ESTIMATED ENERGY COSTS**'][i])\n\t\t\t\n\t\t\tif '$' in graph['ESTIMATED ENERGY COSTS**'][i]:\n\t\t\t\t# print(\"I come into the if statment\")\n\t\t\t\ttmp = graph['ESTIMATED ENERGY COSTS**'][i].split('\u2013$')\n\t\t\t\tgraph['ESTIMATED ENERGY COSTS**'][i] = (float(tmp[0]) + float(tmp[1])) / 2\n\t\t\telif graph['ESTIMATED ENERGY COSTS**'][i] == 'ess':\t\t\t\n\t\t\t\t# catch the exception when less than 0.01 per use\n\t\t\t\tgraph['ESTIMATED ENERGY COSTS**'][i] = 0.0\n\t\t\telse:\n\t\t\t\tgraph['ESTIMATED ENERGY COSTS**'][i] = float(graph['ESTIMATED ENERGY COSTS**'][i])\n\n\t\t\t# print(graph['ESTIMATED ENERGY USAGE*'][i])\n\t\t\t\n\t\t\t# process the estimated energy usage data\n\t\t\tif '\u2013' in graph['ESTIMATED ENERGY USAGE*'][i]:\n\t\t\t\t# print(\"I come into the if statment\")\n\t\t\t\ttmp = graph['ESTIMATED ENERGY USAGE*'][i].split('\u2013')\n\t\t\t\tgraph['ESTIMATED ENERGY USAGE*'][i] = (float(tmp[0]) + float(tmp[1])) / 2\n\t\t\telse:\n\t\t\t\tgraph['ESTIMATED ENERGY USAGE*'][i] = float(graph['ESTIMATED ENERGY USAGE*'][i])\n\t\n\t\t# print(graph['ESTIMATED ENERGY USAGE*'])\n\t\t#transfer the data \n\n\t\treturn graph\n\n\tdef read_user_option(self, graph):\n\t\t#initialize some of the variable used in this method\n\t\tcat1 = graph['CATEGORY1'][0] +\" \" +graph['CATEGORY2'][0]\n\t\tcate_min = ''\n\t\tprice_min = 9999999\n\t\ttotal_save = 0\n\t\tdetail = {}\n\n\t\tprint(\"\\n\\nWelcome to the demo part for calculating your energy consumption and saving money!\\nYou can enter q to exit this flow\")\n\t\tfor i in range(len(graph['CATEGORY1'])):\n\t\t\t# if categories change, then print all the options and let the user choose from them\n\t\t\tif str(graph['CATEGORY1'][i]) +\" \" + str(graph['CATEGORY2'][i]) != cat1:\n\t\t\t\tprint(\"\\n**Category: \", cat1)\n\t\t\t\t# print(detail)\n\t\t\t\tfor j in range(len(detail)):\n\t\t\t\t\tprint(j+1, '. ', list(detail)[j])\n\t\t\t\tchoice = input(\"please Enter your choice or leave it blank: \")\n\t\t\t\tif choice == 'q':\n\t\t\t\t\tbreak\n\t\t\t\tif choice != '':\n\t\t\t\t\tchoice = int(choice) - 1\n\t\t\t\t\tif list(detail.values())[choice][0]*list(detail.values())[choice][1] != price_min:\n\t\t\t\t\t\tprint(\"Your choice is \", list(detail.keys())[choice], \", this would cost \", list(detail.values())[choice][0]*list(detail.values())[choice][1], \n\t\t\t\t\t\t\"dollar per hour\\n\", \"If you choose \", cate_min, \", you will save\", list(detail.values())[choice][0]*list(detail.values())[choice][1]-price_min,\n\t\t\t\t\t\t\"dollar per hour!\")\n\t\t\t\t\t\ttotal_save += list(detail.values())[choice][0]*list(detail.values())[choice][1]-price_min\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint(\"Congratulations! You have made the most energy saving choice!\")\n\t\t\t\t\tprint(\"In this suggestion flow you have saved \", total_save, \"dollars per hour!!\")\n\t\t\t\t\t#demo the least one\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Please choose \", cate_min, \" as your choice! This choice saves energy and will cost you \", price_min, \" dollar per hour!\\nOr you choosing not to involve this category\")\n\t\t\t\tdetail = {}\n\t\t\t\tdetail[graph['DETAIL'][i]] = [graph['ESTIMATED ENERGY USAGE*'][i], graph['ESTIMATED ENERGY COSTS**'][i]]\n\t\t\t\tcate_min = graph['DETAIL'][i]\n\t\t\t\tprice_min = graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i]\n\t\t\t\tcat1 = str(graph['CATEGORY1'][i]) + \" \" + str(graph['CATEGORY2'][i])\n\t\t\telse:\n\t\t\t\tdetail[graph['DETAIL'][i]] = [graph['ESTIMATED ENERGY USAGE*'][i], graph['ESTIMATED ENERGY COSTS**'][i]]\n\t\t\t\tif graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i] < price_min:\n\t\t\t\t\tcate_min = graph['DETAIL'][i]\n\t\t\t\t\tprice_min = graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i] \n\t\tprint(\"\\nYou have saved \", total_save, \"dollars per hour in the whole program flow!!!\")\n\t\tprint(\"\\nThank you for using demo program to reduce your energy costs and your money!\")\n\t\t#return total_save\n\n\tdef co2_emi_capita(self, filename, sheetname):\n\t\tdf1 = pd.read_excel(open(filename, 'rb'), sheet_name=sheetname) \n\t\tdf_USA_individual = df1.iloc[21232:21451]\n\t\tdf_USA_individual.plot(x=\"Year\", y='Per capita CO2 emissions')\n\t\tplt.show()\n\n\n", "description": null, "category": "webscraping", "imports": ["import numpy as np ", "import pandas as pd ", "from matplotlib import pyplot as plt ", "import re ", "import seaborn as sns", "import webscrape"]}], [{"term": "class", "name": "classsuggestion:", "data": "class suggestion:\n\n\tdef __init__(self):\n\t\treturn \n\t\n\tdef readdata(self, filename, sheetname):\n\t\t'''clean data for this part should be a xlsx file so we must specify the filename and sheetname'''\n\t\t#try:\n\t\tgraph = pd.read_excel(filename, sheet_name=sheetname, engine='openpyxl')\n\t\t# except:\n\t\t#\t print(filename)\n\t\t#\t print(\"filename or sheetname is wrong, Please check your input\")\n\t\t#\t return \n\t\t\t\n\t\tgraph = graph[:-6]  #delete the last six lines which is the comment\n\t\t\n\t\tgraph['ESTIMATED ENERGY USAGE*'] = graph['ESTIMATED ENERGY USAGE*'].apply(lambda x: x.split(' ')[0])\n\t\tgraph['ESTIMATED ENERGY COSTS**'] = graph['ESTIMATED ENERGY COSTS**'].apply(lambda x: x.split(' ')[0][1:])\n\t\tgraph['ESTIMATED ENERGY COSTS**'] = graph['ESTIMATED ENERGY COSTS**'].apply(lambda x: x.split('/')[0])\n\t\t\n\t\tfor i in range(len(graph['CATEGORY1'])):\n\n\t\t\t# the column estimated energy costs is cleaned in this process\n\t\t\t# print(graph['ESTIMATED ENERGY COSTS**'][i])\n\t\t\t\n\t\t\tif '$' in graph['ESTIMATED ENERGY COSTS**'][i]:\n\t\t\t\t# print(\"I come into the if statment\")\n\t\t\t\ttmp = graph['ESTIMATED ENERGY COSTS**'][i].split('\u2013$')\n\t\t\t\tgraph['ESTIMATED ENERGY COSTS**'][i] = (float(tmp[0]) + float(tmp[1])) / 2\n\t\t\telif graph['ESTIMATED ENERGY COSTS**'][i] == 'ess':\t\t\t\n\t\t\t\t# catch the exception when less than 0.01 per use\n\t\t\t\tgraph['ESTIMATED ENERGY COSTS**'][i] = 0.0\n\t\t\telse:\n\t\t\t\tgraph['ESTIMATED ENERGY COSTS**'][i] = float(graph['ESTIMATED ENERGY COSTS**'][i])\n\n\t\t\t# print(graph['ESTIMATED ENERGY USAGE*'][i])\n\t\t\t\n\t\t\t# process the estimated energy usage data\n\t\t\tif '\u2013' in graph['ESTIMATED ENERGY USAGE*'][i]:\n\t\t\t\t# print(\"I come into the if statment\")\n\t\t\t\ttmp = graph['ESTIMATED ENERGY USAGE*'][i].split('\u2013')\n\t\t\t\tgraph['ESTIMATED ENERGY USAGE*'][i] = (float(tmp[0]) + float(tmp[1])) / 2\n\t\t\telse:\n\t\t\t\tgraph['ESTIMATED ENERGY USAGE*'][i] = float(graph['ESTIMATED ENERGY USAGE*'][i])\n\t\n\t\t# print(graph['ESTIMATED ENERGY USAGE*'])\n\t\t#transfer the data \n\n\t\treturn graph\n\n\tdef read_user_option(self, graph):\n\t\t#initialize some of the variable used in this method\n\t\tcat1 = graph['CATEGORY1'][0] +\" \" +graph['CATEGORY2'][0]\n\t\tcate_min = ''\n\t\tprice_min = 9999999\n\t\ttotal_save = 0\n\t\tdetail = {}\n\n\t\tprint(\"\\n\\nWelcome to the demo part for calculating your energy consumption and saving money!\\nYou can enter q to exit this flow\")\n\t\tfor i in range(len(graph['CATEGORY1'])):\n\t\t\t# if categories change, then print all the options and let the user choose from them\n\t\t\tif str(graph['CATEGORY1'][i]) +\" \" + str(graph['CATEGORY2'][i]) != cat1:\n\t\t\t\tprint(\"\\n**Category: \", cat1)\n\t\t\t\t# print(detail)\n\t\t\t\tfor j in range(len(detail)):\n\t\t\t\t\tprint(j+1, '. ', list(detail)[j])\n\t\t\t\tchoice = input(\"please Enter your choice or leave it blank: \")\n\t\t\t\tif choice == 'q':\n\t\t\t\t\tbreak\n\t\t\t\tif choice != '':\n\t\t\t\t\tchoice = int(choice) - 1\n\t\t\t\t\tif list(detail.values())[choice][0]*list(detail.values())[choice][1] != price_min:\n\t\t\t\t\t\tprint(\"Your choice is \", list(detail.keys())[choice], \", this would cost \", list(detail.values())[choice][0]*list(detail.values())[choice][1], \n\t\t\t\t\t\t\"dollar per hour\\n\", \"If you choose \", cate_min, \", you will save\", list(detail.values())[choice][0]*list(detail.values())[choice][1]-price_min,\n\t\t\t\t\t\t\"dollar per hour!\")\n\t\t\t\t\t\ttotal_save += list(detail.values())[choice][0]*list(detail.values())[choice][1]-price_min\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint(\"Congratulations! You have made the most energy saving choice!\")\n\t\t\t\t\tprint(\"In this suggestion flow you have saved \", total_save, \"dollars per hour!!\")\n\t\t\t\t\t#demo the least one\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Please choose \", cate_min, \" as your choice! This choice saves energy and will cost you \", price_min, \" dollar per hour!\\nOr you choosing not to involve this category\")\n\t\t\t\tdetail = {}\n\t\t\t\tdetail[graph['DETAIL'][i]] = [graph['ESTIMATED ENERGY USAGE*'][i], graph['ESTIMATED ENERGY COSTS**'][i]]\n\t\t\t\tcate_min = graph['DETAIL'][i]\n\t\t\t\tprice_min = graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i]\n\t\t\t\tcat1 = str(graph['CATEGORY1'][i]) + \" \" + str(graph['CATEGORY2'][i])\n\t\t\telse:\n\t\t\t\tdetail[graph['DETAIL'][i]] = [graph['ESTIMATED ENERGY USAGE*'][i], graph['ESTIMATED ENERGY COSTS**'][i]]\n\t\t\t\tif graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i] < price_min:\n\t\t\t\t\tcate_min = graph['DETAIL'][i]\n\t\t\t\t\tprice_min = graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i] \n\t\tprint(\"\\nYou have saved \", total_save, \"dollars per hour in the whole program flow!!!\")\n\t\tprint(\"\\nThank you for using demo program to reduce your energy costs and your money!\")\n\t\t#return total_save\n\n\tdef co2_emi_capita(self, filename, sheetname):\n\t\tdf1 = pd.read_excel(open(filename, 'rb'), sheet_name=sheetname) \n\t\tdf_USA_individual = df1.iloc[21232:21451]\n\t\tdf_USA_individual.plot(x=\"Year\", y='Per capita CO2 emissions')\n\t\tplt.show()\n\n\n", "description": null, "category": "webscraping", "imports": ["import numpy as np ", "import pandas as pd ", "from matplotlib import pyplot as plt ", "import re ", "import seaborn as sns", "import webscrape"]}], [{"term": "def", "name": "webscrape_images1", "data": "def webscrape_images1():\n\tfor id,animal in enumerate(an_face.keys()):\n\t\tprint(\"1st loop\",animal,\"searching\")\n\t\turl = \"https://search.aol.com/aol/image;_ylt=Awr9Hr57TuJh5GkAKeNjCWVH?q={}+\uc5f0\uc608\uc778&ei=UTF-8&s_it=sb_top&v_t=comsearch&imgty=photo&fr2=p%3As%2Cv%3Ai\".format(animal)\n\t\tres = requests.get(url, headers=headers)\n\t\tres.raise_for_status()\n\t\tsoup = BeautifulSoup(res.text, \"lxml\")\n\t\timages = soup.find_all(\"a\", attrs={\"class\":\"img\"})\n\t\tprint(\"images length:\",len(images))\n\t\tif animal == '\uac15\uc544\uc9c0\uc0c1':\n\t\t\tos.chdir('images/dog')\n\t\telif animal == '\uace0\uc591\uc774\uc0c1':\n\t\t\tos.chdir('images/cat')\n\t\telif animal == '\ud1a0\ub07c\uc0c1':\n\t\t\tos.chdir('images/rabbit')\n\t\telif animal == '\ub9d0\uc0c1':\n\t\t\tos.chdir('images/horse')\n\t\telif animal == '\uc5ec\uc6b0\uc0c1':\n\t\t\tos.chdir('images/fox')\n\t\telif animal == '\ub2e4\ub78c\uc950\uc0c1':\n\t\t\tos.chdir('images/squirrel')\n\t\telif animal == '\uacf0\uc0c1':\n\t\t\tos.chdir('images/bear')\n\t\telif animal == '\ub291\ub300\uc0c1':\n\t\t\tos.chdir('images/wolf')\n\t\telif animal == '\uc6d0\uc22d\uc774\uc0c1':\n\t\t\tos.chdir('images/monkey')\n\t\telif animal == '\uac70\ubd81\uc774\uc0c1':\n\t\t\tos.chdir('images/turtle')\n\t\telif animal == '\ub3fc\uc9c0\uc0c1':\n\t\t\tos.chdir('images/pig')\n\t\telif animal == '\uc0ac\uc2b4\uc0c1':\n\t\t\tos.chdir('images/deer')\n\t\telif animal == '\uac1c\uad6c\ub9ac\uc0c1':\n\t\t\tos.chdir('images/frog')\n\t\tcnt = 0\n\t\tfor i,image in enumerate(images):\n\t\t\tcnt += 1\n\t\t\timage_url = image[\"href\"]\n\t\t\timage_res = requests.get(image_url)\n\t\t\ttry:\n\t\t\t\twith open(\"{}{}.jpg\".format(animal,i), \"wb\") as f:\n\t\t\t\t\tf.write(image_res.content)\n\t\t\texcept image_res.raise_for_status():\n\t\t\t\tcontinue\n\t\tos.chdir(owd)\n", "description": null, "category": "webscraping", "imports": ["from operator import index", "import requests, lxml, re, json", "from bs4 import BeautifulSoup", "import os"]}, {"term": "def", "name": "webscrape_images2", "data": "def webscrape_images2(dog_cnt,cat_cnt,rabbit_cnt,horse_cnt,fox_cnt,squirrel_cnt,bear_cnt,wolf_cnt,monkey_cnt,turtle_cnt,pig_cnt,deer_cnt,frog_cnt):\n\tfor idx1,names in enumerate(an_face.values()):\n\t\tfor name_ind,name in enumerate(names):\n\t\t\tprint(\"2nd loop\",name,\"searching\")\n\t\t\turl1 = \"https://search.aol.com/aol/image;_ylt=Awr9Hr57TuJh5GkAKeNjCWVH?q={}+\uc5bc\uad74&ei=UTF-8&s_it=sb_top&v_t=comsearch&imgty=photo&fr2=p%3As%2Cv%3Ai\".format(name)\n\t\t\tres1 = requests.get(url1, headers=headers)\n\t\t\tres1.raise_for_status()\n\t\t\tsoup1 = BeautifulSoup(res1.text, \"lxml\")\n\t\t\tface_images = soup1.find_all(\"a\", attrs={\"class\":\"img\"})\n\t\t\tif idx1 == 0:\n\t\t\t\tos.chdir('images/dog')\n\t\t\t\tanimal = '\uac15\uc544\uc9c0\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tdog_cnt += temp_cnt\n\t\t\t\tcnt = dog_cnt\n\t\t\telif idx1 == 1:\n\t\t\t\tos.chdir('images/cat')\n\t\t\t\tanimal = '\uace0\uc591\uc774\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tcat_cnt += temp_cnt\n\t\t\t\tcnt = cat_cnt\n\t\t\telif idx1 == 2:\n\t\t\t\tos.chdir('images/rabbit')\n\t\t\t\tanimal = '\ud1a0\ub07c\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\trabbit_cnt += temp_cnt\n\t\t\t\tcnt = rabbit_cnt\n\t\t\telif idx1 == 3:\n\t\t\t\tos.chdir('images/horse')\n\t\t\t\tanimal = '\ub9d0\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\thorse_cnt += temp_cnt\n\t\t\t\tcnt = horse_cnt\n\t\t\telif idx1 == 4:\n\t\t\t\tos.chdir('images/fox')\n\t\t\t\tanimal = '\uc5ec\uc6b0\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tfox_cnt += temp_cnt\n\t\t\t\tcnt = fox_cnt\n\t\t\telif idx1 == 5:\n\t\t\t\tos.chdir('images/squirrel')\n\t\t\t\tanimal = '\ub2e4\ub78c\uc950\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tsquirrel_cnt += temp_cnt\n\t\t\t\tcnt = squirrel_cnt\n\t\t\telif idx1 == 6:\n\t\t\t\tos.chdir('images/bear')\n\t\t\t\tanimal = '\uacf0\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tbear_cnt += temp_cnt\n\t\t\t\tcnt = bear_cnt\n\t\t\telif idx1 == 7:\n\t\t\t\tos.chdir('images/wolf')\n\t\t\t\tanimal = '\ub291\ub300\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\twolf_cnt += temp_cnt\n\t\t\t\tcnt = wolf_cnt\n\t\t\telif idx1 == 8:\n\t\t\t\tos.chdir('images/monkey')\n\t\t\t\tanimal = '\uc6d0\uc22d\uc774\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tmonkey_cnt += temp_cnt\n\t\t\t\tcnt = monkey_cnt\n\t\t\telif idx1 == 9:\n\t\t\t\tos.chdir('images/turtle')\n\t\t\t\tanimal = '\uac70\ubd81\uc774\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tturtle_cnt += temp_cnt\n\t\t\t\tcnt = turtle_cnt\n\t\t\telif idx1 == 10:\n\t\t\t\tos.chdir('images/pig')\n\t\t\t\tanimal = '\ub3fc\uc9c0\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tpig_cnt += temp_cnt\n\t\t\t\tcnt = pig_cnt\n\t\t\telif idx1 == 11:\n\t\t\t\tos.chdir('images/deer')\n\t\t\t\tanimal = '\uc0ac\uc2b4\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tdeer_cnt += temp_cnt\n\t\t\t\tcnt = deer_cnt\n\t\t\telif idx1 == 12:\n\t\t\t\tos.chdir('images/frog')\n\t\t\t\tanimal = '\uac1c\uad6c\ub9ac\uc0c1'\n\t\t\t\tif name_ind > 0:\n\t\t\t\t\tfrog_cnt += temp_cnt\n\t\t\t\tcnt = frog_cnt\n\t\t\ttemp_cnt = 0\n\t\t\tprint(\"length= \",len(face_images))\n\t\t\tfor face_ind,face_image in enumerate(face_images):\n\t\t\t\tprint(face_ind)\n\t\t\t\ttemp_cnt += 1\n\t\t\t\tcnt += 1\n\t\t\t\tface_image_url = face_image[\"href\"]\n\t\t\t\tface_image_res = requests.get(face_image_url)\n\t\t\t\ttry:\n\t\t\t\t\twith open(\"{}{}.jpg\".format(animal,cnt), \"wb\") as f:\n\t\t\t\t\t\tf.write(face_image_res.content)\n\t\t\t\texcept face_image_res.raise_for_status():\n\t\t\t\t\tcontinue\n\t\t\tos.chdir(owd)\n", "description": null, "category": "webscraping", "imports": ["from operator import index", "import requests, lxml, re, json", "from bs4 import BeautifulSoup", "import os"]}], [], [{"term": "class", "name": "classWebscrapeSpiderMiddleware:", "data": "class WebscrapeSpiderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the spider middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_spider_input(self, response, spider):\n\t\t# Called for each response that goes through the spider\n\t\t# middleware and into the spider.\n\n\t\t# Should return None or raise an exception.\n\t\treturn None\n\n\tdef process_spider_output(self, response, result, spider):\n\t\t# Called with the results returned from the Spider, after\n\t\t# it has processed the response.\n\n\t\t# Must return an iterable of Request, or item objects.\n\t\tfor i in result:\n\t\t\tyield i\n\n\tdef process_spider_exception(self, response, exception, spider):\n\t\t# Called when a spider or process_spider_input() method\n\t\t# (from other spider middleware) raises an exception.\n\n\t\t# Should return either None or an iterable of Request or item objects.\n\t\tpass\n\n\tdef process_start_requests(self, start_requests, spider):\n\t\t# Called with the start requests of the spider, and works\n\t\t# similarly to the process_spider_output() method, except\n\t\t# that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n\t\t# Must return only requests (not items).\n\t\tfor r in start_requests:\n\t\t\tyield r\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}, {"term": "class", "name": "classWebscrapeDownloaderMiddleware:", "data": "class WebscrapeDownloaderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the downloader middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_request(self, request, spider):\n\t\t# Called for each request that goes through the downloader\n\t\t# middleware.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this request\n\t\t# - or return a Response object\n\t\t# - or return a Request object\n\t\t# - or raise IgnoreRequest: process_exception() methods of\n\t\t#   installed downloader middleware will be called\n\t\treturn None\n\n\tdef process_response(self, request, response, spider):\n\t\t# Called with the response returned from the downloader.\n\n\t\t# Must either;\n\t\t# - return a Response object\n\t\t# - return a Request object\n\t\t# - or raise IgnoreRequest\n\t\treturn response\n\n\tdef process_exception(self, request, exception, spider):\n\t\t# Called when a download handler or a process_request()\n\t\t# (from other downloader middleware) raises an exception.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this exception\n\t\t# - return a Response object: stops process_exception() chain\n\t\t# - return a Request object: stops process_exception() chain\n\t\tpass\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}], [], [{"term": "def", "name": "CreateCorperationWitJsonFile", "data": "def CreateCorperationWitJsonFile():\n\t#  open the json file\n\twith open(\"./petrol_pumps.json\", \"r\") as f:\n\t\tfor item in json.load(f):\n\t\t\tname = item.get(\"name\")\n\t\t\taddress = item.get(\"address\")\n\t\t\tphone = item.get(\"phone\")\n\t\t\topen_hours = item.get(\"open_hours\")\n\t\t\tlink = item.get(\"link\")\n\t\t\t#  not all has decription since i dont need to webscrape all\n\t\t\tdescription = item.get(\"description\", \"No Description\")\n\t\t\ttry:\n\t\t\t\t#  create the cooperating\n\t\t\t\tCorporation.objects.create(\n\t\t\t\t\tname=name,\n\t\t\t\t\taddress=address,\n\t\t\t\t\tphone=phone,\n\t\t\t\t\topen_hours=open_hours,\n\t\t\t\t\tlink=link,\n\t\t\t\t\tdescription=description,\n\t\t\t\t)\n\t\t\texcept Exception as a:\n\t\t\t\tprint(a)\n\n", "description": null, "category": "webscraping", "imports": ["import json", "import os", "import sys", "import django", "from corporation.models import Corporation"]}], [], [{"term": "def", "name": "webscrape_edicoes", "data": "def webscrape_edicoes(numeros):\r\n\tglobal num_gravados\r\n\turl = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n\tsource = requests.get(url).text\r\n\tsoup = BeautifulSoup(source, 'lxml')\r\n\tlidos = 0\r\n   \r\n\tfor texto in soup.find_all('table', class_='tocArticle'):\r\n\t\tlink = texto.find('a', attrs={'href': re.compile(\"http://\")})\r\n\t\tlink2 = str(link.get('href')[65:70])\r\n\t\tlinks.append(link2)\r\n\t\ttitulo = texto.find('div', class_='tocTitle').text.strip()\r\n\t\tautor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n\t\tprint(f'T\u00c3\u00adtulo do trabalho: {titulo}')\r\n\t\tprint(f'Autores: {autor}')\r\n\t\tprint(f'Link: {link2}')\r\n\t\tprint()\r\n\t\tlidos += 1\r\n\t\tnum_gravados += 1\r\n\treturn lidos > 0\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import csv\r", "import re\r"]}], [], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape:\n\tdef __init__(self):\n\t\tprint(\"WebScrape Imported\")\n\n\tdef lazada_scrape(self,head,category,url):\n\t\tlist_of_rows = []\n\t\turl = \"http://www.lazada.com.ph/\"+ url +\"/\"\n\t\tsource_code = requests.get(url)\n\t\ttxt = source_code.text\n\t\tsoup = BeautifulSoup(txt, 'html.parser')\n\t\tmax_page = int(soup.select(\"span.pages > a:nth-of-type(6)\")[0].get_text())\n\t\tpage = 1\n\t\tmyfile = open(category + \".csv\", 'w', newline='')\n\t\twriter = csv.DictWriter(myfile, fieldnames = [\"url\", \"product_name\", \"product_header\", \"product_category\", \"product_price\", \"product_sale\", \"product_old\", \"installment\", \"rating\"], delimiter=',')\n\t\twriter.writeheader()\n\t\twhile page <= max_page:\n\t\t\t\tprint(page)\n\t\t\t\turl = \"http://www.lazada.com.ph/shop-mobiles/?page=\" + str(page)\n\t\t\t\tsource_code = requests.get(url)\n\t\t\t\ttxt = source_code.text\n\t\t\t\tsoup = BeautifulSoup(txt,'html.parser')\n\t\t\t\tfor div in soup.find_all(\"div\", {\"class\":\"product-card\"}):\n\t\t\t\t\t\tmylist = []\n\t\t\n\t\t\t\t\t\tfor link in div.find_all(\"a\"):\n\t\t\t\t\t\t\t\tmylist.append(str(link.get(\"href\")))\n\t\t\t\t\t\tfor title in div.find_all(\"span\", {\"class\":\"product-card__name\"}):\n\t\t\t\t\t\t\t\tmylist.append(str(title.text).replace(\"\\u200f\",\" \").replace(\"\\uFF08\",\"(\").replace(\"\\uff09\",\")\"))\n\t\t\t\t\t\t\t\tmylist.append(head)\n\t\t\t\t\t\t\t\tmylist.append(category)\n\t\t\t\t\t\tfor price in div.find_all(\"div\", {\"class\":\"product-card__price\"}):\n\t\t\t\t\t\t\t\tmylist.append(str(price.text.replace(\"\\u20B1\",\"Php \")))\n\n\t\t\t\t\t\tsale = div.find_all(\"div\", {\"class\":\"product-card__sale\"})\n\t\t\t\t\t\tif not sale:\n\t\t\t\t\t\t\tmylist.append(\"0%\")\n\t\t\t\t\t\telse:\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tfor sales in sale:\n\t\t\t\t\t\t\t\t\tmylist.append(str(sales.text))\n\n\t\t\t\t\t\told = div.find_all(\"div\", {\"class\":\"old-price-wrap\"})\n\t\t\t\t\t\tif not old:\n\t\t\t\t\t\t\tmylist.append(\"Php 0.00\")\n\t\t\t\t\t\telse:\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tfor olds in old:\n\t\t\t\t\t\t\t\t\tmylist.append(str(olds.text).replace(\"\\u20B1\",\"Php \").replace(\"\\n\",\"\"))\n\n\t\t\t\t\t\tinstallment = div.find_all(\"span\", {\"class\":\"installment-part\"})\n\t\t\t\t\t\tif not installment:\n\t\t\t\t\t\t\tmylist.append(\"Php 0.00\")\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfor installments in installment:\n\t\t\t\t\t\t\t\tmylist.append(str(installments.text).replace(\"\\u20B1\",\"Php \"))\n\n\t\t\t\t\t\trating = div.find_all(\"span\", {\"class\":\"rating__number\"})\n\t\t\t\t\t\tif not rating:\n\t\t\t\t\t\t\tmylist.append(\"(0 reviews)\")\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfor ratings in rating:\n\t\t\t\t\t\t\t\tmylist.append(str(ratings.text))\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tlist_of_rows.append(mylist)\n\t\t\t\tpage+=1\n\t\twr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n", "description": null, "category": "webscraping", "imports": ["import requests", "import csv", "from bs3 import BeautifulSoup"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape():\n\t\"\"\"Webscrapes the worldometers website for coronavirus statistics and saves information into textfile\"\"\"\n\turl = 'https://www.worldometers.info/coronavirus/countries-where-coronavirus-has-spread/'\n\n\t# gets the url's html\n\tpage_content = requests.get(url)\n\tsoup = BeautifulSoup(page_content.text, 'html.parser')\n\n\tweb_data = []\n\n\t# soup.find_all('td') will scrape all elements in the url's table elements\n\tdata_iterator = iter(soup.find_all('td'))\n\n\t# Loop that repeats until there is no data left available for the iterator\n\twhile True:\n\t\ttry:\n\t\t\tcountry = next(data_iterator).text\n\t\t\tconfirmed = next(data_iterator).text\n\t\t\tdeaths = next(data_iterator).text\n\t\t\tcontinent = next(data_iterator).text\n\n\t\t\t# Replaces spaces with underscores in country and continent names to help with saving file content later\n\t\t\tcountry = country.replace(' ', '_')\n\t\t\tcontinent = continent.replace(' ', '_')\n\t\t\t# This just adds stats into the list while also replacing the confirmed and deaths into ints\n\t\t\t# NOTE: This creates a list of tuples\n\t\t\tweb_data.append((country,\n\t\t\t\t\t\t\t int(confirmed.replace(',', '')),  # This allows for numbers in millions\n\t\t\t\t\t\t\t int(deaths.replace(',', '')),\n\t\t\t\t\t\t\t continent\n\t\t\t\t\t\t\t ))\n\n\t\t# StopIteration error occurs when there are no more elements to iterate through\n\t\texcept StopIteration:\n\t\t\tbreak\n\n\t#  Sorts the data by number of confirmed cases\n\tweb_data.sort(key=lambda row: row[1], reverse=True)\n\n\tf = open('webscrape_data.txt', 'w')\n\tfor tuple_unit in web_data:\n\t\tf.write(''.join(str(s) + ' ' for s in tuple_unit) + ' \\n')\n\n\tf.close()\n\n", "description": "Webscrapes the worldometers website for coronavirus statistics and saves information into textfile", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import numpy as np", "import matplotlib.pyplot as plt", "# from texttable import Texttable", "# import matplotlib"]}, {"term": "def", "name": "deaths_of_country", "data": "def deaths_of_country(data, country):\n\t\"\"\"Returns the number of deaths caused by coronavirus of the chosen country\"\"\"\n\tcounter = 0\n\twhile country != data[counter][0]:\n\t\tcounter = counter + 1\n\n\treturn data[counter][2]\n\n", "description": "Returns the number of deaths caused by coronavirus of the chosen country", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import numpy as np", "import matplotlib.pyplot as plt", "# from texttable import Texttable", "# import matplotlib"]}, {"term": "def", "name": "cases_of_country", "data": "def cases_of_country(data, country):\n\t\"\"\"Returns the number of coronavirus cases of the chosen country\"\"\"\n\tcounter = 0\n\twhile country != data[counter][0]:\n\t\tcounter = counter + 1\n\n\treturn data[counter][1]\n\n", "description": "Returns the number of coronavirus cases of the chosen country", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import numpy as np", "import matplotlib.pyplot as plt", "# from texttable import Texttable", "# import matplotlib"]}, {"term": "def", "name": "top_affected_countries", "data": "def top_affected_countries(data):\n\t\"\"\"Returns a list of names of the top 5 countries with the most coronavirus cases\"\"\"\n\tcountries = []\n\tfor tuple_value in data[:5]:\n\t\tcountries.append(tuple_value[0])\n\n\treturn countries\n\n", "description": "Returns a list of names of the top 5 countries with the most coronavirus cases", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import numpy as np", "import matplotlib.pyplot as plt", "# from texttable import Texttable", "# import matplotlib"]}, {"term": "def", "name": "read_data", "data": "def read_data():\n\t\"\"\"Reads data from textfile rather than consistently calling the webscrap function\"\"\"\n\tf = open('webscrape_data.txt')\n\tfile_data = []\n\n\tfor line in f:\n\t\tline = line.rstrip(' \\n')\n\t\t# if line.isdigit():\n\t\t#\t line_edited = tuple(int(line))\n\t\t# else:\n\t\tline_edited = tuple(map(str, line.split(' ')))\n\t\tfile_data.append(line_edited)\n\n\treturn file_data\n\n", "description": "Reads data from textfile rather than consistently calling the webscrap function", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import numpy as np", "import matplotlib.pyplot as plt", "# from texttable import Texttable", "# import matplotlib"]}, {"term": "def", "name": "quick_label", "data": "def quick_label(rects, ax):\n\t\"\"\"Creates a text number label above each bar in *rects*, displaying totals.\"\"\"\n\tfor rect in rects:\n\t\theight = rect.get_height()\n\t\tax.annotate('{}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height),\n\t\t\t\t\txytext=(0, 3),  # 3 points vertical offset\n\t\t\t\t\ttextcoords=\"offset points\",\n\t\t\t\t\tha='center', va='bottom')\n\n", "description": "Creates a text number label above each bar in *rects*, displaying totals.", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import numpy as np", "import matplotlib.pyplot as plt", "# from texttable import Texttable", "# import matplotlib"]}, {"term": "def", "name": "graph_top_affected_countries_c", "data": "def graph_top_affected_countries_c(sent_data):\n\t\"\"\"Creates a singular bar graph that displays the confirmed cases.\"\"\"\n\t# matplotlib setup portion\n\tcountry_labels = []\n\tconfirmed_bars = []\n\tbar_width = 0.5\n\tfig, ax1 = plt.subplots()\n\tfig.suptitle('Top 5 Countries Most Affected By Coronavirus')\n\tfig.set_size_inches(10, 7)\n\n\t# Takes the first 5 countries from data list for now\n\tfor tuple_value in sent_data[:5]:\n\t\tcountry_labels.append(tuple_value[0])\n\t\tconfirmed_bars.append(int(tuple_value[1]))\n\n\tx = np.arange(len(country_labels))\n\n\t# Total Confirmed Cases Portion on Bar Graph\n\trects1 = ax1.bar(x - bar_width / 2, confirmed_bars, bar_width, label='Confirmed', color=['teal'])\n\tax1.set_ylabel('Total Confirmed Cases (Millions)')\n\t# ax1.set_title('Top 5 Countries most affected by Coronavirus')\n\tax1.set_xticks(x)\n\tax1.set_xticklabels(country_labels)\n\tax1.legend()\n\tquick_label(rects1, ax1)\n\n\t# Gets the final graph displayed\n\tfig.tight_layout()\n\t# plt.savefig('images/Coronavirus_Confirmed_Cases_Graph.png')\n\tplt.show()\n\n", "description": "Creates a singular bar graph that displays the confirmed cases.", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import numpy as np", "import matplotlib.pyplot as plt", "# from texttable import Texttable", "# import matplotlib"]}, {"term": "def", "name": "graph_top_affected_countries_d", "data": "def graph_top_affected_countries_d(sent_data):\n\t\"\"\"Creates a singular bar graph that displays the most affected countries' deaths caused by coronavirus.\"\"\"\n\t# matplotlib setup portion\n\tcountry_labels = []\n\tdeaths_bars = []\n\tbar_width = 0.5\n\tfig, ax1 = plt.subplots()\n\tfig.suptitle('Top 5 Countries Most Affected By Coronavirus')\n\tfig.set_size_inches(10, 7)\n\n\t# Takes the first 5 countries from data list for now\n\tfor tuple_value in sent_data[:5]:\n\t\tcountry_labels.append(tuple_value[0])\n\t\tdeaths_bars.append(int(tuple_value[2]))\n\n\tx = np.arange(len(country_labels))\n\n\t# Total Confirmed Cases Portion on Bar Graph\n\trects1 = ax1.bar(x - bar_width / 2, deaths_bars, bar_width, label='Confirmed', color=['red'])\n\tax1.set_ylabel('Total Confirmed Cases (Millions)')\n\t# ax1.set_title('Top 5 Countries most affected by Coronavirus')\n\tax1.set_xticks(x)\n\tax1.set_xticklabels(country_labels)\n\tax1.legend()\n\tquick_label(rects1, ax1)\n\n\t# Gets the final graph displayed\n\tfig.tight_layout()\n\t# plt.savefig('images/Coronavirus_Death_Cases_Graph.png')\n\tplt.show()\n\n", "description": "Creates a singular bar graph that displays the most affected countries' deaths caused by coronavirus.", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import numpy as np", "import matplotlib.pyplot as plt", "# from texttable import Texttable", "# import matplotlib"]}, {"term": "def", "name": "graph_top_affected_countries_cd", "data": "def graph_top_affected_countries_cd(sent_data):\n\t\"\"\"Creates two bar graphs and displays the information side by side.\"\"\"\n\t# matplotlib setup portion\n\tcountry_labels = []\n\tconfirmed_bars = []\n\tdeaths_bars = []\n\tbar_width = 0.45\n\tfig, (ax1, ax2) = plt.subplots(1, 2)  # Note: fig is figure\n\tfig.suptitle('Top 5 Countries Most Affected By Coronavirus')\n\tfig.set_size_inches(10, 7)\n\n\t# Takes the first 5 countries from data list for now\n\tfor tuple_value in sent_data[:5]:\n\t\tcountry_labels.append(tuple_value[0])\n\t\tconfirmed_bars.append(int(tuple_value[1]))\n\t\tdeaths_bars.append(int(tuple_value[2]))\n\n\tx = np.arange(len(country_labels))\n\n\t# Total Confirmed Cases Portion on Bar Graph\n\trects1 = ax1.bar(x - bar_width / 2, confirmed_bars, bar_width, label='Confirmed', color=['teal'])\n\tax1.set_ylabel('Total Confirmed Cases (Millions)')\n\t# ax1.set_title('Top 5 Countries most affected by Coronavirus')\n\tax1.set_xticks(x)\n\tax1.set_xticklabels(country_labels)\n\tax1.legend()\n\n\t# Total Confirmed Death Cases Portion on Bar Graph\n\trects2 = ax2.bar(x + bar_width / 2, deaths_bars, bar_width, label='Deaths', color=['red'])\n\tax2.set_ylabel('Total Death Cases')\n\tax2.set_xticks(x)\n\tax2.set_xticklabels(country_labels)\n\tax2.legend()\n\n\tquick_label(rects1, ax1)\n\tquick_label(rects2, ax2)\n\n\t# Gets the final graph displayed\n\tfig.tight_layout()\n\t#plt.savefig('images/Confirmed_and_Death_Cases_Graph.png')\n\tplt.show()\n\n", "description": "Creates two bar graphs and displays the information side by side.", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import numpy as np", "import matplotlib.pyplot as plt", "# from texttable import Texttable", "# import matplotlib"]}], [{"term": "def", "name": "test_ses_email", "data": "def test_ses_email(aws_credentials):\n\tses = boto3.client(\"ses\", region_name=\"us-east-1\")\n\tlogs = pd.DataFrame({\"errors\": [\"ex1\", \"ex2\", \"ex3\"]})\n\tsend_aws_email(logs)\n\tassert ses.verify_email_identity(EmailAddress=\"jyablonski9@gmail.com\")\n\n", "description": null, "category": "webscraping", "imports": ["from datetime import datetime", "import awswrangler as wr", "import boto3", "from moto import mock_ses, mock_s3", "import numpy as np", "import pandas as pd", "import pytest", "from nba_bbref_webscrape.utils import get_leading_zeroes", "from nba_bbref_webscrape.aws_functions import send_aws_email, execute_email_function, write_to_s3"]}, {"term": "def", "name": "test_ses_execution_logs", "data": "def test_ses_execution_logs(aws_credentials):\n\tses = boto3.client(\"ses\", region_name=\"us-east-1\")\n\tlogs = pd.DataFrame({\"errors\": [\"ex1\", \"ex2\", \"ex3\"]})\n\texecute_email_function(logs)\n\tassert ses.verify_email_identity(EmailAddress=\"jyablonski9@gmail.com\")\n\n", "description": null, "category": "webscraping", "imports": ["from datetime import datetime", "import awswrangler as wr", "import boto3", "from moto import mock_ses, mock_s3", "import numpy as np", "import pandas as pd", "import pytest", "from nba_bbref_webscrape.utils import get_leading_zeroes", "from nba_bbref_webscrape.aws_functions import send_aws_email, execute_email_function, write_to_s3"]}, {"term": "def", "name": "test_ses_execution_no_logs", "data": "def test_ses_execution_no_logs(aws_credentials):\n\tses = boto3.client(\"ses\", region_name=\"us-east-1\")\n\tlogs = pd.DataFrame({\"errors\": []})\n\tsend_aws_email(logs)\n\tassert ses.verify_email_identity(EmailAddress=\"jyablonski9@gmail.com\")\n\n", "description": null, "category": "webscraping", "imports": ["from datetime import datetime", "import awswrangler as wr", "import boto3", "from moto import mock_ses, mock_s3", "import numpy as np", "import pandas as pd", "import pytest", "from nba_bbref_webscrape.utils import get_leading_zeroes", "from nba_bbref_webscrape.aws_functions import send_aws_email, execute_email_function, write_to_s3"]}, {"term": "def", "name": "test_write_to_s3_validated", "data": "def test_write_to_s3_validated(player_stats_data):\n\tconn = boto3.resource(\"s3\", region_name=\"us-east-1\")\n\ttoday = datetime.now().date()\n\tmonth = datetime.now().month\n\tmonth_prefix = get_leading_zeroes(month)\n\tplayer_stats_data.schema = \"Validated\"\n\tconn.create_bucket(Bucket=\"moto_test_bucket\")\n\n\twrite_to_s3(\"player_stats_data\", player_stats_data, bucket=\"moto_test_bucket\")\n\tbucket = conn.Bucket(\"moto_test_bucket\")\n\tcontents = [_.key for _ in bucket.objects.all()]\n\n\tassert (\n\t\tcontents[0]\n\t\t== f\"player_stats_data/validated/{month_prefix}/player_stats_data-{today}.parquet\"\n\t)\n\n", "description": null, "category": "webscraping", "imports": ["from datetime import datetime", "import awswrangler as wr", "import boto3", "from moto import mock_ses, mock_s3", "import numpy as np", "import pandas as pd", "import pytest", "from nba_bbref_webscrape.utils import get_leading_zeroes", "from nba_bbref_webscrape.aws_functions import send_aws_email, execute_email_function, write_to_s3"]}, {"term": "def", "name": "test_write_to_s3_invalidated", "data": "def test_write_to_s3_invalidated(player_stats_data):\n\tconn = boto3.resource(\"s3\", region_name=\"us-east-1\")\n\ttoday = datetime.now().date()\n\tmonth = datetime.now().month\n\tmonth_prefix = get_leading_zeroes(month)\n\tplayer_stats_data.schema = \"Invalidated\"\n\tconn.create_bucket(Bucket=\"moto_test_bucket\")\n\n\twrite_to_s3(\"player_stats_data\", player_stats_data, bucket=\"moto_test_bucket\")\n\tbucket = conn.Bucket(\"moto_test_bucket\")\n\tcontents = [_.key for _ in bucket.objects.all()]\n\n\tassert (\n\t\tcontents[0]\n\t\t== f\"player_stats_data/invalidated/{month_prefix}/player_stats_data-{today}.parquet\"\n\t)\n", "description": null, "category": "webscraping", "imports": ["from datetime import datetime", "import awswrangler as wr", "import boto3", "from moto import mock_ses, mock_s3", "import numpy as np", "import pandas as pd", "import pytest", "from nba_bbref_webscrape.utils import get_leading_zeroes", "from nba_bbref_webscrape.aws_functions import send_aws_email, execute_email_function, write_to_s3"]}], [{"term": "def", "name": "webScrape", "data": "def webScrape():\n\tglobal score\n\tscore[COURIER_NAME[0]] = P2.cityLink(\n\t\t\"http://autoworld.com.my/news/2020/09/25/city-link-express-takes-delivery-of-277-new-isuzu-trucks/\",\n\t\t\"https://www.thestar.com.my/business/business-news/2015/01/05/citylink-mulls-main-market-listing-in-three-years\",\n\t\t\"https://www.thesundaily.my/gear-up/isuzu-lorries--city-link-s-preferred-choice-AK729310\",\n\t)\n\n\tscore[COURIER_NAME[1]] = P2.posLaju(\n\t\t\"https://www.thestar.com.my/business/business-news/2021/02/22/pos-malaysia-records-rm233b-revenue-in-fy20\",\n\t\t\"https://soyacincau.com/2020/08/15/pos-malaysia-e-consignment-notes-qr-code-available/\",\n\t\t\"https://www.theborneopost.com/2020/07/08/poslaju-customers-urged-to-bear-with-longer-waiting-time/\",\n\t)\n\n\tscore[COURIER_NAME[2]] = P2.gdex(\n\t\t\"https://www.theedgemarkets.com/article/gdex-stands-benefit-pickup-ecommerce-activities-says-kenanga-research\",\n\t\t\"https://www.theedgemarkets.com/article/gdex-look-creating-industrial-reit-part-next-growth-phase\",\n\t\t\"https://www.theedgemarkets.com/article/gdex-2q-net-profit-down-absence-gain-warns-covid19-impact\",\n\t)\n\n\tscore[COURIER_NAME[3]] = P2.jnt(\n\t\t\"https://www.straitstimes.com/asia/se-asia/pandemic-fuelled-e-shopping-boom-spurs-courier-firms-growth\",\n\t\t\"https://www.theedgemarkets.com/article/indonesias-jt-express-said-weigh-us1-billionplus-us-ipo\",\n\t\t\"https://kr-asia.com/one-masters-two-apprentices-how-indonesias-jt-express-rose-in-china-on-the-back-of-pinduoduo\",\n\t)\n\n\tscore[COURIER_NAME[4]] = P2.dhl(\n\t\t\"https://www.theedgemarkets.com/article/tech-digitalisation-way-forward-dhl-express\",\n\t\t\"https://www.theedgemarkets.com/article/dhl-predicts-strong-growth-b2b-ecommerce\",\n\t\t\"https://www.theedgemarkets.com/article/special-report-rocky-road-ahead-logistics-operators-amid-pandemic\",\n\t)\n\n\n", "description": null, "category": "webscraping", "imports": ["# from Problem2 import problem2dhl as P2", "from Problem2 import problem2 as P2", "from Problem1 import p1Q1 as P1q1", "from Problem1 import p1Q2 as P1q2", "from Problem1 import p1Q4 as P1q4", "import problem3 as P3", "# from Problem3 import problem3 as P3"]}], [{"term": "def", "name": "get_date", "data": "def get_date(Datestr):\n\tDateList=Datestr.split('. ')\n\tif (len(DateList)==2):\n\t\tMonthNameDict = {\t\n\t\t'ledna' : 1,\n\t\t'\u00fanora' : 2,\n\t\t'b\u0159ezna' : 3,\n\t\t'dubna' : 4,\n\t\t'kv\u011btna' : 5,\n\t\t'\u010dervna' : 6,\n\t\t'\u010dervence' : 7,\n\t\t'srpna' : 8,\n\t\t'z\u00e1\u0159\u00ed' : 9,\n\t\t'\u0159\u00edjna' : 10,\n\t\t'listopadu' : 11,\n\t\t'prosince' : 12,\n\t\t}\n\t\tConvertedDateStr=str(DateList[0])+'|'+str(MonthNameDict[DateList[1]])+'|'+str(datetime.datetime.now().year)\n\t\treturn datetime.datetime.strptime(ConvertedDateStr,'%d|%m|%Y')\n\telif(len(DateList)>2):\n\t\tif(len(Datestr.split(','))>0):\n\t\t\treturn datetime.datetime.strptime(Datestr, '%d. %m. %Y, %H:%M')\n\n\telse:\n\t\tToday=datetime.datetime.now()\n\t\tDayNameDict = {\n\n\t\t'p\u0159edev\u010d\u00edrem':2,\n\t\t'v\u010dera':1,\n\t\t'dnes':0,\n\t\t}\n\n\t\tif(len(Datestr.split('.'))>1):\n\t\t\treturn datetime.datetime.strptime(Datestr,'%d.%m.%Y')\n\t\telse:\n\t\t\treturn datetime.datetime.now()-datetime.timedelta(days=DayNameDict[Datestr])\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen as uReq", "from bs4 import BeautifulSoup as soup\t", "import os, sys, datetime, time"]}, {"term": "def", "name": "Webscrape_head", "data": "def Webscrape_head(page_soup):\n\tNazev = page_soup.find(\"h1\",{\"itemprop\":\"name\"}).text\n\tBookInfoFile=open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/BookInfo.tsv\", \"a\",encoding=\"utf-8\")\n\n\tAutor = page_soup.find(\"span\",{\"itemprop\":\"author\"}).text\n\tZanry = page_soup.select('a[href*=\"zanr\"]')\n\n\ttry:\n\t\tAnotacePart1 = page_soup.find(\"span\",{\"class\":\"start_text\"})\n\t\tif (AnotacePart1 is not None):\n\t\t\tAnotacePart1=AnotacePart1.text.replace(chr(13),'').replace('\\n',' ').replace('  ','').strip()\n\t\t\tAnotacePart2 = page_soup.find(\"span\",{\"class\":\"end_text\"})\n\t\t\tAnotacePart2=AnotacePart2.text.replace(chr(13),'').replace('\\n',' ').replace('  ','').strip()\n\t\t\tAnotace = str(AnotacePart1)+str(AnotacePart2)\n\t\telse:\n\t\t\tAnotace = page_soup.find(\"p\",{\"id\":\"bdetdesc\"}).text.replace(chr(13),'').replace('\\n',' ').replace('  ','').strip()\n\n\texcept AttributeError:\n\t\tAnotace = '??'\n\n\tBookInfoFile.write(\"Nazev: \"+Nazev+'\\n')\n\tfor genre in Zanry:\n\t\tBookInfoFile.write(\"Zanr: \"+genre.text+'\\n')\n\tBookInfoFile.write(\"Autor: \"+Autor+'\\n')\n\tBookInfoFile.write(\"Anotace: \"+Anotace+'\\n')\n\n\tgenre_list=[]\n\tfor genre in Zanry:\n\t\tgenre_list.append(genre.text)\n\n\tBookInfoFile.write(Nazev+'\\t'+author+'\\t'+','.join(genre_list)+'\\t'+Anotace+'\\n')\n\n\tBookInfoFile.close()\n\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen as uReq", "from bs4 import BeautifulSoup as soup\t", "import os, sys, datetime, time"]}, {"term": "def", "name": "Webscrape_reviews", "data": "def Webscrape_reviews(my_url):\n\tif('c=all' not in my_url):\n\t\tmy_url = my_url + '?c=all'\n\t#otevre url a precte html zadaneho url\n\tuClient = uReq(my_url)\n\tpage_html = uClient.read()\n\tuClient.close()\n\n\t#vyhledani hledanych dat v html\n\tpage_soup = soup(page_html, \"html.parser\")\n\n\tNazev = page_soup.find(\"h1\",{\"itemprop\":\"name\"}).text\n\n\t#Webscrape_head(page_soup)\n\n\tDatabazeKnihReviews = open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/Reviews.tsv\", \"a\",encoding=\"utf-8\")\n\n\trewiev_page_count=1\n\tfor rewiev_page in range(1,rewiev_page_count+1):\n\t\tif(rewiev_page != 1):\n\t\t\t#zmena url a precteni\n\t\t\tmy_url=my_url+'&str='+str(rewiev_page)\n\t\t\t#vyhledani recenzi v html\n\t\t\tuClient = uReq(my_url)\n\t\t\tpage_html = uClient.read()\n\t\t\tuClient.close()\n\t\t\tpage_soup = soup(page_html, \"html.parser\")\n\t\treviews = page_soup.findAll(\"div\",{\"class\":\"komentars_user komover\"})\n\t\treviews+= page_soup.findAll(\"div\",{\"class\":\"komentars_user_last komover\"})\n\t\tfor review in reviews:\n\t\t\tusername=review.div.a.text\n\n\t\t\tlikes=review.div.div.em\n\t\t\tif(likes is not None):\n\t\t\t\tlikes=likes.text\n\t\t\telse:\n\t\t\t\tlikes=0\n\t\t\tdate=review.div.div.span\n\t\t\tif(date is not None):\n\t\t\t\tdate=get_date(date.text)\n\t\t\telse:\n\t\t\t\tdate='??'\n\n\t\t\trating=review.div.div.img\n\t\t\tif(rating is not None):\n\t\t\t\trating=rating[\"src\"][:-4]\n\t\t\t\trating=int(rating[-1])*20\n\t\t\telse:\n\t\t\t\trating='??'\n\t\t\t\n\t\t\tcomment=review.div.p.text.replace(chr(13),'').replace('\\n',' ').replace('  ','').strip()\n\n\t\t\tDatabazeKnihReviews.write(\"DatabazeKnih\"+'\\t'+Nazev+'\\t'+username+'\\t'+str(likes)+'\\t'+str(date)+'\\t'+str(rating)+'\\t'+comment+'\\n') \n\t\ttime.sleep(2)#delay mezi pristupy aby nespadl server\n\ttime.sleep(2)\n\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen as uReq", "from bs4 import BeautifulSoup as soup\t", "import os, sys, datetime, time"]}], [{"term": "def", "name": "_next_page", "data": "def _next_page(page, results):\n\t#check to see if the page exists\n\tif (page > parser.get_last_page()):\n\t\tprint(\"Page does not exist\")\n\t\t#view the results again\n\t\tview_results(results)\n\t\treturn\n\t\n\t#move the parser to the next page\n\tparser.move_to_page(page)\n\t\n\t#get the new review results from the parser\n\tresults = parser.get_results()\n\t#view them\n\tview_results(results)\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.gamedetail.gameparse.game_user_parser import game_user_parser", "from webscrape.searchresults.categorydetails.gamedetail.gamedisplay import game_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "print_page_reviews", "data": "def print_page_reviews(results):\n\t#clear the command line output\n\tclear()\n\t\n\t#number of reviews on the page\n\tnumber = 1\n\t#loop through each review and print it\n\tfor result in results:\n\t\tprint(str(number) + \". \" + result.to_string())\n\t\tnumber = number + 1\n\t#if the number is still 1, then the for loop was never entered due to results being empty\n\tif (number == 1):\n\t\tprint(\"There are no user reviews on this page\")\n\t\t\n\t#print the current page\n\tprint(\"Page \" + str(parser.get_current_page()) + \" of \" + str(parser.get_last_page()))  \n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.gamedetail.gameparse.game_user_parser import game_user_parser", "from webscrape.searchresults.categorydetails.gamedetail.gamedisplay import game_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "view_results", "data": "def view_results(results):\n\t\n\t#prints the reviews\n\tprint_page_reviews(results)\n\t\n\t#prompts the user to either enter a page number or view a review\n\tnumber = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n\t\n\t#if the number is invalid\n\tif (number.isdigit() == False):\n\t\tnumber = -1\n\t#if the number is zero then the user wants to navigate to a different page\n\tif (number == \"0\"):\n\t\t#prompt user for page number\n\t\tpage = input(\"Enter page number\\n\")\n\t\t#if the page is not valid\n\t\tif (page.isdigit() == False):\n\t\t\tpage = 0\n\t\t\n\t\tpage =  int(page)\n\t\t#go to the next page\t\n\t\t_next_page(page, results)\n\t\t\n\t\treturn\n\t#cast the user input into a number\n\tnumber = int(number)\n\t\n\t#while the number is a valid number\n\twhile (number > 0 and number < len(results) + 1):\n\t\t\n\t\t#get the chosen review\n\t\tchosenresult = results[number-1]\n\t\t\n\t\t#view the details of the review here\n\t\tgame_view_review_detail.view_review_detail(chosenresult)\n\t\t\n\t\t\n\t\tprint_page_reviews(results)\n\t\t\n\t\t\n\t\t#prompts the user to either view a new page or to view a search result\n\t\tnumber = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n\t\t\t#if the number is invalid\n\t\tif (number.isdigit() == False):\n\t\t\tnumber = -1\n\t\t\n\t\t#if the number is zero then the user wants to navigate to a different page\n\t\tif (number == \"0\"):\n\t\t\t#prompt user for page number\n\t\t\tpage = input(\"Enter page number\\n\")\n\t\t\t#if the page is not valid\n\t\t\tif (page.isdigit() == False):\n\t\t\t\tpage = 0\n\t\t\t\n\t\t\tpage =  int(page)\n\t\t\t\t\n\t\t\t#go to the next page \n\t\t\t_next_page(page, results)\n\t\t\n\t\t\treturn\n\t\n\t\tnumber = int(number)\n\t\t\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.gamedetail.gameparse.game_user_parser import game_user_parser", "from webscrape.searchresults.categorydetails.gamedetail.gamedisplay import game_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "view_game_user_reviews", "data": "def view_game_user_reviews(user_link):\n\tglobal parser\n\t\n\t#create the parser for the review\n\tparser = game_user_parser(user_link)\n\t\n\t#get the reviews on the page\n\tresults = parser.get_results()\n\t\n\t#view the results\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.gamedetail.gameparse.game_user_parser import game_user_parser", "from webscrape.searchresults.categorydetails.gamedetail.gamedisplay import game_view_review_detail", "from webscrape import clear"]}], [{"term": "def", "name": "_next_page", "data": "def _next_page(page, results):\n\t#check to see if the page exists\n\tif (page > parser.get_last_page()):\n\t\tprint(\"Page does not exist\")\n\t\t#view the results again\n\t\tview_results(results)\n\t\treturn\n\t\n\t#move the parser to the next page\n\tparser.move_to_page(page)\n\t\n\t#get the new review results from the parser\n\tresults = parser.get_results()\n\t#view them\n\tview_results(results)\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.tvdetail.tvparse.tv_user_parser import tv_user_parser", "from webscrape.searchresults.categorydetails.tvdetail.tvdisplay import tv_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "print_page_reviews", "data": "def print_page_reviews(results):\n\t#clears the command line output\n\tclear()\n\t#number of reviews on the page\n\tnumber = 1\n\t#loop through each review and print it\n\tfor result in results:\n\t\tprint(str(number) + \". \" + result.to_string())\n\t\tnumber = number + 1\n\t#if the number is still 1, then the for loop was never entered due to results being empty\n\tif (number == 1):\n\t\tprint(\"There are no user reviews on this page\")\n\t\t\n\t#print the current page\n\tprint(\"Page \" + str(parser.get_current_page()) + \" of \" + str(parser.get_last_page()))  \n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.tvdetail.tvparse.tv_user_parser import tv_user_parser", "from webscrape.searchresults.categorydetails.tvdetail.tvdisplay import tv_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "view_results", "data": "def view_results(results):\n\t\n\t#prints the reviews\n\tprint_page_reviews(results)\n\t\n\t#prompts the user to either enter a page number or view a review\n\tnumber = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n\t\n\t#if the number is invalid\n\tif (number.isdigit() == False):\n\t\tnumber = -1\n\t#if the number is zero then the user wants to navigate to a different page\n\tif (number == \"0\"):\n\t\t#prompt user for page number\n\t\tpage = input(\"Enter page number\\n\")\n\t\t#if the page is not valid\n\t\tif (page.isdigit() == False):\n\t\t\tpage = 0\n\t\t\n\t\tpage =  int(page)\n\t\t#go to the next page\t\n\t\t_next_page(page, results)\n\t\t\n\t\treturn\n\t#cast the user input into a number\n\tnumber = int(number)\n\t\n\t#while the number is a valid number\n\twhile (number > 0 and number < len(results) + 1):\n\t\t\n\t\t#get the chosen review\n\t\tchosenresult = results[number-1]\n\t\t\n\t\t#view the details of the review here\n\t\ttv_view_review_detail.view_review_detail(chosenresult)\n\t\t\n\t\t\n\t\tprint_page_reviews(results)\n\t\t\n\t\t\n\t\t#prompts the user to either view a new page or to view a search result\n\t\tnumber = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n\t\t\t#if the number is invalid\n\t\tif (number.isdigit() == False):\n\t\t\tnumber = -1\n\t\t\n\t\t#if the number is zero then the user wants to navigate to a different page\n\t\tif (number == \"0\"):\n\t\t\t#prompt user for page number\n\t\t\tpage = input(\"Enter page number\\n\")\n\t\t\t#if the page is not valid\n\t\t\tif (page.isdigit() == False):\n\t\t\t\tpage = 0\n\t\t\t\n\t\t\tpage =  int(page)\n\t\t\t\t\n\t\t\t#go to the next page \n\t\t\t_next_page(page, results)\n\t\t\n\t\t\treturn\n\t\n\t\tnumber = int(number)\n\t\t\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.tvdetail.tvparse.tv_user_parser import tv_user_parser", "from webscrape.searchresults.categorydetails.tvdetail.tvdisplay import tv_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "view_tv_user_reviews", "data": "def view_tv_user_reviews(user_link):\n\tglobal parser\n\t\n\t#create the parser for the review\n\tparser = tv_user_parser(user_link)\n\t\n\t#get the reviews on the page\n\tresults = parser.get_results()\n\t\n\t#view the results\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.tvdetail.tvparse.tv_user_parser import tv_user_parser", "from webscrape.searchresults.categorydetails.tvdetail.tvdisplay import tv_view_review_detail", "from webscrape import clear"]}], [{"term": "def", "name": "_next_page", "data": "def _next_page(page, results):\n\t#check to see if the page exists\n\tif (page > parser.get_last_page()):\n\t\tprint(\"Page does not exist\")\n\t\t#view the results again\n\t\tview_results(results)\n\t\treturn\n\t\n\t#move the parser to the next page\n\tparser.move_to_page(page)\n\t\n\t#get the new review results from the parser\n\tresults = parser.get_results()\n\t#view them\n\tview_results(results)\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.moviedetail.movieparse.movie_user_parser import movie_user_parser", "from webscrape.searchresults.categorydetails.moviedetail.moviedisplay import movie_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "print_page_reviews", "data": "def print_page_reviews(results):\n\t#clears the command line output\n\tclear()\n\t#number of reviews on the page\n\tnumber = 1\n\t#loop through each review and print it\n\tfor result in results:\n\t\tprint(str(number) + \". \" + result.to_string())\n\t\tnumber = number + 1\n\t#if the number is still 1, then the for loop was never entered due to results being empty\n\tif (number == 1):\n\t\tprint(\"There are no user reviews on this page\")\n\t\t\n\t#print the current page\n\tprint(\"Page \" + str(parser.get_current_page()) + \" of \" + str(parser.get_last_page()))  \n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.moviedetail.movieparse.movie_user_parser import movie_user_parser", "from webscrape.searchresults.categorydetails.moviedetail.moviedisplay import movie_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "view_results", "data": "def view_results(results):\n\t\n\t#prints the reviews\n\tprint_page_reviews(results)\n\t\n\t#prompts the user to either enter a page number or view a review\n\tnumber = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n\t\n\t#if the number is invalid\n\tif (number.isdigit() == False):\n\t\tnumber = -1\n\t#if the number is zero then the user wants to navigate to a different page\n\tif (number == \"0\"):\n\t\t#prompt user for page number\n\t\tpage = input(\"Enter page number\\n\")\n\t\t#if the page is not valid\n\t\tif (page.isdigit() == False):\n\t\t\tpage = 0\n\t\t\n\t\tpage =  int(page)\n\t\t#go to the next page\t\n\t\t_next_page(page, results)\n\t\t\n\t\treturn\n\t#cast the user input into a number\n\tnumber = int(number)\n\t\n\t#while the number is a valid number\n\twhile (number > 0 and number < len(results) + 1):\n\t\t\n\t\t#get the chosen review\n\t\tchosenresult = results[number-1]\n\t\t\n\t\t#view the details of the review here\n\t\tmovie_view_review_detail.view_review_detail(chosenresult)\n\t\t\n\t\t\n\t\tprint_page_reviews(results)\n\t\t\n\t\t\n\t\t#prompts the user to either view a new page or to view a search result\n\t\tnumber = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n\t\t\t#if the number is invalid\n\t\tif (number.isdigit() == False):\n\t\t\tnumber = -1\n\t\t\n\t\t#if the number is zero then the user wants to navigate to a different page\n\t\tif (number == \"0\"):\n\t\t\t#prompt user for page number\n\t\t\tpage = input(\"Enter page number\\n\")\n\t\t\t#if the page is not valid\n\t\t\tif (page.isdigit() == False):\n\t\t\t\tpage = 0\n\t\t\t\n\t\t\tpage =  int(page)\n\t\t\t\t\n\t\t\t#go to the next page \n\t\t\t_next_page(page, results)\n\t\t\n\t\t\treturn\n\t\n\t\tnumber = int(number)\n\t\t\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.moviedetail.movieparse.movie_user_parser import movie_user_parser", "from webscrape.searchresults.categorydetails.moviedetail.moviedisplay import movie_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "view_movie_user_reviews", "data": "def view_movie_user_reviews(user_link):\n\tglobal parser\n\t\n\t#create the parser for the review\n\tparser = movie_user_parser(user_link)\n\t\n\t#get the reviews on the page\n\tresults = parser.get_results()\n\t\n\t#view the results\n\tview_results(results)\n\t\t\n\t\t\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.moviedetail.movieparse.movie_user_parser import movie_user_parser", "from webscrape.searchresults.categorydetails.moviedetail.moviedisplay import movie_view_review_detail", "from webscrape import clear"]}], [{"term": "def", "name": "_next_page", "data": "def _next_page(page, results):\n\t#check to see if the page exists\n\tif (page > parser.get_last_page()):\n\t\tprint(\"Page does not exist\")\n\t\t#view the results again\n\t\tview_results(results)\n\t\treturn\n\t\n\t#move the parser to the next page\n\tparser.move_to_page(page)\n\t\n\t#get the new review results from the parser\n\tresults = parser.get_results()\n\t#view them\n\tview_results(results)\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.albumdetail.albumparse.album_user_parser import album_user_parser", "from webscrape.searchresults.categorydetails.albumdetail.albumdisplay import album_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "print_page_reviews", "data": "def print_page_reviews(results):\n\t#clear command line output\n\tclear()\n\t#number of reviews on the page\n\tnumber = 1\n\t#loop through each review and print it\n\tfor result in results:\n\t\tprint(str(number) + \". \" + result.to_string())\n\t\tnumber = number + 1\n\t#if the number is still 1, then the for loop was never entered due to results being empty\n\tif (number == 1):\n\t\tprint(\"There are no user reviews on this page\")\n\t\t\n\t#print the current page\n\tprint(\"Page \" + str(parser.get_current_page()) + \" of \" + str(parser.get_last_page()))  \n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.albumdetail.albumparse.album_user_parser import album_user_parser", "from webscrape.searchresults.categorydetails.albumdetail.albumdisplay import album_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "view_results", "data": "def view_results(results):\n\t\n\t#prints the reviews\n\tprint_page_reviews(results)\n\t\n\t#prompts the user to either enter a page number or view a review\n\tnumber = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n\t\n\t#if the number is invalid\n\tif (number.isdigit() == False):\n\t\tnumber = -1\n\t#if the number is zero then the user wants to navigate to a different page\n\tif (number == \"0\"):\n\t\t#prompt user for page number\n\t\tpage = input(\"Enter page number\\n\")\n\t\t#if the page is not valid\n\t\tif (page.isdigit() == False):\n\t\t\tpage = 0\n\t\t\n\t\tpage =  int(page)\n\t\t#go to the next page\t\n\t\t_next_page(page, results)\n\t\t\n\t\treturn\n\t#cast the user input into a number\n\tnumber = int(number)\n\t\n\t#while the number is a valid number\n\twhile (number > 0 and number < len(results) + 1):\n\t\t\n\t\t#get the chosen review\n\t\tchosenresult = results[number-1]\n\t\t\n\t\t#view the details of the review here\n\t\talbum_view_review_detail.view_review_detail(chosenresult)\n\t\t\n\t\t\n\t\tprint_page_reviews(results)\n\t\t\n\t\t\n\t\t#prompts the user to either view a new page or to view a search result\n\t\tnumber = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n\t\t\t#if the number is invalid\n\t\tif (number.isdigit() == False):\n\t\t\tnumber = -1\n\t\t\n\t\t#if the number is zero then the user wants to navigate to a different page\n\t\tif (number == \"0\"):\n\t\t\t#prompt user for page number\n\t\t\tpage = input(\"Enter page number\\n\")\n\t\t\t#if the page is not valid\n\t\t\tif (page.isdigit() == False):\n\t\t\t\tpage = 0\n\t\t\t\n\t\t\tpage =  int(page)\n\t\t\t\t\n\t\t\t#go to the next page \n\t\t\t_next_page(page, results)\n\t\t\n\t\t\treturn\n\t\n\t\tnumber = int(number)\n\t\t\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.albumdetail.albumparse.album_user_parser import album_user_parser", "from webscrape.searchresults.categorydetails.albumdetail.albumdisplay import album_view_review_detail", "from webscrape import clear"]}, {"term": "def", "name": "view_album_user_reviews", "data": "def view_album_user_reviews(user_link):\n\tglobal parser\n\t\n\t#create the parser for the review\n\tparser = album_user_parser(user_link)\n\t\n\t#get the reviews on the page\n\tresults = parser.get_results()\n\t\n\t#view the results\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults.categorydetails.albumdetail.albumparse.album_user_parser import album_user_parser", "from webscrape.searchresults.categorydetails.albumdetail.albumdisplay import album_view_review_detail", "from webscrape import clear"]}], [], [{"term": "def", "name": "func_faixae", "data": "def func_faixae():\n\tfaixa_etaria = Faixa('obitos','fx_etaria','total')\n\tfaixa_etaria.get().columns = ['faixa etaria', 'obitos']\n\tfaixa_etaria.agrupar('faixa etaria')\n\treturn faixa_etaria\n", "description": null, "category": "webscraping", "imports": ["from classes import *"]}, {"term": "def", "name": "c_diarios", "data": "def c_diarios():\n\t#fazendo dataframe apenas com data e casos confirmados\n\tcasos_diarios = Faixa('municipios','data','confirmados')\n\tcasos_diarios.get().columns = ['data','casos confirmados']\n\treturn casos_diarios\n", "description": null, "category": "webscraping", "imports": ["from classes import *"]}, {"term": "def", "name": "o_diarios", "data": "def o_diarios():\n\t#fazendo dataframe apenas com data e obitos\n\tobitos_diarios = Faixa('municipios','data','obitos')\n\treturn obitos_diarios\n\n", "description": null, "category": "webscraping", "imports": ["from classes import *"]}, {"term": "def", "name": "funcao_webscrape", "data": "def funcao_webscrape():\n\tpopulacao = WebScraping()\n\n\t#adicionando nomes das colunas, porque o pandas n\u00e3o os via como string, ainda aproveitando pra remover o que n preciso\n\tpopulacao.get().columns = ['Posicao', 'Municipio', 'Populacao', 'lixo']\n\n\t#removendo todas as linhas que cont\u00e9m \"habitantes\"\n\tpopulacao.dropColumnValue('Posicao','habitantes')\n\n\t#removendo as colunas que n\u00e3o preciso\n\tpopulacao.dropColumn('Posicao','lixo')\n\n\t#Colocando valores em ordem alfabetica\n\tpopulacao.sort('Municipio')\n\n\t#tirando espa\u00e7o em branco dos numeros\n\tpopulacao.rmSpaceInt('Populacao')\n\n\t#renomeando algumas coisas escritas erradas.\n\tpopulacao.replace(9,'Municipio', 'Ar\u00eas')\n\tpopulacao.replace(10,'Municipio','A\u00e7u')\n\tpopulacao.replace(18,'Municipio', 'Brejinho')\n\tpopulacao.replace(96,'Municipio', 'Passa e Fica')\n\tpopulacao.replace(133,'Municipio', 'S\u00e3o Bento do Trair\u00ed')\n\treturn populacao\n\n", "description": null, "category": "webscraping", "imports": ["from classes import *"]}, {"term": "def", "name": "func_mediaCem", "data": "def func_mediaCem():\n\t#fazendo dataframe apenas com municipio e casos confirmados\n\tcemMil = Media100mil('municipios','mun_residencia','obitos')\n\tcemMil.get().columns = ['Municipio','obitos']\n\n\n\t#agrupar todos os valores, sort para deixar em ordem alfabetica baseada nos munic\u00edpios e resetar o \u00edndice\n\tcemMil.agruparSort('Municipio')\n\n\tcemMil.replace(10,'Municipio','Campo Grande')\n\tcemMil.replace(55,'Municipio','Boa Sa\u00fade')\n\n\tcemMil.sort('Municipio')\n\n\tpopulacao = funcao_webscrape()\n\n\t#concatenar a coluna do dataframe obtido pelo webscraping\n\tcemMil.createColumn('Populacao',populacao.get(),'Populacao')\n\tcemMil.createColumnMedia('Mortes por 100 mil','obitos',populacao.get(),'Populacao')\n\t#cemMil.get().head()\n", "description": null, "category": "webscraping", "imports": ["from classes import *"]}], [{"term": "def", "name": "OCR", "data": "def OCR(yest):\r\n\tcustom_config = r'-l eng --oem 3 --psm 6'\r\n\tpytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\r\n\ttext = pytesseract.image_to_string(yest, config=custom_config)\r\n\treturn text\r\n", "description": null, "category": "webscraping", "imports": ["import cv2\r", "import numpy as np\r", "from PIL import ImageGrab\r", "import pytesseract\r", "from webscrape import findWord\r", "import pydirectinput\r", "import time\r"]}, {"term": "def", "name": "typeword", "data": "def typeword(word):\r\n\tprint(word)\r\n\ttry:\r\n\t\tfor letter in word:\r\n\t\t\tpydirectinput.write(letter)\r\n\texcept TypeError:\r\n\t\treturn(\"rbeoiwbghbewioh\")\r\n", "description": null, "category": "webscraping", "imports": ["import cv2\r", "import numpy as np\r", "from PIL import ImageGrab\r", "import pytesseract\r", "from webscrape import findWord\r", "import pydirectinput\r", "import time\r"]}], [{"term": "def", "name": "webscrape_DRG", "data": "def webscrape_DRG(*code):\n\t\n\t#Load required libraries\n\timport pandas as pd\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\timport re\n\n\t#Set max table column width and allow text wrapping. \n\tpd.set_option('display.max_colwidth', 0)\n\n\t#Define the DRG codes that will be webscraped from the website\n\tDRG_codes = [*code]\n\n\t#Base url that will be used to generate the final url in the build_url function. \n\tbaseurl = \"https://www.findacode.com/tools/code-print.php?set=DRG&c=\"\n\n\t#Define the function to create the url that will be scraped.\n\tdef build_url(DRGcode):\n\t\treturn baseurl + DRGcode\n\turls = []\n\tfor DRGcode in DRG_codes:\n\t\turls.append(build_url(DRGcode))\n\n\t#Table to display the results\t\n\tdata = {\"URLS\": urls[:]}\n\n\tdf = pd.DataFrame(data)\n\n\t#Print the table in the output\n\tdf\n\t\n\t#Create empty lists that will hold data from webscraped DRG codes\n\tcode = []\n\tFCP = []\n\tFOP = []\n\n\t#Web scraping using a for loop that passes through all URLs\n\tfor url in urls[:]:\n\t\tpage = requests.get(url).text\n\t\tsoup = BeautifulSoup(page, 'html.parser')\n\t\tname = soup.find('blockquote')\n\t\tname.br.extract()\n\t\tcode.append(name.text[1:-213])\n\t\tFOP.append(soup.find(id=\"drg_t_op_pmt\").getText()) #id in the DRG website for the Federal Operating Payment\n\t\tFCP.append(soup.find(id=\"drg_t_cp_pmt\").getText()) #id in the DRG website for the Federal Capital Payment\n\n\tFCP = [item.strip() for item in FCP]\n\tFOP = [item.strip() for item in FOP]\n\n\t#Column names for the df table \n\tdf[\"DRG Codes\"] = code\n\tdf[\"Federal Capital Payment\"] = FCP\n\tdf[\"Federal Operating Payment\"] = FOP\n\n\tintFCP = []\n\n\t#Extracting only the numbers from the extracted data. \n\tfor element in FCP:\n\t\tx= re.findall(r\"\\$[^\\]]+\", element)\n\t\tx = ''.join(x)\n\t\tx = x.replace(\"$\",\"\").replace(\",\",\"\")\n\t\tintFCP.append(float(x))\n\n\tintFOP = []\n\n\tfor element in FOP:\n\t\tx= re.findall(r\"\\$[^\\]]+\", element)\n\t\tx = ''.join(x)\n\t\tx= x.replace(\"$\",\"\").replace(\",\",\"\")\n\t\tintFOP.append(float(x))\n\t\t\n\t#Sum of the two numbers FOP + FCP\n\tsum_list = [a + b for a, b in zip(intFCP, intFOP)]\n\n\t#Empty list for the Total Federal Payment\n\tTFP = []\n\n\t#Apply the $ formatting to the number\n\tfor element in sum_list:\n\t\tTFP.append(\"${:,.2f}\".format(element))\n\t\n\t#Column name\t \n\tdf[\"Total Federal Payment\"] = TFP\n\n\t#Return the data table\n\treturn(df)\n", "description": null, "category": "webscraping", "imports": ["\timport pandas as pd", "\timport requests", "\tfrom bs4 import BeautifulSoup", "\timport re"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape():\n\n\tdata = []\n\t# Accessing table\n\ttable = soup.find(\"table\", attrs={'class', 'wikitable'})\n\ttable_body = table.find('tbody')\n\t# Table Row Tags\n\ttr_tags = table_body.find_all('tr')\n\tfor i in tr_tags:\n\t\ttd_tags = i.find_all('td')\n\t\ttemp = []\n\t\t# Looping through row\n\t\tfor index, j in enumerate(td_tags):\n\t\t\t# For  tag value\n\t\t\tif(index == 0):\n\t\t\t\ta_tag = j.find_all('a')\n\t\t\t\t# Some of them are plain text, not \n\t\t\t\tif(a_tag):\n\t\t\t\t\ttemp.append(a_tag[0].contents[0])\n\t\t\t\telse:\n\t\t\t\t\ttemp.append(j.text)\n\t\t\t#Distance, Mass, Radius\n\t\t\telif(index == 5 or index == 7 or index == 8):\n\t\t\t\ttemp.append(j.text.strip(\"0\").strip('\\n'))\n\n\t\tdata.append(temp)\n\t\tprint(data)\n\t\twith open('data.csv', 'w') as f:\n\t\t\tfs = csv.writer(f)\n\t\t\tfs.writerow(headers)\n\t\t\tfs.writerows(data)\n\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import csv"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(numeros):\r\n\tglobal num_gravados\r\n\turl = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n\tsource = requests.get(url).text\r\n\tsoup = BeautifulSoup(source, 'lxml')\r\n\tlidos = 0\r\n   \r\n\tfor texto in soup.find_all('table', class_='tocArticle'):\r\n\t\tlink = texto.find('a', attrs={'href': re.compile(\"http://\")})\r\n\t\tlink2 = str(link.get('href')[65:70])\r\n\t\ttitulo = texto.find('div', class_='tocTitle').text.strip()\r\n\t\tautor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n\t\tprint(f'T\u00edtulo do trabalho: {titulo}')\r\n\t\tprint(f'Autores: {autor}')\r\n\t\tprint(f'Link: {link2}')\r\n\t\tprint()\r\n\t\tcsv_writer.writerow([titulo, autor])\r\n\t\tlidos += 1\r\n\t\tnum_gravados += 1\r\n\treturn lidos > 0\r\n\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import csv\r", "import re\r"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('webscrape', '0003_auto_20190323_1501'),\n\t\t('dataprocess', '0001_initial'),\n\t]\n\n\toperations = [\n\t\tmigrations.CreateModel(\n\t\t\tname='TempProcLandData',\n\t\t\tfields=[\n\t\t\t\t('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n\t\t\t\t('price', models.IntegerField(default=0, null=True)),\n\t\t\t\t('latitude', models.FloatField(default=0)),\n\t\t\t\t('longitude', models.FloatField(default=0)),\n\t\t\t\t('land_type', models.CharField(max_length=20, null=True)),\n\t\t\t\t('land_size', models.FloatField(default=0, null=True)),\n\t\t\t\t('land_availability', models.BooleanField(default=0)),\n\t\t\t\t('date_collected', models.DateTimeField(default=django.utils.timezone.now, null=True)),\n\t\t\t\t('raw', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, to='webscrape.RawLandData')),\n\t\t\t],\n\t\t),\n\t]\n", "description": null, "category": "webscraping", "imports": ["from django.db import migrations, models", "import django.db.models.deletion", "import django.utils.timezone"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(numeros):\r\n\tglobal num_gravados\r\n\turl = f'http://www.uel.br/revistas/uel/index.php/informacao/article/view/{numeros}'\r\n\tsource = requests.get(url).text\r\n\tsoup = BeautifulSoup(source, 'lxml')\r\n\tlidos = 0\r\n   \r\n\tfor texto in soup.find_all('div', id='content'):\r\n\t\ttitulo = texto.find('div', id='articleTitle').text.strip()\r\n\t\tautor = texto.find('div', id='authorString').text.strip()\r\n\t\tresumo = texto.find('div', id='articleAbstract').text.strip().replace('Resumo', '').replace('\\n', '')\r\n\t\tkeywords = texto.find('div', id='articleSubject').text.strip().replace('Palavras-chave', '').replace('\\n', '')\r\n\t\tprint(f'T\u00edtulo do trabalho: {titulo}')\r\n\t\tprint(f'Autores: {autor}')\r\n\t\tprint(f'Resumo: {resumo}')\r\n\t\tprint(f'Palavras-Chave: {keywords}')\r\n\t\tprint()\r\n\t\t#csv_writer.writerow([titulo, autor])\r\n\t\tlidos += 1\r\n\t\tnum_gravados += 1\r\n\treturn lidos > 0\r\n\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import csv\r"]}], [{"term": "class", "name": "Movie", "data": "class Movie():\n\tdef __init__(self, url: str, csv: HandleCSV):\n\t\t# add check to verify the domain is \"imdb.com\"\n\t\tself.url = url\n\t\tprint(f\"Processing: {self.url}\")\n\t\tself.repeat = False\n\t\tif csv.isRepeat(self.url):\n\t\t\tself.repeat = True\n\t\t\treturn\n\n\t\tself.csv = csv\n\t\tself.attributes = {}\n\n\t\tself.page = requests.get(self.url) # get page HTML through request\n\t\tself.soup = BeautifulSoup(self.page.content, \"html.parser\") # parse content\n\n\t\tself.repeat = False # have webscraped this movie already\n\n\t\tself.webScrape()\n\n\tdef setMovie(self, url: str):\n\t\t# add check to verify the domain is \"imdb.com\"\n\t\tself.url = url\n\t\tself.webScrape()\n\n\tdef webScrape(self):\n\t\t# may need to add try-except block\n\t\tself.attributes[\"url\"] = self.url # identifier\n\t\tself.attributes[\"title\"] = self.soup.select(\".TitleHeader__TitleText-sc-1wu6n3d-0\")\n\t\tself.attributes[\"rating\"] = self.soup.select(\".AggregateRatingButton__RatingScore-sc-1ll29m0-1\")\n\t\tself.attributes[\"numOfRatings\"] = self.soup.select(\".AggregateRatingButton__TotalRatingAmount-sc-1ll29m0-3\")\n\t\tself.attributes[\"popularity\"] = self.soup.select(\".TrendingButton__TrendingScore-bb3vt8-1\")\n\t\tself.attributes[\"numOfAwards\"] = self.soup.select(\"li[data-testid=\\\"award_information\\\"] div ul li span\")\n\t\tself.attributes[\"parentalRating\"] = self.soup.select(\".TitleBlock__TitleMetaDataContainer-sc-1nlhx7j-2 ul li a\")\n\t\tself.attributes[\"yearMade\"] = self.soup.select(\".TitleBlock__TitleMetaDataContainer-sc-1nlhx7j-2 ul li a\")\n\t\tself.attributes[\"genre\"] = self.soup.select(\"div[data-testid=\\\"genres\\\"] a span\")\n\t\tself.attributes[\"language\"] = self.soup.select(\"li[data-testid=\\\"title-details-languages\\\"] div ul li a\")\n\t\tself.attributes[\"length\"] = self.soup.select(\"li[data-testid=\\\"title-techspec_runtime\\\"] div ul li span\")\n\t\tself.attributes[\"gross\"] = self.soup.select(\"li[data-testid=\\\"title-boxoffice-cumulativeworldwidegross\\\"] div ul li span\")\n\t\t# ...\n\n\t\tself.parse()\n\n\t\tself.addRowToPandas()\n\n\t\"\"\" format attributes \"\"\"\n\tdef parse(self):\n\t\t# title\n\t\tif len(self.attributes[\"title\"]) == 0:\n\t\t\tself.attributes[\"title\"] = None\n\t\telse:\n\t\t\tself.attributes[\"title\"] = str(self.attributes[\"title\"][0].text)\n\n\t\t# rating\n\t\tif len(self.attributes[\"rating\"]) == 0:\n\t\t\tself.attributes[\"rating\"] = None\n\t\telse:\n\t\t\tself.attributes[\"rating\"] = float(self.attributes[\"rating\"][0].text)\n\n\t\t# numOfRatings\n\t\tif len(self.attributes[\"numOfRatings\"]) == 0:\n\t\t\tself.attributes[\"numOfRatings\"] = None\n\t\telse:\n\t\t\tself.attributes[\"numOfRatings\"] = str(self.attributes[\"numOfRatings\"][0].text).lower()\n\t\t\tif 'k' in self.attributes[\"numOfRatings\"]:\n\t\t\t\tself.attributes[\"numOfRatings\"] = float(self.attributes[\"numOfRatings\"][:-1]) * 1000\n\t\t\telif 'm' in self.attributes[\"numOfRatings\"]:\n\t\t\t\tself.attributes[\"numOfRatings\"] = float(self.attributes[\"numOfRatings\"][:-1]) * 1_000_000\n\t\t\telse:\n\t\t\t\tself.attributes[\"numOfRatings\"] = float(self.attributes[\"numOfRatings\"])\n\n\t\t# popularity\n\t\tif len(self.attributes[\"popularity\"]) == 0:\n\t\t\tself.attributes[\"popularity\"] = None\n\t\telse:\n\t\t\tself.attributes[\"popularity\"] = self.attributes[\"popularity\"][0].text\n\t\t\tself.attributes[\"popularity\"] = int(self.attributes[\"popularity\"].replace(',', \"\"))\n\n\t\t# numOfAwards\n\t\tif len(self.attributes[\"numOfAwards\"]) == 0:\n\t\t\tself.attributes[\"numOfAwards\"] = None\n\t\telse:\n\t\t\tself.attributes[\"numOfAwards\"] = str(self.attributes[\"numOfAwards\"][0].text)\n\n\t\t\twins = self.attributes[\"numOfAwards\"].find(\"wins\")\n\t\t\tif wins == -1:\n\t\t\t\twins = 0\n\t\t\telse:\n\t\t\t\twins = int(self.attributes[\"numOfAwards\"][:wins - 1])\n\n\t\t\tnominations = self.attributes[\"numOfAwards\"].find(\"nominations\")\n\t\t\tif nominations == -1:\n\t\t\t\tnominations = 0\n\t\t\telse:\n\t\t\t\tif wins == 0: # \"x wins\" does not exist in string\n\t\t\t\t\tnominations = self.attributes[\"numOfAwards\"][:nominations - 1]\n\t\t\t\telse:\n\t\t\t\t\tstart = self.attributes[\"numOfAwards\"].find(\"&\") + 2\n\t\t\t\t\tnominations = int(self.attributes[\"numOfAwards\"][start:nominations - 1])\n\t\t\t\n\t\t\tself.attributes[\"numOfAwards\"] = [wins, nominations]\n\n\t\t# parentalRating\n\t\tif len(self.attributes[\"parentalRating\"]) == 0:\n\t\t\tself.attributes[\"parentalRating\"] = None\n\t\telse:\n\t\t\tself.attributes[\"parentalRating\"] = str(self.attributes[\"parentalRating\"][1].text)\n\n\t\t# yearMade\n\t\tif len(self.attributes[\"yearMade\"]) == 0:\n\t\t\tself.attributes[\"yearMade\"] = None\n\t\telse:\n\t\t\tself.attributes[\"yearMade\"] = int(self.attributes[\"yearMade\"][0].text)\n\n\t\t# genre\n\t\tif len(self.attributes[\"genre\"]) == 0:\n\t\t\tself.attributes[\"genre\"] = None\n\t\telse:\n\t\t\t# there might be multiple genres!\n\t\t\ttemp = []\n\t\t\tfor element in self.attributes[\"genre\"]:\n\t\t\t\ttemp.append(str(element.text))\n\t\t\tself.attributes[\"genre\"] = temp\n\n\t\t# language\n\t\tif len(self.attributes[\"language\"]) == 0:\n\t\t\tself.attributes[\"language\"] = None\n\t\telse:\n\t\t\t# there might be multiple languages!\n\t\t\ttemp = []\n\t\t\tfor element in self.attributes[\"language\"]:\n\t\t\t\ttemp.append(str(element.text))\n\t\t\tself.attributes[\"language\"] = temp\n\n\t\t# length\n\t\tif len(self.attributes[\"length\"]) == 0:\n\t\t\tself.attributes[\"length\"] = None\n\t\telse:\n\t\t\tself.attributes[\"length\"] = str(self.attributes[\"length\"][0].text)\n\t\t\t\n\t\t\thours = self.attributes[\"length\"].find('h')\n\t\t\tif hours == -1:\n\t\t\t\thours = 0\n\t\t\telse:\n\t\t\t\thours = int(self.attributes[\"length\"][:hours])\n\t\t\t\n\t\t\tminutes = self.attributes[\"length\"].find(\"min\")\n\t\t\tif minutes == -1:\n\t\t\t\tminutes = 0\n\t\t\telse:\n\t\t\t\tif hours == 0: # \"h\" doesn't exist\n\t\t\t\t\tminutes = int(self.attributes[\"length\"][:minutes])\n\t\t\t\telse:\n\t\t\t\t\tstart = self.attributes[\"length\"].find('h') + 2\n\t\t\t\t\tminutes = int(self.attributes[\"length\"][start:minutes])\n\n\t\t\tself.attributes[\"length\"] = (hours * 60) + minutes # total number of minutes\n\n\t\t# gross\n\t\tif len(self.attributes[\"gross\"]) == 0:\n\t\t\tself.attributes[\"gross\"] = None\n\t\telse:\n\t\t\tself.attributes[\"gross\"] = str(self.attributes[\"gross\"][0].text)\n\n\t\t\tself.attributes[\"gross\"] = self.attributes[\"gross\"].replace(',', \"\")\n\t\t\tself.attributes[\"gross\"] = int(self.attributes[\"gross\"].replace('$', \"\"))\n\n\tdef setAttribute(self, attribute: str, value):\n\t\t# may need to include try-except block\n\t\tself.attributes[attribute] = value\n\n\tdef getAttribute(self, attribute: str):\n\t\t# may need to include try-except block\n\t\treturn self.attributes[attribute]\n\n\tdef prettyPrint(self):\n\t\t# header\n\t\ttext = \"Here's what I found about \\\"{}\\\" on IMDb:\".format(self.attributes[\"title\"])\n\t\tlength = len(text)\n\t\tprint(text)\n\t\tprint(('_' * length) + '\\n')\n\n\t\tprint(\"URL: {}\".format(self.attributes[\"url\"]))\n\t\tprint(\"Title: {}\".format(self.attributes[\"title\"]))\n\t\tprint(\"Rating (_/10): {}\".format(self.attributes[\"rating\"]))\n\t\tprint(\"Number of Ratings: {}\".format(self.attributes[\"numOfRatings\"]))\n\t\tprint(\"Popularity (see IMDb): {}\".format(self.attributes[\"popularity\"]))\n\t\tprint(\"Number of Awards [wins, nominations]: {}\".format(self.attributes[\"numOfAwards\"]))\n\t\tprint(\"Parental Rating: {}\".format(self.attributes[\"parentalRating\"]))\n\t\tprint(\"Made in: {}\".format(self.attributes[\"yearMade\"]))\n\t\tprint(\"Genre(s): {}\".format(self.attributes[\"genre\"]))\n\t\tprint(\"Language(s): {}\".format(self.attributes[\"language\"]))\n\t\tprint(\"Length: {} minutes\".format(self.attributes[\"length\"]))\n\t\tprint(\"Gross Worldwide: ${}\".format(self.attributes[\"gross\"]))\n\n\tdef addRowToPandas(self):\n\t\ttry:\n\t\t\tself.csv.addRow(self.attributes)\n\t\texcept:\n\t\t\tpass\n\n\tdef findSimilar(self) -> list:\n\t\tqueue = []\n\t\ttemp = self.soup.select(\"section[data-testid=\\\"MoreLikeThis\\\"] div div div div a\")\n\t\tfor item in temp:\n\t\t\titem = str(item[\"href\"])\n\n\t\t\tend = item.find(\"/?\") # clean url\n\t\t\tif end == -1:\n\t\t\t\tend = len(item)\n\n\t\t\titem = item[:end]\n\t\t\titem = \"https://imdb.com\" + item\n\t\t\tqueue.append(item)\n\n", "description": " format attributes ", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "from handleCSV import HandleCSV"]}], [{"term": "def", "name": "getInput", "data": "def getInput():\n\tcollege_name = u_input_box.get()\n\n\tglobal global_Label\n\n\t# check for empty input\n\tif (len(college_name) != 0):\n\t\tu_input_box.delete(0, END)\n\n\t\tlabel2.pack_forget()\n\t\tgetData(college_name)\n\telse:\n\t\tlabel2.config(text=\"Input cannot be empty\", font=(\"Montserrat\", 12, font.BOLD), foreground=\"red\")\n\t\tlabel2.pack()\n\n", "description": null, "category": "webscraping", "imports": ["import tkinter", "from tkinter import *", "import tkinter.font as font", "from webscrape import *"]}, {"term": "def", "name": "getData", "data": "def getData(college_name):\n\tret_val = webscrape((college_name))\n\n\tif (ret_val == -1):\n\t\tlabel2.config(text=\"COLLEGE NOT FOUND!\", font=(\"Montserrat\", 12, font.BOLD), foreground=\"red\")\n\t\tlabel2.pack()\n\telse:\n\t\tblank = \"\\n\"\n\t\tarray_string = blank.join(ret_val)\n\t\tnew_L.configure(text=array_string)\n\n", "description": null, "category": "webscraping", "imports": ["import tkinter", "from tkinter import *", "import tkinter.font as font", "from webscrape import *"]}], [{"term": "class", "name": "TechSpider", "data": "class TechSpider(scrapy.Spider):\n\tname = 'tech'\n\tallowed_domains = ['techcrunch.com']\n\tstart_urls = ['https://techcrunch.com/2022/04/01/staten-island-amazon-workers-vote-to-unionize/']\n\t\n\tdef parse(self, response):\n\t\t#c=response.css('.content a::attr(href)').getall()\n\t\t#print(type(c))\n\t\t#for i in c:\n\t\t#i=response.urljoin(i)\n\t\tparse_content(response)   \n\t\t#yield response.follow(i,self.parse_content)\n\t\t   \n\t\t \n\t\n\n\n\tdef parse_content(self,response):\n\t\tq={}\n\t\tittem = WebscrapeItem()\n\t\titem = {}\n\t\tTitle=response.css('h1.article__title::text').extract()\n\t\tAuthor=response.css('div.article__byline a::text').extract()\n\t\tpost=response.css('div.article-content ::text').extract()\n\t\t\t\n\t\titem['title']= Title\n\t\titem['author']=Author\n\t\titem['post']=post\n\n\t\t\n\t\t  \n\t\ta = item['title']\n\t\t#print(type(a))\n\n\t\tb = item['author']\n\t\t\n\n\t\tlistt = list(item['post'])\n\t\tresult= ''\n\t\tfor element in listt:\n\t\t\tresult += str(element)\n\t\tc=[]\n\t\tc.append(result)\n\t\tif len(a)==1 :\n\t\t\tittem['title']= a[0]\n\t\t\tittem['author']=b[0]\n\t\t\tittem['post']=c[0]\n\t\t\n\t\t\n\t\t\t\t\t \n\t\tyield ittem\n\t\t\n\n", "description": null, "category": "webscraping", "imports": ["from gettext import install", "from twisted.internet import reactor", "from scrapy.crawler import CrawlerRunner", "from scrapy.utils.log import configure_logging", "import time", "import scrapy", "from scrapy.crawler import CrawlerProcess", "from scrapy.utils.project import get_project_settings", "import os", "from crochet import setup", "from items import WebscrapeItem", "\t#scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   "]}, {"term": "def", "name": "fs", "data": "#def s():\n\n\t#process = CrawlerProcess(get_project_settings())\n\t# 'followall' is the name of one of the spiders of the project.\n\t#process.crawl(TechSpider)\n\t#process.start(stop_after_crawl=True) # the script will block here until the crawling is finished\n   \n\t#configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n\t#runner = CrawlerRunner(get_project_settings())\n\t#runner.crawl(TechSpider)\n\t#d.addBoth(lambda _: reactor.stop())\n", "description": null, "category": "webscraping", "imports": ["from gettext import install", "from twisted.internet import reactor", "from scrapy.crawler import CrawlerRunner", "from scrapy.utils.log import configure_logging", "import time", "import scrapy", "from scrapy.crawler import CrawlerProcess", "from scrapy.utils.project import get_project_settings", "import os", "from crochet import setup", "from items import WebscrapeItem", "\t#scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   "]}, {"term": "def", "name": "frun_spider", "data": "#def run_spider(spiderName):\n\t#module_name=\"first_scrapy.spiders.{}\".format(spiderName)\n\t#scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   \n\t#spiderObj=scrapy_var.mySpider()\t\t   #get mySpider-object from spider module\n\t#crawler = CrawlerRunner(get_project_settings())   #from Scrapy docs\n\t#crawler.crawl(spiderObj)  \n", "description": null, "category": "webscraping", "imports": ["from gettext import install", "from twisted.internet import reactor", "from scrapy.crawler import CrawlerRunner", "from scrapy.utils.log import configure_logging", "import time", "import scrapy", "from scrapy.crawler import CrawlerProcess", "from scrapy.utils.project import get_project_settings", "import os", "from crochet import setup", "from items import WebscrapeItem", "\t#scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   "]}, {"term": "def", "name": "crawl", "data": "def crawl(runner):\n\td = runner.crawl(TechSpider)\n\td.addBoth(lambda _: reactor.stop())\n\treturn d\n\n", "description": null, "category": "webscraping", "imports": ["from gettext import install", "from twisted.internet import reactor", "from scrapy.crawler import CrawlerRunner", "from scrapy.utils.log import configure_logging", "import time", "import scrapy", "from scrapy.crawler import CrawlerProcess", "from scrapy.utils.project import get_project_settings", "import os", "from crochet import setup", "from items import WebscrapeItem", "\t#scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   "]}, {"term": "def", "name": "loop_crawl", "data": "def loop_crawl():\n\trunner = CrawlerRunner(get_project_settings())\n\tcrawl(runner)\n\treactor.run()\n", "description": null, "category": "webscraping", "imports": ["from gettext import install", "from twisted.internet import reactor", "from scrapy.crawler import CrawlerRunner", "from scrapy.utils.log import configure_logging", "import time", "import scrapy", "from scrapy.crawler import CrawlerProcess", "from scrapy.utils.project import get_project_settings", "import os", "from crochet import setup", "from items import WebscrapeItem", "\t#scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   "]}, {"term": "def", "name": "startspider", "data": "def startspider():\n\tloop_crawl()\n", "description": null, "category": "webscraping", "imports": ["from gettext import install", "from twisted.internet import reactor", "from scrapy.crawler import CrawlerRunner", "from scrapy.utils.log import configure_logging", "import time", "import scrapy", "from scrapy.crawler import CrawlerProcess", "from scrapy.utils.project import get_project_settings", "import os", "from crochet import setup", "from items import WebscrapeItem", "\t#scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   "]}], [{"term": "class", "name": "Search", "data": "class Search(forms.Form):\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from .models import PatternTable, Category", "from django.http import HttpResponseRedirect, HttpResponse", "from django.urls import reverse", "from django import forms", "\timport bs4", "\tfrom urllib.request import urlopen as uReq", "\tfrom bs4 import BeautifulSoup as soup", "\timport sqlite3"]}, {"term": "def", "name": "webScrape", "data": "def webScrape():\n\timport bs4\n\tfrom urllib.request import urlopen as uReq\n\tfrom bs4 import BeautifulSoup as soup\n\timport sqlite3\n\tmy_url = 'https://www.amigurumi.com/search/free/'\n  \n\tfor page in range (1,11): #parse 10 pages\n\t\tnew_url = my_url + str(page) + '/'\n\t\tuClient = uReq(new_url)\n\t\tpage_html = uClient.read()\n\t\tuClient.close()\n\t\tpage_soup=soup(page_html, \"html.parser\")\n\t\tcontainers = page_soup.findAll(\"div\", {\"class\":\"item\"})\n\t\t\n\t\tfor i in range(len(containers)): #Go through each item\n\t\t\tcontainer = containers[i]\n\t\t\t# Name of pattern\n\t\t\tname = container.img.get('title')\n\t\t\t# Link to pattern\n\t\t\tlink = container.a.get('href')\n\n\t\t\t#Parse each link to pattern\n\t\t\tnewClient = uReq(link)\n\t\t\tnew_html = newClient.read()\n\t\t\tnewClient.close()\n\t\t\tpattern_soup = soup(new_html, \"html.parser\")\n\n\t\t\t# Description of pattern\n\t\t\tdescription = pattern_soup.findAll(\"div\", {\"id\": \"patterndescription\"})\n\t\t\t\n\t\t\tif len(description)!=0:\n\t\t\t\tdes = description[0]\n\t\t\t\tdes_text = des.find('p').text.strip()\n\t\t\t\tnew_des_text=\"\".join(des_text.splitlines())\n\t\t\telse: \n\t\t\t\tnew_des_text = \" \"\n\t\t\t# What category pattern belongs in\n\t\t\tcategory = pattern_soup.findAll(\"span\", {\"itemprop\": \"title\"})\n\t\t\t\n\t\t\tgroup = category[1].text\n\t\t\tobj, created = Category.objects.get_or_create(cate=group)\n\t\t\tp = PatternTable(name = name, link = link, description = new_des_text, category = obj)\n\t\t\tp.save()\n\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from .models import PatternTable, Category", "from django.http import HttpResponseRedirect, HttpResponse", "from django.urls import reverse", "from django import forms", "\timport bs4", "\tfrom urllib.request import urlopen as uReq", "\tfrom bs4 import BeautifulSoup as soup", "\timport sqlite3"]}, {"term": "def", "name": "index", "data": "def index(request):\n\t#webScrape()\n\t#return(HttpResponse(\"hello\"))\n\tif request.GET.get('q'):\n\t\tquery = request.GET.get(\"q\", \"\")\n\t\treturn(search(request, query))\n\telse:\n\t\treturn render(request, \"patterns/index.html\", {\"patterns\": PatternTable.objects.all()})\n\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from .models import PatternTable, Category", "from django.http import HttpResponseRedirect, HttpResponse", "from django.urls import reverse", "from django import forms", "\timport bs4", "\tfrom urllib.request import urlopen as uReq", "\tfrom bs4 import BeautifulSoup as soup", "\timport sqlite3"]}, {"term": "def", "name": "pattern", "data": "def pattern(request, pattern_id):\n\tpattern = PatternTable.objects.get(id=pattern_id)\n\treturn render(request, \"patterns/pattern.html\", {\"pattern\":pattern})\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from .models import PatternTable, Category", "from django.http import HttpResponseRedirect, HttpResponse", "from django.urls import reverse", "from django import forms", "\timport bs4", "\tfrom urllib.request import urlopen as uReq", "\tfrom bs4 import BeautifulSoup as soup", "\timport sqlite3"]}, {"term": "def", "name": "search", "data": "def search(request, query):\n\tpatterns = PatternTable.objects.all()\n\tresults=[]\n\tfor pattern in patterns:\n\t\tif query.lower() == pattern.name.lower():\n\t\t\treturn render(request, \"patterns/pattern.html\",{\"pattern\": pattern})\n\t\tif query.lower() in pattern.name.lower() or query.lower() in pattern.description.lower():\n\t\t\tresults.append(pattern)\n\n\treturn render(request,\"patterns/search.html\", {\"results\":results})   \n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from .models import PatternTable, Category", "from django.http import HttpResponseRedirect, HttpResponse", "from django.urls import reverse", "from django import forms", "\timport bs4", "\tfrom urllib.request import urlopen as uReq", "\tfrom bs4 import BeautifulSoup as soup", "\timport sqlite3"]}, {"term": "def", "name": "categories", "data": "def categories(request):\n\tresults = Category.objects.all()\n\treturn render(request, \"patterns/categories.html\",{\"results\": results})\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from .models import PatternTable, Category", "from django.http import HttpResponseRedirect, HttpResponse", "from django.urls import reverse", "from django import forms", "\timport bs4", "\tfrom urllib.request import urlopen as uReq", "\tfrom bs4 import BeautifulSoup as soup", "\timport sqlite3"]}, {"term": "def", "name": "category", "data": "def category(request,cate_id):\n\t#patterns_match = PatternTable.objects.filter(category=cate)\n\tc = Category.objects.get(id = cate_id)\n\tall_patterns = c.patterns_incategory.all()\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from .models import PatternTable, Category", "from django.http import HttpResponseRedirect, HttpResponse", "from django.urls import reverse", "from django import forms", "\timport bs4", "\tfrom urllib.request import urlopen as uReq", "\tfrom bs4 import BeautifulSoup as soup", "\timport sqlite3"]}], [{"term": "class", "name": "RegistryServer", "data": "class RegistryServer(cli.Application):\n\tmode = cli.SwitchAttr([\"-m\", \"--mode\"], cli.Set(\"UDP\", \"TCP\"), default = \"UDP\",\n\t\thelp = \"Serving mode\")\n\t\n\tipv6 = cli.Flag([\"-6\", \"--ipv6\"], help=\"use ipv6 instead of ipv4\")\n\n\tport = cli.SwitchAttr([\"-p\", \"--port\"], cli.Range(0, 65535), default = REGISTRY_PORT, \n\t\thelp = \"The UDP/TCP listener port\")\n\t\n\tlogfile = cli.SwitchAttr([\"--logfile\"], str, default = None, \n\t\thelp = \"The log file to use; the default is stderr\")\n\t\n\tquiet = cli.SwitchAttr([\"-q\", \"--quiet\"], help = \"Quiet mode (only errors are logged)\")\n\t\n\tpruning_timeout = cli.SwitchAttr([\"-t\", \"--timeout\"], int, \n\t\tdefault = DEFAULT_PRUNING_TIMEOUT, help = \"Set a custom pruning timeout (in seconds)\")\n\n\tdef main(self):\n\t\tif self.mode == \"UDP\":\n\t\t\tserver = UDPRegistryServer(host = '::' if self.ipv6 else '0.0.0.0', port = self.port, \n\t\t\t\tpruning_timeout = self.pruning_timeout)\n\t\telif self.mode == \"TCP\":\n\t\t\tserver = TCPRegistryServer(port = self.port, pruning_timeout = self.pruning_timeout)\n\t\tsetup_logger(self.quiet, self.logfile)\n\t\tserver.start()\n\n", "description": null, "category": "webscraping", "imports": ["from plumbum import cli", "from rpyc.utils.registry import REGISTRY_PORT, DEFAULT_PRUNING_TIMEOUT", "from rpyc.utils.registry import UDPRegistryServer, TCPRegistryServer", "from rpyc.lib import setup_logger"]}], [{"term": "def", "name": "scrapin", "data": "def scrapin(url,user):\r\n\tres = requests.get(str(url),headers = user)\r\n\tres.status_code == requests.codes.ok\r\n\t#USED TO HALT A BAD DOWNLOAD\r\n\ttry:\r\n\t\tres.raise_for_status()\r\n\texcept Exception as exc:\r\n\t\tprint('There was a problem: %s' % (exc))\r\n\tres_web = BeautifulSoup(res.text,'html.parser')\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import sys\r", "from automate_email import email_alert\r", "import datetime\r"]}, {"term": "def", "name": "webscrapeWeather", "data": "def webscrapeWeather(city):\r\n\t#define our user agent for scraping the website\r\n\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\r\n\r\n\tcity = city +' weather'\r\n\tcity=city.replace(' ','+')\r\n\t#find the weather for the city using google\r\n\tread = scrapin(f'https://www.google.com/search?q={city}&oq={city}&aqs=chrome.0.35i39l2j0l4j46j69i60.6128j1j7&sourceid=chrome&ie=UTF-8',headers)\r\n\t\r\n\ttry:\r\n\t\t#get all of the correct data from the wbsite\r\n\t\tlocation = read.select('#wob_loc')[0].getText().strip()  \r\n\t\t#info = read.select('#wob_dc')[0].getText().strip() \r\n\t\t#weather = read.select('#wob_tm')[0].getText().strip()\r\n\t\t#this is where the high and lows are located\r\n\t\tmore_Weather = read.select('#wob_dp')[0].getText().strip()\r\n\r\n\t\tdegree_sign = u'\\N{DEGREE SIGN}'\r\n\t\t#parse the string to find todays temperatures\r\n\t\thigh = more_Weather[more_Weather.find(today)+3:more_Weather.find(tomorrow)][:2]+degree_sign\r\n\t\tlow = more_Weather[more_Weather.find(today)+3:more_Weather.find(tomorrow)][5:7]+degree_sign\r\n\t\t#put it all together \r\n\t\tweather_up = location +' Weather\\n'+len(location+' Weather')*'-'+'\\nHigh: '+high+'\\nLow: '+low+'\\n'\r\n\r\n\texcept: \r\n\t\tprint('invalid search')\r\n\treturn(weather_up)\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import sys\r", "from automate_email import email_alert\r", "import datetime\r"]}, {"term": "def", "name": "webscrapeNews", "data": "def webscrapeNews():\r\n\ttry:\r\n\t\t\r\n\t\theaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\r\n\t\t#website of choise\r\n\t\tread_news = scrapin('https://www.allsides.com/',headers)\r\n\t\thyper_set=[]\r\n\t\ttitle_set = []\r\n\t\t#compile list of titles from the entire website\r\n\t\turl = 'https://www.allsides.com'\r\n\t\tarticles = read_news.find_all('a',class_='main-link')\r\n\t\t#read what the news was from yesterday\r\n\t\twith open('C:\\\\Users\\josh.smith\\Desktop\\yesterdays_news.txt','r') as f:\r\n\t\t\tyesterdays_news=f.read()\r\n\r\n\t\tfor article in articles:\r\n\t\t\t#we're shooting for five articles and we want to make sure to not repeat yesterdays news\r\n\t\t\tif len(title_set) < 5 and article.getText() not in yesterdays_news:\t\r\n\t\t\t\t#we also dont want to insert the wrong urls (this could be added to the above line instead)\r\n\t\t\t\tif url not in article['href']:\r\n\t\t\t\t\t#print(len(title_set))\r\n\t\t\t\t\ttitle_set.append(article.getText())\r\n\t\t\t\t\thyper_set.append(url+article['href'])\r\n\t\t#save the news articles we found for today for later use\r\n\t\twith open('C:\\\\Users\\josh.smith\\Desktop\\yesterdays_news.txt','w') as f:\r\n\t\t\tfor title in title_set:\r\n\t\t\t\tf.write(str(title+'\\n'))\r\n\t#just in case anything goes wrong\r\n\texcept Exception as err: \r\n\t\tprint(err)\r\n\t\tprint('invalid search')\r\n\treturn(hyper_set,title_set)\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import sys\r", "from automate_email import email_alert\r", "import datetime\r"]}, {"term": "def", "name": "getQuote", "data": "def getQuote():\r\n\t#same drill, define user agents and scrape the website\r\n\tuser_agent={'User-agent': 'Mozilla/5.0'}\r\n\tquotes_web = scrapin('https://www.brainyquote.com/quote_of_the_day',user_agent)\r\n\r\n\ttry:\r\n\t\t#find the quote\r\n\t\tqotd= quotes_web.find('div',class_='grid-item qb clearfix bqQt').getText()\r\n\t#if anything goes wrong we insert this qotd instead\r\n\texcept Exception as err:\r\n\t\tqotd = '\\nNo quote today.'\r\n\t\temail_alert('Error with qotd','File: '+__file__+'\\n'+str(err),None,['josh.smith@kennypipe.com'])\r\n\t\t#print(err)\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import sys\r", "from automate_email import email_alert\r", "import datetime\r"]}, {"term": "def", "name": "newCity", "data": "def newCity(city,emails):\r\n\t#use func from above\r\n\tweather_info = webscrapeWeather(city)\r\n\tlinks,titles = webscrapeNews()\r\n\tqotd = getQuote()\r\n\t\r\n\t#this counter is to number the articles\r\n\tcounter = 1\r\n\temail_stuff = []\r\n\t#run through all the links and titles to compile the email\r\n\tfor title in zip(titles,links):\r\n\t\t \r\n\t\temail = '\\n'+str(counter)+'. '+title[0]+' '\r\n\t\temail_stuff.append(email)\r\n\t\tdashes=len(str(title[0]))*'-'\r\n\t\tcounter +=1\r\n\t#further compiling\r\n\temail_stuff.insert(0,''+weather_info+'\\n')\r\n\temail_stuff.append('\\n\\n'+qotd.replace('\\n\\n\\n','').replace('.','.\\n-')+'')\r\n\temail_alert('Daily News', 'placeholder',','.join(email_stuff).replace(',',''), emails)\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import sys\r", "from automate_email import email_alert\r", "import datetime\r"]}], [{"term": "def", "name": "getMovie", "data": "def getMovie(movieName, year):\n\turl = \"https://movie-database-imdb-alternative.p.rapidapi.com/\"\n\tmovieName = movieName.replace(\"&\", \"&\")\n\tmovieName = movieName.replace(\"-\", \"\")\n\tquerystring = {\"s\":movieName,\"page\":\"1\",\"y\":year,\"r\":\"json\"}\n\n\theaders = {\n\t\t'x-rapidapi-host': \"movie-database-imdb-alternative.p.rapidapi.com\",\n\t\t'x-rapidapi-key': \"8ca55b21e6msha37e66df719a38ap1fc3c2jsn802dc9396eed\"\n\t}\n\n\tresponse = requests.request(\"GET\", url, headers=headers, params=querystring)\n\n\tdata = response.text\n\tdata_dict = json.loads(data)\n\ttry:\n\t\tresults = data_dict['Search']\n\texcept KeyError:\n\t\timdb = \"failed\"\n\t\treturn imdb, data_dict\n\tmovie = results[0]\n\timdb = movie['imdbID']\n\n\tconn = http.client.HTTPSConnection(\"movie-database-imdb-alternative.p.rapidapi.com\")\n\n\theaders = {\n\t\t'x-rapidapi-host': \"movie-database-imdb-alternative.p.rapidapi.com\",\n\t\t'x-rapidapi-key': \"8ca55b21e6msha37e66df719a38ap1fc3c2jsn802dc9396eed\"\n\t\t}\n\n\trequest = \"/?i=\" + imdb + \"&r=json\"\n\tconn.request(\"GET\", request, headers=headers)\n\n\tres = conn.getresponse()\n\tdata = res.read().decode(\"utf-8\")\n\tdata_dict = json.loads(data)\n\n\tprint(movieName + \" details have been found\")\n\n\treturn imdb, data_dict\n\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "import http.client", "import json", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "intoDB", "data": "def intoDB(mydb, imdb, data_dict):\n\tmycursor = mydb.cursor()\n\n\tsql = \"INSERT INTO Movie(MovieID, Title, ReleaseYear, Rating, Runtime, Metascore, imdbRating) \" \\\n\t\t  \"VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n\tval = (imdb, data_dict['Title'], data_dict['Year'], data_dict['Rated'], data_dict['Runtime'],\n\t\t   data_dict['Metascore'], data_dict['imdbRating'])\n\n\ttry:\n\t\tmycursor.execute(sql, val)\n\t\tmydb.commit()\n\texcept mysql.connector.errors.DatabaseError:\n\t\treturn \"failed\"\n\n\tsql = \"INSERT INTO Directors(Name, MovieID) VALUES (%s, %s)\"\n\tsplit = data_dict['Director'].split(',')\n\tvalues = []\n\tfor x in split:\n\t\tx = x.replace(\" \", \"\")\n\t\tval = (x, imdb)\n\t\tvalues.append(val)\n\n\tmycursor.executemany(sql, values)\n\tmydb.commit()\n\n\tsql = \"INSERT INTO Writers(Name, MovieID) VALUES (%s, %s)\"\n\tsplit = data_dict['Writer'].split(',')\n\tvalues = []\n\tfor x in split:\n\t\tx = x.replace(\" \", \"\")\n\t\tval = (x, imdb)\n\t\tvalues.append(val)\n\n\tmycursor.executemany(sql, values)\n\tmydb.commit()\n\n\tsql = \"INSERT INTO Genres(Name, MovieID) VALUES (%s, %s)\"\n\tsplit = data_dict['Genre'].split(',')\n\tvalues = []\n\tfor x in split:\n\t\tx = x.replace(\" \", \"\")\n\t\tval = (x, imdb)\n\t\tvalues.append(val)\n\n\tmycursor.executemany(sql, values)\n\tmydb.commit()\n\n\tsql = \"INSERT INTO Actors(Name, MovieID) VALUES (%s, %s)\"\n\tsplit = data_dict['Actors'].split(',')\n\tvalues = []\n\tfor x in split:\n\t\tx = x.replace(\" \", \"\")\n\t\tval = (x, imdb)\n\t\tvalues.append(val)\n\n\tmycursor.executemany(sql, values)\n\tmydb.commit()\n\n\treturn \"\"\n\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "import http.client", "import json", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "webScrape", "data": "def webScrape():\n\tmovies = []\n\tresponse = requests.get(\"https://www.boxofficemojo.com/year/world/2020/?sort=domesticGrossToDate&ref_=bo_ydw__resort#table\")\n\tsoup = BeautifulSoup(response.text, 'html.parser')\n\ttitles = soup.findAll('td', attrs={\"a-text-left mojo-field-type-release_group\"})\n\tfor title in titles:\n\t\ttmp = str(title)\n\t\tparsed = tmp.split('>')\n\t\tremaining = parsed[2]\n\t\tclean = remaining.split('<')\n\t\tmovies.append(clean[0])\n\n\treturn movies\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "import http.client", "import json", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "fillDB", "data": "def fillDB(year):\n\tmovies = webScrape()\n\tfailed = []\n\tmydb = mysql.connector.connect(\n\t\t\thost = \"localhost\",\n\t\t\tuser = \"root\",\n\t\t\tpassword = \"\",\n\t\t\tdatabase = \"MovieFinder\"\n\t\t)\n\n\tfor movie in movies[:50]:\n\t\tprint(\"Currently adding: \" + movie)\n\t\timbd, data = getMovie(movie, year)\n\t\tif (imbd == \"failed\"):\n\t\t\tfailed.append(movie)\n\t\t\tprint(\"Could not find: \" + movie)\n\t\telse:\n\t\t\tstatus = intoDB(mydb, imbd, data)\n\t\t\tif (status == \"failed\"):\n\t\t\t\tfailed.append(movie)\n\t\t\t\tprint(\"Could not add: \" + movie)\n\t\t\telse:\n\t\t\t\tprint(\"Successfully added: \" + movie)\n\n\tprint(\"Program failed to add the following movies:\")\n\tfor f in failed:\n\t\tprint(f)\n\n\tmydb.close()\n\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "import http.client", "import json", "import requests", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "addMovie", "data": "def addMovie(title):\n\tmydb = mysql.connector.connect(\n\t\thost=\"localhost\",\n\t\tuser=\"root\",\n\t\tpassword=\"\",\n\t\tdatabase=\"MovieFinder\"\n\t)\n\n\timbd, data = getMovie(title)\n\tintoDB(mydb, imbd, data)\n\n\tmydb.close()\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "import http.client", "import json", "import requests", "from bs4 import BeautifulSoup"]}], [{"term": "class", "name": "classNovelAlertsModel:", "data": "class NovelAlertsModel:\n\t\"\"\"\n\tA class that represents the model for the Model-View-Controller(MVC) design pattern.\n\n\t:param FIELD_NAMES: List of dict string key headings\n\t:type FIELD_NAMES: List[str]\n\t:param _URL_file_path: File path of URL data\n\t:type _URL_file_path: str\n\t:param EMAIL_FILE_PATH: File path of email data\n\t:type EMAIL_FILE_PATH: str\n\t:param _user_email: Users email\n\t:type _user_email: str\n\t:param _url_data: List of dictionaries in the format: {\"URL\": \"url_Link, \"latestChapter\": \"chapter\"}\n\t:type _url_data: list[dict[str, str]]\n\t:param _password: users email password\n\t:type _password: str\n\t:param _message_box: GUI error msg method that brings up a message box\n\t:type _message_box: NovelAlertsView method\n\t\"\"\"\n\n\tFIELD_NAMES = [\"URL\", \"latestChapter\"]\n\n\tdef __init__(self, message_box: Callable=print, URL_path: str=\"data/URL_log.csv\", email_path: str=\"data/email.txt\") -> None:\n\t\t\"\"\"Model Initializer\"\"\"\n\n\t\tself._URL_file_path = URL_path\n\t\tself._email_file_path = email_path\n\t\tself._user_email = self._load_email()\n\t\tself._url_data = self._load_URL_Data()\n\t\tself._password = \"\"\n\t\tself._message_box = message_box \n\t\t\n\t\t# Initializes the csv file with column headers if there was no previous data.\n\t\tif not self._url_data:\n\t\t\tself._write_URL_data_to_file()\n\n\t# Add function that figures out which website is added and get call different versions of latest chapter functions\n\tdef _get_Latest_Chapter_URL_Filtered(self, URL: str) -> Union[str, None]:\n\t\t\"\"\"Choose a different version of the get latest chapter function by using the URL\"\"\"\n\n\t\tif \"wln\" in URL and \"series-id\" in URL: \n\t\t\treturn self._webscrape_WLN_Latest_Chapter(URL)\n\t\telif \"novelupdates\" in URL and \"series\" in URL:\n\t\t\treturn self._webscrape_Novelupdates_Latest_Chapter(URL)\n\t\telse:\n\t\t\tself._message_box(\"ERROR: URL is not correct or from wlnupdates or novelupdates domain\")\n\n\tdef _webscrape_WLN_Latest_Chapter(self, URL: str) -> Union[str, None]:\n\t\t\"\"\"Web scrapes the latest chapter from the URL link and must be from domain https://www.wlnupdates.com/\"\"\"\n\n\t\ttry:\n\t\t\t# Title: How to Web Scrape using Beautiful Soup in Python without running into HTTP error 403\n\t\t\t# Author: Raiyan Quaium\n\t\t\t# Availability: https://medium.com/@raiyanquaium/how-to-web-scrape-using-beautiful-soup-in-python-without-running-into-http-error-403-554875e5abed\n\n\t\t\t# Requests the URL data with disguised headers \n\t\t\treq = Request(URL, headers={\"User-Agent\": \"Mozilla/5.0\"})\n\t\t\t# Opens the url and reads the html as a string\n\t\t\twebpage = urlopen(req).read()\n\t\t\t# Creates Bs4 object with arguments consisting of html to be parsed and which parser to use.\n\t\t\tpage_soup = soup(webpage, \"html.parser\")\n\t\t\t# Uses the soup object to find 'h5' tags within the html\n\t\t\t# .text is used to grab the text within the tag and nothing else.\n\t\t\t# [17:] is used to splice the text string to not include \"Latest release - \"\n\t\t\t# Ex. Latest release - vol 2.0  chp. 351.0\n\t\t\tlatest_chapter = page_soup.find(\"h5\").text[17:]\n\t\t\treturn latest_chapter\n\t\texcept Exception:\n\t\t\t# Returns None if latest chapter could not be found\n\t\t\tself._message_box(\"ERROR: Could not find the latest chapter and was not entered into the data.\")\n\n\tdef _webscrape_Novelupdates_Latest_Chapter(self, URL: str) -> Union[str, None]:\n\t\t\"\"\"Web scrapes the latest chapter from the URL link and must be from domain https://www.novelupdates.com/\"\"\"\n\n\t\ttry:\n\t\t\treq = Request(URL, headers={\"User-Agent\": \"Mozilla/5.0\"})\n\t\t\twebpage = urlopen(req).read()\n\t\t\tpage_soup = soup(webpage, \"html.parser\")\n\t\t\t# Uses the soup object to find all 'a' tags with the class 'chp-release'\n\t\t\t# Uses the bracket to access the first result which is the latest chp\n\t\t\t# .text is used to grab the text within the tag and nothing else.\n\t\t\t# Ex.  text \n\t\t\tlatest_chapter = page_soup.findAll(\"a\", \"chp-release\")[0].text\n\t\t\treturn latest_chapter\n\t\texcept Exception:\n\t\t\tself._message_box(\"ERROR: Could not find the latest chapter and was not entered into the data.\")\n\n\tdef _integrate_Updated_URLS(self, updated_URLS: list[str]) -> str:\n\t\t\"\"\"Integrate updated urls into a string\"\"\"\n\n\t\tmessage = \"\"\"\"\"\"\n\n\t\tfor url in updated_URLS:\n\t\t\tmessage += f\"\"\"{url}\\n\"\"\"\n\n\t\treturn message\n\n\tdef _send_Email(self, updated_URLS: list[str]) -> None:\n\t\t\"\"\"Sends a email to user with a list of URL's that have new updates\"\"\"\n\n\t\t# Title: Sending Emails with Python\n\t\t# Author: Joska de Langen\n\t\t# Availability: https://realpython.com/python-send-email/\n\n\t\t# For SSL\n\t\tport = 465 \n\n\t\tmessage = self._integrate_Updated_URLS(updated_URLS)\n\n\t\t# default context validates host name, certificates, and optimizes security of connection\n\t\tcontext = ssl.create_default_context()\n\n\t\ttry:\n\t\t\t# Initiates a TLS-encrypted connection\n\t\t\twith smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n\t\t\t\ttry:\n\t\t\t\t\tserver.login(self._user_email, self._password)\n\t\t\t\t\tserver.sendmail(self._user_email, self._user_email, message)\n\t\t\t\texcept Exception:\n\t\t\t\t\tself._message_box(\"ERROR: Email or password is incorrect!\")\n\t\texcept Exception:\n\t\t\tself._message_box(\"ERROR: Server connection could not be established!\")\n\n\tdef _compile_updated_URLS(self) -> list[str]:\n\t\t\"\"\"Compiles a list of updated URLS by comparing current chapters with new chapters\"\"\"\n\n\t\tupdated_URLS = []\n\t\tfor dict_ in self._url_data:\n\t\t\t# Gets the latestchapter and compare it to the current one in object\n\t\t\t# If it is less than the latest chapter then append URL to list of updated URL's and set new chapter into object\n\t\t\t# No need to check for None from function call because webscraping the URL has worked if it is already in data structure.\n\t\t\tlatest_chapter = self._get_Latest_Chapter_URL_Filtered(dict_[self.FIELD_NAMES[0]])\n\t\t\tif latest_chapter == None:\n\t\t\t\tcontinue\n\n\t\t\tif dict_[self.FIELD_NAMES[1]] < latest_chapter:\n\t\t\t\tupdated_URLS.append(dict_[self.FIELD_NAMES[0]])\n\t\t\t\tdict_[self.FIELD_NAMES[1]] = latest_chapter\n\n\t\treturn updated_URLS\n\n\tdef _webscrape_Check(self) -> Union[int, None]:\n\t\t\"\"\"Check if there are new updates and sends that data to _send_Email\"\"\"\n\n\t\tif self._url_data:\n\t\t\ttry:\n\t\t\t\tupdated_URLS = self._compile_updated_URLS()\n\t\t\n\t\t\t\t# If there were new updated novels\n\t\t\t\tif updated_URLS:\n\t\t\t\t\t# write the new latest chapter data into the csv file.\n\t\t\t\t\tself._write_URL_data_to_file()\n\t\t\t\t\tself._send_Email(updated_URLS)\n\t\t\texcept Exception:\n\t\t\t\tself._message_box(\"Error: Webscraper did not work. If this continues then restart program.\")\n\n\tdef setEmail(self, email: str) -> None:\n\t\t\"\"\"Set the new email and saves it to email file\"\"\"\n\n\t\tself._user_email = email\n\n\t\twith open(self._email_file_path, \"w\") as email_file:\n\t\t\temail_file.write(self._user_email)\n\n\tdef _load_email(self) -> str:\n\t\t\"\"\"Loads the email from email file into class property\"\"\"\n\n\t\twith open(self._email_file_path, \"r\") as email_file:\n\t\t\treturn email_file.read()\n\n\tdef getEmail(self) -> str:\n\t\t\"\"\"Returns email of the user\"\"\"\n\n\t\treturn self._user_email\n\t\n\tdef setPassword(self, password: str) -> None:\n\t\t\"\"\"Set the password\"\"\"\n\n\t\tself._password = password\n\n\tdef getPassword(self) -> str:\n\t\t\"\"\"Gets the password\"\"\"\n\n\t\treturn self._password\n\n\tdef _load_URL_Data(self) -> list[dict[str, str]]:\n\t\t\"\"\"Opens csv file to be read into a list of dictionarys\"\"\"\n\n\t\twith open(self._URL_file_path, mode='r') as csv_file:\n\t\t\treader = csv.DictReader(csv_file)\n\t\t\treturn list(map(dict, reader))\n\n\tdef addURLData(self, URL: str) -> None:\n\t\t\"\"\"Add the new URL to the dictionary and csv file\"\"\"\n\n\t\tfor dict_ in self._url_data:\n\t\t\tif dict_[\"URL\"] == URL:\n\t\t\t\tself._message_box(\"ERROR: URL is already in data structure\")\n\t\t\t\treturn\n\t\t\n\t\t# Gets the latest chapter and if return type is None, then function call did not get latest chapter and doesn't add it to data.\n\t\tlatest_chapter = self._get_Latest_Chapter_URL_Filtered(URL)\n\t\tif latest_chapter == None: \n\t\t\treturn\n\t\t\n\t\tdict_row = {self.FIELD_NAMES[0]: URL, self.FIELD_NAMES[1]: latest_chapter}\n\n\t\tself._url_data.append(dict_row)\n\n\t\twith open(self._URL_file_path, mode='a') as csv_file:\n\t\t\twriter = csv.DictWriter(csv_file, fieldnames=self.FIELD_NAMES)\n\t\t\twriter.writerow(dict_row)\n\n\tdef _get_URL_data(self) -> list[dict[str, str]]:\n\t\t\"\"\"Gets the current URL data within the object\"\"\"\n\t\treturn self._url_data\n\n\tdef _set_URL_data(self, URL_data: list[dict[str, str]]) -> None:\n\t\t\"\"\"Sets the URL data to the object and file\"\"\"\n\t\tself._url_data = URL_data\n\t\tself._write_URL_data_to_file()\n\n\tdef _write_URL_data_to_file(self) -> None:\n\t\t\"\"\"Writes current object _url_data into the csv file\"\"\"\n\n\t\twith open(self._URL_file_path, mode='w') as csv_file:\n\t\t\t# DictWriter object that allows for file output with dictionary keys as fieldnames/columns/headers\n\t\t\twriter = csv.DictWriter(csv_file, fieldnames=self.FIELD_NAMES)\n\n\t\t\twriter.writeheader()\n\t\t\tfor dict_ in self._url_data:\n\t\t\t\twriter.writerow(dict_)\n\n\tdef deleteURLData(self, URL: str) -> None:\n\t\t\"\"\"Delete the URL from the class object and rewrite data into the csv file \"\"\"\n\n\t\tfor dict_ in self._url_data:\n\t\t\tif dict_[self.FIELD_NAMES[0]] == URL:\n\t\t\t\tself._url_data.remove(dict_)\n\t\t\t\tself._message_box(\"Success\")\n\t\t\t\tself._write_URL_data_to_file()\n\t\t\t\treturn\n\t\t\n\t\t# Calls msgBox because URL data has been iterated through and match was not found\n\t\tself._message_box(\"Error: URL is not within existing data or not correct!\")\n", "description": "\n\tA class that represents the model for the Model-View-Controller(MVC) design pattern.\n\n\t:param FIELD_NAMES: List of dict string key headings\n\t:type FIELD_NAMES: List[str]\n\t:param _URL_file_path: File path of URL data\n\t:type _URL_file_path: str\n\t:param EMAIL_FILE_PATH: File path of email data\n\t:type EMAIL_FILE_PATH: str\n\t:param _user_email: Users email\n\t:type _user_email: str\n\t:param _url_data: List of dictionaries in the format: {\"URL\": \"url_Link, \"latestChapter\": \"chapter\"}\n\t:type _url_data: list[dict[str, str]]\n\t:param _password: users email password\n\t:type _password: str\n\t:param _message_box: GUI error msg method that brings up a message box\n\t:type _message_box: NovelAlertsView method\n\t", "category": "webscraping", "imports": ["import csv", "import smtplib", "import ssl", "from typing import Callable, Union", "from urllib.request import Request, urlopen", "from bs4 import BeautifulSoup as soup"]}], [{"term": "def", "name": "scrapper1", "data": "def scrapper1(link):\n\t\"\"\"\n\tFirst version of this project. Kind of buggy, and sometimes puts the results into wrong columns\n\tbecause the site doesn't always provide the same details. For example sometime the fuel type is missing thus\n\tputting the next detail into the fuel type column.\n\t\"\"\"\n\twb = Workbook()\n\tws = wb.active\n\tws.title = 'Vehicles'\n\tprint('Excel file has been created!')\n\n\twebscraping = WebScrape(link)\n\twebscraping.results()\n\twebscraping.main_results()\n\n\tresult_dictionay = webscraping.get_dict()\n\n\theadings = ['car0'] + list(result_dictionay['car1'].keys()) + ['soodushind']\n\n\tws.append(headings)\n\n\tfor vehicle in result_dictionay:\n\t\tvalues = list(result_dictionay[vehicle].values())\n\t\tws.append([vehicle] + values)\n\n\twb.save('Auto-24-Py.xlsx')\n\tprint('Excel file has been saved and is ready to use!')\n\n", "description": "\n\tFirst version of this project. Kind of buggy, and sometimes puts the results into wrong columns\n\tbecause the site doesn't always provide the same details. For example sometime the fuel type is missing thus\n\tputting the next detail into the fuel type column.\n\t", "category": "webscraping", "imports": ["from openpyxl import Workbook", "from Bs4 import WebScrape, Webscrape2"]}, {"term": "def", "name": "scrapper2", "data": "def scrapper2(link):\n\t\"\"\"\n\tSecond version of this project. Works flawlessly.\n\t\"\"\"\n\twb = Workbook()\n\tws = wb.active\n\tws.title = 'Vehicles'\n\tprint('Excel file has been created!')\n\n\twebscraping = Webscrape2(link)\n\twebscraping.results()\n\twebscraping.main_results()\n\n\tresult_dictionay = webscraping.get_dict()\n\theadings = [0] + list(webscraping.get_example_dict().keys())\n\n\tws.append(headings)\n\n\tprint('Converting Dictionary -> Excel file!')\n\tfor vehicle in result_dictionay:\n\t\tvalues = list(result_dictionay[vehicle].values())\n\t\tws.append([vehicle] + values)\n\twb.save('Auto-24-Py.xlsx')\n\tprint('Excel file has been saved and is ready to use!')\n", "description": "\n\tSecond version of this project. Works flawlessly.\n\t", "category": "webscraping", "imports": ["from openpyxl import Workbook", "from Bs4 import WebScrape, Webscrape2"]}], [{"term": "def", "name": "webScrape", "data": "def webScrape(routeURL, routeName):\n\tglobal completeFileName\n\tglobal soup\n\n\t#gets and requests data from the url\n\turl = routeURL\n\turlRequest = requests.get(url)\n\tsoup = BeautifulSoup(urlRequest.content, 'html5lib') \n\n\t#configures file and path naming\n\ttimeFileName = routeName+\".txt\"\n\tdataFileName = routeName+\"Data.txt\"\n\tpathName = (os.getcwd()+'/timetableFiles/txtFiles')\n\tcompleteFileName = os.path.join(pathName, timeFileName)\n\t#finds all elements with tag 'td' and writes them into the text file\n\ttdValue = 0\n\ttdContainer = soup.findAll('td')\n\n\topenFile=open(completeFileName, 'w')\n\t# goes through each td in the list and writes each value to a new line on the text file\n\tfor td in tdContainer:\n\t\topenFile.write((tdContainer[tdValue].text+'\\n'))\n\t\ttdValue+=1\n\topenFile.close()\n\n\ttotalTables()\n\ttableHeight()\n\ttableWidth()\n\tlineCount()\n\n\ttableData = [str(widthPerTable)+\"\\n\", str(heightPerTable)+\"\\n\", str(fileLength)]\n\n\ttableDataFile = (os.path.join(pathName, dataFileName))\n\topenFile = open(tableDataFile, 'w')\n\topenFile.writelines(tableData)\n\topenFile.close()\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import os", "import webbrowser", "#^^functions were called to test - no longer needed to be called, messes with the program when importing module"]}, {"term": "def", "name": "tableHeight", "data": "def tableHeight():\n\tglobal totalHeight\n\tglobal heightPerTable\n\ttableHeightContainer = soup.findAll('tr', attrs={\"class\": \"table_alt\"})\n\tfor tr in tableHeightContainer:\n\t\ttotalHeight+=1\n\ttableHeightContainer = soup.findAll('tr', attrs={\"class\" : \"table_alt\"})\n\tfor tr in tableHeightContainer:\n\t\ttotalHeight+=1\n\n\t#calculates the average height of each table\n\tfirstColumns = soup.findAll(\"td\", attrs={\"class\":\"first-column\"})\n\tfor td in firstColumns:\n\t\theightPerTable+=1\n\theightPerTable=int(heightPerTable/totalTableCount)\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import os", "import webbrowser", "#^^functions were called to test - no longer needed to be called, messes with the program when importing module"]}, {"term": "def", "name": "tableWidth", "data": "def tableWidth():\n\tglobal widthPerTable\n\ttotal=0\n\tfirstRow = soup.find(\"span\", attrs={\"class\":\"magenta_on\"})\n\tfor i in range (1, (len(firstRow)+1)):\n\t\ttotal+=1\n\ttotal+=(totalTableCount)\n\twidthPerTable = total+3\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import os", "import webbrowser", "#^^functions were called to test - no longer needed to be called, messes with the program when importing module"]}, {"term": "def", "name": "totalTables", "data": "def totalTables():\n\tglobal totalTableCount\n\ttotalTableCount = 0\n\ttableCount = soup.findAll(\"tr\", attrs={\"class\":\"bodytextbold\"})\n\tfor tr in tableCount:\n\t\ttotalTableCount+=1\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import os", "import webbrowser", "#^^functions were called to test - no longer needed to be called, messes with the program when importing module"]}, {"term": "def", "name": "fileInput", "data": "def fileInput():\n\tglobal tableDict\n\tglobal totalTableCount\n\tglobal heightPerTable\n\ttableDict = []\n\tlineTracker = 0\n\tfile = open(completeFileName)\n\tfileContent = file.readlines()\n\t#makes array 3D for amount of tables\n\tfor i in range(0,totalTableCount):\n\t\ttableDict.append([])\n\n\t#makes the smaller arrays, one for each table line\n\tfor i in range(0, totalTableCount):\n\t\tfor x in range(0,heightPerTable):\n\t\t\ttableDict[i].append([])\n\t\t\tfor b in range(0,11):\n\t\t\t\ttableDict[i][x].append(fileContent[lineTracker])\n\t\t\t\twhile lineTracker != fileLength-1:\n\t\t\t\t\tlineTracker+=1\n\t\t\t\t\tbreak\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import os", "import webbrowser", "#^^functions were called to test - no longer needed to be called, messes with the program when importing module"]}, {"term": "def", "name": "lineCount", "data": "def lineCount():\n\tglobal fileLength\n\twith open(completeFileName, 'r') as openFile:\n\t\tfileLength = len(openFile.readlines())\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import os", "import webbrowser", "#^^functions were called to test - no longer needed to be called, messes with the program when importing module"]}, {"term": "def", "name": "tableDimensions", "data": "def tableDimensions():\n\tglobal dimensionArray\n\tglobal heightPerTable\n\tfor i in range(0, totalTableCount):\n\t\tdimensionArray.append([])\n\t\tdimensionArray[i].append(widthPerTable)\n\t\tdimensionArray[i].append(heightPerTable)\n\tprint(dimensionArray)\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import os", "import webbrowser", "#^^functions were called to test - no longer needed to be called, messes with the program when importing module"]}, {"term": "def", "name": "cancellations", "data": "def cancellations():\n\t#takes user to website in order to \n\twebbrowser.open_new(\"https://www.firstbus.co.uk/doncaster/plan-journey/timetables/journey-cancellations\")\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests", "import os", "import webbrowser", "#^^functions were called to test - no longer needed to be called, messes with the program when importing module"]}], [{"term": "def", "name": "index", "data": "def index():\r\n\tif request.method == 'POST':\r\n\t\tkeyword = request.data\r\n\t\tpresent_prices(keyword)\r\n\t\treturn send_file(\"static/js/amazon-output.json\")\r\n\r\n\treturn send_file(\"templates/index.html\")\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, send_file, make_response, request\r", "from webscrape import present_prices\r"]}], [], [], [{"term": "class", "name": "classWebScrape:\r", "data": "class WebScrape:\r\n\tdef __init__(self, in_path, out_path):\r\n\t\tself.rows = open(in_path).read().strip().split(\"\\n\")\r\n\t\tself.out_path = out_path\r\n\r\n\tdef download_image(self):\r\n\t\t# print(rows)\r\n\t\ttotal = 0\r\n\t\t# loop the URLs\r\n\t\tfor url in self.rows:\r\n\t\t\ttry:\r\n\t\t\t\t# try to download the image\r\n\t\t\t\tr = requests.get(url, timeout=60)\r\n\r\n\t\t\t\t# save the image to disk\r\n\t\t\t\tp = os.path.sep.join([self.out_path + \"{}.jpg\".format(str(total).zfill(8))])\r\n\t\t\t\tf = open(p, \"wb\")\r\n\t\t\t\tf.write(r.content)\r\n\t\t\t\tf.close()\r\n\r\n\t\t\t\t# update the counter\r\n\t\t\t\tprint(\"[INFO] downloaded: {}\".format(p))\r\n\t\t\t\ttotal += 1\r\n\r\n\t\t\t# handle if any exceptions are thrown during the download process\r\n\t\t\texcept:\r\n\t\t\t\tprint(\"[INFO] error downloading {}...skipping\".format(p))\r\n\r\n\tdef delete_invalid_image(self):\r\n\t\t# loop over the image paths we just downloaded\r\n\t\tfor imagePath in paths.list_images(self.out_path):\r\n\t\t\t# initialize if the image should be deleted or not\r\n\t\t\tdelete = False\r\n\r\n\t\t\t# try to load the image\r\n\t\t\ttry:\r\n\t\t\t\timage = cv2.imread(imagePath)\r\n\r\n\t\t\t\t# if the image is `None` then we could not properly load it\r\n\t\t\t\t# from disk, so delete it\r\n\t\t\t\tif image is None:\r\n\t\t\t\t\tdelete = True\r\n\r\n\t\t\t# if OpenCV cannot load the image then the image is likely\r\n\t\t\t# corrupt so we should delete it\r\n\t\t\texcept:\r\n\t\t\t\tprint(\"Except\")\r\n\t\t\t\tdelete = True\r\n\r\n\t\t\t# check to see if the image should be deleted\r\n\t\t\tif delete:\r\n\t\t\t\tprint(\"[INFO] deleting {}\".format(imagePath))\r\n\t\t\t\tos.remove(imagePath)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["# import the necessary packages\r", "from imutils import paths\r", "import argparse\r", "import requests\r", "import cv2\r", "import os\r", "import random\r", "from sklearn.model_selection import train_test_split\r", "import numpy as np\r"]}, {"term": "class", "name": "classDataPreprocess:\r", "data": "class DataPreprocess:\r\n\tdef resize(self, out_path, write_loc):\r\n\t\t# loop over the image paths we just downloaded\r\n\t\tcount = 0\r\n\t\tfor imagePath in paths.list_images(out_path):\r\n\t\t\ttry:\r\n\t\t\t\timage = cv2.imread(imagePath)\r\n\t\t\t\timage = cv2.resize(image, (224, 224))\r\n\t\t\t\tcv2.imwrite(write_loc+str(count)+\".jpg\", image)\r\n\t\t\t\tcount += 1\r\n\t\t\texcept Exception as e:\r\n\t\t\t\tprint(e)\r\n\t\t\t\tprint(\"couldn't write this image, skipping\")\r\n\r\n\r\n\tdef prepare(self, file_loc, test_size=0.25):\r\n\r\n\t\t# grab the image paths and randomly shuffle them\r\n\t\timagePaths = sorted(list(paths.list_images(file_loc)))\r\n\t\trandom.seed(42)\r\n\t\trandom.shuffle(imagePaths)\r\n\t\tdata = []\r\n\t\tlabels = []\r\n\t\t# count = 0\r\n\t\tlabel1 = 0\r\n\t\tlabel0 = 0\r\n\t\tlabel2 = 0\r\n\t\tlabel3 = 0\r\n\t\tlabel4 = 0\r\n\t\ttotal = 0\r\n\t\t# loop over the input images\r\n\t\tfor imagePath in imagePaths:\r\n\t\t\t# load the image, pre-process it, and store it in the data list\r\n\t\t\timage = cv2.imread(imagePath)\r\n\t\t\tdata.append(image)\r\n\r\n\t\t\t# extract the class label from the image path and update the\r\n\t\t\t# labels list\r\n\t\t\tlabel = imagePath.split(os.path.sep)[-2]\r\n\t\t\t# print(label)\r\n\t\t\ttotal += 1\r\n\t\t\tif label == \"Batman\":\r\n\t\t\t\tlabel = 0\r\n\t\t\t\tlabel0 += 1\r\n\t\t\telif label == \"Superman\":\r\n\t\t\t\tlabel = 1\r\n\t\t\t\tlabel1 += 1\r\n\t\t\telif label == \"Wonderwoman\":\r\n\t\t\t\tlabel = 2\r\n\t\t\t\tlabel2 += 1\r\n\t\t\telif label == \"Joker\":\r\n\t\t\t\tlabel = 3\r\n\t\t\t\tlabel3 += 1\r\n\t\t\telif label == \"Persons\":\r\n\t\t\t\tlabel = 4\r\n\t\t\t\tlabel4 += 1\r\n\t\t\tlabels.append(label)\r\n\t\t# print(count)\r\n\t\tdata = np.array(data)\r\n\t\tlabels = np.array(labels)\r\n\t\tprint(\"Batman : \" + str(label0))\r\n\t\tprint(\"Superman : \" + str(label1))\r\n\t\tprint(\"Wonderwoman : \" + str(label2))\r\n\t\tprint(\"Joker : \" + str(label3))\r\n\t\tprint(\"Persons : \" + str(label4))\r\n\t\tprint(\"Total images : \" + str(total))\r\n\t\t# partition the data into training and testing splits using 75% of\r\n\t\t# the data for training and the remaining 25% for testing\r\n\t\t(trainX, testX, trainY, testY) = train_test_split(data,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  labels, test_size=test_size, random_state=42)\r\n\t\tprint(\"Split : \" + str(1-test_size) + \" : \" + str(test_size))\r\n\t\tprint(\"Splitting the set into Training : \" + str(int(total * (1-test_size))) + \" Test : \" + str(int(total * test_size)))\r\n\t\treturn trainX, testX, trainY, testY\r\n\r\n", "description": null, "category": "webscraping", "imports": ["# import the necessary packages\r", "from imutils import paths\r", "import argparse\r", "import requests\r", "import cv2\r", "import os\r", "import random\r", "from sklearn.model_selection import train_test_split\r", "import numpy as np\r"]}, {"term": "def", "name": "main", "data": "def main():\r\n\tin_path = \"url/persons.txt\"\r\n\tout_path = \"Output/Persons/\"\r\n\twrite_loc = \"Data/Persons/\"\r\n\tfile_loc = \"Data\"\r\n\tws = WebScrape(in_path, out_path)\r\n\t# ws.download_image()\r\n\t# ws.delete_invalid_image()\r\n\tprocess = DataPreprocess()\r\n\tprocess.resize(out_path, write_loc)\r\n\t# print(process.prepare(\"Data\"))\r\n\t# process.prepare(\"Data\")\r\n", "description": null, "category": "webscraping", "imports": ["# import the necessary packages\r", "from imutils import paths\r", "import argparse\r", "import requests\r", "import cv2\r", "import os\r", "import random\r", "from sklearn.model_selection import train_test_split\r", "import numpy as np\r"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(url):\n\t\"\"\" Grabs all the pdf files in a website and converts them into text files, storing them in an array that preserves\n\torder.\n\n\tInputs: website url\n\tOutput: array of text <- CAN BE CHANGED CORRESPONDINGLY\n\n\t\"\"\"\n\t# connect to website and get list of all pdfs\n\tresponse = requests.get(url)\n\tsoup = BeautifulSoup(response.text, \"html.parser\")\n\tlinks = soup.select(\"a[href$='.pdf']\")\n\n\t# clean the pdf link names\n\turl_list = []\n\tfor el in links:\n\t\tif el['href'].startswith('http'):\n\t\t\turl_list.append(el['href'])\n\t\telse:\n\t\t\turl_list.append(url + el['href'])\n\n\t# TODO: have to change the pdf to text conversion here. (add the pdf2txt function)\n\t# TO RITVIK: i had it so that we add in the text itself, but we can have it so that it appends the txt file\n\tpdf_txts = []\n\ti = 0\n\tfor url in url_list:\n\t\turl_str = \"{}.pdf\".format(i) # should think about naming as well. Maybe just store it as numbers for now?\n\t\turlretrieve(url, url_str)\n\t\ttxt = pdf2txt(url_str)\n\t\tpdf_txts.append(txt)\n\t\ti = i + 1\n\n\treturn pdf_txts\n", "description": " Grabs all the pdf files in a website and converts them into text files, storing them in an array that preserves\n\torder.\n\n\tInputs: website url\n\tOutput: array of text <- CAN BE CHANGED CORRESPONDINGLY\n\n\t", "category": "webscraping", "imports": ["import os", "import requests", "from urllib.parse import urljoin", "from bs4 import BeautifulSoup", "from urllib.request import urlretrieve", "import codecs", "from io import StringIO", "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter", "from pdfminer.layout import LAParams", "from pdfminer.converter import TextConverter", "from pdfminer.pdfpage import PDFPage"]}, {"term": "def", "name": "pdf2txt", "data": "def pdf2txt(url):\n\trsrcmgr = PDFResourceManager()\n\tretstr = StringIO()\n\tcodec = 'utf-8'\n\tlaparams = LAParams()\n\tdevice = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n\tfp = open(url, 'rb')\n\tinterpreter = PDFPageInterpreter(rsrcmgr, device)\n\tpassword = \"\"\n\tmaxpages = 0\n\tcaching = True\n\tpagenos=set()\n\n\ti = 1\n\tfor page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n\t\t#print(i)\n\t\tinterpreter.process_page(page)\n\t\ti = i + 1\n\n\ttext = retstr.getvalue()\n\n\tfp.close()\n\tdevice.close()\n\tretstr.close()\n\treturn text\n", "description": null, "category": "webscraping", "imports": ["import os", "import requests", "from urllib.parse import urljoin", "from bs4 import BeautifulSoup", "from urllib.request import urlretrieve", "import codecs", "from io import StringIO", "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter", "from pdfminer.layout import LAParams", "from pdfminer.converter import TextConverter", "from pdfminer.pdfpage import PDFPage"]}], [{"term": "class", "name": "WebscrapeConfig", "data": "class WebscrapeConfig(AppConfig):\n\tname = 'webscrape'\n", "description": null, "category": "webscraping", "imports": ["from django.apps import AppConfig"]}], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('webscrape', '0001_initial'),\n\t]\n\n\toperations = [\n\t\tmigrations.DeleteModel(\n\t\t\tname='SavedProducts',\n\t\t),\n\t]\n", "description": null, "category": "webscraping", "imports": ["from django.db import migrations"]}], [{"term": "def", "name": "text", "data": "def text(index, string):\n\t# I do all 3 so it stays disabled again like default after inserting\n\ttext1.configure(state='normal')\n\ttext1.insert(index, string)\n\ttext1.configure(state='disabled')\n\n", "description": null, "category": "webscraping", "imports": ["import tkinter as tk", "from tkinter.filedialog import askopenfilename", "from tkinter.ttk import Progressbar", "from webscrape.webscrape.SeleniumSD.automate_servicedesk import AutoSD  # importing my own script class", "import threading", "import asyncio", "import time", "import subprocess", "import emoji", "\timport os"]}, {"term": "def", "name": "submit", "data": "def submit():\n\trow = groupUnassigned_entry.get() # previously hardcoded '0', now gets value from input field\n\tprint('Script running...')\n\n\ttext(0.0, '\\n \\n ')\n\ttext(1.0, '---- Group Unassigned, row:' + ' ' + row + '----' + '\\n')\n\ttext(2.0, 'Script running...' + '\\n')\n", "description": null, "category": "webscraping", "imports": ["import tkinter as tk", "from tkinter.filedialog import askopenfilename", "from tkinter.ttk import Progressbar", "from webscrape.webscrape.SeleniumSD.automate_servicedesk import AutoSD  # importing my own script class", "import threading", "import asyncio", "import time", "import subprocess", "import emoji", "\timport os"]}, {"term": "def", "name": "start_submit_thread", "data": "def start_submit_thread(event):\n\tglobal submit_thread\n\tsubmit_thread = threading.Thread(target=event)\n\tsubmit_thread.daemon = True\n\tprogressbar.start()\n\tsubmit_thread.start()\n\twindow.after(20, check_submit_thread)\n\n", "description": null, "category": "webscraping", "imports": ["import tkinter as tk", "from tkinter.filedialog import askopenfilename", "from tkinter.ttk import Progressbar", "from webscrape.webscrape.SeleniumSD.automate_servicedesk import AutoSD  # importing my own script class", "import threading", "import asyncio", "import time", "import subprocess", "import emoji", "\timport os"]}, {"term": "def", "name": "check_submit_thread", "data": "def check_submit_thread():\n\tif submit_thread.is_alive():\n\t\twindow.after(20, check_submit_thread)\n\telse:\n\t\t# progressbar.stop()\n\t\tprogressbar.stop()\n\n", "description": null, "category": "webscraping", "imports": ["import tkinter as tk", "from tkinter.filedialog import askopenfilename", "from tkinter.ttk import Progressbar", "from webscrape.webscrape.SeleniumSD.automate_servicedesk import AutoSD  # importing my own script class", "import threading", "import asyncio", "import time", "import subprocess", "import emoji", "\timport os"]}, {"term": "def", "name": "close_app", "data": "def close_app():\n\twindow.destroy()\n\n", "description": null, "category": "webscraping", "imports": ["import tkinter as tk", "from tkinter.filedialog import askopenfilename", "from tkinter.ttk import Progressbar", "from webscrape.webscrape.SeleniumSD.automate_servicedesk import AutoSD  # importing my own script class", "import threading", "import asyncio", "import time", "import subprocess", "import emoji", "\timport os"]}, {"term": "def", "name": "print_val", "data": "def print_val():\n\tprint(inputDc_entry.get())  # gets the value from inputDc input field from user\n\ttext1.insert(0.0, \"Shazaam\")\n", "description": null, "category": "webscraping", "imports": ["import tkinter as tk", "from tkinter.filedialog import askopenfilename", "from tkinter.ttk import Progressbar", "from webscrape.webscrape.SeleniumSD.automate_servicedesk import AutoSD  # importing my own script class", "import threading", "import asyncio", "import time", "import subprocess", "import emoji", "\timport os"]}, {"term": "def", "name": "hello", "data": "def hello():\n\ttext(0.0, \"hello!\")\n", "description": null, "category": "webscraping", "imports": ["import tkinter as tk", "from tkinter.filedialog import askopenfilename", "from tkinter.ttk import Progressbar", "from webscrape.webscrape.SeleniumSD.automate_servicedesk import AutoSD  # importing my own script class", "import threading", "import asyncio", "import time", "import subprocess", "import emoji", "\timport os"]}, {"term": "def", "name": "open_logs", "data": "def open_logs():\n\t# filename = askopenfilename(parent=window) # this is how you open File Explorer\n\timport os\n\tos.system('C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\logs.txt') # this is how you open a file directly\n\n", "description": null, "category": "webscraping", "imports": ["import tkinter as tk", "from tkinter.filedialog import askopenfilename", "from tkinter.ttk import Progressbar", "from webscrape.webscrape.SeleniumSD.automate_servicedesk import AutoSD  # importing my own script class", "import threading", "import asyncio", "import time", "import subprocess", "import emoji", "\timport os"]}], [{"term": "def", "name": "insertSpaces", "data": "def insertSpaces(name):\n\t\n\tfor i in range(len(name)):\n\t\tif (name[i] == \" \"):\n\t\t\tnum = i + 1\n\t\t\tname = name[:i] + \"%20\" + name[num:]  \n\t\n\t##print(name)\n\tnewN = name\n\treturn newN\n\t\n\t\n\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults import prompt_user_search_results_script", "from webscrape import clear"]}, {"term": "def", "name": "prompt_user", "data": "def prompt_user():\n\tprint(\"Welcome to Metacritic Webscraping!\\n\\n\")\n   \n\t\n\t\n\tcategories = [\"movie\", \"tv\", \"game\", \"album\", \"all\"]\n\tprint(\"1. Movies\")\n\tprint(\"2. TV Shows\")\n\tprint(\"3. Video Games\")\n\tprint(\"4. Album\")\n\tprint(\"5. All\")\n\t#prompt the user to enter their search category\n\tindex = input(\"Enter the number that corresponds with the category you would like to search in\\nOr\\nEnter invalid number to exit\\n\")\n\t\n\tif (index.isdigit() == False):\n\t\tindex = -1\n\tindex = int(index)\n\t\n\twhile(index > 0 and index <= len(categories)):\n\t\t\n\t\tcategory = categories[index - 1]\n\t\t\n\t\t#convert the user input to lower-case for consistency\n\t\tname = input(\"Enter the name of what you wish to search\\n\")\n\t\tname = name.lower()\n\t\t\n\t\tsearchName = name\n\t\tif \" \" in name:\n\t\t\tsearchName = insertSpaces(name)\n\t\t \n\t\t \n\t\t#clear output here\n\t\tclear()\n\t\t \n\t\tif (searchName != None):\n\t\t\tprompt_user_search_results_script.find_results(searchName, category)\n\t\t\n\t\t#clear output here\n\t\tclear()\n\t\t\n\t\tprint(\"1. Movies\")\n\t\tprint(\"2. TV Shows\")\n\t\tprint(\"3. Video Games\")\n\t\tprint(\"4. Album\")\n\t\tprint(\"5. All\")\n\t\t\n\t\tindex = input(\"Enter the number that corresponds with the category you would like to search in\\nOr\\nEnter invalid number to exit\\n\")\n\t\t#check to see if the index is a valid number\n\t\tif (index.isdigit() == False):\n\t\t\tindex = -1\n\t\t\n\t\tindex = int(index)\n\t\t\n\t\n\tprint(\"Thanks for using my program!\")\n\t\n\t\n\n\n\t\n", "description": null, "category": "webscraping", "imports": ["from webscrape.searchresults import prompt_user_search_results_script", "from webscrape import clear"]}], [{"term": "def", "name": "findCourses", "data": "def findCourses(string):\n\t# [A-Z]{4}  match a 4 character string from A to Z\n\t# ' '?\t  match space or no space\n\t# [0-9]{4}  match 4 digit number from 0 to 9\n\tmatches = re.findall(r'[A-Z]{4} ?[0-9]{4}', string.upper())\n\n\t# get all the courses and seperate sub and number (COMP2160 -> COMP 2160)\n\tfor key, value in enumerate(matches):\n\t\tif (len(matches[key].split()) != 2):\n\t\t\tmatches[key] = ' '.join(value[i:i + 4] for i in range(0,len(value), 4))\n\n\treturn matches\n", "description": null, "category": "webscraping", "imports": ["from src.config import Config", "from src.sql import SQL", "from src import webscrape", "import praw", "import os", "import re", "import time"]}, {"term": "def", "name": "log", "data": "def log(logType, message):\n\t# store time and prepare log message\n\tt = time.gmtime()\n\toutput = f'[{t.tm_year}-{t.tm_mon}-{t.tm_mday} {t.tm_hour}:{t.tm_min}:{t.tm_sec}][{logType}] {message}'\n\n\t# open log file for writing (append)\n\tf = open('f.log', 'a')\n\n\t# write log and print\n\tf.write(output + '\\n')\n\tprint(output)\n\n\t# clean up our toys\n\tf.close()\n", "description": null, "category": "webscraping", "imports": ["from src.config import Config", "from src.sql import SQL", "from src import webscrape", "import praw", "import os", "import re", "import time"]}, {"term": "def", "name": "customStream", "data": "def customStream(subreddit, **kwargs):\n\t# create array of comments and submissions\n\tresults = []\n\tresults.extend(subreddit.new(**kwargs))\n\tresults.extend(subreddit.comments(**kwargs))\n\tresults.sort(key = lambda post: post.created_utc, reverse = True)\n\n\treturn results\n", "description": null, "category": "webscraping", "imports": ["from src.config import Config", "from src.sql import SQL", "from src import webscrape", "import praw", "import os", "import re", "import time"]}, {"term": "def", "name": "__run__", "data": "def __run__():\n\tdoReply = False\n\n\tstream = praw.models.util.stream_generator(lambda **kwargs: submissions_and_comments(reddit.subreddit('umanitoba'), **kwargs), skip_existing = True)\n\tfor newPost in stream:\n\t\tentirePostContent = None\n\n\t\tif (type(newPost) is praw.models.Submission):\n\t\t\tentirePostContent = newPost.title + ' ' + newPost.selftext\n\t\t\tlog('READ', 'Reading submission ' + str(newPost))\n\t\telif (type(newPost) is praw.models.Comment):\n\t\t\t# TODO find comments with [ABCD 1234]\n\t\t\t#entirePostContent = newPost.body\n\t\t\tlog('READ', 'Reading comment ' + str(newPost))\n\t\t\tentirePostContent = None\n\n\t\tif (entirePostContent != None):\n\t\t\tcourses = findCourses(entirePostContent)\n\t\t\treplyCourseInfo = []\n\n\t\t\tfor course in courses:\n\t\t\t\tcourseSplit = course.split()\n\n\t\t\t\tresult = webscrape.getAuroraCourse(courseSplit[0], courseSplit[1])\n\t\t\t\tif (result == None):\n\t\t\t\t\tcontinue\n\n\t\t\t\tdoReply = True\n\n\t\t\t\tget = sql.getCourseInfo(courseSplit[0], courseSplit[1])\n\n\t\t\t\tif (get != None):\n\t\t\t\t\tif (get['last_update'] + (60 * 1) < time.time()):\n\t\t\t\t\t\tsql.updateCourseInfo(get['id'], result['title'], result['desc'], result['notHeld'], result['preReq'])\n\t\t\t\t\t\treplyCourseInfo.append((result['title'], result['desc'], result['notHeld'], result['preReq']))\n\t\t\t\t\telse:\n\t\t\t\t\t\treplyCourseInfo.append((get['title'], get['desc'], get['notHeld'], get['preReq']))\n\t\t\t\telse:\n\t\t\t\t\tsql.insertCourseInfo(courseSplit[0], courseSplit[1], result['title'], result['desc'], result['notHeld'], result['preReq'])\n\t\t\t\t\treplyCourseInfo.append((result['title'], result['desc'], result['notHeld'], result['preReq']))\n\t\t\t\t\t\n\t\t\tif (doReply):\n\t\t\t\tlog('REPLY', 'Replying to ' + str(new))\n\n\t\t\t\tdoReply = False\n\t\t\t\t\n\t\t\t\treplyStr = ''\n\n\t\t\t\tfor courseInfo in replyCourseInfo:\n\t\t\t\t\treplyStr = replyStr + '\\n' + courseInfo[0] + '|' + courseInfo[1] + '|' + courseInfo[2] + '|' + courseInfo[3]\n\n\t\t\t\tnewPost.reply('Course|Description|Not Held With|Prerequisite(s)\\n-|-|-|-' + replyStr)\n", "description": null, "category": "webscraping", "imports": ["from src.config import Config", "from src.sql import SQL", "from src import webscrape", "import praw", "import os", "import re", "import time"]}], [{"term": "def", "name": "winexp", "data": "def winexp(t1, t2):\n\twe2 = 1 / (1 + 10 ** ((t1 - t2) / 400))  # winning expectancy of team 2\n\twe1 = 1-we2\n\treturn round(we1, 2), round(we2, 2)\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from tabulate import tabulate", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(week, tab):\n\ttab2 = tab[week - 1]\n\ttab2.dropna(inplace=True)  # cleaning data\n\ttab2.drop(tab2.columns[[3, 4]], axis=1, inplace=True)  # removing NaNs\n\ttab2.reset_index(drop=True, inplace=True)\n\ttab2.columns = ['Team1', 'Score1', 'Score2', 'Team2']\n\treturn tab2\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from tabulate import tabulate", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "elocalc", "data": "def elocalc():\n\tk = 47  # k-factor\n\twhile True:\n\t\tregion = input(\"Ranking kt\u00f3rej ligi chcesz modyfikowa\u0107? \")\n\t\ttry:\n\t\t\tseason = pd.read_html('https://lol.fandom.com/%s/2021_Season/Spring_Season' % region,\n\t\t\t\t\t\t\t\t  # getting table with results\n\t\t\t\t\t\t\t\t  attrs={'class': 'wikitable2 matchlist'})\n\t\texcept Exception as e:\n\t\t\tprint(e)\n\t\t\tcontinue\n\t\tbreak\n\tdf = pd.DataFrame(index=['Rating'])\n\tweek = 1\n\twhile True:\n\t\ttry:\n\t\t\tresults = webscrape(week, season)\n\t\texcept (IndexError, ValueError):\n\t\t\tbreak\n\t\tfor i in range(len(results.index)):\n\t\t\tteam1 = results.loc[i, 'Team1'].replace('\\u2060', '')\n\t\t\tteam2 = results.loc[i, 'Team2'].replace('\\u2060', '')\n\t\t\tif team1 not in df.columns:\n\t\t\t\tdf[team1] = 1000\n\t\t\tif team2 not in df.columns:\n\t\t\t\tdf[team2] = 1000\n\t\t\twin1, win2 = winexp(df.loc['Rating', team1], df.loc['Rating', team2])\n\n\t\t\tif results.loc[i, 'Score1'] > results.loc[i, 'Score2']:\n\t\t\t\tdf.loc['Rating', team1] += k*(1-win1)\n\t\t\t\tdf.loc['Rating', team2] += k*(0-win2)\n\t\t\telse:\n\t\t\t\tdf.loc['Rating', team1] += k*(0-win1)\n\t\t\t\tdf.loc['Rating', team2] += k*(1-win2)\n\t\tweek += 1\n\n\tdf = df.T\n\tdf = df.sort_values(by='Rating', ascending=False)\n\tdfprint = df.reset_index()  # dataframe for pretty print\n\tdfprint.index += 1\n\tdfprint.rename(columns={'index': 'Team'}, inplace=True)\n\tdf = df.round(1)\n\tprint(tabulate(dfprint, headers='keys', tablefmt=\"fancy_grid\", floatfmt='.1f'))\n\tdf.to_csv(region + '.csv', index='True')\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from tabulate import tabulate", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "stats", "data": "def stats():\n\twhile True:\n\t\tregion = input(\"Dla kt\u00f3rej ligi chcesz statystyki: \").upper()\n\t\ttry:\n\t\t\tdf = pd.read_csv(region + '.csv', index_col=0)\n\t\texcept FileNotFoundError as e:\n\t\t\tprint(e)\n\t\t\tcontinue\n\t\tbreak\n\n\tprint(df.describe())\n\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "from tabulate import tabulate", "import matplotlib.pyplot as plt"]}, {"term": "def", "name": "main", "data": "def main():\n\tprint(\"\"\"\\t\\t\\tWitaj w programie ESPORTS ELO\n\tCo chcesz zrobi\u0107?\n\t1. Policzy\u0107 ranking ELO\n\t2. Inne\n\t\t\"\"\")\n\toption = int(input(\"\"))\n\tif option == 1:\n\t\telocalc()\n\telif option == 2:\n\t\tstats()\n\n\tinput()\n\n", "description": "\\t\\t\\tWitaj w programie ESPORTS ELO\n\tCo chcesz zrobi\u0107?\n\t1. Policzy\u0107 ranking ELO\n\t2. Inne\n\t\t", "category": "webscraping", "imports": ["import pandas as pd", "from tabulate import tabulate", "import matplotlib.pyplot as plt"]}], [], [{"term": "class", "name": "classLessonScrape:", "data": "class LessonScrape:\n\tdef __init__(self, session, dictionary, file_list):\n\t\tself.session = session\n\t\tself.file_list = file_list\n\t\tself.dictionary = dictionary\n\t\tself.start()\n\n\tdef start(self):\n\t\twb = WebScrape(self.session)\n\t\twb.init_driver()\n\t\tword_expansion = []\n\n\t\tfor c_lesson in self.file_list:\n\t\t\twb.run_webdriver(c_lesson)\n\t\t\tlesson = wb.get_source()\n\t\t\ttempfile_path = WriteFile.write_file(\n\t\t\t\t\"./data/temp/temp.html\", lesson[\"source\"]\n\t\t\t)\n\t\t\tdata = open(tempfile_path, \"r\")\n\t\t\tsoup = BeautifulSoup(data, \"html.parser\")\n\t\t\twcpod = ScrapeCpod(soup)\n\t\t\twcpod.scrape_dialogues()\n\t\t\tdialogues = wcpod.get_dialogues()\n\t\t\tif len(dialogues) > 0:\n\t\t\t\tself.dictionary.add_sentences(dialogues)\n\n\t\t\tif \"Vocabulary\" not in lesson[\"not_available\"]:\n\t\t\t\twords = wcpod.scrape_lesson_vocab()\n\t\t\t\tnon_dup_words = [\n\t\t\t\t\tword.chinese\n\t\t\t\t\tfor word in words\n\t\t\t\t\tif not self.dictionary.check_for_dup(word, False)\n\t\t\t\t]\n\t\t\t\tword_expansion.extend(non_dup_words)\n\t\t\tif \"Expansion\" not in lesson[\"not_available\"]:\n\t\t\t\twcpod.scrape_expansion()\n\t\t\t\texpand = wcpod.get_expansion()\n\t\t\t\tself.dictionary.add_sentences(expand)\n\n\t\t\tif \"Grammar\" not in lesson[\"not_available\"]:\n\t\t\t\twcpod.scrape_lesson_grammar()\n\t\t\t\tgrammar = wcpod.get_grammar()\n\t\t\t\tself.dictionary.add_sentences(grammar)\n\n\t\t\ttry:\n\t\t\t\tos.remove(tempfile_path)\n\t\t\texcept OSError as error:\n\t\t\t\traise RuntimeError(error)\n\t\twb.close()\n\t\tsent_csv_path = WriteFile.write_to_csv(\n\t\t\t\"./out/lessons.csv\", self.dictionary.get_all_sentences()\n\t\t)\n\t\tsent_audio = TerminalOptions(\n\t\t\t[\"Yes\", \"No\"], \"Do You Download the Audio for the Sentences?\"\n\t\t).get_selected()\n\t\tif sent_audio == \"Yes\":\n\t\t\tAudio(sent_csv_path, \"sentences\")\n\t\tif len(word_expansion) > 0:\n\t\t\tsave_exp_vocab = TerminalOptions(\n\t\t\t\t[\"Yes\", \"No\"], \"Do you want add the Lesson Vocabs?\"\n\t\t\t).get_selected()\n\t\t\tif save_exp_vocab == \"Yes\":\n\t\t\t\tkeepAll = TerminalOptions(\n\t\t\t\t\t[\"Yes\", \"No\"], \"Keep All of the Words?\"\n\t\t\t\t).get_selected()\n\t\t\t\tif keepAll == \"No\":\n\t\t\t\t\tword_list = TerminalOptions(\n\t\t\t\t\t\t[word for word in word_expansion],\n\t\t\t\t\t\t\"Select the Words You Want to Keep:\",\n\t\t\t\t\t\tTrue,\n\t\t\t\t\t).get_selected()\n\t\t\t\t\tWordScrape(self.session, self.dictionary, word_list)\n\t\t\t\telse:\n\t\t\t\t\tWordScrape(self.session, self.dictionary, word_expansion)\n", "description": null, "category": "webscraping", "imports": ["import os", "from audio import Audio", "from bs4 import BeautifulSoup", "from cpod_scrape import ScrapeCpod", "from terminal_opts import TerminalOptions", "from web_scrape import WebScrape", "from word_scrape import WordScrape", "from write_file import WriteFile"]}], [], [{"term": "class", "name": "Migration", "data": "class Migration(migrations.Migration):\n\n\tdependencies = [\n\t\t('webscrape', '0004_trendsearch'),\n\t]\n\n\toperations = [\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='trendsearch',\n\t\t\told_name='linK',\n\t\t\tnew_name='link',\n\t\t),\n\t]\n", "description": null, "category": "webscraping", "imports": ["from django.db import migrations"]}], [], [], [{"term": "def", "name": "initiateDriver", "data": "def initiateDriver():\r\n\twebdriver_path = 'C:\\\\IEDriverServer.exe'\r\n\tdriver = webdriver.Ie(webdriver_path)\r\n\tdriver.maximize_window()\r\n\treturn driver\r\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium.webdriver.support.ui import WebDriverWait\r", "from selenium.webdriver.support import expected_conditions as EC\r", "from selenium.common.exceptions import TimeoutException\r", "from selenium.webdriver.common.by import By\r", "from bs4 import BeautifulSoup\r", "import numpy as np\r", "import pandas as pd\r", "import time\r", "import requests\r"]}, {"term": "def", "name": "login", "data": "def login(driver, user, pwd):\r\n\r\n\t# Open IE browser and login to e-portal\r\n\tdriver.get(\"http://***\")\r\n\tid = driver.find_element_by_name(\"textfield32\")\r\n\tpassword = driver.find_element_by_name(\"textfield33\")\r\n\tid.send_keys(user)\r\n\tpassword.send_keys(pwd)\r\n\tpassword.send_keys(Keys.RETURN)\r\n\r\n\t# Dismiss alert if any popup alert happens\r\n\ttry:\r\n\t\tWebDriverWait(driver, 2).until(EC.alert_is_present(), 'Timed out waiting for popup to appear')\r\n\t\tdriver.switch_to.alert.dismiss()\r\n\t\tprint(\"alert dismissed!\")\r\n\texcept TimeoutException:\r\n\t\tprint(\"no dismissed\")\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium.webdriver.support.ui import WebDriverWait\r", "from selenium.webdriver.support import expected_conditions as EC\r", "from selenium.common.exceptions import TimeoutException\r", "from selenium.webdriver.common.by import By\r", "from bs4 import BeautifulSoup\r", "import numpy as np\r", "import pandas as pd\r", "import time\r", "import requests\r"]}, {"term": "def", "name": "webScrape", "data": "def webScrape(driver, start, end):\r\n\tid = [ j + 1 for j in range(start, end)]\r\n\tcolumns = ['\u54e1\u5de5\u7de8\u78bc', '\u59d3\u540d', 'First Name', 'Last Name', \r\n\t\t\t   '\u8fa6\u516c\u5ba4\u96fb\u8a71', 'EMAIL', '\u90e8\u9580\u4ee3\u78bc', '\u90e8\u9580\u540d\u7a31', '\u8077\u7a31']\r\n\tdf = pd.DataFrame(np.zeros((end - start, 9)), columns=columns, index=id)\r\n\tfor i in range(start, end):\r\n\t\turl = \"http://***?ID=\" + str(i + 1)\r\n\t\treq = requests.get(url)\r\n\r\n\t\t# Check if url works\r\n\t\tif req.status_code == requests.codes.ok: #pylint: disable=no-member\r\n\t\t\ttry:\r\n\t\t\t\tdriver.get(url)\r\n\t\t\t\tsoup = BeautifulSoup(driver.page_source, \"html.parser\")\r\n\r\n\t\t\t# Invalid ID\r\n\t\t\texcept:\r\n\t\t\t\tWebDriverWait(driver, 2).until(EC.alert_is_present(), 'Timed out waiting for popup to appear')\r\n\t\t\t\tdriver.switch_to.alert.dismiss()\r\n\t\t\t\tdataRow = [\"InvalidID\"] * 9\r\n\t\t\t\tdf.iloc[i - start, :] = dataRow\r\n\t\t\telse:\r\n\t\t\t\tdata = soup.find_all(\"nobr\")\r\n\t\t\t\tif len(data) == 18:\r\n\t\t\t\t\tdataRow = [data[2 * j + 1].text for j in range(9)]\r\n\t\t\t\t\tdf.iloc[i - start, :] = dataRow\r\n\r\n\t\t\t\t# No data\r\n\t\t\t\telse:\r\n\t\t\t\t\tdataRow = [None] * 9\r\n\t\t\t\t\tdf.iloc[i - start, :] = dataRow\r\n\t\telse:\r\n\t\t\tprint(\"url invalid for id \" + str(i + 1))\r\n\r\n\tdf.to_excel(\"Employee List.xlsx\")\r\n\r\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver\r", "from selenium.webdriver.common.keys import Keys\r", "from selenium.webdriver.support.ui import WebDriverWait\r", "from selenium.webdriver.support import expected_conditions as EC\r", "from selenium.common.exceptions import TimeoutException\r", "from selenium.webdriver.common.by import By\r", "from bs4 import BeautifulSoup\r", "import numpy as np\r", "import pandas as pd\r", "import time\r", "import requests\r"]}], [], [], [{"term": "class", "name": "ToScrapeCSSSpider", "data": "class ToScrapeCSSSpider(scrapy.Spider):\r\n\tname = \"toscrape-css\"\r\n\tstart_urls = [\r\n\t\t'http://quotes.toscrape.com/',\r\n\t]\r\n\r\n\tdef parse(self, response):\r\n\t\titems = WebscrapeItem()\r\n\t\tfor quote in response.css(\"div.quote\"):\r\n\r\n\t\t\t\titems['text'] = quote.css('span.text::text').extract()\r\n\t\t\t\titems['author']= quote.css('small.author::text').extract()\r\n\t\t\t\titems['tags'] = quote.css('div.tags > a.tag::text').extract()\r\n\r\n\t\t\t\tyield items\r\n\r\n\t\tnext_page_url = response.css(\"li.next > a::attr(href)\").extract_first()\r\n\t\tif next_page_url is not None:\r\n\t\t\tyield scrapy.Request(response.urljoin(next_page_url))\r\n", "description": null, "category": "webscraping", "imports": ["import scrapy\r", "from ..items import WebscrapeItem\r"]}], [{"term": "def", "name": "webscrape_url_apiguide", "data": "def webscrape_url_apiguide(savepath=''):\n\t'''Webscraper function for getting updated api statics from guide url and save to file.'''\n\n", "description": null, "category": "webscraping", "imports": []}], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape:\n\n\tobjects = []\n\tdef __init__(self, url, header):\n\t\tself.url = url\n\t\tself.response = requests.get(self.url, headers = header)\n\t\tself.objects.append(self)\n\t\tself.scraper = bs4.BeautifulSoup(self.response.text, features=\"html.parser\")\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests, bs4, re "]}], [], [{"term": "def", "name": "home", "data": "def home():\n\timport webscrape as ws\n\tdata = ws.scrapeData()\n\treturn render_template('index.html', out=data)\n\n", "description": null, "category": "webscraping", "imports": ["import numpy as np", "import pymongo", "from flask import Flask, jsonify, render_template,redirect,url_for,request,make_response", "\timport webscrape as ws"]}], [{"term": "def", "name": "sample_responses", "data": "def sample_responses(input_text):\r\n\tuser_message = str(input_text).lower()\r\n\t\r\n\tif user_message in( 'hello', 'hi'):\r\n\t\treturn \"hello there!\"\r\n\tif user_message in( 'who are you?'):\r\n\t\treturn \"I am webscrape bot!\"\r\n\t\t\r\n\tif user_message in( 'time'):\r\n\t\tnow = datetime.now()\r\n\t\tdate_time = now.strftime(\"%d/%m/%y, %H:%M:%S \")\r\n\t\treturn date_time\r\n\treturn 'I dont understand what you just said'\r\n", "description": null, "category": "webscraping", "imports": ["from datetime import datetime\r"]}], [{"term": "def", "name": "main", "data": "def main():\n\tdh.wsgToJson()\n\tdh.wsmToJson()\n\n", "description": null, "category": "webscraping", "imports": ["import webscraper.webscrapeGymgrossisten as wsg", "import json", "import data_handling.dataHandler as dh"]}], [{"term": "def", "name": "webscrape_centauro", "data": "def webscrape_centauro():\r\n\tglobal log\r\n\tselector = '_3teszy'\r\n\tsources_list = []\r\n\tbrowser = webdriver.Chrome(ChromeDriverManager().install())\r\n\tbrowser.get(url)\r\n\tbrowser.fullscreen_window()\r\n\r\n\ttry:\r\n\t\t# wait for data to be loaded\r\n\t\tWebDriverWait(browser, delay).until(\r\n\t\t\tEC.presence_of_element_located((By.CLASS_NAME, selector))\r\n\t\t)\r\n\t\t# list of colors of item\r\n\t\tbtns = browser.find_elements(By.CLASS_NAME, 'box-color')\r\n\r\n\t\t# iterate through all colors and get html\r\n\t\tfor btn in btns:\r\n\t\t\t# WebDriverWait(browser, delay).until(\r\n\t\t\t#\t EC.element_to_be_clickable((By.CLASS_NAME, 'box-color')))\r\n\r\n\t\t\tsleep(3)\r\n\t\t\tbtn.click()\r\n\t\t\tsources_list.append(browser.page_source)\r\n\r\n\texcept TimeoutException:\r\n\t\tlog = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Link not working for {url}\\n'\r\n\t\tprint(\"Loading took too much time!\")\r\n\r\n\tfinally:\r\n\t\tsleep(3)\r\n\t\tbrowser.quit()\r\n\r\n\tprices_log = pd.read_excel('itens.xlsx')\r\n\r\n\tfor source in sources_list:\r\n\r\n\t\t# scrape properties from html\r\n\t\tsoup = BeautifulSoup(source, features=\"lxml\")  # turns into BeatifulSoup object\r\n\t\tspan_tags = soup.find('span', {'class': '_3teszy'})\r\n\t\tcurr_price = float(span_tags.text.strip('R$ ').replace(',', '.'))\r\n\t\titem_name = soup.find('h1', {'class': '_gjoabl'}).text\r\n\t\tsizes = [x.text for x in soup.find_all('div', {'class': '_1uax8x0'})]\r\n\t\tmy_size = sum([x in sizes for x in target_sizes]) > 0\r\n\t\tcurr_color = soup.find('h3', {'class': '_3lyjer color-selected-label'}).text[5:]\r\n\t\tcurr_datetime = datetime.now()\r\n\t\tmail_check = False\r\n\r\n\t\t# check if item and color is already in the file\r\n\t\tif len(prices_log.query('name == @item_name and color == @curr_color').index) == 0:\r\n\t\t\tprice_dict = {'name': item_name, 'my_size': my_size, 'color': curr_color,\r\n\t\t\t\t\t\t  'last_price': curr_price, 'date_last_price': curr_datetime,\r\n\t\t\t\t\t\t  'high_price': curr_price, 'date_high_price': curr_datetime,\r\n\t\t\t\t\t\t  'low_price': curr_price, 'date_low_price': curr_datetime,\r\n\t\t\t\t\t\t  'store': 'centauro'\r\n\t\t\t\t\t\t  }\r\n\t\t\tprices_log = prices_log.append(price_dict, ignore_index=True)\r\n\t\t\tlog = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Added item {item_name} - {curr_color}\\n'\r\n\t\t\tcontinue\r\n\r\n\t\t# fill values\r\n\t\tcurrent_item = (prices_log['name'] == item_name) & (prices_log['color'] == curr_color)\r\n\t\tposition = prices_log[current_item].index\r\n\t\tprices_log.at[position[0], 'last_price'] = curr_price\r\n\t\tprices_log.at[position[0], 'date_last_price'] = curr_datetime\r\n\t\tprices_log.at[position[0], 'my_size'] = my_size\r\n\r\n\t\t# checks for new low price\r\n\t\tif target_price >= curr_price:\r\n\t\t\tif not mail_check:\r\n\t\t\t\ttry:\r\n\t\t\t\t\tsend_mail(url, item_name, curr_price, my_size, curr_color, curr_store)\r\n\t\t\t\t\tmail_check = True\r\n\r\n\t\t\t\texcept Exception as erro:\r\n\t\t\t\t\tprint(f'Error {erro.__class__}')\r\n\r\n\t\tif float(prices_log.loc[current_item, 'low_price']) > curr_price:\r\n\r\n\t\t\tprices_log.loc[current_item, 'low_price'] = curr_price\r\n\t\t\tprices_log.loc[current_item, 'date_low_price'] = curr_datetime\r\n\r\n\t\t\tlog = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - New low price on item {item_name} - {curr_color}\\n'\r\n\r\n\t\t\tif not mail_check:\r\n\t\t\t\ttry:\r\n\t\t\t\t\tsend_mail(url, item_name, curr_price, my_size, curr_color, curr_store)\r\n\t\t\t\t\tmail_check = True\r\n\r\n\t\t\t\texcept Exception as erro:\r\n\t\t\t\t\tprint(f'Error {erro.__class__}')\r\n\r\n\t\t\tprint('alert new low price')\r\n\r\n\t\telif float(prices_log.loc[current_item, 'high_price']) < curr_price:\r\n\r\n\t\t\tprices_log.loc[current_item, 'high_price'] = curr_price\r\n\t\t\tprices_log.loc[current_item, 'date_high_price'] = curr_datetime\r\n\r\n\tprices_log.to_excel('itens.xlsx', index=False)\r\n\r\n", "description": null, "category": "webscraping", "imports": ["# imports\r", "from bs4 import BeautifulSoup\r", "from selenium import webdriver\r", "from selenium.webdriver.support.ui import WebDriverWait\r", "from selenium.webdriver.support import expected_conditions as EC\r", "from selenium.webdriver.common.by import By\r", "from selenium.common.exceptions import TimeoutException\r", "from webdriver_manager.chrome import ChromeDriverManager\r", "from datetime import datetime\r", "import pandas as pd\r", "from time import sleep\r", "import smtplib\r", "import ssl\r", "from email.mime.text import MIMEText\r", "from email.mime.multipart import MIMEMultipart\r", "from email.utils import formataddr\r"]}, {"term": "def", "name": "webscrape_amazon", "data": "def webscrape_amazon():\r\n\t\"\"\"\r\n\tScrapes the price, item name, color/model and checks if prefered size is available. Register current price,\r\n\tand size check to excel file \"itens\" in the same directory.\r\n\r\n\tCompares current price with lower price registered. Alerts in case of new low price.\r\n\t\"\"\"\r\n\r\n\tglobal log\r\n\tselector = 'productTitle'\r\n\tbrowser = webdriver.Chrome(ChromeDriverManager().install())\r\n\tbrowser.get(url)\r\n\tbrowser.fullscreen_window()\r\n\r\n\ttry:\r\n\t\t# wait for data to be loaded\r\n\t\tWebDriverWait(browser, delay).until(\r\n\t\t\tEC.presence_of_element_located((By.ID, selector))\r\n\t\t)\r\n\t\tsleep(3)\r\n\t\tsource = browser.page_source\r\n\r\n\texcept TimeoutException:\r\n\t\tlog = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Link not working for {url}\\n'\r\n\t\tprint(\"Loading took too much time!\")\r\n\t\treturn\r\n\r\n\tfinally:\r\n\t\tsleep(1)\r\n\t\tbrowser.quit()\r\n\r\n\tprices_log = pd.read_excel('itens.xlsx')\r\n\r\n\t# scrape properties from html\r\n\tsoup = BeautifulSoup(source, features=\"lxml\")  # turns into BeatifulSoup object\r\n\ttry:\r\n\t\tspan_tags = soup.find('div', {'id': 'corePrice_desktop'}).select('span[class*=\"apexPriceToPay\"]')[0].find('span', {\"aria-hidden\": \"true\"})\r\n\texcept IndexError:\r\n\t\titem_name = soup.find('span', {'id': 'productTitle'}).text.strip()\r\n\t\tlog = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Item {item_name} is unavailable on {curr_store.capitalize()}\\n'\r\n\t\treturn\r\n\texcept AttributeError:\r\n\t\titem_name = soup.find('span', {'id': 'productTitle'}).text.strip()\r\n\t\tlog = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Couldnt locate price for item {item_name} on {curr_store.capitalize()}\\n'\r\n\t\treturn\r\n\r\n\tcurr_price = float(span_tags.text.strip('R$ ').replace(',', '.'))\r\n\titem_name = soup.find('span', {'id': 'productTitle'}).text.strip()\r\n\t# sizes = [x.text for x in soup.find_all('div', {'class': '_1uax8x0'})]\r\n\tmy_size = True\r\n\ttry:\r\n\t\tcurr_color = soup.find('div', {'id': 'variation_color_name'}).find('span', {'class': 'selection'}).text\r\n\texcept AttributeError:\r\n\t\tcurr_color = ' '\r\n\r\n\tcurr_datetime = datetime.now()\r\n\tmail_check = False\r\n\r\n\t# check if item and color is already in the file\r\n\tif len(prices_log.query('name == @item_name and color == @curr_color').index) == 0:\r\n\t\tprice_dict = {'name': item_name, 'my_size': my_size, 'color': curr_color,\r\n\t\t\t\t\t  'last_price': curr_price, 'date_last_price': curr_datetime,\r\n\t\t\t\t\t  'high_price': curr_price, 'date_high_price': curr_datetime,\r\n\t\t\t\t\t  'low_price': curr_price, 'date_low_price': curr_datetime,\r\n\t\t\t\t\t  'store': 'amazon'\r\n\t\t\t\t\t  }\r\n\t\tprices_log = prices_log.append(price_dict, ignore_index=True)\r\n\t\tlog = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Added item {item_name} - {curr_color}\\n'\r\n\t\tprices_log.to_excel('itens.xlsx', index=False)\r\n\t\treturn\r\n\r\n\t# fill values\r\n\tcurrent_item = (prices_log['name'] == item_name) & (prices_log['color'] == curr_color)\r\n\tposition = prices_log[current_item].index\r\n\tprices_log.at[position[0], 'last_price'] = curr_price\r\n\tprices_log.at[position[0], 'date_last_price'] = curr_datetime\r\n\tprices_log.at[position[0], 'my_size'] = my_size\r\n\r\n\t# checks for new low price\r\n\tif target_price >= curr_price:\r\n\t\tif not mail_check:\r\n\t\t\ttry:\r\n\t\t\t\tsend_mail(url, item_name, curr_price, my_size, curr_color, curr_store)\r\n\t\t\t\tmail_check = True\r\n\r\n\t\t\texcept Exception as erro:\r\n\t\t\t\tprint(f'Error {erro.__class__}')\r\n\r\n\tif float(prices_log.loc[current_item, 'low_price']) > curr_price:\r\n\r\n\t\tprices_log.loc[current_item, 'low_price'] = curr_price\r\n\t\tprices_log.loc[current_item, 'date_low_price'] = curr_datetime\r\n\r\n\t\tlog = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - New low price on item {item_name} - {curr_color}\\n'\r\n\r\n\t\tif not mail_check:\r\n\t\t\ttry:\r\n\t\t\t\tsend_mail(url, item_name, curr_price, my_size, curr_color, curr_store)\r\n\t\t\t\tmail_check = True\r\n\r\n\t\t\texcept Exception as erro:\r\n\t\t\t\tprint(f'Error {erro.__class__}')\r\n\r\n\t\tprint('alert new low price')\r\n\r\n\telif float(prices_log.loc[current_item, 'high_price']) < curr_price:\r\n\r\n\t\tprices_log.loc[current_item, 'high_price'] = curr_price\r\n\t\tprices_log.loc[current_item, 'date_high_price'] = curr_datetime\r\n\r\n\tprices_log.to_excel('itens.xlsx', index=False)\r\n\r\n", "description": "\r\n\tScrapes the price, item name, color/model and checks if prefered size is available. Register current price,\r\n\tand size check to excel file \"itens\" in the same directory.\r\n\r\n\tCompares current price with lower price registered. Alerts in case of new low price.\r\n\t", "category": "webscraping", "imports": ["# imports\r", "from bs4 import BeautifulSoup\r", "from selenium import webdriver\r", "from selenium.webdriver.support.ui import WebDriverWait\r", "from selenium.webdriver.support import expected_conditions as EC\r", "from selenium.webdriver.common.by import By\r", "from selenium.common.exceptions import TimeoutException\r", "from webdriver_manager.chrome import ChromeDriverManager\r", "from datetime import datetime\r", "import pandas as pd\r", "from time import sleep\r", "import smtplib\r", "import ssl\r", "from email.mime.text import MIMEText\r", "from email.mime.multipart import MIMEMultipart\r", "from email.utils import formataddr\r"]}, {"term": "def", "name": "send_mail", "data": "def send_mail(link, product, price, size, color, store):\r\n\t\"\"\"\r\n\tSends an email through gmail account alerting of a price found on item.\r\n\t\"\"\"\r\n\tport = 465  # For SSL\r\n\tpassword = \"password\"  # generated app password\r\n\r\n\tsender_email = \"email@gmail.com\"\r\n\treceiver_email = \"email@provider.com\"\r\n\r\n\tmessage = MIMEMultipart(\"alternative\")\r\n\tmessage[\"Subject\"] = f\"Alert for {product} on {store.capitalize()}\"\r\n\tmessage[\"From\"] = formataddr(('Your WebScraper', sender_email))\r\n\tmessage[\"To\"] = receiver_email\r\n\r\n\t# send both HTML and plain text version of the same email\r\n\ttext = f\"\"\"\\\r\n\t\tHi,\r\n\t\tI found the item {product} {color} for R$ {price}! Your size {\"is\" if size else \"isn't\"} available\r\n\t\tClick the link below for more information\r\n\t\t{link}\r\n\t\t\"\"\"\r\n\r\n\tmail_html = f\"\"\"\\\r\n\t\t\r\n", "description": "\r\n\tSends an email through gmail account alerting of a price found on item.\r\n\t", "category": "webscraping", "imports": ["# imports\r", "from bs4 import BeautifulSoup\r", "from selenium import webdriver\r", "from selenium.webdriver.support.ui import WebDriverWait\r", "from selenium.webdriver.support import expected_conditions as EC\r", "from selenium.webdriver.common.by import By\r", "from selenium.common.exceptions import TimeoutException\r", "from webdriver_manager.chrome import ChromeDriverManager\r", "from datetime import datetime\r", "import pandas as pd\r", "from time import sleep\r", "import smtplib\r", "import ssl\r", "from email.mime.text import MIMEText\r", "from email.mime.multipart import MIMEMultipart\r", "from email.utils import formataddr\r"]}], [{"term": "def", "name": "Intro", "data": "def Intro():\n\tprint(\" \")\n\tprint(\">>>>>>>>>>>>> 0-60 Time import by Kordelle Walker <<<<<<<<<<<<\")\n\tprint(\"This Script webscrapes 0-60 times for every make and model car and\")\n\tprint(\"sends the output to local directory in multiple .csv format files\")\n\twhile True:\n\t   answer = input('Do you want to create/update all csv files?: (y/n)')\n\t   if answer.lower().startswith(\"y\"):\n\t\t  print(\"Creating/Updating csv files now...\")\n\t\t  break\n\t   elif answer.lower().startswith(\"n\"):\n\t\t  print(\"Creation/Update Cancelled\")\n\t\t  exit()\n", "description": null, "category": "webscraping", "imports": ["from operator import ilshift", "from sqlite3 import Date", "from bs4 import BeautifulSoup", "import requests, time, re, csv, S3_Upload", "from sqlalchemy import null", "\tprint(\">>>>>>>>>>>>> 0-60 Time import by Kordelle Walker <<<<<<<<<<<<\")"]}, {"term": "def", "name": "time_func", "data": "def time_func():\n\t   for j in list:\n\t\t\tjam = j.find_all('span', \"stats__list__accordion__body__stat__top__right__stat-time\")\n\t\t\tfire = j.find_all('div', \"stats__list__accordion__body__stat__top__title\")\n\t\t\tif not jam:\n\t\t\t\tcontinue\n\t\t\tmash = [jam[0].text.replace(\"[\",\"\")]\n\t\t\tseparator = \", \"\n\t\t\thot = separator.join(mash)\n\t\t\tpotato = [fire[0].text.strip()]\n\t\t\tchocolate = separator.join(potato)\n\t\t\tvanilla = separator.join(chocolate.split(\" \", maxsplit=1))\n\t\t\t#cheese = [jam[1].text]\n\t\t\tdataset = [hot,vanilla[0:4],car[i].upper(),vanilla[6:].upper()]\n\t\t\twriter.writerow(dataset)\n", "description": null, "category": "webscraping", "imports": ["from operator import ilshift", "from sqlite3 import Date", "from bs4 import BeautifulSoup", "import requests, time, re, csv, S3_Upload", "from sqlalchemy import null", "\tprint(\">>>>>>>>>>>>> 0-60 Time import by Kordelle Walker <<<<<<<<<<<<\")"]}], [{"term": "class", "name": "Webscrape", "data": "class Webscrape():\n\t\n\tdef __init__(self):\n\t\tself.unrated = {\n\t\t\t'Not Rated',\n\t\t\t'Unrated',\n\t\t}\n\n\t# year, imdb rating, intended audience, runtime\n\tdef get_info(self, imdb_num):\n\t\turl = f'https://www.imdb.com/title/tt{imdb_num}/'\n\t\tpage = requests.get(url) # Replace with other imdb link\n\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\n\t\tli = [x.get_text() for x in soup.find_all(class_='ipc-inline-list__item')]\n\t\tx = 0\n\t\twhile not li[x][-4:].isnumeric():\n\t\t\tx += 1\n\t\t# shift = -1 if li[2][-1].isnumeric() else 0 # some movies don't have trivia\n\t\thas_audience = 1 if not li[x+1][0].isnumeric() else 0 # some movies don't have an \"Unrated\" option...\n\t\t'''\n\t\t0 Cast & crew\n\t\t1 User reviews\n\t\t2 Trivia\n\t\t3 Episode aired Apr 6, 1997\n\t\t4 PGPG\n\t\t5 1h 47m\n\t\t'''\n\t\t# print(li[:6])\n\t\t# print(f'date: {x}', f'missing_audience: {has_audience}')\n\t\tyear = li[x][-4:]\n\t\tif has_audience: audience = li[x+1][:len(li[x+1])//2]\n\t\telse: audience = audience = 'Unrated'\n\t\truntime = self.string_to_minutes(li[has_audience + x + 1])\n\n\t\timdb_rating = soup.find_all(class_='sc-7ab21ed2-1 jGRxWM')[0].get_text() # rating from imdb\n\t\t\n\t\t# print(year, imdb_rating, audience, runtime)\n\t\treturn year, imdb_rating, self.unrated_helper(audience), runtime\n\n\tdef string_to_minutes(self, timestring):\n\t\t# 1h 48m\n\t\t# 2h 03,\n\t\thours_mins = timestring.strip().split()\n\t\tif (len(hours_mins) == 2) : return int(hours_mins[0][-2]) * 60 + int(hours_mins[1][-3:-1])\n\t\tif (hours_mins[0][-1] == 'm'): return int(hours_mins[0][:-1]) # if only minutes runtime\n\t\treturn int(hours_mins[0][-2]) * 60\n\n\tdef unrated_helper(self, s):\n\t\tif s in self.unrated:\n\t\t\treturn 'Unrated'\n\t\treturn s\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup"]}], [], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape:\n\t\n\tdef get_converted_value(value_list:list[list[str,str,int]])->list[int]:\n\t\t\"\"\"Get the converted values for passed required conversions in a list\n\t\tArgs : list containing the conversion details \n\t\treturns conversion values for the given list\"\"\"\n\n\t\tcf.browser_activate()\n\t\tthis_list = []\n\t\tfor val_list in value_list:\n\t\t\thelium.go_to(f\"https://www.xe.com/currencyconverter/convert/?Amount={val_list[2]}&From={val_list[0]}&To={val_list[1]}\")\n\t\t\tvalue = cf.browser_locate_element_h(selector=r\"//*[@id='__next']/div[2]/div[2]/section/div[2]/div/main/form/div[2]/div[1]/p[2]\",get_text = True)\n\t\t\tthis_list.append(round(locale.atof(value.split()[0]),3))\n\t\tcf.browser_quit_h()\n\t\treturn this_list\n\t\n\t\n", "description": "Get the converted values for passed required conversions in a list\n\t\tArgs : list containing the conversion details \n\t\treturns conversion values for the given list", "category": "webscraping", "imports": ["import ClointFusion as cf", "import helium", "import locale", "import time", "import os "]}, {"term": "class", "name": "classExcelSheet:", "data": "class ExcelSheet:\n   \n\tdef __init__(self,pathtofolder:str,filename:str,sheet_name:str,pathtosourcefolder=None)->None:\n\t\t\"\"\"Initialising function to create an excel doc at the given location (which acts as an object of the class throughout)\n\t\t\tif source folder path is mentioned then it will be copied into the newly created excel document\n\t\t\tParams : pathtofolder : Path where the excel sheet has to be created\n\t\t\t\t\tfilename : filename of the excel document\n\t\t\t\t\tsheet_name :Sheet name of the sheet created in excel sheets by default\n\t\t\t\t\tpathtosourcefolder :Path from which the excel sheet is to be copied (None by default)\n\n\t\t\tReturns : None\n\t\t\t\t\t\"\"\"\n\t\tself.path = cf.os.path.join(pathtofolder, filename)\n\t\tself.name = sheet_name\n  \n\t\tcf.excel_create_excel_file_in_given_folder(fullPathToTheFolder=pathtofolder,excelFileName=filename,sheet_name=sheet_name)\n\t\tprint(f\"New Excel Sheet created at {pathtofolder} with name {self.name}\")\n\t\tif pathtosourcefolder:\n\t\t\tcf.shutil.copyfile(pathtosourcefolder,self.path)\n\n\n\tdef insert_data(self,data_list:list)->None:\n\t\t\"\"\"Inserts the given data in dictionary format into the excel sheet\n\t\t\tParams : data list\n\t\t\tReturns : None\"\"\"\n\t\trow_count,_ = cf.excel_get_row_column_count(excel_path = self.path,sheet_name = self.name,header = 0)\n\t\n\t\tif row_count==1:\n\t\t\tfor i in range(len(data_list)):\n\t\t\t\tcf.excel_set_single_cell(excel_path = self.path, sheet_name = self.name,header = 0, columnName = \"\",cellNumber = i,setText = data_list[i])\n", "description": "Initialising function to create an excel doc at the given location (which acts as an object of the class throughout)\n\t\t\tif source folder path is mentioned then it will be copied into the newly created excel document\n\t\t\tParams : pathtofolder : Path where the excel sheet has to be created\n\t\t\t\t\tfilename : filename of the excel document\n\t\t\t\t\tsheet_name :Sheet name of the sheet created in excel sheets by default\n\t\t\t\t\tpathtosourcefolder :Path from which the excel sheet is to be copied (None by default)\n\n\t\t\tReturns : None\n\t\t\t\t\t", "category": "webscraping", "imports": ["import ClointFusion as cf", "import helium", "import locale", "import time", "import os "]}], [], [], [], [], [{"term": "def", "name": "__init__", "data": "  def __init__(self, gpp_type=DEUCE_GARAGE_GPP, workout_dates=['2021-12-01', '2021-12-02']):\n\tself.gpp_type = gpp_type\n\tself.workout_dates = workout_dates\n\tself.wod_urls = []\n\tself.get_wod_url()\n\tself.web_data = WebData()\n\t#self.cycle_wods_json = self._web_data.cycle_wods_json()\n", "description": null, "category": "webscraping", "imports": ["# import urllib.request", "# from urllib.request import urlopen", "from html.parser import HTMLParser", "#import re", "import requests", "from requests_html import HTMLSession", "from requests_html import HTML", "import pandas as pd"]}, {"term": "def", "name": "add_wod_url", "data": "  def add_wod_url(self, a_href: str):\n\tself.wod_urls.append(a_href)\n", "description": null, "category": "webscraping", "imports": ["# import urllib.request", "# from urllib.request import urlopen", "from html.parser import HTMLParser", "#import re", "import requests", "from requests_html import HTMLSession", "from requests_html import HTML", "import pandas as pd"]}, {"term": "def", "name": "get_wod_url", "data": "  def get_wod_url(self) -> None:\n\t# TODO: get this working for singleton then loop it => for wod_date in workout_dates:\n\twod_date = self.workout_dates[0]\n\twod_url_base = Deuce.DEUCE_URL + wod_date\n\tself.web_data = WebData(wod_url_base)\n\tobj_html = self.web_data.html\n\tsel_url = obj_html.xpath(Deuce.A_BLOG_XPATH)\n\t#list_wod_links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", xhtml)\n\twod_url = sel_url[0].links.pop()\n\t# for url in list_wod_links:\n\t#\t if wod_url_base in url:\n\t#\t   wod_url = url\n\t#\t   break\n\t#print(\"wod_url => \", wod_url)\n\t#a_href =  url_get_contents(wod_url[0]).decode('utf-8')\n\tself.add_wod_url(wod_url)\n", "description": null, "category": "webscraping", "imports": ["# import urllib.request", "# from urllib.request import urlopen", "from html.parser import HTMLParser", "#import re", "import requests", "from requests_html import HTMLSession", "from requests_html import HTML", "import pandas as pd"]}, {"term": "class", "name": "classWebData:", "data": "class WebData:\n\t\"\"\" This class serves as a storage mechanism for an HTTPResponse object data decoded to utf-8 string\"\"\"\t\n\tdef __init__(self, url: str = ''):\n\t\tself.url = url\n\t\tif url != '':\n\t\t  self.html: HTML = self.webscrape_html_data(self.url)\n\n\tdef webscrape_html_data(self, url) -> HTML:\n\t\t\"\"\" Opens a website and read its binary contents (HTTP Response Body)\"\"\"   \n\t\ttry:\n\t\t\tsession = HTMLSession()\n\t\t\traw_html_data = session.get(url)\t \n\t\t\tclean_html_data = HTML(html=replace_chars(raw_html_data.text))\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\tprint(e)\n\t\treturn clean_html_data\n", "description": " This class serves as a storage mechanism for an HTTPResponse object data decoded to utf-8 string", "category": "webscraping", "imports": ["# import urllib.request", "# from urllib.request import urlopen", "from html.parser import HTMLParser", "#import re", "import requests", "from requests_html import HTMLSession", "from requests_html import HTML", "import pandas as pd"]}, {"term": "class", "name": "HTMLDeuceParser", "data": "class HTMLDeuceParser(HTMLParser):\n\t\"\"\" This class serves as a html Deuce GPP parser. It extends HTMLParser and is able to parse div\n\ttags containing class=\"wod_block\" which you feed in. You can access the result per .wod_table field.\n\t\"\"\"\n\tdef __init__(self, html: HTML):\n\t\tself.recording = False\n\t\tself.html = html\n\t\tself.wodblock = self.html.xpath(Deuce.DIV_WODBLOCK_XPATH) \n\t\tself.tag = ''\n\t\tself.wod_table = []\n\t\t#self.convert_charrefs = False\n\t\t# initialize the base class\n\t\tHTMLParser.__init__(self)\t\t \n\n\tdef handle_starttag(self, tag, attrs):\t  \n\t\tif tag == 'div':\n\t\t\tself.tag = 'th'\n\t\t\t# for value in attrs:\n\t\t\t\t# print(value)\n\t\t\t\t# print(\"Encountered the beginning of a %s tag\" % tag)\n\t\t\tself.recording = True \n\t\telif tag == 'h2':\n\t\t\tself.tag = 'th'\n\t\t\t# for value in attrs:\n\t\t\t\t# print(value)\n\t\t\t\t# print(\"Encountered the beginning of a %s tag\" % tag)\n\t\t\tself.recording = True  \n\t\telif tag == 'p':\n\t\t\tself.tag = 'tr'\n\t\t\t# for value in attrs:\n\t\t\t\t# print(value)\n\t\t\t\t# print(\"Encountered the beginning of a %s tag\" % tag)\n\t\t\tself.recording = True\t\t\t\t \n\t\telif tag == 'span':\n\t\t\tself.tag = 'td'\n\t\t\t# for value in attrs:\n\t\t\t\t# print(value)\n\t\t\t\t# print(\"Encountered the beginning of a %s tag\" % tag)\n\t\t\tself.recording = True\t\t\t\t\t\t\t\n\t\telse:\n\t\t\tself.recording = True\n\t\t\treturn\n\n\tdef handle_endtag(self, tag):\n\t\tif tag == 'div' and self.recording == True:\n\t\t\tself.tag = 'th'\n\t\t\tself.recording = False \n\t\t\t# print(\"Encountered the end of a %s tag\" % tag)\n\t\telif tag == 'h2' and self.recording == True:\n\t\t\tself.tag = 'th'\n\t\t\tself.recording = False \n\t\t\t# print(\"Encountered the end of a %s tag\" % tag)\t \n\t\telif (tag == 'b' or tag == 'br') and self.recording == True:\n\t\t\tpass # FIXME: Does this break the table?\n\t\t\t# self.recording = False\n\t\telif tag == 'p' and self.recording == True:\n\t\t\tself.tag = 'tr'\n\t\t\tself.recording = False \n\t\t\t# print(\"Encountered the end of a %s tag\" % tag)\t   \n\t\telif tag == 'span' and self.recording == True:\n\t\t\tself.tag = 'td'\n\t\t\tself.recording = False \n\t\t\t# print(\"Encountered the end of a %s tag\" % tag)\t\t\t  \n\t\telse:\n\t\t\treturn # We don't want\t\t\t   \n\n\tdef handle_data(self, data):\n\t\tif self.recording == True:\n\t\t\tself.wod_table.append('<' + self.tag + '>' + data + '' + self.tag + '>')\n", "description": " This class serves as a html Deuce GPP parser. It extends HTMLParser and is able to parse div\n\ttags containing class=\"wod_block\" which you feed in. You can access the result per .wod_table field.\n\t", "category": "webscraping", "imports": ["# import urllib.request", "# from urllib.request import urlopen", "from html.parser import HTMLParser", "#import re", "import requests", "from requests_html import HTMLSession", "from requests_html import HTML", "import pandas as pd"]}, {"term": "def", "name": "replace_chars", "data": "def replace_chars(s: str) -> str:\n\ts = s.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\0', '')\n\treturn s  \n", "description": null, "category": "webscraping", "imports": ["# import urllib.request", "# from urllib.request import urlopen", "from html.parser import HTMLParser", "#import re", "import requests", "from requests_html import HTMLSession", "from requests_html import HTML", "import pandas as pd"]}, {"term": "def", "name": "create_table", "data": "def create_table(table_data: list) -> str:\n\ttable_data[0] = '' + str(table_data[0]) + ''\n\tgarage_idx = table_data.index('' + Deuce.DEUCE_GARAGE_GPP + '')\n\ttable_data[garage_idx] = '' + Deuce.DEUCE_GARAGE_GPP + ''\n\ttable_data[-1] = str(table_data[-1]) + ''\n\tcreated_table = ''.join(table_data)\n", "description": null, "category": "webscraping", "imports": ["# import urllib.request", "# from urllib.request import urlopen", "from html.parser import HTMLParser", "#import re", "import requests", "from requests_html import HTMLSession", "from requests_html import HTML", "import pandas as pd"]}], [], [], [], [], [], [], [], [], [{"term": "class", "name": "classEngine:", "data": "class Engine:\n\tdef __init__(self):\n\n\t\t# The basics of creating a window here... Nothing fancy\n\t\tself.window = Tk()\n\t\tself.window.minsize(1028, 720)\n\t\tself.window.title(\"Stock Trader\")\n\t\tself.window.configure(bg=\"#1c1c1c\")\n\t\tself.window.resizable(False, False) # Keeping it non-resizeable so it doesn't alter how the content will look\n\t\tself.currentTicker = 'SPY' # Default stock shown\n\t\tself.changed = False # To identify if the stock has been changed\n\t\tself.window.iconphoto(False, PhotoImage(file='favicon.png'))\n\t\t#\n\n\t\t# Search\n\t\tself.searchBar = searchBarGUI.SearchBarGUI(self.window,self)\n\n\t\t# Top Stocks\n\t\tself.topStocks = topStocksGUI.TopStocksGUI(self.window,self)\n\n\t\t# Graph -- Much of the graph config has to be done here since the variables have to be accessed in this file.\n\n\t\t\t# Colors\n\n\t\trcParams['axes.labelcolor'] = 'white'\n\t\trcParams['xtick.color'] = 'white'\n\t\trcParams['axes.titleweight'] = \"bold\"\n\t\trcParams['ytick.color'] = 'white'\n\t\trcParams['text.color'] = 'white'\n\n\t\t\t# Configs\n\t\tself.fig = plt.figure(figsize=(7, 3), dpi=100) # Basically a 700x300 i,age\n\t\tself.fig.patch.set_facecolor(\"#1c1c1c\")\n\t\tself.graph = self.fig.add_subplot(1, 1, 1)\n\t\tself.graph.set_facecolor('#454444')\n\n\t\t\t# Graph Gui\n\t\tself.graphGUI =  graphGUI.GraphGUI(self.window,self.fig,self.graph)\n\n\t\t# Stock info\n\t\tself.urlFinviz = \"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker) # URL FOR FINVIZ\n\t\tself.urlYahoo = 'https://ca.finance.yahoo.com/quote/{0}'.format(self.currentTicker) # URL FOR YAHOO FINANCE\n\n\t\tself.pageFinviz = webScrapeURL(self.urlFinviz) # webscrape the finviz page\n\t\tself.pageYahoo= webScrapeURL(self.urlYahoo) # webscrape the yahoo page\n\n\t\t# News feed GUI\n\t\tself.newsfeed = newsGUI.NewsfeedGUI(self.window, self.pageFinviz) # create the news feed\n\n\t\t# Stock info GUI\n\t\tself.stockInfo = stockinfoGUI.StockInfo(self.window, self.pageFinviz)\n\n\t# Modify Methods #\n\n\tdef changeTicker(self,ticker): # Self-explanatory\n\t\tself.currentTicker = ticker\n\t\tself.graphGUI.ticker = ticker\n\n\tdef updateEverything(self): # Re-Init stuff which changes based on stocks\n\t\tself.newsfeed.__init__(self.window,webScrapeURL(\"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker)))\n\t\tself.stockInfo.__init__(self.window,webScrapeURL(\"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker)))\n\n\t# Accessor Methods #\n\n\tdef getWindow(self):\n\t\treturn self.window\n\tdef getGraphClass(self):\n\t\treturn self.graphGUI\n\tdef getTicker(self):\n", "description": null, "category": "webscraping", "imports": ["from tkinter import *", "from GUI import topStocksGUI", "from GUI import searchBarGUI", "from GUI import graphGUI", "from GUI import newsGUI", "from GUI import stockinfoGUI", "from matplotlib import rcParams", "import matplotlib.pyplot as plt", "from webscraping import *"]}], [{"term": "def", "name": "process_image", "data": "def process_image(url, image_path, filename, item_id, description = \"Image\", ordering = 0):\n\tlsr._manage_rate()\n\twith open(image_path+filename, \"wb\") as f:\n\t\tf.write(requests.get(url).content)\n\turl = lsr.api_url+'Image.json'\n\t\n\tfiles = {'image': (filename, open(image_path + filename, 'rb'), 'image/jpeg')}\n\tpayload = {'data': '{\"description\": \"' + description + '\", \"ordering\": ' + ordering +', \"itemID\": ' + str(item_id) +'}'}\n\tr = requests.post(url, files=files, data=payload, headers=lsr.headers)\n\t#print(r.text)\n", "description": null, "category": "webscraping", "imports": ["import requests", "import os", "import json", "import sys", "from bs4 import BeautifulSoup", "import urllib.parse", "import mysql.connector", "import re", "import math", "from selenium import webdriver", "from lsretail import api_dev as lsretail", "#from lsecom import api as lsecom"]}, {"term": "def", "name": "round_up", "data": "def round_up(n, decimals=0):\n\tmultiplier = 10 ** decimals\n\treturn math.ceil(n * multiplier) / multiplier\n", "description": null, "category": "webscraping", "imports": ["import requests", "import os", "import json", "import sys", "from bs4 import BeautifulSoup", "import urllib.parse", "import mysql.connector", "import re", "import math", "from selenium import webdriver", "from lsretail import api_dev as lsretail", "#from lsecom import api as lsecom"]}, {"term": "def", "name": "mark_as_added", "data": "def mark_as_added(itemID, status='true'):\n\t#changes the local copy of the item to show that the item has been added to ecom incase the script starts overdef round_up(n, decimals=0):\n\tupdate_ecom = f\"UPDATE item SET publishToEcom = '{status}' WHERE itemID = {itemID}\"\n\tcursor.execute(update_ecom)\n", "description": null, "category": "webscraping", "imports": ["import requests", "import os", "import json", "import sys", "from bs4 import BeautifulSoup", "import urllib.parse", "import mysql.connector", "import re", "import math", "from selenium import webdriver", "from lsretail import api_dev as lsretail", "#from lsecom import api as lsecom"]}], [{"term": "def", "name": "striphtml", "data": "def striphtml(data):\n\tp = re.compile(r'')\n\treturn p.sub('', data)\n", "description": null, "category": "webscraping", "imports": ["import joblib", "import json ", "import random as rand", "import requests ", "import os", "import re", "from flask import Flask", "from geolib import geohash", "from google.cloud import firestore", "from label import LabelNews", "from newspaper import Article, ArticleException", "\timportant_tags = [\"city\",'datePublished','name',", "\t\tmoney = dict((k, item[k]) for k in important_tags)"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(city, key, url):\n\t\"\"\" Creates a request for news article that contain a city, gets the \n\tresponse in JSON format, and both adds and removes information based \n\ton what we need.\n\t\n\tRETURNS\n\t-------\n\tA list of headline items, each item is a dictionary.\n\t\"\"\"\n\t\n\t# header contains info to give us access to the site at url\n\theaders = {\"Ocp-Apim-Subscription-Key\": key}  \n\tparams = {\"q\":city,\"textDecorations\": True,\n\t\t\t  \"textFormat\":\"HTML\",\"freshness\":\"Week\"}  \n\tprint(\"pulling from Microsoft ...\\n\")\n\tresponse = requests.get(url, headers=headers, params=params)\n\tresponse.raise_for_status()\n\tresponse_json = response.json()\n\t# clean up response json - only keep 'value' tag\n\tprint(\"isolating 'value' data ...\\n\")\n\ttags_to_remove = ['_type','readLink','queryContext',\n\t\t\t\t\t  'totalEstimatedMatches','sort']\n\tfor tag in tags_to_remove:\n\t\tresponse_json.pop(tag)\n\t# articles are under the 'value\" key in response_json\n\tarticles = response_json['value']  \n\t# get summaries\n\tprint(\"retrieving summaries ...\\n\")\n\tfor item in articles :\n\t\ttry:\n\t\t\tnews = Article(item[\"url\"])\n\t\t\tnews.download()\n\t\t\tnews.parse()\n\t\t\tnews.nlp()\n\t\t\titem[\"summary\"] = news.summary\n\t\texcept ArticleException:\n\t\t\tprint(\"Forbidden Article\")\n\t\t\titem[\"summary\"] = \"no summary\" #-!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n\timportant_tags = [\"city\",'datePublished','name',\n\t\t\t\t\t  'organization','summary','url']\n\t# clean each article\n\tprint(\"\\ncollecting payload ...\\n\")\n\tpayload = []\n\tfor item in articles :\n\t\t# want to save organization name from 'provider' key, ... \n\t\t# ... then remove it and save under key 'organization'\n\t\torganization = item['provider'][0]['name']\n\t\titem.pop('provider')\n\t\titem['organization'] = organization\n\t\titem[\"city\"] = city\n\t\t# remove other unneccessary tags in headline name \n\t\titem[\"name\"] = striphtml(item['name'])\n\t\t\n\t\tmoney = dict((k, item[k]) for k in important_tags)\n\n\t\tpayload.append(money)\n\t\t\t\t\n", "description": " Creates a request for news article that contain a city, gets the \n\tresponse in JSON format, and both adds and removes information based \n\ton what we need.\n\t\n\tRETURNS\n\t-------\n\tA list of headline items, each item is a dictionary.\n\t", "category": "webscraping", "imports": ["import joblib", "import json ", "import random as rand", "import requests ", "import os", "import re", "from flask import Flask", "from geolib import geohash", "from google.cloud import firestore", "from label import LabelNews", "from newspaper import Article, ArticleException", "\timportant_tags = [\"city\",'datePublished','name',", "\t\tmoney = dict((k, item[k]) for k in important_tags)"]}, {"term": "def", "name": "gen_id", "data": "def gen_id(news_item) :\n\t'''generates a unique id for a news headline by concatenating the article's\n\tgeohash with its publication date.\n\t\n\tARGS\n\t---\n\tnews_item (dict) : news headline object in format ...\n\t\n\tRETURNS\n\t---\n\tunique id as a string\n\t'''\n\treturn news_item[\"geohash\"] + '_' + news_item[\"datePublished\"]\n", "description": null, "category": "webscraping", "imports": ["import joblib", "import json ", "import random as rand", "import requests ", "import os", "import re", "from flask import Flask", "from geolib import geohash", "from google.cloud import firestore", "from label import LabelNews", "from newspaper import Article, ArticleException", "\timportant_tags = [\"city\",'datePublished','name',", "\t\tmoney = dict((k, item[k]) for k in important_tags)"]}, {"term": "def", "name": "locate", "data": "def locate(news_lst, city) :\n\t'''Temporary geohasher assigns random locations based on city parameter; also \n\tassigns unique id's\n\t\n\tARGS\n\t---\n\tnews_lst : list of news articles in format ...\n\tcity (string) : collection id\n\t\n\tRETURNS\n\t---\n\tlist of news articles in format ...\n\t'''\n\t#long beach values\n\tif (city == \"long-beach\"):\n\t\tmax_lat = 33.8765\n\t\tmin_lat = 33.7664\n\t\tmax_lon = -118.0997\n\t\tmin_lon = -118.2042\n\n\t#cerritos values\n\telif (city == \"cerritos\"):\n\t\tmax_lat = 33.8879\n\t\tmin_lat = 33.8459\n\t\tmax_lon = -118.0287\n\t\tmin_lon = -118.1085\n\t#\n\t#bellflower values\n\telif (city == \"bellflower\"):\n\t\tmax_lat = 33.9105\n\t\tmin_lat = 33.8656\n\t\tmax_lon = -118.1067\n\t\tmin_lon = -118.1514\n\t#\n\t#lakewood values\n\telif (city == \"lakewood\"):\n\t\tmax_lat = 33.8692\n\t\tmin_lat = 33.8202\n\t\tmax_lon = -118.0590\n\t\tmin_lon = -118.1677\n\n\n\trand.seed()\n\t#\n\tfor item in news_lst :\n\t\tlati = min_lat + (max_lat-min_lat)*rand.random()\n\t\tlon = min_lon + (max_lon-min_lon)*rand.random()\n\t\tghash = geohash.encode(lati, lon, 7)\n\t\tloncoord = float(geohash.decode(ghash).lon)\n\t\tlatcoord = float(geohash.decode(ghash).lat)\n\t\titem[\"geohash\"] = ghash\n\t\titem[\"id\"] = gen_id(item)\n\t\titem[\"coordinates\"] = firestore.GeoPoint(latcoord, loncoord)\n", "description": null, "category": "webscraping", "imports": ["import joblib", "import json ", "import random as rand", "import requests ", "import os", "import re", "from flask import Flask", "from geolib import geohash", "from google.cloud import firestore", "from label import LabelNews", "from newspaper import Article, ArticleException", "\timportant_tags = [\"city\",'datePublished','name',", "\t\tmoney = dict((k, item[k]) for k in important_tags)"]}, {"term": "def", "name": "collect", "data": "def collect() :\n\t'''docstring'''\n\ttext = []\n\t#\n\tfor c in cities:\n\t\t# scrape\n\t\tnews = webscrape(c, subscription_key, search_url)\n\t\t# geohash and id\n\t\tnews_lst = locate(news,c)\n\t\t# label\n\t\tln = LabelNews(labeler, model, news_lst, vectorizer)\n\t\tnews_lst = ln.assign_topics()\n\t\t#\n\t\t# Project ID determined by GCLOUD_PROJECT environment variable \n\t\tprint(\"writing to Firestore ...\\n\")\n\t\tdb = firestore.Client()\t\n\t\tfor item in news_lst :\n\t\t\tdoc_ref = db.collection(\"Testing Collections\").document(c).collection(\"Articles\").document(item[\"id\"])\n\t\t\tdoc_ref.set({\n\t\t\t\t\"datePublished\" : item[\"datePublished\"],\n\t\t\t\t\"name\" : item[\"name\"],\n\t\t\t\t\"organization\" : item[\"organization\"],\n\t\t\t\t\"summary\" : item[\"summary\"],\n\t\t\t\t\"url\" : item[\"url\"],\n\t\t\t\t\"coordinates\" : item[\"coordinates\"],\n\t\t\t\t\"topic\" : item[\"topic\"],\n\t\t\t\t\"g\" : {\n\t\t\t\t\t\"geohash\" : item[\"geohash\"],\n\t\t\t\t\t\"geopoint\" : item[\"coordinates\"]\n\t\t\t\t}\n\t\t\t})\n\t\t#\t\n\tfor i in news_lst :\n\t\ttext.append(i[\"topic\"] + \" : \" + i[\"name\"])\n\t#\t\n\ttext = \";_____\".join(text)\n\tname = os.environ.get('NAME', text)\n\treturn \"{}\".format(name)\n\n\n", "description": null, "category": "webscraping", "imports": ["import joblib", "import json ", "import random as rand", "import requests ", "import os", "import re", "from flask import Flask", "from geolib import geohash", "from google.cloud import firestore", "from label import LabelNews", "from newspaper import Article, ArticleException", "\timportant_tags = [\"city\",'datePublished','name',", "\t\tmoney = dict((k, item[k]) for k in important_tags)"]}], [{"term": "def", "name": "scrape", "data": "def scrape(url):\n\t#requests one url\n\tr = requests.get(url)\n\t#uses regex to parse through all data for valid URLs\n\tmatches = re.findall(r'href=[\\'|\\\"](\\S*)[\\'|\\\"]', r.text)\n\tfixed = []\n\tfor x in matches:\n\t\ttry:\n\t\t\t#turns relative url paths into absolute paths\n\t\t\tif x[0] == '/':\n\t\t\t\tfixed.append(f'{url}{x}')\n\t\t\t# looks for urls starting with http and includes https\n\t\t\telif x[0:4] == 'http':\n\t\t\t\tfixed.append(x)\n\t\t\t#passes on broken or invalid links\n\t\t\telse:\n\t\t\t\tpass\n\t\texcept IndexError:\n\t\t\tpass\n\n\t# returs results into a list called fixed\n\treturn fixed\n", "description": null, "category": "webscraping", "imports": ["#this section imports external functions", "from fileinput import close", "import requests", "import re"]}], [{"term": "class", "name": "classWebScrape:", "data": "class WebScrape:\n\n\tdef scrape(self, URL):\n\t\t\"\"\"function for getting the web content\"\"\"\n\t\thtml_text = requests.get(URL).text\n\t\thtml_content = BeautifulSoup(html_text, \"lxml\").text\n\t\treturn html_content\n\n\n\n\n", "description": "function for getting the web content", "category": "webscraping", "imports": ["from bs4 import BeautifulSoup", "import requests"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape():\n\n\tpath = os.environ['PATH_CHROMEDRIVER']\n\n\tchrome_options = Options()  \n\tchrome_options.add_argument(\"--headless\") #headless browsing\n\n\t#updates the driver object\n\tdriver = webdriver.Chrome(executable_path=path, chrome_options=chrome_options)\n\n\tdriver.get('https://pogoapi.net/Is_It_Shiny/') #async\n\n\ttime.sleep(1) #forces 1-second wait for driver.get to complete\n\t\t\n\thtml_rendered = driver.page_source #page source sould be rendered by now\n\n\t#Beautiful Soup: Build a Web Scraper With Python https://realpython.com/beautiful-soup-web-scraper-python/\n\tsoup = BeautifulSoup(html_rendered, 'html.parser') \n\tresults = soup.find(id='pokemonList') # pull out html for pokemonList id that lists all pokemon currently in the game and their shiny status\n\n\t\n\t#\tprint(\"this is results: \", results.prettify()) # nicely format with .prettify() dataquest.io/blog/web-scraping-tutorial-python/\n\t#extract the text of all  tags within results \"Finding all instances of a tag at once\" dataquest.io/blog/web-scraping-tutorial-python/\n\t# Beautifulsoup loop through HTML https://stackoverflow.com/questions/38519975/beautifulsoup-loop-through-html\n\n\ttd_tags = results.find_all('td')\n\n\tlist_pokemon_and_shiny_status = []\n\n\tfor td_tag in td_tags:\n\t\n\t\t#\"Once we\u00e2\u20ac\u2122ve isolated the tag, we can use the get_text method to extract all of the text inside the tag\" dataquest.io/blog/web-scraping-tutorial-python/\n\n\t\t#The strip() method returns a copy of the string with both leading and trailing characters removed (based on the string argument passed).\n\t\t#e.g. empty spaces\n\t\t#https://towardsdatascience.com/top-5-beautiful-soup-functions-7bfe5a693482#:~:text=One%20of%20them%20is%20Beautiful,order%20to%20get%20data%20easily.&text=The%20basic%20process%20goes%20something,it%20any%20way%20you%20want.\n\t\tlist_pokemon_and_shiny_status.append(td_tag.get_text().strip())\n\n\n\tdict_pokemon_shinystatus = {}\n\n\t#slice list for pokemon names https://www.xspdf.com/resolution/52771228.html\n\tpokemon_list = list_pokemon_and_shiny_status[1::3]\n\n\t#slice list for shiny status\n\tshiny_status = list_pokemon_and_shiny_status[2::3]\n\n\t## Merge the two lists to create a dictionary\n\t#https://thispointer.com/python-6-different-ways-to-create-dictionaries/\n\tdict_pokemon_shinystatus = dict(zip(pokemon_list, shiny_status))\n\n\tlist_shinies_newer = [key for key,value \\\n\tin dict_pokemon_shinystatus.items() if value == 'Yes'] \n\n\n\twith open('shiny_pokemon.txt', 'a+') as shiny_pokemon_file:\n\t\tshiny_pokemon_file.seek(0)\n\n\t\t#read WHOLE file to see what's in it, first. first time, should be empty list\n\t\tlines = shiny_pokemon_file.readlines()\n\n\t   \t#clean up \\n's\n\t\tlist_stripped = [line.strip() for line in lines]\n\t\tprint(f\"list_stripped:\\n {list_stripped}\")\n\n\n\t\tfor pokemon in list_shinies_newer:\n\t   \t\tif pokemon not in list_stripped: \n\n\t   \t\t\tprint(f\"{pokemon} not in file currently!\")\n\n\n\t   \t\t\tmessage = Mail(\n\t\t\t\t\tfrom_email=email_from, \n\t\t\t\t\tto_emails=email_to,\n\t\t\t\t\tsubject='Test: PokemonGo Shinies new (via SendGrid)',\n\t\t\t\t\thtml_content=f\"and easy to do anywhere, {pokemon} even with Python\")\n\t\t\t\t\n\t\t\t\ttry:\n\t\t\t\t\tsg = SendGridAPIClient(os.environ.get('SENDGRID_API_KEY'))\n\t\t\t\t\tresponse = sg.send(message)\n\t\t\t\t\tprint(response.status_code)\n\t\t\t\t\tprint(response.body)\n\t\t\t\t\tprint(response.headers)\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tprint(e)\n\n\n\t   \t\t\tshiny_pokemon_file.write(f\"{pokemon}\\n\")\n\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import time", "import requests ", "from bs4 import BeautifulSoup ", "from selenium import webdriver", "from selenium.webdriver.chrome.options import Options  "]}, {"term": "def", "name": "getPokemonList", "data": "def getPokemonList():\n\n\n\n\tprint(\"next steps...\")\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import time", "import requests ", "from bs4 import BeautifulSoup ", "from selenium import webdriver", "from selenium.webdriver.chrome.options import Options  "]}], [{"term": "def", "name": "webscrape", "data": "def webscrape():\n\t\"\"\"\n\tWeb scraping 3 character ISO and country name from holidays\n\tdocumentation site\n\n\t:return countries:  {3 character ISO: [Country, states]}\n\t:type countries: dict\n\n\t\"\"\"\n\turl = 'https://pypi.org/project/holidays/'\n\tcountries = {}\n\tr = requests.get(url)\n\n\tsoup = BeautifulSoup(r.text, 'html.parser')\n\tiso_table = soup.find('table')\n\n\tfor country in iso_table.find_all('tbody'):\n\t\trows = country.find_all('tr')\n\t\tfor row in rows:\n\t\t\tc = row.find_all('td')\n\t\t\tif '/' not in c[1].text:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tif len(c[1].text.split('/')[1]) == 3:\n\t\t\t\t\tprov = ''\n\t\t\t\t\tif c[2].text != None:\n\t\t\t\t\t\tprov = c[2].text.strip()\n\t\t\t\t\t\tif '\\n' in prov:\n\t\t\t\t\t\t\tprov = prov.replace('\\n', ' ')\n\t\t\t\t\t\tif ' (default)' in prov:\n\t\t\t\t\t\t\tprov = prov.replace(' (default)', '')\n\t\t\t\t\t\tif 'prov = ' in prov:\n\t\t\t\t\t\t\tprov = prov.replace('prov = ', '')\n\t\t\t\t\t\tif 'state = ' in prov:\n\t\t\t\t\t\t\tprov = prov.replace('state = ', '')\n\t\t\t\t\t\tif prov == '' or prov == 'None':\n\t\t\t\t\t\t\tprov = None\n\n\t\t\t\t\tcountries[c[1].text.split('/')[1]] = [c[0].text, prov]\n\treturn countries\n", "description": "\n\tWeb scraping 3 character ISO and country name from holidays\n\tdocumentation site\n\n\t:return countries:  {3 character ISO: [Country, states]}\n\t:type countries: dict\n\n\t", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import holidays", "from datetime import date", "import csv"]}, {"term": "def", "name": "getHolidays", "data": "def getHolidays(countries):\n\t\"\"\"\n\tGet holiday dates for years in range 2016-2022 for\n\tsupported countries\n\n\t:param countries:\n\t:type countries: dict\n\t:return country_holidays:\n\t:type country_holidays: dict\n\n\t\"\"\"\n\tcountry_holidays = {}\n\n\tfor c in countries.keys():\n\t\tcountry_holidays[c] = {}\n\t\tfor year in range(2016, 2022+1):\n\t\t\tcountry_holidays[c][year] = holidays.CountryHoliday(countries[c][0], years=year)\n\n\n\treturn country_holidays\n", "description": "\n\tGet holiday dates for years in range 2016-2022 for\n\tsupported countries\n\n\t:param countries:\n\t:type countries: dict\n\t:return country_holidays:\n\t:type country_holidays: dict\n\n\t", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import holidays", "from datetime import date", "import csv"]}, {"term": "def", "name": "writeHolidays", "data": "def writeHolidays(countries, country_holidays):\n\t\"\"\"\n\tWrites countries and their holidays to csv file holidays.csv\n\n\t:param countries, country_holidays:\n\t:type countries, country_holidays: dict\n\n\t\"\"\"\n\n\twith open('holidays.csv', mode='w', newline='') as csv_file:\n\t\theaders = ['ISO', 'Country', 'States', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\n\t\twriter = csv.DictWriter(csv_file, fieldnames=headers)\n\t\twriter.writeheader()\n\n\t\tfor c in country_holidays:\n\t\t\tholidays16 = [k for k in country_holidays[c][2016]]\n\t\t\tholidays17 = [k for k in country_holidays[c][2017]]\n\t\t\tholidays18 = [k for k in country_holidays[c][2018]]\n\t\t\tholidays19 = [k for k in country_holidays[c][2019]]\n\t\t\tholidays20 = [k for k in country_holidays[c][2020]]\n\t\t\tholidays21 = [k for k in country_holidays[c][2021]]\n\t\t\tholidays22 = [k for k in country_holidays[c][2022]]\n\t\t\twriter.writerow({'ISO': c, 'Country': countries[c][0], 'States': countries[c][1], '2016': str(holidays16), '2017': str(holidays17),\n\t\t\t'2018': str(holidays18), '2019': str(holidays19), '2020': str(holidays20), '2021': str(holidays21), '2022': str(holidays22)})\n", "description": "\n\tWrites countries and their holidays to csv file holidays.csv\n\n\t:param countries, country_holidays:\n\t:type countries, country_holidays: dict\n\n\t", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import holidays", "from datetime import date", "import csv"]}, {"term": "def", "name": "summary", "data": "def summary(country_holidays):\n\t\"\"\"\n\tSummary of number of holidays in each supported\n\tcountry for each year in the range 2016-2022\n\n\t:param country_holidays:\n\t:type country_holidays: dict\n\t:return nbr_holidays: {ISO: [2016, ..., 2022]\n\t:type nbr_holidays: dict\n\n\t\"\"\"\n\n\tnbr_holidays = {}\n\tfor c in country_holidays:\n\n\t\tif c != 'SWE' or c != 'NOR':\n\t\t\tnbr_holidays[c] = [len(country_holidays[c][2016]), len(country_holidays[c][2017]), len(country_holidays[c][2018]), len(country_holidays[c][2019]), len(country_holidays[c][2020]), len(country_holidays[c][2021]), len(country_holidays[c][2022])]\n\n\t\t# Remove regular Sundays from holidays count for Sweden and Norway\n\t\tif c == 'SWE' or c == 'NOR':\n\t\t\tnbr_holidays[c] = []\n\t\t\tfor y in range(2016, 2022+1):\n\t\t\t\tnbr_holidays_y = len(country_holidays[c][y])\n\t\t\t\tfor dt in country_holidays[c][y]:\n\t\t\t\t\tdate_ = dt.strftime(\"%Y-%m-%d\")\n\t\t\t\t\tif country_holidays[c][y].get(date_) == 'S\u00f6ndag' or country_holidays[c][y].get(date_) == 'S\u00f8ndag':\n\t\t\t\t\t\tnbr_holidays_y -= 1\n\t\t\t\tnbr_holidays[c].append(nbr_holidays_y)\n\n\treturn nbr_holidays\n", "description": "\n\tSummary of number of holidays in each supported\n\tcountry for each year in the range 2016-2022\n\n\t:param country_holidays:\n\t:type country_holidays: dict\n\t:return nbr_holidays: {ISO: [2016, ..., 2022]\n\t:type nbr_holidays: dict\n\n\t", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import holidays", "from datetime import date", "import csv"]}, {"term": "def", "name": "writeSummary", "data": "def writeSummary(nbr_holidays):\n\t\"\"\"\n\tWrites summary of number of holidays to csv file nbr_holidays.csv\n\n\t:param nbr_holidays:\n\t:tye nbr_holidays: dict\n\n\t\"\"\"\n\n\twith open('nbr_holidays.csv', mode='w', newline='') as csv_file:\n\t\theaders = ['ISO', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\n\t\twriter = csv.DictWriter(csv_file, fieldnames=headers)\n\t\twriter.writeheader()\n\n\t\tfor c in nbr_holidays:\n\t\t\twriter.writerow({'ISO': c, '2016': nbr_holidays[c][0], '2017': nbr_holidays[c][1], '2018': nbr_holidays[c][2], '2019': nbr_holidays[c][3], '2020': nbr_holidays[c][4], '2021': nbr_holidays[c][5], '2022': nbr_holidays[c][6]})\n\n\n", "description": "\n\tWrites summary of number of holidays to csv file nbr_holidays.csv\n\n\t:param nbr_holidays:\n\t:tye nbr_holidays: dict\n\n\t", "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import holidays", "from datetime import date", "import csv"]}, {"term": "def", "name": "run", "data": "def run():\n\tcountries = webscrape()\n\tcountry_holidays = getHolidays(countries)\n\twriteHolidays(countries, country_holidays)\n\tnbr_holidays = summary(country_holidays)\n\twriteSummary(nbr_holidays)\n \n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import holidays", "from datetime import date", "import csv"]}], [{"term": "def", "name": "mainFunc", "data": "def mainFunc():\n\tprovider = \"\"  # starts with netflix\n\tpageNumber = 0  # Should start with 0\n\n\tstart = time.time()\n\tWs.loopThroughScrapePages(provider, pageNumber)\n\tPd.writeToJson(badAPIcalls)\n\tend = time.time()\n\tprint('total time: ' + str(end - start) + ' seconds.')\n\tprint(\"BAD calls \" + str(badCalls))\n\n", "description": null, "category": "webscraping", "imports": ["import time", "import ParseData as Pd", "import WebScrape as Ws"]}], [{"term": "def", "name": "check_if_scraper_works", "data": "def check_if_scraper_works():\n\tTEST_URL = \"https://www.otodom.pl/pl/oferty/sprzedaz/mieszkanie/warszawa?limit=10&page=1\"\n\tpage_count = how_many_pages_to_scrape(TEST_URL)\n\tif type(page_count) != int or page_count <= 0:\n\t\tprint('page count does not work properly')\n\t\treturn False\n\tscraping_results = otodom_page_scraper(1,TEST_URL)\n\tif len(scraping_results) <= 1:\n\t\tprint('scraping results are broken')\n\t\treturn False\n\tprint('Scraper passed the test')\n\treturn True\n\n", "description": null, "category": "webscraping", "imports": ["from googlemapsgeocoding import geocode_single_location", "from otodomestateprices import otodom_web_scraper, otodom_page_scraper, how_many_pages_to_scrape", "from requests import post, get", "import json", "import sys", "import time", "import os", "import schedule"]}, {"term": "def", "name": "post_data", "data": "def post_data(city, results):\n\tpayload = {\"key\" : '1234','city': city, 'data' : results}\n\tprint(f'Length is {len(results)}')\n\tdata = json.dumps(payload)\n\tprint(f'json data in memory {sys.getsizeof(data)}')\n\tr = post(f\"{os.environ.get('API_URL')}/api/\", json=data)\n\tprint(str(f'Status code {r.status_code}'))\n\n", "description": null, "category": "webscraping", "imports": ["from googlemapsgeocoding import geocode_single_location", "from otodomestateprices import otodom_web_scraper, otodom_page_scraper, how_many_pages_to_scrape", "from requests import post, get", "import json", "import sys", "import time", "import os", "import schedule"]}, {"term": "def", "name": "get_new_offers", "data": "def get_new_offers(cities):\n\tfor city,url in cities.items():\n\t\tprint(url)\n\t\tprint(city)\n\t\tresults = otodom_web_scraper(url)\n\t\tpost_data(city, results)\n\n", "description": null, "category": "webscraping", "imports": ["from googlemapsgeocoding import geocode_single_location", "from otodomestateprices import otodom_web_scraper, otodom_page_scraper, how_many_pages_to_scrape", "from requests import post, get", "import json", "import sys", "import time", "import os", "import schedule"]}, {"term": "def", "name": "geocode_offers", "data": "def geocode_offers(offers, city_geodata):\n\toffers_geocoded = []\n\tfor offer in offers:\n\t\tgeodata = [data for data in city_geodata if data['location'] == offer['location'].lower()]\n\t\tif len(geodata) == 0:\n\t\t\tgeodata = geocode_single_location(offer['location'].lower())\n\t\t\tif geodata is None:\n\t\t\t\tcontinue\n\t\t\tlat = geodata['lat']\n\t\t\tlng = geodata['lng']\n\t\telse:\n\t\t\tgeodata = geodata[0]\n\t\t\tlat = geodata['latitude']\n\t\t\tlng = geodata['longtitude']\n\t\t\n\t\tgeocoded_offer = offer\n\t\tgeocoded_offer['lat'] = lat\n\t\tgeocoded_offer['lng'] = lng\n\t\toffers_geocoded.append(geocoded_offer)\n\treturn offers_geocoded\n\n", "description": null, "category": "webscraping", "imports": ["from googlemapsgeocoding import geocode_single_location", "from otodomestateprices import otodom_web_scraper, otodom_page_scraper, how_many_pages_to_scrape", "from requests import post, get", "import json", "import sys", "import time", "import os", "import schedule"]}, {"term": "def", "name": "get_city_geodata", "data": "def get_city_geodata(city):\n\tAPI_URL = os.environ.get('API_URL')\n\tr = get(f\"{os.environ.get('API_URL')}/api/getgeodata/?city={city}\")\n\treturn json.loads(r.content)\n\n", "description": null, "category": "webscraping", "imports": ["from googlemapsgeocoding import geocode_single_location", "from otodomestateprices import otodom_web_scraper, otodom_page_scraper, how_many_pages_to_scrape", "from requests import post, get", "import json", "import sys", "import time", "import os", "import schedule"]}, {"term": "def", "name": "get_new_offers", "data": "def get_new_offers(cities):\n\tfor city,url in cities.items():\n\t\tcity_geodata = get_city_geodata(city);\n\t\tprint('got city_geodata')\n\t\toffers = otodom_web_scraper(url)\n\t\tprint('offers scraped')\n\t\toffers_geocoded = geocode_offers(offers, city_geodata)\n\t\tpost_data(city, offers_geocoded)\n\n", "description": null, "category": "webscraping", "imports": ["from googlemapsgeocoding import geocode_single_location", "from otodomestateprices import otodom_web_scraper, otodom_page_scraper, how_many_pages_to_scrape", "from requests import post, get", "import json", "import sys", "import time", "import os", "import schedule"]}, {"term": "def", "name": "fwebscrape", "data": "\tdef webscrape():\n\t\tif not check_if_scraper_works():\n\t\t\tsys.exit(\"Webscraper stopped functioning properly. Possible changes at the scraped site\")\n\t\tget_new_offers(cities=CITIES_URL)\n", "description": null, "category": "webscraping", "imports": ["from googlemapsgeocoding import geocode_single_location", "from otodomestateprices import otodom_web_scraper, otodom_page_scraper, how_many_pages_to_scrape", "from requests import post, get", "import json", "import sys", "import time", "import os", "import schedule"]}], [{"term": "class", "name": "classWebscrapeSpiderMiddleware:", "data": "class WebscrapeSpiderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the spider middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_spider_input(self, response, spider):\n\t\t# Called for each response that goes through the spider\n\t\t# middleware and into the spider.\n\n\t\t# Should return None or raise an exception.\n\t\treturn None\n\n\tdef process_spider_output(self, response, result, spider):\n\t\t# Called with the results returned from the Spider, after\n\t\t# it has processed the response.\n\n\t\t# Must return an iterable of Request, or item objects.\n\t\tfor i in result:\n\t\t\tyield i\n\n\tdef process_spider_exception(self, response, exception, spider):\n\t\t# Called when a spider or process_spider_input() method\n\t\t# (from other spider middleware) raises an exception.\n\n\t\t# Should return either None or an iterable of Request or item objects.\n\t\tpass\n\n\tdef process_start_requests(self, start_requests, spider):\n\t\t# Called with the start requests of the spider, and works\n\t\t# similarly to the process_spider_output() method, except\n\t\t# that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n\t\t# Must return only requests (not items).\n\t\tfor r in start_requests:\n\t\t\tyield r\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}, {"term": "class", "name": "classWebscrapeDownloaderMiddleware:", "data": "class WebscrapeDownloaderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the downloader middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_request(self, request, spider):\n\t\t# Called for each request that goes through the downloader\n\t\t# middleware.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this request\n\t\t# - or return a Response object\n\t\t# - or return a Request object\n\t\t# - or raise IgnoreRequest: process_exception() methods of\n\t\t#   installed downloader middleware will be called\n\t\treturn None\n\n\tdef process_response(self, request, response, spider):\n\t\t# Called with the response returned from the downloader.\n\n\t\t# Must either;\n\t\t# - return a Response object\n\t\t# - return a Request object\n\t\t# - or raise IgnoreRequest\n\t\treturn response\n\n\tdef process_exception(self, request, exception, spider):\n\t\t# Called when a download handler or a process_request()\n\t\t# (from other downloader middleware) raises an exception.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this exception\n\t\t# - return a Response object: stops process_exception() chain\n\t\t# - return a Request object: stops process_exception() chain\n\t\tpass\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}], [], [{"term": "def", "name": "Main", "data": "def Main():\n\tinfo = 'PIA 063 PC E2022'\n\tparser = argparse.ArgumentParser(info)\n\n\tparser.add_argument('-opt', '--option', required = True, help = 'Opcion para elegir script a utilizar.', dest = 'opt')\n\n\t\n\t\n\t#ENCRIPTAR\n\tparser.add_argument('-enc', '--enc', help = 'Ruta del directorio a encriptar.')\n\t#DESENCRIPTAR\n\tparser.add_argument('-dec', '--dec', help = 'Ruta del directorio a desencriptar.')\n\t#WEBSCRAPE\n\tparser.add_argument('-ws', '--ws', help= 'Url de la pagina a escanear.')\n\t#METADATA\n\tparser.add_argument('-md', '--md', help= 'Ruta del imagen para extraer informacion. ')\n\t#ESCANEO DE PUERTOS \n\tparser.add_argument('-ip', '--ip', help= 'IP a escanear')\n\t#ESCANEO DE PUERTOS POWERSHELL\n\tparser.add_argument('-ep', '--ep', help= 'IP a escanear CON PS')\n\t#APi \n\tparser.add_argument('-api', '--api', help= 'API y busqueda de correos')\n\t\n\t\n   \n\n\targs = parser.parse_args()\n\t\n\t\n\t\t\n\tif args.opt.upper()== 'ENCRIPTAR':\n\t\t path_to_encrypt = args.enc\n\t\t encrypt.main(path_to_encrypt)\n\n\tif args.opt.upper()== 'DESENCRIPTAR':\n\t\t path_to_encrypt = args.dec\n\t\t decrypt.main(path_to_encrypt)\n\t \n\tif args.opt.upper()== 'ESCANEO_DE_PUERTOS':\n\t\ttarget = args.ip\n\t\tpuertos.main(target)\n\n\tif args.opt.upper()== 'WEBSCRAPE':\n\t\t url = args.ws\n\t\t webscrape.main(url)\n\n\tif args.opt.upper()== 'METADATA':\n\t\t directorio = args.md\n\t\t metadata.main(directorio)\n\t  \n\tif args.opt.upper()== 'API':\n\t\t email = args.api\n\t\t api.main(email)\n\t   \n\n\tif args.opt.upper()== 'ESCANEOPUERTOSPOWERSHELL':\n\t\t command = \"powershell -ExecutionPolicy ByPass -File EscaneoPuertosPowershell.ps1\"\n\t\t print(command)\n\t\t powerShellResult = subprocess.run(command)\n", "description": null, "category": "webscraping", "imports": ["\timport argparse", "\timport encrypt", "\timport decrypt", "\timport puertos", "\timport metadata", "\timport api", "\timport webscrape", "\timport subprocess\t"]}], [{"term": "def", "name": "search", "data": "def search(stockName):\n\turl = get_url(stockName.lower())\n\tprint(url)\n\tprice = current_price(url)\n\tif price == \"Error. Can't find stock name. Make sure name is correct.\" or price == None:\n\t\tprint(\"Stock not Found\")\n\t\treturn False\n\telse:\n\t\tprint(current_price(url))\n\t\tprint(\"THIS IS THECURRENT PRIce\")\n\t\tprint(\"Stock Full Name: \" + get_company_name(url))\n\t\t# stockName = get_company_name(url)\n\t\tt = time.localtime()\n\t\tcurrentTime = time.strftime(\"%H:%M:%S\", t)\n\t\tprint(get_company_name(url) + \"Stock Data Updated at \" + currentTime)\n\t\treturn True\n\n", "description": null, "category": "webscraping", "imports": ["from webscrape import *", "import pygame", "from pygame.locals import *", "import time"]}, {"term": "def", "name": "searchBarInitalize", "data": "def searchBarInitalize():\n\tsearchBarButton = pygame.Rect(198, 17, 983, 56)\n\tfont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 50)\n\ttimeFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 30)\n\tfavFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 60)\n\tverFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 30)\n\tcompanyFont = pygame.font.Font(\n\t\t\"../course-project-a8-mcm/Fonts/times.ttf\", 50)\n\tt = time.localtime()\n\tcurrentTime = time.strftime(\"%H:%M:%S\", t)\n\tupdatedTime = \"Stock Data Updated at \" + currentTime\n\treturn searchBarButton, font, updatedTime, timeFont, companyFont, favFont, verFont\n\n", "description": null, "category": "webscraping", "imports": ["from webscrape import *", "import pygame", "from pygame.locals import *", "import time"]}, {"term": "def", "name": "updateSearchBarOnKeyPress", "data": "def updateSearchBarOnKeyPress(theGuiEvent, searchBarText):\n\tif theGuiEvent.unicode == \"\\b\":\n\t\treturn searchBarText[:-1]\n\n\telif theGuiEvent.unicode.isalpha() and len(searchBarText) <= 5 or theGuiEvent.unicode.isdigit() and len(searchBarText) <= 5:\n\t\treturn searchBarText+theGuiEvent.unicode\n\telse:\n\t\tif len(searchBarText) >= 5:\n\t\t\tprint(\"max length of string reached\")\n\n\t\treturn searchBarText\n\n", "description": null, "category": "webscraping", "imports": ["from webscrape import *", "import pygame", "from pygame.locals import *", "import time"]}, {"term": "def", "name": "timeStamp", "data": "def timeStamp():\n\tt = time.localtime()\n\tcurrentTime = time.strftime(\"%H:%M:%S\", t)\n\tupdatedTime = \"Updated Stock Data at \" + str(currentTime)\n\treturn updatedTime\n", "description": null, "category": "webscraping", "imports": ["from webscrape import *", "import pygame", "from pygame.locals import *", "import time"]}], [{"term": "def", "name": "search", "data": "def search(stockName):\n\turl = get_url(stockName.lower())\n\tprint(url)\n\tprice=current_price(url)\n\tif price == \"Error. Can't find stock name. Make sure name is correct.\" or price==None:\n\t\tprint(\"Stock not Found\")\n\t\treturn False\n\telse:\n\t\tprint(current_price(url))\n\t\tprint(\"THIS IS THECURRENT PRIce\")\n\t\tprint(\"Stock Full Name: \" + get_company_name(url))\n\t\t# stockName = get_company_name(url)\n\t\tt = time.localtime()\n\t\tcurrentTime = time.strftime(\"%H:%M:%S\", t)\n\t\tprint(get_company_name(url) + \"Stock Data Updated at \" + currentTime)\n\t\treturn True\n\n", "description": null, "category": "webscraping", "imports": ["from webscrape import *", "import pygame", "from pygame.locals import *", "import time"]}, {"term": "def", "name": "searchBarInitalize", "data": "def searchBarInitalize():\n\tsearchBarButton = pygame.Rect(200, 10, 1000, 70)\n\tfont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 50)\n\ttimeFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 30)\n\tfavFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 60)\n\tverFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 30)\n\tcompanyFont = pygame.font.Font(\n\t\t\"../course-project-a8-mcm/Fonts/times.ttf\", 50)\n\tt = time.localtime()\n\tcurrentTime = time.strftime(\"%H:%M:%S\", t)\n\tupdatedTime = \"Stock Data Updated at \" + currentTime\n\treturn searchBarButton, font, updatedTime, timeFont, companyFont, favFont, verFont\n\n", "description": null, "category": "webscraping", "imports": ["from webscrape import *", "import pygame", "from pygame.locals import *", "import time"]}, {"term": "def", "name": "updateSearchBarOnKeyPress", "data": "def updateSearchBarOnKeyPress(theGuiEvent, searchBarText):\n\tif theGuiEvent.unicode == \"\\b\":\n\t\treturn searchBarText[:-1]\n\n\telif theGuiEvent.unicode.isalpha() and len(searchBarText) <= 5 or theGuiEvent.unicode.isdigit() and len(searchBarText) <= 5:\n\t\treturn searchBarText+theGuiEvent.unicode\n\telse:\n\t\tif len(searchBarText) >= 5:\n\t\t\tprint(\"max length of string reached\")\n\n\t\treturn searchBarText\n\n", "description": null, "category": "webscraping", "imports": ["from webscrape import *", "import pygame", "from pygame.locals import *", "import time"]}, {"term": "def", "name": "timeStamp", "data": "def timeStamp():\n\tt = time.localtime()\n\tcurrentTime = time.strftime(\"%H:%M:%S\", t)\n\tupdatedTime = \"Updated Stock Data at \" + str(currentTime)\n\treturn updatedTime\n", "description": null, "category": "webscraping", "imports": ["from webscrape import *", "import pygame", "from pygame.locals import *", "import time"]}], [], [{"term": "class", "name": "WebscrapeItem", "data": "class WebscrapeItem(scrapy.Item):\n\t# define the fields for your item here like:\n\t title = scrapy.Field()\n\t author= scrapy.Field()\n\t post = scrapy.Field()\n", "description": null, "category": "webscraping", "imports": ["import scrapy", "from multiprocessing import AuthenticationError", "from matplotlib.pyplot import title"]}], [{"term": "def", "name": "cli", "data": "def cli():\n\tpass\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import time", "from pathlib import Path", "import click", "import django", "import pandas as pd", "from selenium import webdriver", "from selenium.common.exceptions import NoSuchElementException", "from selenium.webdriver.chrome.options import Options", "\tfrom summary.models import DataPivot"]}, {"term": "def", "name": "get_pivot_objects", "data": "def get_pivot_objects():\n\n\tos.environ[\"DJANGO_SETTINGS_MODULE\"] = \"main.settings.dev\"\n\tdjango.setup()\n\n\tfrom summary.models import DataPivot\n\n\tdata = []\n\tfor d in DataPivot.objects.all().order_by(\"assessment_id\"):\n\t\tdata.append((d.id, d.get_absolute_url(), False, False, False, False))\n\n\tdf = pd.DataFrame(data=data, columns=(\"id\", \"url\", \"loaded\", \"error\", \"png\", \"svg\"))\n\tdf.to_pickle(FN)\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import time", "from pathlib import Path", "import click", "import django", "import pandas as pd", "from selenium import webdriver", "from selenium.common.exceptions import NoSuchElementException", "from selenium.webdriver.chrome.options import Options", "\tfrom summary.models import DataPivot"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(base_url: str):\n\tdf = pd.read_pickle(FN)\n\n\tmax_sleep = 60 * 10  # 10 min\n\n\tchrome_options = Options()\n\tchrome_options.add_argument(\"--headless\")\n\tdriver = webdriver.Chrome(options=chrome_options)\n\tdriver.set_window_size(2000, 1500)\n\tdriver.implicitly_wait(max_sleep)\n\n\turl = f\"{base_url}/user/login/\"\n\tdriver.get(url)\n\n\tdriver.find_element_by_id(\"id_username\").clear()\n\tdriver.find_element_by_id(\"id_username\").send_keys(os.environ[\"HAWC_USERNAME\"])\n\tdriver.find_element_by_id(\"id_password\").clear()\n\tdriver.find_element_by_id(\"id_password\").send_keys(os.environ[\"HAWC_PW\"])\n\tdriver.find_element_by_id(\"submit-id-login\").submit()\n\n\tfor key, data in df.iterrows():\n\t\tif data.loaded is True:\n\t\t\tcontinue\n\n\t\tdriver.implicitly_wait(max_sleep)\n\t\turl = f\"{base_url}{data.url}\"\n\t\tprint(f\"Trying {key+1} of {df.shape[0]}: {url}\")\n\t\tdriver.get(url)\n\t\tel = driver.find_element_by_id(\"dp_display\")\n\t\tloading_div = driver.find_element_by_id(\"loading_div\")\n\t\twhile True:\n\t\t\tif not loading_div.is_displayed():\n\t\t\t\tdriver.implicitly_wait(10)\n\t\t\t\ttry:\n\t\t\t\t\tsvg = driver.find_element_by_xpath(\"//*[local-name()='svg']\")\n\t\t\t\t\tif svg:\n\t\t\t\t\t\tdf.loc[key, \"loaded\"] = True\n\t\t\t\t\t\tdf.loc[key, \"error\"] = False\n\t\t\t\t\t\tdf.loc[key, \"png\"] = svg.screenshot_as_png\n\t\t\t\t\t\tdf.loc[key, \"svg\"] = svg.get_attribute(\"innerHTML\")\n\n\t\t\t\texcept NoSuchElementException:\n\t\t\t\t\tdf.loc[key, \"loaded\"] = True\n\t\t\t\t\tdf.loc[key, \"error\"] = True\n\t\t\t\t\tdf.loc[key, \"svg\"] = el.get_attribute(\"innerHTML\")\n\n\t\t\t\tdf.to_pickle(FN)\n\t\t\t\tbreak\n\n\t\t\ttime.sleep(0.1)\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import sys", "import time", "from pathlib import Path", "import click", "import django", "import pandas as pd", "from selenium import webdriver", "from selenium.common.exceptions import NoSuchElementException", "from selenium.webdriver.chrome.options import Options", "\tfrom summary.models import DataPivot"]}], [{"term": "def", "name": "main", "data": "def main():\n\t\n\tparser = argparse.ArgumentParser(description= 'This is main driver.')\n\t# tried to use \"nargs = '1' \" but error\n\tparser.add_argument(\"-source\", choices=[\"local\", \"remote\", \"test\"], required = True, help=\"where data should be gotten from\")\n\targs = parser.parse_args()\n\n\tlocation = args.source\n\t\n\t#if local: grab raw data from csv files\n\tif location == \"local\":\n\t\t#check if pickle files have been created before grabbing data\n\t\ttry:\n\t\t\t# read pickle files as pandas df\n\t\t\tdata_folder = Path(\"./data/\")\n\t\t\tcollege_data = pd.read_pickle(data_folder / \"college_pickle.pkl\")\n\t\t\tboba_shops_data = pd.read_pickle(data_folder / \"boba_pickle.pkl\")\n\t\t#if csv files do not exist, must run remote first to create csv files\n\t\texcept FileNotFoundError:\n\t\t\tprint('Please run meng_lisa.py on remote/test first to create data files.')\n\t\t\treturn\n\t#if remote: grab raw data by webscraping and API requests\n\telse:\n\t\t# input websites that I want scrape \n\t\t# collect data as dictionary\n\t\twiki_dict = webscrape_wiki.wiki_tables('https://en.wikipedia.org/wiki/List_of_colleges_and_universities_in_California')\n\t\tfree4u_dict = webscrape_free4u.free4u_table('https://www.free-4u.com/Colleges/California-Colleges.html')\n\n\t\t# create CSV databases from raw data\n\t\twebscrape_wiki.wiki_data(wiki_dict)\n\t\twebscrape_free4u.free4u_data(free4u_dict)\n\t\t\n\t\twiki_df = pd.read_csv('wiki_raw_data.csv')\n\t\tfree4u_df = pd.read_csv('free4u_raw_data.csv')\n\t\t\n\t\n\t\t## # DATA CLEANING\n\t\t\n\t\t# check for duplicate data from wiki\n\t\twiki_df[wiki_df.duplicated(['college_name']) == True]\n\t\t# wiki data has no duplicates!\n\t\t\n\t\t# check for duplicate data from free4u\n\t\tfree4u_df[free4u_df.duplicated(['college_name'], keep=False) == True]\n\t\t\n\t\t# remove duplicate data!\n\t\t#pd.set_option('display.max_rows', 300)\n\t\tupdated_free4u_df = free4u_df.drop_duplicates(['college_name'], keep = 'last')\n\t\t\n\t\t# use GoogleMap API \"Places Autocomplete\" on both raw dataframes to get full name of colleges and place IDs to normalize\n\t\twiki_normalize = api_autocomplete.normalize_college_name(wiki_df)\n\t\tfree4u_normalize = api_autocomplete.normalize_college_name(updated_free4u_df)\n\t\n\t\t# add full names and place IDs into individual dataframes\n\t\t# turn off warning as I am aware I am operating on the copy of the dataframe, not original\n\t\tpd.set_option('mode.chained_assignment', None)\n\t\twiki_df['normalize_name'] = wiki_normalize['normalize_name']\n\t\twiki_df['place_id'] = wiki_normalize['place_id']\n\n\t\tupdated_free4u_df['normalize_name'] = free4u_normalize['normalize_name']\n\t\tupdated_free4u_df['place_id'] = free4u_normalize['place_id']\n\t\n\t\t# remove rows from dataframe if place_id is None because can't be used with Yelp API to find nearby boba shops\n\t\tnew_wiki_df = wiki_df.dropna(subset=['place_id'])\n\t\tnew_free4u_df = updated_free4u_df.dropna(subset=['place_id'])\n\t\t\n\t\t# combine dataframes\n\t\t# reset index\n\t\tconcat_df = pd.concat([new_wiki_df, new_free4u_df], axis=0, sort=False).reset_index(drop=True)\n\n\t\t# clean duplicates by:\n\t\t# 1. group by place_id (same location/campus)\n\t\t# 2. combine duplicate rows by filling missing data in wiki with free4u data (url and tuition) (because wiki data is more up to date)\n\t\t# will use enrollment data from free4u if none in wiki\n\t\t# removed \"college_name\" column because no longer needed\n\t\tfinal_df = concat_df.groupby('place_id').agg({'normalize_name':'first',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'college_city':'first',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'enrollment': 'first',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'place_id':'first',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'url':'last',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'tuition':'last',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t   'year_founded':'first'\n\t\t\t\t\t\t\t\t\t\t\t\t\t }).sort_values(by=['normalize_name']).reset_index(drop=True)\n\t\n\t\t# use GoogleMaps API \"Geocoding\" to get latitude, longitude\n\t\tcoordinates_list = api_geocode.coordinates(final_df)\n\t\t\n\t\t# add coordinates to dataframe\n\t\tfinal_df['latitude'] = coordinates_list['latitude']\n\t\tfinal_df['longitude'] = coordinates_list['longitude']\n\n\t\t# impute missing values in city_names from city in normalized_name\n\t\tfinal_df['college_city'] = final_df.apply(\n\t\t\tlambda row: row['normalize_name'].split(\",\")[-3] if pd.isnull(row['college_city']) \n\t\t\telse row['college_city'], axis=1)\n\n\t\t# store cleaned college data as pickle file\n\t\tfinal_df.to_pickle(\"college_pickle.pkl\")\n\n\t\t# pandas read college pickle file to use data for Yelp API\n\t\tcollege_data = pd.read_pickle(\"college_pickle.pkl\")\n\n\t\t# use Yelp API with parameters: boba (business category) within 10 mile of colleges (radius)\n\t\t# to get data on store names, rating, number of reviews, price, coordinates, and distance from colleges\n\t\t# NOTE: Yelp maxes you out at 20 stores (so total stores don't matter)\n\t\tboba_data = api_yelp.boba_shops(college_data)\n\n\t\t# store boba_data (type = nested dictionaty) as pickle file\n\t\tboba_pickle = open(\"boba_pickle.pkl\", \"wb\")\n\t\tpickle.dump(boba_data, boba_pickle)\n\t\tboba_pickle.close()\n\n\t\t# pandas read boba pickle file \n\t\tboba_shops_data = pd.read_pickle(\"boba_pickle.pkl\")\n\n\t# use SQLite to insert both boba and college pickle dataframe into SQL data model due to relational data of colleges to boba shops\n\t# create SQL tables\n\tsql_tables.create_tables()\n\tprint(\"total_data.db as been created!\")\n\n\t# create separate college data into separate panda df's with foreign/primary key \n\tcity_name = []\n\tfor name in college_data[\"college_city\"]:\n\t\tif name not in city_name:\n\t\t\tcity_name.append(name)\n\n\t# create foreign key lists\n\tcolleges_city_id = create_fk_list.fk_dict(college_data[\"college_city\"])\n\n\tcity_fk_id = create_fk_list.city_fk_list(college_data, colleges_city_id)\n\n\tcollege_data = college_data.drop(columns=['college_city'])\n\tcollege_data[\"city_id\"] = city_fk_id\n\t\n\t# add pandas df to sql Colleges and Cities table\n\tconn = sqlite3.connect(\"total_data.db\")\n\tcur = conn.cursor()\n\tcollege_data.to_sql('Colleges', conn, if_exists='append', index = False)\n\tfor c_name in city_name:\n\t\tcur.execute(\"INSERT INTO Cities (city_name) VALUES (?)\", (c_name,))\n\tconn.commit()\n\n\t# make df for Yelp boba data to insert into SQL table\n\tboba_dict = convert_boba_data.boba_data_dict(boba_shops_data)   \n\tboba_df = pd.DataFrame.from_dict(boba_dict)\n\t\n\t# create Boba df \n\tboba_table = boba_df[['store_name', 'latitude', 'longitude']]\n\t# filter out duplicates based on name and coordinates and fit into SQL table with primary key\n\tboba_table = boba_table.groupby(['store_name', 'latitude', 'longitude']).size().reset_index(name='count')\n\tboba_table = boba_table.rename(columns={\"store_name\":\"shop_name\"})\n\tboba_table = boba_table.drop(columns=['count'])\n\t# add primary key for None (to account for colleges with no shops nearby)\n\tboba_table = boba_table.append({'shop_name': None}, ignore_index=True)\n\tboba_table['shop_id'] = range(1,len(boba_table)+1)\n\tboba_table.to_sql('Shops', conn, if_exists='replace', index = False)\n\n\t# create foreign key list\n\tshop_fkid = boba_table.drop(columns=['latitude','longitude'])\n\tshop_fkid = shop_fkid.set_index(['shop_name'])\n\tshop_fkid = shop_fkid.to_dict()\n\n\tshop_fkid_list = []\n\tfor shop_name in boba_df['store_name']:\n\t\tshop_fkid_list.append(shop_fkid['shop_id'][shop_name])\n\n\tboba_df['store_name'] = shop_fkid_list\n\tboba_df = boba_df.rename(columns={'store_name':'shop_id'})\n\tboba_df.to_sql('Boba_near_Colleges', conn, if_exists='replace', index = False)\n\n\tprint(\"All clean data as been inserted into total_data.db!\")\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "import requests", "import json", "import argparse", "import pickle", "from pathlib import Path", "# import scraper modules", "import webscrape_wiki", "import webscrape_free4u", "# import api modules", "import api_autocomplete", "import api_geocode", "import api_yelp", "# import sqlite for final db", "import sqlite3", "import sql_tables", "import convert_boba_data", "import create_fk_list", "import folium # need to \"conda install -c conda-forge folium\"", "from folium import plugins", "from folium.plugins import HeatMap", "import maps #module", "import matplotlib", "import matplotlib.pyplot as plt", "import numpy as np", "import seaborn as sns"]}], [], [], [{"term": "def", "name": "timeout", "data": "def timeout(event, context):\r\n\traise Exception('Execution is about to time out, exiting...')\r\n", "description": null, "category": "webscraping", "imports": ["import hashlib\r", "import base64\r", "import logging\r", "import uuid\r", "import boto3\r", "import json\r"]}, {"term": "def", "name": "store_deidentified_message", "data": "def store_deidentified_message(message, entity_map, ddb_table):\r\n\thashed_message = hashlib.sha3_256(message.encode()).hexdigest()\r\n\tfor entity_hash in entity_map:\r\n\t\tddb.put_item(\r\n\t\t\tTableName=ddb_table,\r\n\t\t\tItem={\r\n\t\t\t\t'EntityHash': {\r\n\t\t\t\t\t'S': entity_hash\r\n\t\t\t\t},\r\n\t\t\t\t'Hash_Message': {\r\n\t\t\t\t\t'S': hashed_message\r\n\t\t\t\t},\r\n\t\t\t\t'Entity': {\r\n\t\t\t\t\t'S': entity_map[entity_hash]\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t)\r\n\treturn hashed_message\r\n", "description": null, "category": "webscraping", "imports": ["import hashlib\r", "import base64\r", "import logging\r", "import uuid\r", "import boto3\r", "import json\r"]}, {"term": "def", "name": "deidentify_entities_in_message", "data": "def deidentify_entities_in_message(message, entity_list):\r\n\tentity_map = dict()\r\n\tfor entity in entity_list:\r\n\t  salted_entity = entity['Text'] + str(uuid.uuid4())\r\n\t  hashkey = hashlib.sha3_256(salted_entity.encode()).hexdigest()\r\n\t  entity_map[hashkey] = entity['Text']\r\n\t  message = message.replace(entity['Text'], hashkey)\r\n\treturn message, entity_map\r\n", "description": null, "category": "webscraping", "imports": ["import hashlib\r", "import base64\r", "import logging\r", "import uuid\r", "import boto3\r", "import json\r"]}, {"term": "def", "name": "lambda_handler", "data": "def lambda_handler(event, context):\r\n\tbucket = 'assign2-scrape-bucket'\r\n\ttry:\r\n\t\t# Extract the entities and message from the event\r\n\t\t#originalDataObject= s3.get_object(Bucket = bucket, Key= 'ScrapedFolder/webscrape.txt')\r\n\t\t#Entityobject= s3.get_object(Bucket = bucket, Key= 'EntityExtractionOutput/webscrape.txt.txt')\r\n\t\t#entities_json_data = json.loads(Entityobject['Body'].read())\r\n\t\t#originalData = originalDataObject['Body'].read().decode()\r\n\t\t\r\n\t\toriginalData = event['data']\r\n\t\tentities_json_data = event['entities']\r\n\t\t# Mask entities\r\n\t\tdeidentified_message, entity_map = deidentify_entities_in_message(originalData, entities_json_data)\r\n\t\thashed_message = store_deidentified_message(deidentified_message, entity_map, 'LookUp')\r\n\t\treturn deidentified_message\r\n\texcept Exception as e:\r\n\t  logging.error('Exception: %s. Unable to extract entities from message' % e)\r\n", "description": null, "category": "webscraping", "imports": ["import hashlib\r", "import base64\r", "import logging\r", "import uuid\r", "import boto3\r", "import json\r"]}], [{"term": "class", "name": "classWebscrapeSpiderMiddleware:", "data": "class WebscrapeSpiderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the spider middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_spider_input(self, response, spider):\n\t\t# Called for each response that goes through the spider\n\t\t# middleware and into the spider.\n\n\t\t# Should return None or raise an exception.\n\t\treturn None\n\n\tdef process_spider_output(self, response, result, spider):\n\t\t# Called with the results returned from the Spider, after\n\t\t# it has processed the response.\n\n\t\t# Must return an iterable of Request, or item objects.\n\t\tfor i in result:\n\t\t\tyield i\n\n\tdef process_spider_exception(self, response, exception, spider):\n\t\t# Called when a spider or process_spider_input() method\n\t\t# (from other spider middleware) raises an exception.\n\n\t\t# Should return either None or an iterable of Request or item objects.\n\t\tpass\n\n\tdef process_start_requests(self, start_requests, spider):\n\t\t# Called with the start requests of the spider, and works\n\t\t# similarly to the process_spider_output() method, except\n\t\t# that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n\t\t# Must return only requests (not items).\n\t\tfor r in start_requests:\n\t\t\tyield r\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}, {"term": "class", "name": "classWebscrapeDownloaderMiddleware:", "data": "class WebscrapeDownloaderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the downloader middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_request(self, request, spider):\n\t\t# Called for each request that goes through the downloader\n\t\t# middleware.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this request\n\t\t# - or return a Response object\n\t\t# - or return a Request object\n\t\t# - or raise IgnoreRequest: process_exception() methods of\n\t\t#   installed downloader middleware will be called\n\t\treturn None\n\n\tdef process_response(self, request, response, spider):\n\t\t# Called with the response returned from the downloader.\n\n\t\t# Must either;\n\t\t# - return a Response object\n\t\t# - return a Request object\n\t\t# - or raise IgnoreRequest\n\t\treturn response\n\n\tdef process_exception(self, request, exception, spider):\n\t\t# Called when a download handler or a process_request()\n\t\t# (from other downloader middleware) raises an exception.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this exception\n\t\t# - return a Response object: stops process_exception() chain\n\t\t# - return a Request object: stops process_exception() chain\n\t\tpass\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}], [{"term": "def", "name": "categoryURLWebscraper", "data": "def categoryURLWebscraper(url, textDoc): \n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--ignore-certificate-errors')\n\toptions.add_argument(\"--test-type\")\n\tdriver = webdriver.Chrome(executable_path=\"./chromedriver\",options=options)\n\tdriver.get(url)\n\n\tmenuElements = driver.find_elements_by_class_name('link-page')\n\tmenuElementLnks = set([element.get_attribute('href') for element in menuElements])\n\n\twith open (textDoc, 'w') as f:\n\t\tfor line in menuElementLnks:\n\t\t\tf.write(line)\n\t\t\tf.write('\\n')\n\treturn\n\t\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver"]}], [{"term": "def", "name": "main", "data": "def main():\n\n\trunner()\n", "description": null, "category": "webscraping", "imports": ["from webscrape_backend.backend_runner import runner"]}], [], [{"term": "class", "name": "IMDB", "data": "class IMDB(SCRAPE):\n\n\tdef __init__(self) -> None:\n\t\tsuper().__init__()\n\n", "description": null, "category": "webscraping", "imports": ["from webscrape import SCRAPE"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(link):\n\tproductLink = link\n\tprint(productLink)\n\t# we will be using a headless implementation of chrome\n\toption = Options()\n\toption.headless = True\n\tdriver = webdriver.Chrome(options=option)\n\tdriver.get(productLink)\n\n\t# We're now going to find the availability tag which is a link tag with an href\n\tavailability = driver.find_element(\n\t\tBy.XPATH, '//link[contains(@href, \"http://schema.org\")]').get_attribute(\"href\")\n\n\tif \"InStock\" in availability:\n\t\t# we now have to send ourselves an email notification about the availability\n\t\texec(open(\"/home/mrugank/Documents/Python Projects/Monitoring-and-Alert-System/alert.py\", \"r\").read())\n\t\tdriver.quit()\n\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.by import By", "import re"]}, {"term": "def", "name": "main", "data": "def main():\n\twith open(\"/home/mrugank/Documents/Python Projects/Monitoring-and-Alert-System/Monitor.txt\", 'r') as monitorFile:\n\t\tlines = monitorFile.readlines()\n\t\tregSearch = re.compile(r'https://www.bestbuy.c(a|om)(\\S)+(\\s)?')\n\t\tfor line in lines:\n\t\t\turlMatch = regSearch.search(line)\n\t\t\turl = urlMatch.group(0) if urlMatch else None\n\t\t\tif url is not None:\n\t\t\t\tprocessedUrl = re.sub(\n\t\t\t\t\t'^(.*)(?=https://www.bestbuy.c(a|om))', \"\", url)\n\t\t\t\twebscrape(processedUrl)\n\n", "description": null, "category": "webscraping", "imports": ["from selenium import webdriver", "from selenium.webdriver.chrome.options import Options", "from selenium.webdriver.common.by import By", "import re"]}], [{"term": "def", "name": "webScrape", "data": "def webScrape(ticker):\n\tif ticker == 'BTC' or ticker == 'ETH' or ticker == 'DOGE' or ticker == 'GME' or ticker == 'AMC' or ticker == 'TSLA' or ticker == 'BB':\n\t\tif ticker == 'BTC':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/BTC-USD?p=BTC-USD&.tsrc=fin-srch'\n\t\telif ticker == 'ETH':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/ETH-USD?p=ETH-USD&.tsrc=fin-srch'\n\t\telif ticker == 'DOGE':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/DOGE-USD?p=DOGE-USD&.tsrc=fin-srch'\n\t\telif ticker == 'GME':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/GME?p=GME&.tsrc=fin-srch'\n\t\telif ticker == 'AMC':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/AMC?p=AMC&.tsrc=fin-srch'\n\t\telif ticker == 'TSLA':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/TSLA?p=TSLA&.tsrc=fin-srch'\n\t\telif ticker == 'BB':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/BB?p=BB&.tsrc=fin-srch'\n\n\t\t#opening up connection and grab page\n\t\tuClient = uReq(my_url)\n\t\tpage_html = uClient.read()\n\t\tuClient.close()\n\n\t\t#html parsing\n\t\tpage_soup = soup(page_html, \"html.parser\")\n\n\t\t#grab price diff %\n\t\tif ticker == 'GME' or ticker == 'AMC' or ticker == 'TSLA' or ticker == 'BB':\n\t\t\tdata = str(page_soup.findAll(\"span\", {\"data-reactid\":\"51\"})[0].text)\n\t\telse:\n\t\t\tdata = str(page_soup.findAll(\"span\", {\"data-reactid\":\"34\"})[1].text)\n\n\t\tdata = data[:-2]\n\t\tnewData = \"\"\n\t\tfor element in reversed(data):\n\t\t\tif element == '(':\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tnewData += element\n\t\tnewData = newData[::-1]\n\t\treturn float(newData)\n\n\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request, redirect, url_for", "import smtplib", "from email.message import EmailMessage", "from urllib.request import urlopen as uReq", "from bs4 import BeautifulSoup as soup", "import time", "import sys"]}, {"term": "def", "name": "sendMessage", "data": "def sendMessage(body, subject, to):\n\t#use EmailMessage library and set variables based on function arguments\n\tmsg.set_content(body)\n\tmsg['subject'] = subject\n\tmsg['to'] = to\n\n\tserver.send_message(msg)\n\n\tdel msg['to']\n\tdel msg['subject']\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request, redirect, url_for", "import smtplib", "from email.message import EmailMessage", "from urllib.request import urlopen as uReq", "from bs4 import BeautifulSoup as soup", "import time", "import sys"]}, {"term": "def", "name": "driver", "data": "def driver(ticker, number, carrier, threshold, duration):\n\t#email and login setup\n\tuser = \"your sent from email\"\n\tmsg['from'] = user\n\tpassword = \"your sent from email app password\"\n\tcarrierEmail = \"some email string\"\n\t\n\tif carrier == \"VERIZON\" or carrier == \"XFINITY\":\n\t\tcarrierEmail = \"vtext.com\"\n\telif carrier == \"AT&T\":\n\t\tcarrierEmail = \"txt.att.net\"\n\telif carrier == \"SPRINT\":\n\t\tcarrierEmail = \"messaging.sprintpcs.com\"\n\telif carrier == \"TMOBILE\":\n\t\tcarrierEmail = \"tmomail.net\"\n\telif carrier == \"VIRGIN\":\n\t\tcarrierEmail = \"vmobl.com\"\n\n\tto = number + \"@\" + carrierEmail\n\n\t#server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n\tserver.starttls()\n\tserver.login(user, password)\n\n\tcount = 0\n\n\n\n\t#get initial % change in price \n\tpriorChange = webScrape(ticker)\n\tsubject = 'INITIAL'\n\tbody = 'Welcome to Cereal! You will get alerts for {} when a change in daily price differs by 1%, starting at {}%.'.format(ticker, priorChange)\n\tsendMessage(body, subject, to)\n\n\n\n\t#enter while loop that will continue to run program\n\twhile count != (int(30 * duration)):\n\t\t#get current 24 hour % change\n\t\tcurrentChange = webScrape(ticker)\n\n\t\tif currentChange != priorChange:\n\t\t\tif priorChange > 0 and currentChange < 0:\n\t\t\t\t#output message saying it's now down today\n\t\t\t\tsubject = 'DOWN'\n\t\t\t\tbody = '{} is now down today at {}%'.format(ticker, currentChange)\n\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t#set prior to now be current\n\t\t\t\tpriorChange = currentChange\n\t\t\telif priorChange < 0 and currentChange > 0:\n\t\t\t\t#output message saying it's now up today\n\t\t\t\tsubject = 'UP'\n\t\t\t\tbody = '{} is now up today at {}%'.format(ticker, currentChange)\n\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t#set prior to now be current\n\t\t\t\tpriorChange = currentChange\n\t\t\telif currentChange > 0:\n\t\t\t\tif float(currentChange - priorChange) >= threshold:\n\t\t\t\t\t#output message saying that it's now up even more today at ___\n\t\t\t\t\tsubject = 'UP'\n\t\t\t\t\tbody = '{} is now up even more today at {}%'.format(ticker, currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\t\t\t\telif float(currentChange - priorChange) <= (-1 * threshold):\n\t\t\t\t\t#output message saying that it's now up less today at ___\n\t\t\t\t\tsubject = 'UP'\n\t\t\t\t\tbody = '{} is now up less today at {}%'.format(ticker, currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\t\t\telif currentChange < 0:\n\t\t\t\tif float(currentChange - priorChange) >= threshold:\n\t\t\t\t\t#ouptut message saying that it's now down even more today at ___\n\t\t\t\t\tsubject = 'DOWN'\n\t\t\t\t\tbody = '{} is now down less today at {}%'.format(ticker, currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\t\t\t\telif float(currentChange - priorChange) <= (-1 * threshold):\n\t\t\t\t\t#output message saying that it's now down less today at ___\n\t\t\t\t\tsubject = 'DOWN'\n\t\t\t\t\tbody = '{} is now down even more today at {}%'.format(ticker, currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\n\t\t#wait 2 minutes before entering loop again\n\t\ttime.sleep(120)\n\n\t\tcount = count + 1\n\n\t#exits while loop, we are done\t\t\t\n\tserver.quit()\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request, redirect, url_for", "import smtplib", "from email.message import EmailMessage", "from urllib.request import urlopen as uReq", "from bs4 import BeautifulSoup as soup", "import time", "import sys"]}, {"term": "def", "name": "home", "data": "def home():\n\tif request.method == \"POST\":\n\n\t\t#get info from html form\n\t\tticker = request.form.get(\"ticker\")\n\t\tnumber = request.form.get(\"number\")\n\t\tcarrier = request.form.get('carrier')\n\t\tthreshold = float(request.form.get('alerts'))\n\t\tduration = float(request.form.get('duration'))\n\n\t\tdriver(ticker, number, carrier, threshold, duration)\n\n\t\treturn render_template(\"buttonpressed.html\")\n\n\treturn render_template(\"home.html\")\n\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, render_template, request, redirect, url_for", "import smtplib", "from email.message import EmailMessage", "from urllib.request import urlopen as uReq", "from bs4 import BeautifulSoup as soup", "import time", "import sys"]}], [], [{"term": "class", "name": "CrawlType", "data": "class CrawlType(Enum):\n\t'''\n\tpossible supported storage modes\n\t'''\n\tEVENT = \"Event\"\t  \n\tSERIES = \"Series\"\n\t\n\t@property\n\tdef urlPrefix(self):\n\t\tbaseUrl=\"http://www.wikicfp.com/cfp\"\n\t\tif self.value is CrawlType.EVENT.value:\n\t\t\turl= f\"{baseUrl}/servlet/event.showcfp?eventid=\"\n\t\telif self.value is CrawlType.SERIES.value:\n\t\t\turl= f\"{baseUrl}/program?id=\"\n\t\treturn url\n\t\n \n\t@classmethod\n\tdef isValid(cls,value:str)->bool:\n\t\t'''\n\t\tcheck whether the given value is valid\n\t\t\n\t\tArgs:\n\t\t\tvalue(str): the value to check\n\t\t\n\t\tReturn:\n\t\t\tbool: True if the value is a valid value of this enum\n\t\t'''\n\t\treturn value in [\"Event\",\"Series\"]\n\n\t@classmethod\n\tdef ofValue(cls,value:str):\n\t\tif value==\"Event\":\n\t\t\treturn CrawlType.EVENT\n\t\telif value==\"Series\":\n\t\t\treturn CrawlType.SERIES\n\t\telse:\n\t\t\treturn None\n", "description": null, "category": "webscraping", "imports": ["from corpus.datasources.webscrape import WebScrape", "from corpus.event import EventStorage,EventManager, EventSeriesManager", "import datetime", "from enum import Enum", "import glob", "import re", "import os", "import sys", "import threading", "import time", "from argparse import ArgumentParser", "from argparse import RawDescriptionHelpFormatter", "#from lodstorage.jsonpicklemixin import JsonPickleMixin", "from lodstorage.storageconfig import StorageConfig", "import corpus.datasources.wikicfp as wcfp", "#import jsonpickle"]}, {"term": "class", "name": "CrawlBatch", "data": "class CrawlBatch(object):\n\t'''\n\tdescribe a batch of pages to fetch metadata from\n\t'''\n\t\n\tdef __init__(self,threads:int,startId:int,stopId:int,crawlTypeValue:str,threadIndex=None,):\n\t\t'''\n\t\tconstruct me with the given number of threads, startId and stopId\n\t\t\n\t\tArgs:\n\t\t   threads(int): number of threads to use\n\t\t   startId(int): id of the event to start crawling with\n\t\t   stopId(int): id of the event to stop crawling\n\t\t   crawlTypeValue(str): the type of crawling (Event or Series)\n\t\t   threadIndex(int): the index of this batch\n\t\t'''\n\t\tself.threads=threads\n\t\tself.threadIndex=threadIndex\n\t\tself.startId=startId\n\t\tself.stopId=stopId\t \n\t\tif startId <= stopId: \n\t\t\tself.step = +1\n\t\t\tself.total = stopId-startId+1\n\t\telse: \n\t\t\tself.step = -1\n\t\t\tself.total = startId-stopId+1\n\t\tself.batchSize = self.total // threads\n\t\tif not CrawlType.isValid(crawlTypeValue):\n\t\t\traise Exception(f\"Invalid crawlType {crawlTypeValue}\")\n\t\tself.crawlType=CrawlType.ofValue(crawlTypeValue)\n\t\t\n\tdef split(self)->list:\n\t\t'''\n\t\tsplit me for my threads\n\t\t'''\n\t\tcrawlBatches=[]\n\t\tfor threadIndex in range(self.threads):\n\t\t\ts = self.startId + threadIndex * self.batchSize\n\t\t\te = s + self.batchSize-1\n\t\t\tsplitBatch=CrawlBatch(1,s, e,self.crawlType.value,threadIndex)\n\t\t\tcrawlBatches.append(splitBatch)\n\t\treturn crawlBatches\n\t  \n\t\n\tdef __str__(self):\n\t\t'''\n\t\tget my string representation\n\t\tReturn\n\t\t\tstr: my string representation\n\t\t'''\n\t\ttext=f\"WikiCFP {self.crawlType.value} IDs {self.startId} - {self.stopId} ({self.threads} threads of {self.batchSize} IDs each)\"\n\t\treturn text\n", "description": null, "category": "webscraping", "imports": ["from corpus.datasources.webscrape import WebScrape", "from corpus.event import EventStorage,EventManager, EventSeriesManager", "import datetime", "from enum import Enum", "import glob", "import re", "import os", "import sys", "import threading", "import time", "from argparse import ArgumentParser", "from argparse import RawDescriptionHelpFormatter", "#from lodstorage.jsonpicklemixin import JsonPickleMixin", "from lodstorage.storageconfig import StorageConfig", "import corpus.datasources.wikicfp as wcfp", "#import jsonpickle"]}, {"term": "class", "name": "WikiCfpScrape", "data": "class WikiCfpScrape(object):\n\t'''\n\tsupport events from http://www.wikicfp.com/cfp/\n\t'''\n\n\tdef __init__(self,jsonEventManager,jsonEventSeriesManager,profile:bool=True,debug:bool=False,jsondir:str=None,limit=200000,batchSize=1000,showProgress=True):\n\t\t'''\n\t\tConstructor\n\t\t\n\t\tArgs:\n\t\t\tconfig(StorageConfig): the storage configuration to use\n\t\t\tdebug(bool): if True debug for crawling is on\n\t\t\tlimit(int): maximum number of entries to be crawled\n\t\t\tbatchSize(int): default size of batches\n\t\t\tshowProgress(bool): if True show Progress\n\t\t'''\n\t\tself.debug=debug\n\t\tself.limit=limit\n\t\tself.batchSize=batchSize\n\t\tself.showProgress=showProgress\n\t\tself.jsonEventManager=jsonEventManager\n\t\tself.jsonEventSeriesManager=jsonEventSeriesManager\n\t\tself.jsonManagers={\n\t\t\t\"Event\": jsonEventManager,\n\t\t\t\"Series\": jsonEventSeriesManager\n\t\t}\n\t\tself.profile=profile\n\t\tif jsondir is not None:\n\t\t\tself.jsondir=jsondir\n\t\telse:\n\t\t\tcachePath=self.jsonEventManager.config.getCachePath()\n\t\t\tself.jsondir=f\"{cachePath}/wikicfp\"\n\t\t\tif not os.path.exists(self.jsondir):\n\t\t\t\t\tos.makedirs(self.jsondir)\n\t\t\n\tdef getManager(self,crawlType:CrawlType):\n\t\t'''\n\t\tget the manager for the given crawlType\n\t\t\n\t\tArgs:\n\t\t   crawlType(CrawlType) the manager for the given CrawlType\n\t\t'''\n\t\tmanager=self.jsonManagers[crawlType.value]\n\t\treturn manager\n\t\n\tdef cacheToJsonManager(self,crawlType:CrawlType):\n\t\t'''\n\t\tcache the crawlType entities to my json manager for that crawlType\n\t\t\n\t\tArgs:\n\t\t\tcrawlType(CrawlType): the CrawlType to work with\n\t\t\t\n\t\tReturn:\n\t\t\tEntityBasemanager: the entityManager\n\t\t\t\n\t\t'''\n\t\tjsonEm=self.getManager(crawlType)\n\t\tif jsonEm.isCached():\n\t\t\tjsonEm.fromStore()\n\t\telse:\t\n\t\t\tself.crawlFilesToJson(crawlType=crawlType,withStore=True)\n\t\treturn jsonEm\t\n\t\t\n\tdef crawlFilesToJson(self,crawlType:CrawlType,withStore=True):\t\n\t\t'''\n\t\tconvert the crawlFiles to Json\n\t\t\n\t\tArgs:\n\t\t\tcrawlType(CrawlType): the CrawlType to work with\n\t\t\twithStore(bool): True if the data should be stored after collecting\n\t\t\n\t\tReturn:\n\t\t\tjsonEm(EventBaseManager): the JSON - storage based EventBaseManager to use to collect the results\n\t\t'''\n\t\t# crawling is not done on startup but need to be done\n\t\t# in command line mode ... we just collect the json crawl result files here\n\t\t#self.crawl(startId=startId,stopId=stopId)\n\t\tjsonEm=self.getManager(crawlType)\n\t\tentityList=jsonEm.getList()\n\t\tstartTime=time.time()\n\t\tjsonFiles=self.jsonFiles(crawlType)\n\t\tif len(jsonFiles)==0:\n\t\t\tif self.profile or self.debug:\n\t\t\t\tprint(f\"No wikiCFP crawl json backups for {crawlType.value} available\")\n\t\telse:\n\t\t\t# loop over all crawled files\n\t\t\tfor jsonFilePath in jsonFiles:\n\t\t\t\tconfig=StorageConfig.getJSON(debug=self.debug)\n\t\t\t\tconfig.cacheFile=jsonFilePath\n\t\t\t\tif crawlType.value is CrawlType.EVENT.value:\n\t\t\t\t\tbatchEm=EventManager(name=jsonFilePath,clazz=jsonEm.clazz,config=config)\n\t\t\t\telif crawlType.value is CrawlType.SERIES.value:\n\t\t\t\t\tbatchEm=EventSeriesManager(name=jsonFilePath,clazz=jsonEm.clazz,config=config)\n\t\t\t\t# legacy mode -file were created before ORM Mode\n\t\t\t\t# was available - TODO: make new files available in ORM mode with jsonable\n\t\t\t\t#jsonPickleEm=JsonPickleMixin.readJsonPickle(jsonFileName=jsonFilePath,extension='.json')\n\t\t\t\t#jsonPickleEvents=jsonPickleEm['events']\n\t\t\t\t#if self.showProgress:\n\t\t\t\t#\tprint(\"%4d: %s\" % (len(jsonPickleEvents),jsonFilePath))\n\t\t\t\tbatchEm.fromStore(cacheFile=jsonFilePath)\n\t\t\t\tfor entity in batchEm.getList():\n\t\t\t\t\tif hasattr(entity, \"startDate\"):\n\t\t\t\t\t\tif entity.startDate is not None:\n\t\t\t\t\t\t\tentity.year=entity.startDate.year  \n\t\t\t\t\tdoAdd=not(entity.deleted)\n\t\t\t\t\t# SPAM Filter\n\t\t\t\t\tif hasattr(entity, \"locality\"):\n\t\t\t\t\t\tif entity.locality.startswith(\"1\"):\n\t\t\t\t\t\t\tdoAdd=False\n\t\t\t\t\t# Series Filter\n\t\t\t\t\tif hasattr(entity, \"title\"):\n\t\t\t\t\t\tif entity.title.startswith(\"WikiCFP : Call For Papers of Conferences, Workshops and Journals\"):\n\t\t\t\t\t\t\tdoAdd=False\n\t\t\t\t\t\n\t\t\t\t\tif doAdd:\n\t\t\t\t\t\tentityList.append(entity)\n\t\t\tif self.profile:\n\t\t\t\telapsed=time.time()-startTime\n\t\t\t\tprint (f\"read {len(entityList)} {crawlType.value} records in {elapsed:5.1f} s\")\n\t\t\tif withStore:\n\t\t\t\tjsonEm.store(limit=self.limit,batchSize=self.batchSize)\n\t\treturn jsonEm\n\t\t\n\tdef jsonFiles(self,crawlType:CrawlType)->list:  \n\t\t'''\n\t\tget the list of the json files that have my data\n\t\t\n\t\tArgs:\n\t\t\tcrawlType(CrawlType): the tpe of the files\n\t\t\t\n\t\tReturn:\n\t\t\tlist: a list of json file names\n\t\t'''\n\t\tjsonFiles=sorted(glob.glob(f\"{self.jsondir}/wikicfp_{crawlType.value}*.json\"),key=lambda path:int(re.findall(r'\\d+',path)[0]))\n\t\treturn jsonFiles\t\n\t\t\n\tdef getJsonFileName(self,crawlBatch):\n\t\t'''\n\t\tget my the JsonFileName \n\t\t\n\t\tArgs:\n\t\t\tcrawlBatch(CrawlBatch): the batch to crawl):\n\t\t\t\n\t\tReturn:\n\t\t\tstr: the json file name for this batch\n\t\t'''\n\t\tjsonFilePath=f\"{self.jsondir}/wikicfp_{crawlBatch.crawlType.value}{crawlBatch.startId:06d}-{crawlBatch.stopId:06d}.json\"\n\t\treturn jsonFilePath\n\t\n\tdef getBatchEntityManager(self,crawlBatch:CrawlBatch):\n\t\t'''\n\t\tget the batch Entity Manager for the given crawlBatch\n\t\t\n\t\tArgs:\n\t\t\tcrawlBatch(CrawlBatch): the batch to crawl):\n\t\t\t\n\t\tReturn:\n\t\t\tEntityManager: either a Event or a Series Manager\n\t\t'''\n\t\tjsonFilepath=self.getJsonFileName(crawlBatch)\n\t\tconfig=EventStorage.getStorageConfig(debug=self.debug, mode=\"json\")\n\t\tconfig.cacheFile=jsonFilepath\n\t\tcrawlType=crawlBatch.crawlType\n\t\tprint(f\"CrawlBatch has crawlType {type(crawlType)}{crawlType}/{crawlType.value}\")\n\t\tif crawlType.value is CrawlType.EVENT.value:\n\t\t\tbatchEm=wcfp.WikiCfpEventManager(config=config)\n\t\telif crawlType.value is CrawlType.SERIES.value:\n\t\t\tbatchEm=wcfp.WikiCfpEventSeriesManager(config=config)\n\t\telse:\n\t\t\traise Exception(f\"Invalid crawlType {crawlType}\")\n\t\treturn batchEm\n\t\t\n\tdef crawl(self,crawlBatch:CrawlBatch):\n\t\t'''\n\t\tsee https://github.com/TIBHannover/confIDent-dataScraping/blob/master/wikicfp.py\n\t\t\n\t\tArgs:\n\t\t\tcrawlBatch(CrawlBatch): the batch to crawl\n\t\t'''\n\t   \n\t\tprint(f'crawling {crawlBatch}')\n\t\tbatchEm=self.getBatchEntityManager(crawlBatch)\n \n\t\t# get all ids\n\t\tcrawlType=crawlBatch.crawlType\n\t\tfor eventId in range(int(crawlBatch.startId), int(crawlBatch.stopId+1), crawlBatch.step):\n\t\t\twEvent=WikiCfpEventFetcher(crawlType=crawlType)\n\t\t\tretry=1\n\t\t\tmaxRetries=3\n\t\t\tretrievedResult=False\n\t\t\twhile not retrievedResult:\n\t\t\t\ttry:\n\t\t\t\t\trawEvent=wEvent.fromEventId(eventId)\n\t\t\t\t\tif crawlType.value is CrawlType.EVENT.value:\n\t\t\t\t\t\tevent=wcfp.WikiCfpEvent()\n\t\t\t\t\t\tevent.fromDict(rawEvent)\n\t\t\t\t\t\ttitle=\"? deleted: %r\" %event.deleted if not 'title' in rawEvent else event.title\n\t\t\t\t\t\tbatchEm.getList().append(event)\n\t\t\t\t\telif crawlType.value is CrawlType.SERIES.value:\n\t\t\t\t\t\teventSeries=wcfp.WikiCfpEventSeries()\n\t\t\t\t\t\teventSeries.fromDict(rawEvent)\n\t\t\t\t\t\ttitle=\"?\" if not 'title' in rawEvent else eventSeries.title\n\t\t\t\t\t\tbatchEm.getList().append(eventSeries)\n\t\t\t\t\tretrievedResult=True\n\t\t\t\texcept Exception as ex:\n\t\t\t\t\tif \"HTTP Error 500\" in str(ex):\n\t\t\t\t\t\tprint(f\"{eventId} inaccessible due to HTTP Error 500\")\n\t\t\t\t\t\tretrievedResult=True\n\t\t\t\t\telif \"timed out\" in str(ex):\n\t\t\t\t\t\tprint(f\"{eventId} access timed Out on retry attempt {retry}\")\n\t\t\t\t\t\tretry+=1\n\t\t\t\t\t\tif retry>maxRetries:\n\t\t\t\t\t\t\traise ex\n\t\t\t\t\telse:\n\t\t\t\t\t\traise ex\n\t\t\t\t\tpass\n\t\t\t\t\n\t\t\tprint(f\"{eventId:06d}: {title}\")\n\t\t   \n\t\t   \n\t\tbatchEm.store()\n\t\treturn batchEm\n\t\t\t\n\tdef threadedCrawl(self,crawlBatch:CrawlBatch):\n\t\t'''\n\t\tcrawl with the given number of threads, startId and stopId\n\t\t\n\t\tArgs:\n\t\t\tcrawlBatch(CrawlBatch): the batch to crawl\n\t\t'''\n\t\t# determine the eventId range for each threaded job\n\t\tstartTime=time.time()\n\t\t\n\t\tmsg=f'Crawling {crawlBatch}'\n\t\tprint(msg)\n\n\t\t# this list will contain all threads -> we can wait for all to finish at the end\n\t\tjobs = []\n\n\t\t# now start each thread with its id range and own filename\n\t\tfor crawlBatch in crawlBatch.split(): \n\t\t\n\t\t\tthread = threading.Thread(target = self.crawl, args=(crawlBatch,))\n\t\t\tjobs.append(thread)\n\t\t\t\n\t\tfor job in jobs:\n\t\t\tjob.start()   \n\n\t\t# wait till all threads have finished before print the last output\n\t\tfor job in jobs:\n\t\t\tjob.join()\n\n\t\tif self.debug:\n\t\t\telapsed=time.time()-startTime\n\t\t\tprint(f'crawling done after {elapsed:5.1f} s')\n\t\t\t   \n", "description": null, "category": "webscraping", "imports": ["from corpus.datasources.webscrape import WebScrape", "from corpus.event import EventStorage,EventManager, EventSeriesManager", "import datetime", "from enum import Enum", "import glob", "import re", "import os", "import sys", "import threading", "import time", "from argparse import ArgumentParser", "from argparse import RawDescriptionHelpFormatter", "#from lodstorage.jsonpicklemixin import JsonPickleMixin", "from lodstorage.storageconfig import StorageConfig", "import corpus.datasources.wikicfp as wcfp", "#import jsonpickle"]}, {"term": "class", "name": "WikiCfpEventFetcher", "data": "class WikiCfpEventFetcher(object):\n\t'''\n\ta single WikiCfpEentFetcher to fetch and event or series\n\t'''\n\tdef __init__(self,crawlType=CrawlType.EVENT,debug=False,showProgress:bool=True,timeout=20):\n\t\t'''\n\t\tconstruct me\n\t\t\n\t\tArgs:\n\t\t\tshowProgress(bool): if True show progress\n\t\t\ttimeout(float): the default timeout\n\t\t\n\t\t'''\n\t\tself.debug=debug\n\t\tself.crawlType=crawlType\n\t\tself.showProgress=showProgress\n\t\tself.progressCount=0\n\t\tself.timeout=timeout\n\t\t\t\n\tdef fromTriples(self,rawEvent,triples): \n\t\t'''\n\t\tget the rawEvent dict from the given triple e.g.:\n\t\t\n\t\tv:Event(v:summary)=IDC 2009\n\t\tv:Event(v:eventType)=Conference\n\t\tv:Event(v:startDate)=2009-06-03T00:00:00\n\t\tv:Event(v:endDate)=2009-06-05T23:59:59\n\t\tv:Event(v:locality)=Milano, Como, Italy\n\t\tv:Event(v:description)= IDC  2009 : The 8th International Conference on Interaction Design and Children\n\t\tv:Address(v:locality)=Milano, Como, Italy\n\t\tv:Event(v:summary)=Submission Deadline\n\t\tv:Event(v:startDate)=2009-01-19T00:00:00\n\t\tv:Event(v:summary)=Notification Due\n\t\tv:Event(v:startDate)=2009-02-20T00:00:00\n\t\tv:Event(v:summary)=Final Version Due\n\t\tv:Event(v:startDate)=2009-03-16T00:00:00\n\t\t'''\n\t\trecentSummary=None\n\t\tfor s,p,o in triples:\n\t\t\ts=s.replace(\"v:\",\"\")\n\t\t\tp=p.replace(\"v:\",\"\")\n\t\t\tif self.debug:\n\t\t\t\tprint (\"%s(%s)=%s\" % (s,p,o)) \n\t\t\tif recentSummary in ['Submission Deadline','Notification Due','Final Version Due']:\t\t\t \n\t\t\t\tkey=recentSummary.replace(\" \",\"_\")\n\t\t\telse:\n\t\t\t\tkey=p \n\t\t\tif p.endswith('Date'):\n\t\t\t\tdateStr=o\n\t\t\t\tif dateStr==\"TBD\":\n\t\t\t\t\to=None\n\t\t\t\telse:\t\n\t\t\t\t\to=datetime.datetime.strptime(\n\t\t\t\t\t\tdateStr, \"%Y-%m-%dT%H:%M:%S\").date()\n\t\t\tif not key in rawEvent: \n\t\t\t\trawEvent[key]=o\t\n\t\t\tif p==\"summary\": \n\t\t\t\trecentSummary=o \n\t\t\telse: \n\t\t\t\trecentSummary=None\n\t\t\t\t\n\t@staticmethod\t   \n\tdef getUrl(cfpid,crawlType:CrawlType=CrawlType.EVENT)->str:\n\t\t'''\n\t\tArgs:\n\t\t\tcfpid(int): the WikiCFP id of the event or series\n\t\t\tcrawlType(CrawlType): Event or Series\n\t\t\t\n\t\tReturns:\n\t\t\tthe WikiCfP url\n\t\t'''\n\t\turl=f\"{crawlType.urlPrefix}{cfpid}\"\n\t\treturn url   \n\t\n\t@classmethod\n\tdef getLatestEvent(cls,debug=False,showProgress=True):\n\t\t'''\n\t\tget the latest Event doing a binary search\n\t\t'''\n\t\twikicfp=WikiCfpEventFetcher(debug,showProgress=showProgress)\n\t\twikicfp.progressCount=0\n\t\twikicfp.getLatesEvetFromPair()\n\t\t\n\tdef getHighestNonDeletedIdInRange(self,fromId:int,toId:int)->int:\n\t\t'''\n\t\tget the event with the highest id int the range fromId to toId that is not deleted\n\t\t\n\t\tArgs:\n\t\t\tfromId(int): minimum id to search from\n\t\t\ttoId(int): maximum id to search to\n\t\t\t\n\t\tReturns:\n\t\t\tint: maxium id of an event that is not deleted or None if there is none in this range\n\t\t'''\n\t\tmaxId=None\n\t\tfor eventId in range(fromId,toId+1):\n\t\t\tif self.showProgress:\n\t\t\t\tprint(\".\",end='',flush=True)\n\t\t\t\tself.progressCount+=1\n\t\t\t\tif self.progressCount % 50 == 0:\n\t\t\t\t\tprint(flush=True)\n\t\t\trawEvent=self.fromEventId(eventId)\n\t\t\tif not rawEvent['deleted']:\n\t\t\t\tmaxId=eventId\n\t\treturn maxId\n\t\n\tdef getLatesEvetFromPair(self,low=5000,high=300000,margin=40):\n\t\t'''\n\t\tget the latest Event doing a binary search\n\t\t\n\t\tArgs:\n\t\t\tlow(int): lower index to search from\n\t\t\thight(int): upper index boundary\n\t\t'''\n\t\tif high-low>margin+1:\n\t\t\tmid=(high+low)//2\n\t\t\tmidId=(self.getHighestNonDeletedIdInRange(mid-margin, mid))\n\t\t\tif midId:\n\t\t\t\treturn self.getLatesEvetFromPair(mid+1,high)\n\t\t\telse:\n\t\t\t\treturn self.getLatesEvetFromPair(low, mid-1)\n\t\telse:\n\t\t\treturn mid\n\t\tpass\n\t\t\t\t\t\t   \n\tdef fromEventId(self,cfpid:int):\n\t\t'''\n\t\tsee e.g. https://github.com/andreeaiana/graph_confrec/blob/master/src/data/WikiCFPCrawler.py\n\t\t\n\t\tArgs:\n\t\t\tcfpid(int): the wikicfp id to use\n\t\t'''\n\t\turl=WikiCfpEventFetcher.getUrl(cfpid,self.crawlType)\n\t\treturn self.fromUrl(url)\n\t\n\tdef rawEventFromWebScrape(self,rawEvent:dict,triples:list,scrape:WebScrape):\n\t\t'''\n\t\tfill the given rawEvent with the data derived from the scrape \n\t\t\n\t\tArgs:\n\t\t\trawEvent(dict): the event dictionary\n\t\t\ttriples(list): the triples found\n\t\t\tscrape(WebScrape): the webscrape object to be used for parsing\n\t\t'''\n\t\tif len(triples)==0:\n\t\t\t#scrape.printPrettyHtml(scrape.soup)\n\t\t\tfirstH3=scrape.fromTag(scrape.soup, 'h3')\n\t\t\tif \"This item has been deleted\" in firstH3:\n\t\t\t\trawEvent['deleted']=True\n\t\telse:\t\t\n\t\t\tself.fromTriples(rawEvent,triples)\n\t\t\t# add series information\n\t\t\t# Tag: International Semantic Web Conference\n\t\t\tm,seriesText=scrape.findLinkForRegexp(r'/cfp/program\\?id=([0-9]+).*')\n\t\t\tif m:\n\t\t\t\tseriesId=m.group(1)\n\t\t\t\trawEvent['seriesId']=seriesId\n\t\t\t\trawEvent['series']=seriesText\n\t\t\t\tpass\n\t\t\t\t\n\t\t\tif 'summary' in rawEvent:\n\t\t\t\trawEvent['acronym']=rawEvent.pop('summary').strip()\n\t\t\tif 'description' in rawEvent:\n\t\t\t\trawEvent['title']=rawEvent.pop('description').strip()\n\t\t\t\t\n\tdef rawEventSeriesFromWebScrape(self,rawEvent:dict,scrape:WebScrape):\n\t\t'''\n\t\tfill the given rawEventSeries with the data derived from the scrape \n\t\t\n\t\tArgs:\n\t\t\trawEvent(dict): the event dictionary\n\t\t\tscrape(WebScrape): the webscrape object to be used for parsing\n\t\t'''\n\t\ttitle=scrape.soup.find(\"title\")\n\t\trawEvent[\"title\"]=title.text.strip()\n\t\tdblpM,_text=scrape.findLinkForRegexp(r'http://dblp.uni-trier.de/db/([a-zA-Z0-9/-]+)/index.html')\n\t\tif dblpM:\n\t\t\tdblpSeriesId=dblpM.group(1)\n\t\t\trawEvent['dblpSeriesId']=dblpSeriesId\n\t\tpass\n \n\t\n\tdef fromUrl(self,url:str)->dict:\n\t\t'''\n\t\tget the event form the given url\n\t\t\n\t\tArgs:\n\t\t\turl(str): the url to get the event from\n\t\t\n\t\tReturns:\n\t\t\tdict: a raw event dict or None if an error occured\n\t\t\n\t\t'''\n\t\tregexp=r\"^\"+self.crawlType.urlPrefix.replace(\"?\",\"\\?\")+\"(\\d+)$\"\n\t\tm=re.match(regexp,url)\n\t\tif not m:\n\t\t\traise Exception(\"Invalid URL %s\" % (url))\n\t\telse:\n\t\t\tcfpId=int(m.group(1))\n\t\trawEvent={}\n\t\tif self.crawlType.value is CrawlType.EVENT.value:\n\t\t\trawEvent['eventId']=f\"{cfpId}\" \n\t\telif self.crawlType.value is CrawlType.SERIES.value:\n\t\t\trawEvent['seriesId']=f\"{cfpId}\" \n\t\trawEvent['url']=url\n\t\trawEvent['wikiCfpId']=cfpId\n\t\trawEvent['deleted']=False\n\t\tscrape=WebScrape(debug=self.debug,timeout=self.timeout)\n\t\ttriples=scrape.parseRDFa(url)\n\t\tif scrape.err:\n\t\t\traise Exception(f\"fromUrl {url} failed {scrape.err}\")\n\t\tif self.crawlType.value is CrawlType.EVENT.value:\n\t\t\tself.rawEventFromWebScrape(rawEvent, triples, scrape)\n\t\telse:\n\t\t\tself.rawEventSeriesFromWebScrape(rawEvent,scrape)\n\t\treturn rawEvent\n", "description": null, "category": "webscraping", "imports": ["from corpus.datasources.webscrape import WebScrape", "from corpus.event import EventStorage,EventManager, EventSeriesManager", "import datetime", "from enum import Enum", "import glob", "import re", "import os", "import sys", "import threading", "import time", "from argparse import ArgumentParser", "from argparse import RawDescriptionHelpFormatter", "#from lodstorage.jsonpicklemixin import JsonPickleMixin", "from lodstorage.storageconfig import StorageConfig", "import corpus.datasources.wikicfp as wcfp", "#import jsonpickle"]}, {"term": "def", "name": "main", "data": "def main(argv=None): # IGNORE:C0111\n\t'''main program.'''\n\n\tif argv is None:\n\t\targv=sys.argv[1:]\n\t\t\n\tprogram_name = os.path.basename(__file__)\n\tprogram_version = \"v%s\" % __version__\n\tprogram_build_date = str(__updated__)\n\tprogram_version_message = '%%(prog)s %s (%s)' % (program_version, program_build_date)\n\tprogram_shortdesc = \"script to read event metadata from http://wikicfp.com\"\n\tuser_name=\"Wolfgang Fahl\"\n\tprogram_license = '''%s\n", "description": null, "category": "webscraping", "imports": ["from corpus.datasources.webscrape import WebScrape", "from corpus.event import EventStorage,EventManager, EventSeriesManager", "import datetime", "from enum import Enum", "import glob", "import re", "import os", "import sys", "import threading", "import time", "from argparse import ArgumentParser", "from argparse import RawDescriptionHelpFormatter", "#from lodstorage.jsonpicklemixin import JsonPickleMixin", "from lodstorage.storageconfig import StorageConfig", "import corpus.datasources.wikicfp as wcfp", "#import jsonpickle"]}], [{"term": "class", "name": "classprotein3Dcompare:", "data": "class protein3Dcompare:\n\t\n\tdef __init__(self):\n\t\tpass\n\t\t\t\n\tdef testClass(self,mystring):\n\t\tprint(mystring)\n\t\t\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fwebscrape_gene", "data": "\tdef webscrape_gene(self,uniprot_ids):\n\t\tLIST_OF_GENES = [] \n\t\tfor items in uniprot_ids:\n\t\t\t# WEB SCRAPES GENE \n\t\t\tURL = 'https://www.uniprot.org/uniprot/' + items\n\t\t\tpage = requests.get(URL)\n\t\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\t\tresults = soup.find_all('a')\n\t\t\tfor result in results:\n\t\t\t\tif 'eco' in result.text: \n\t\t\t\t\ts = result.text\n\t\t\t\t\tgene = s.strip('eco: ')\n\t\t\t\t\tLIST_OF_GENES.append(gene)\n\t\t\t\t\tbreak\n\t\t\t\t\n\t\treturn LIST_OF_GENES\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fget_geneDict", "data": "\tdef get_geneDict(self,uniprot_ids): \n\t\tgene_dict = {}\n\t\tfor items in uniprot_ids:\n\t\t\t# WEB SCRAPES GENE \n\t\t\tURL = 'https://www.uniprot.org/uniprot/' + items\n\t\t\tpage = requests.get(URL)\n\t\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\t\tresults = soup.find_all('a')\n\t\t\tfor result in results:\n\t\t\t\tif 'eco' in result.text: \n\t\t\t\t\ts = result.text\n\t\t\t\t\tgene = s.strip('eco: ')\n\t\t\t\t\tgene_dict[items] = gene\n\t\t\t\t\tbreak\n\t\t\n\t\treturn gene_dict\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fpdbidToUniprot", "data": "\tdef pdbidToUniprot(self, pdbid): \n\t\tURL = 'https://www.rcsb.org/structure/' + pdbid\n\t\tpage = requests.get(URL)\n\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\tuniprot = 'none'\n\t\tresults = soup.find_all('a')\n\t\tfor result in results:\n\t\t\tu = result.text\n\t\t\tif len(u) == 6 and u[1].isdigit():\n\t\t\t\tuniprot = u\n\t\t\t\tbreak\n\t\t\n\t\t\t\n\t\treturn uniprot \n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fgetOrganism", "data": "\tdef getOrganism(self, uniprot):\n\t\tif uniprot == 'none': \n\t\t\torganism = 'none'\n\t\telse: \n\t\t\tURL = 'https://www.uniprot.org/uniprot/' + uniprot \n\t\t\tpage = requests.get(URL)\n\t\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\t\tresult = soup.find_all('em')\n\t\t\torganism = str(result)\n\t\t\torganism = organism[5:]\n\t\t\torganism = organism[:-6]\n\t\t\t\n\t\treturn organism\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fgetName", "data": "\tdef getName(self, pdbid):\n\t\tURL = 'https://www.rcsb.org/structure/' + pdbid\n\t\tpage = requests.get(URL)\n\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\tresult = soup.find_all(\"span\", id= \"structureTitle\")  \n\t\tnew_string = str(result)\n\t\tindex = new_string.find('>')\n\t\tnew_string = new_string[index+1:]\n\t\tindex = new_string.find('<')\n\t\tname = new_string[:index]\n\n\t\treturn name\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fgetLigand", "data": "\tdef getLigand(self, pdbid):\n\t\tURL = 'https://www.rcsb.org/structure/' + pdbid\n\t\tpage = requests.get(URL)\n\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\tresults = soup.find_all(\"a\") \n\t\tligands = ''\n\t\tfor result in results: \n\t\t\tif 'Query' in result.text:\n\t\t\t\tstring = result.text\n\t\t\t\tindex = string.find('on')\n\t\t\t\tnew_string = string[index+2:]\n\t\t\t\tligands = ligands + new_string + ','\n\t\tligands = ligands[:-1]\n\n\t\treturn ligands \n\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fgetTaxonomy", "data": "\tdef getTaxonomy(self, uniprot):\n\t\tif uniprot == 'none': \n\t\t\ttaxonomyList = 'none'\n\t\telse:\n\t\t\ttaxonomyList = []\n\t\t\thiddenTaxonlist = []\n\t\t\tURL = 'https://www.uniprot.org/uniprot/' + uniprot\n\t\t\tpage = requests.get(URL)\n\t\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\t\tresults = soup.find(id = 'names_and_taxonomy')\n\t\t\tNewresults= results.find_all('span')\n\t\t\tfor result in Newresults:\n\t\t\t\tif 'hiddenTaxon' in str(result):\n\t\t\t\t\tresultFound = False\n\t\t\t\t\tfinal = result.text\n\t\t\t\t\twhile resultFound == False:\n\t\t\t\t\t\tif final[-1].isalpha():\n\t\t\t\t\t\t\tfinal = final\n\t\t\t\t\t\t\tresultFound = True\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfinal = final[:-1] \n\t\t\t\t\thiddenTaxonlist.append(final)\n\t\t\t\t\t\n\t\t\tNewerresults= results.find_all('a') \n\t\t\tfor result in Newerresults:\n\t\t\t\tif 'taxonomy' in str(result):\n\t\t\t\t\ttaxonomyList.append(result.text)\n\t\t\ttaxonomyList = taxonomyList[2:]\n\t\t\tfor item in taxonomyList:\n\t\t\t\tif item in hiddenTaxonlist:\n\t\t\t\t\ttaxonomyList.remove(item)\n\t\t\t\t\t\n\t\treturn taxonomyList\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fgetPubmed", "data": "\tdef getPubmed(self, pdbid):\n\t\ttry:\n\t\t\tURL = 'https://www.rcsb.org/structure/' + pdbid\n\t\t\tpage = requests.get(URL)\n\t\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\t\tresults = soup.find(id = 'pubmedLinks')\n\t\t\tNewresults= results.find_all('a')\n\t\t\tfor result in Newresults:\n\t\t\t\tif 'querySearchLink' in str(result):\n\t\t\t\t\tpubmedID = result.text\t   \n\t\t\t\t\tPubmedLink = 'https://pubmed.ncbi.nlm.nih.gov/' + pubmedID +'/'\n\t\texcept AttributeError:\n\t\t\tPubmedLink = ''\n\t\t\n\t\treturn PubmedLink\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fgetSequence", "data": "\tdef getSequence(self, uniprot):\n\t\tif uniprot == 'none': \n\t\t\tseq = 'none'\n\t\telse:\n\t\t\tURL = 'https://www.uniprot.org/uniprot/' + uniprot\n\t\t\tpage = requests.get(URL)\n\t\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\t\tresults = soup.find(id = 'entrySequence')\n\t\t\tNewresults= results.find_all('pre')\n\t\t\tfor result in Newresults:\n\t\t\t\tseq = result.text \n\t\t\t\t\n\t\treturn seq\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fgetLength", "data": "\tdef getLength(self, pdbid):\n\t\tURL = 'https://www.rcsb.org/structure/' + pdbid\n\t\tpage = requests.get(URL)\n\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n\t\tresults = soup.find(id=\"MacromoleculeTable\")\n\t\tNewresults= results.find_all('td')\n\t\tfor result in Newresults:\n\t\t\tif result.text.isdigit():\n\t\t\t\tlength = result.text\n\t\t\t\n\t\treturn length \n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fwebscrapeData", "data": "\tdef webscrapeData(self, pdbid):\n\t\tfeatureslist = ['PDB', 'Uniprot', 'Organism', 'Name', 'Ligands', 'Global Stoichiometry', 'Sequence','Sequence Length', 'PubMed link', 'Taxonomy'] \n\t\tpdb = pdbid\n\t\tuni = self.pdbidToUniprot(pdbid)\n\t\torg = self.getOrganism(uni)\n\t\tn = self.getName(pdbid)\n\t\tl = self.getLigand(pdbid)\n\t\tg = self.get_global_stoich(pdbid)\n\t\ts = self.getSequence(uni)\n\t\tsl = self.getLength(pdbid)\n\t\tp = self.getPubmed(pdbid)\n\t\tt = self.getTaxonomy(uni)\n\t\t\n\t\tdf1 = pd.DataFrame([[pdb,uni,org,n,l,g,s,sl,p,t]], columns = featureslist )\n\t\t\n\t\treturn df1\n\t\t\t\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fgetHomologData", "data": "\tdef getHomologData(self, genename, targetprotein, pdbidlist):\n\t\tdf = pd.DataFrame()\n\t\ttargetdf = self.webscrapeData(targetprotein)\n\t\tdf = df.append(targetdf)\n\t\t\n\t\tfor pdbid in pdbidlist: \n\t\t\tdf1 = self.webscrapeData(pdbid)\n\t\n\t\t\tdf = df.append(df1)\n\t\t\t\n\t\tfilename = genename +'data.xlsx'\t\n\t\tdf.to_excel(filename, index=False)\t\n\t\treturn df \n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fimportGlob", "data": "\tdef importGlob(self, pdb1, pdb2):\n\t\t\n\t\t# Running FATCAT alignment, saving .cif file in output directory  \n\t\tos.system(\"bash runFATCAT.sh -pdb1 \" + pdb1 + \" -pdb2 \" + pdb2 + \" -pdbFilePath \" + pdbDirectory + \" -autoFetch -flexible -outputPDB -outFile \" + outputDirectory + \"pdbtemporary.pdb\")\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n\t\t\t\t\t\t\t\t\n\t\tproteinz = outputDirectory + 'pdbtemporary.pdb'\n\t\t\n\t\tcmd.load(proteinz, \"proteinz\")\n\t\tcmd.split_states(\"proteinz\")\n\t\tcmd.delete(\"proteinz\")\n\t\tcmd.set_name(\"proteinz_0001\", \"target\")\n\t\tcmd.set_name(\"proteinz_0002\", \"homolog\")\n\t\tcmd.color(\"red\", \"target\")\n\t\tcmd.save(str(outputDirectory) + str(pdb1)+ \"_\" + str(pdb2) + \".cif\")\n\t\t\n\t\ttemppdb = outputDirectory + 'pdbtemporary.pdb'\n\t\tos.remove(temppdb)\n\t\tprint(\"deleted temporary pdb\")\n\t\tprint(\"done\")\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fshortFatcat", "data": "\tdef shortFatcat(self, pdb1, pdb2):\n\n\t\tos.system(\"bash runFATCAT.sh -pdb1 \" + pdb1+ \" -pdb2 \" + pdb2 + \" -pdbFilePath \" + pdbDirectory + \" -autoFetch -flexible -printFatCat -show3d\")\n\t\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fspecificSingleChainAlignment", "data": "\tdef specificSingleChainAlignment(self, pdb1, pdb2, chain1, chain2):\n\t\t\n\t\t# Running FATCAT alignment, saving .cif file in output directory  \n\t\tos.system(\"bash runFATCAT.sh -pdb1 \" + pdb1 + \".\" + chain1 + \" -pdb2 \" + pdb2 + \".\" + chain2 + \" -pdbFilePath \" + pdbDirectory + \" -autoFetch -flexible -outputPDB -outFile \" + outputDirectory + \"pdbtemporary.pdb\")\n\t  \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\tproteinz = outputDirectory + 'pdbtemporary.pdb'\n\t\t\n\t\tcmd.load(proteinz, \"proteinz\")\n\t\tcmd.split_states(\"proteinz\")\n\t\tcmd.delete(\"proteinz\")\n\t\tcmd.set_name(\"proteinz_0001\", \"target\")\n\t\tcmd.set_name(\"proteinz_0002\", \"homolog\")\n\t\tcmd.color(\"red\", \"target\")\n\t\tcmd.save(outputDirectory + pdb1 + \".\" + chain1 + \"_\" + pdb2 + \".\" + chain2 + \".cif\")\n\n\t\ttemppdb = outputDirectory + 'pdbtemporary.pdb'\n\t\tos.remove(temppdb)\n\t\tprint(\"deleted temporary pdb\")\n\t\tprint(\"done\")\n\t\t\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "fChainByChain", "data": "\tdef ChainByChain(self, pdb1, pdb2):\n\t\tcmd.fetch(pdb1)\n\t\tfor ch1 in cmd.get_chains(pdb1):\n\t\t\tcmd.delete(pdb1)\n\t\t\tcmd.fetch(pdb2)\n\t\t\tfor ch2 in cmd.get_chains(pdb2):\n\t\t\t\tcmd.delete(pdb2)\n\t\t\t\tself.specificSingleChainAlignment(pdb1,pdb2,ch1, ch2)\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}, {"term": "def", "name": "ffatcat_savexml", "data": "\tdef fatcat_savexml(self, pdb1, pdb2):  \n\t\t\n\t\tos.system(\"bash runFATCAT.sh -pdb1 \" + pdb1 + \" -pdb2 \" + pdb2 + \" -pdbFilePath \" + pdbDirectory + \" -autoFetch -flexible -printFatCat -printXML \" + outputDirectory + pdb1 + \"_\" + pdb2 + \".xml\")\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import pandas as pd", "from pymol import cmd", "import os", "\tdef importGlob(self, pdb1, pdb2):"]}], [{"term": "class", "name": "classgetUrl:", "data": "class getUrl:\n\turl =None\n\tpayload =None\n\t\n\tdef __init__(self,url,payload=None):\n\t\tself.url=url\n\t\tself.payload=payload\n\n\n", "description": null, "category": "webscraping", "imports": ["# import csv, pandas,\t\tsqlalchemy, os", "import os", "import scrape_utils", "import db_utils", "import requests", "import re", "import pandas as pd", "import db_logging as lg", "import datetime as dt", "from bs4 import BeautifulSoup", "from pprint import pprint"]}], [{"term": "def", "name": "create_connection", "data": "def create_connection():\n\tmydb = mysql.connector.connect(\n\t\thost=\"localhost\",\n\t\tdatabase=\"PenTestingProject\",\n\t\tuser=\"root\",\n\t\tpassword=\"123\"\n\t)\n\tconnector = mydb.cursor()\n\tgetExploits = 'SELECT * FROM Vulnerabilities'\n\tconnector.execute(getExploits)\n\treturn connector.fetchall()\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "from mysql.connector import Error", "import time"]}, {"term": "def", "name": "init", "data": "def init():\n\t#create_connection()\n\tprint('Doop')\n", "description": null, "category": "webscraping", "imports": ["import mysql.connector", "from mysql.connector import Error", "import time"]}], [], [{"term": "class", "name": "classWebscrapeSpiderMiddleware:", "data": "class WebscrapeSpiderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the spider middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_spider_input(self, response, spider):\n\t\t# Called for each response that goes through the spider\n\t\t# middleware and into the spider.\n\n\t\t# Should return None or raise an exception.\n\t\treturn None\n\n\tdef process_spider_output(self, response, result, spider):\n\t\t# Called with the results returned from the Spider, after\n\t\t# it has processed the response.\n\n\t\t# Must return an iterable of Request, or item objects.\n\t\tfor i in result:\n\t\t\tyield i\n\n\tdef process_spider_exception(self, response, exception, spider):\n\t\t# Called when a spider or process_spider_input() method\n\t\t# (from other spider middleware) raises an exception.\n\n\t\t# Should return either None or an iterable of Request or item objects.\n\t\tpass\n\n\tdef process_start_requests(self, start_requests, spider):\n\t\t# Called with the start requests of the spider, and works\n\t\t# similarly to the process_spider_output() method, except\n\t\t# that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n\t\t# Must return only requests (not items).\n\t\tfor r in start_requests:\n\t\t\tyield r\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}, {"term": "class", "name": "classWebscrapeDownloaderMiddleware:", "data": "class WebscrapeDownloaderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the downloader middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_request(self, request, spider):\n\t\t# Called for each request that goes through the downloader\n\t\t# middleware.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this request\n\t\t# - or return a Response object\n\t\t# - or return a Request object\n\t\t# - or raise IgnoreRequest: process_exception() methods of\n\t\t#   installed downloader middleware will be called\n\t\treturn None\n\n\tdef process_response(self, request, response, spider):\n\t\t# Called with the response returned from the downloader.\n\n\t\t# Must either;\n\t\t# - return a Response object\n\t\t# - return a Request object\n\t\t# - or raise IgnoreRequest\n\t\treturn response\n\n\tdef process_exception(self, request, exception, spider):\n\t\t# Called when a download handler or a process_request()\n\t\t# (from other downloader middleware) raises an exception.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this exception\n\t\t# - return a Response object: stops process_exception() chain\n\t\t# - return a Request object: stops process_exception() chain\n\t\tpass\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}], [{"term": "class", "name": "WebscrapeItem", "data": "class WebscrapeItem(scrapy.Item):\n\t# define the fields for your item here like:\n\t# name = scrapy.Field()\n\tpass\n", "description": null, "category": "webscraping", "imports": ["import scrapy"]}], [{"term": "def", "name": "webscrape", "data": "def webscrape(t):\n\t#print(t)\n\t#for site_url in sitemap_url:\n\t#print(site_url)\n\trequest_1 = requests.get(t)\n\tsrc_1 = request_1.content\n\tsoup_1  = BeautifulSoup(src_1, 'html.parser')\n\th1_list = []\n\th2_list = []\n\ttitle_list = []\n\timg_list = []\n\tfor h1 in soup_1.find_all(\"h1\"):\n\t\tif None in h1:\n\t\t\th1_list.append(None)\n\t\telse: \n\t\t\th1_list.append(h1.text)\n\tfor h2 in soup_1.find_all(\"h2\"):\n\t\tif None in h2:\n\t\t\th2_list.append(None)\n\t\telse:\n\t\t\th2_list.append(h2.text)\n\tfor title in soup_1.find_all(\"title\"):\n\t\tif None in title:\n\t\t\ttitle_list.append(None)\n\t\telse:\n\t\t\ttitle_list.append(title.text)\n\tfor img1 in soup_1.find_all(\"img\"):\n\t\tif img1.has_attr('alt'):\n\t\t\timg_list.append(img1['alt'])\n\tdicts_list[\"Website Name\"].append(t) \n\tdicts_list[\"H1 TAG\"].append(h1_list) \n\tdicts_list[\"H2 TAG\"].append(h2_list) \n\tdicts_list[\"TITLE\"].append(title_list) \n\tdicts_list[\"IMG-ALT\"].append(img_list)\n\tprint(\"done\")\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import time", "from time import sleep", "import pandas as pd", "from tabulate import tabulate", "from multiprocessing import Pool", "from multiprocessing import cpu_count", "import concurrent.futures", "from functools import lru_cache ", "import pickle", "import os", "import datetime", "from datetime import date"]}, {"term": "def", "name": "cache_to_disk", "data": "def cache_to_disk(func):\n\tdef wrapper(*args):\n\t\tcache = '.{}{}.pkl'.format(func.__name__, args).replace('/', '_')\n\t\ttry:\n\t\t\twith open(cache, 'rb') as f:\n\t\t\t\ttime_created2 = os.stat(cache).st_mtime\n\t\t\t\tdate_time = datetime.datetime.fromtimestamp(time_created2)\n\t\t\t\tnow = datetime.datetime.today()\n\t\t\t\tdelta  = now - date_time\n\t\t\t\tprint(\"modified date \"+ str(date_time))\n\t\t\t\tprint(\"today's date: \"+ str(now))\n\t\t\t\tprint(delta.days)\n\t\t\t\tdays = delta.days \n\t\t\t\tif days>0:\n\t\t\t\t\tprint(\"A\")\n\t\t\t\t\tresult = func(*args)\n\t\t\t\t\twith open(cache, 'wb') as g:\n\t\t\t\t\t\tpickle.dump(result, g)\n\t\t\t\t\treturn result\n\n\t\t\t\tprint(\"B\")\n\t\t\t\tt3 = time.time()\n\t\t\t\tprint(args)\n\t\t\t\tprint(\"This site has been scraped before\")\n\t\t\t\ttotal_time = t3-t0\n\t\t\t\tprint(\"The total time for scraping\" + \" \" + str(total_time))\n\t\t\t\treturn pickle.load(f)\t \t\t \t\t\n\t\texcept IOError:\n\t\t\tresult = func(*args)\n\t\t\twith open(cache, 'wb') as f:\n\t\t\t\tpickle.dump(result, f)\n\t\t\treturn result\n\n\treturn wrapper\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import time", "from time import sleep", "import pandas as pd", "from tabulate import tabulate", "from multiprocessing import Pool", "from multiprocessing import cpu_count", "import concurrent.futures", "from functools import lru_cache ", "import pickle", "import os", "import datetime", "from datetime import date"]}, {"term": "def", "name": "get_links", "data": "def get_links(url):\n\tresponse = requests.get(url)\n\tsrc_2 = response.content\n\tsoup_get_links = BeautifulSoup(src_2, 'html.parser')\n\tlocal_links = []\n\n\tfor links in soup_get_links.find_all('loc'):\n\t\tlink_url = links.text\n\n\t\tif link_url is  not  None: \n\t\t\tif link_url not in sitemap_url:\n\t\t\t\tsitemap_url.append(link_url) \n\t\t\t\tget_links(link_url)\n\t\t\t\tprint(link_url)\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import time", "from time import sleep", "import pandas as pd", "from tabulate import tabulate", "from multiprocessing import Pool", "from multiprocessing import cpu_count", "import concurrent.futures", "from functools import lru_cache ", "import pickle", "import os", "import datetime", "from datetime import date"]}, {"term": "def", "name": "multipro", "data": "def multipro(site_string):\n\tsite_string = str(site_1)\n\tget_links(site_string)\n\n\tt2 = time.time()\n\tthreads = min(MAX_THREADS, len(sitemap_url))\n\n\twith concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n\t\texecutor.map(webscrape, sitemap_url)\n\n\n\tt1=time.time()\n\ttotal_time=t1-t2\n\ttotal_time2=t1-t0\n\tprint(site_string)\n\tprint(\"The total time for web scraping is\" + \" \" + str(total_time))\n\tprint(\"The total time with recursion is\" + \" \" + str(total_time2))\n\treturn dicts_list\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import time", "from time import sleep", "import pandas as pd", "from tabulate import tabulate", "from multiprocessing import Pool", "from multiprocessing import cpu_count", "import concurrent.futures", "from functools import lru_cache ", "import pickle", "import os", "import datetime", "from datetime import date"]}, {"term": "def", "name": "main", "data": "def main():\n\tsite_string = str(site_1)\n\n\ttemp_dict_list = multipro(site_string)\n\n\tdf = pd.DataFrame(temp_dict_list)\n\tdf.to_html(\"data.html\")\n\n\t#webscrape()\n", "description": null, "category": "webscraping", "imports": ["import requests", "from bs4 import BeautifulSoup", "import time", "from time import sleep", "import pandas as pd", "from tabulate import tabulate", "from multiprocessing import Pool", "from multiprocessing import cpu_count", "import concurrent.futures", "from functools import lru_cache ", "import pickle", "import os", "import datetime", "from datetime import date"]}], [{"term": "class", "name": "ApiTest", "data": "class ApiTest(unittest.TestCase):\r\n\tapi_url = \"http://127.10.0.0:9000/\"\r\n\r\n\tdef test_get_all_reviews(self):\r\n\t\tr = requests.get(search_url)\r\n\t\tself.assertEqual(r.status_code, 200)\r\n", "description": null, "category": "webscraping", "imports": ["import unittest\r", "import requests\r", "from webscrape import search_url\r"]}], [], [{"term": "def", "name": "index", "data": "def index(request):\n\treturn render(request,'index.html')\n", "description": null, "category": "webscraping", "imports": ["from inspect import formatannotationrelativeto", "from django.http import HttpResponse", "from django.shortcuts import render", "from selenium import webdriver", "from django.views.decorators.csrf import csrf_exempt", "from .webscrape import flipkartscrape,amazonscrape", "from .dbhandler import readIntoDatabase, writeIntoDatabase", "from selenium.webdriver.chrome.options import Options"]}, {"term": "def", "name": "webscrape", "data": "def webscrape(request):\n\n\t\n\tif request.method == 'POST':\n\t\tamz_link = request.POST.get('amazon')\n\t\tflip_link = request.POST.get('flipkart')\n\t\tc = webdriver.ChromeOptions()\n\t\t# c.add_argument(\"--headless\")\n\t\tc.add_argument(\"--ignore-certificate-error\")\n\t\tc.add_argument(\"--ignore-ssl-errors\")\n\t\tc.add_experimental_option('excludeSwitches', ['enable-logging'])\n\t\tdriver = webdriver.Chrome(chrome_options = c, executable_path= \"C:/drivers/chromedriver.exe\")\n\t\n\n\n\t\tamz_prodcut = amazonscrape(amz_link, driver)\n\t\tflip_prodcut = flipkartscrape(flip_link, driver)\n\t\twriteIntoDatabase(amz_prodcut ,flip_prodcut )\n\t\tdict={}\n\t\tcount =0\n\t\tfor row in readIntoDatabase():\n\t\t\tdict[count] = row\n\t\t\tcount +=1\n\t\t\n\t\treturn render(request, 'dashboard.html' , {\"contex\": dict })\n", "description": null, "category": "webscraping", "imports": ["from inspect import formatannotationrelativeto", "from django.http import HttpResponse", "from django.shortcuts import render", "from selenium import webdriver", "from django.views.decorators.csrf import csrf_exempt", "from .webscrape import flipkartscrape,amazonscrape", "from .dbhandler import readIntoDatabase, writeIntoDatabase", "from selenium.webdriver.chrome.options import Options"]}, {"term": "def", "name": "dashboard", "data": "def dashboard(request):\n\tdict={}\n\tcounter =0\n\tfor row in readIntoDatabase():\n\t\tdict[counter]=row\n\t\tcounter+=1\n\t \n\treturn render(request, 'dashboard.html',{\"contex\": dict})\n\t   \n", "description": null, "category": "webscraping", "imports": ["from inspect import formatannotationrelativeto", "from django.http import HttpResponse", "from django.shortcuts import render", "from selenium import webdriver", "from django.views.decorators.csrf import csrf_exempt", "from .webscrape import flipkartscrape,amazonscrape", "from .dbhandler import readIntoDatabase, writeIntoDatabase", "from selenium.webdriver.chrome.options import Options"]}], [{"term": "class", "name": "Project1", "data": "class Project1():\n\t\n\tdef __init__(self, mylink, table_num=1, data='cotest', r=None):\n\t\tself.mylink = mylink\n\t\tself.table_num = table_num\n\t\tself.data = data\n\t\tif r == None:\n\t\t\tself.r = ''\n\t\telse:\n\t\t\tself.r = r\n\n\tdef webscrape(self):\n\t\t\n\t\t#This code was taken from tyler_getWebData.py\n\t\t# Save URL of interest\n\t\turl = self.mylink\n\t\t\n\n\t\tself.r = requests.get(url)  # Obtain handle to URL\n\t\t\n\t\tdf_list = pd.read_html(self.r.text)  # this parses all the tables in webpage\n\t\tdf = df_list[self.table_num] \n\n\t\t# Below line creates new column names based on flattened MultiIndex\n\t\t# Ex: \"Fossil CO2 ... \" / \"1990\" becomes \"Fossil_CO2_..._1990\"\n\t\tdf.columns = [\"_\".join(a) for a in df.columns.to_flat_index()]\n\t\t\n\t\t# Save DF to CSV\n\t\tdf.to_csv(f'{self.data}_data.csv')\n\t\treturn self.r\n\tdef merge(self, csv1, csv2, title1='csv1', title2='csv2'):\n\t\t\n\t\t#This code is taken from merge_data.py\n\t\tcsv1 = pd.read_csv(f'{csv1}')\n\t\tcsv2 = pd.read_csv(f'{csv2}')\n\t\n\t\n\t\tco2_comp_renew = csv1['Country'].isin(csv2['Country'])\n\t\tdif_df = csv1[~co2_comp_renew]\n\t\n\t\tdatabase = csv2['Country']\n\t\tsimilar = [difflib.get_close_matches(word, database, n = 1, cutoff = 0.8) for word in dif_df['Country']]\n\t\n\t\tdif_df['Country'] = similar\n\t\tdif_df = dif_df[dif_df['Country'].astype(str) != '[]']\n\t\tdif_df['Country'] = [country[0] for country in dif_df['Country']]\n\t\tcsv1.loc[dif_df.index, 'Country'] = dif_df['Country']\n\t\n\t\n\t\t#SECOND PASS\n\t\tco2_comp_renew = csv1['Country'].isin(csv2['Country'])\n\t\tdif_df = csv1[~co2_comp_renew]\n\t\n\t\n\t\n\t\ttranslation = {\n\t\t\t'Czech Republic':'Czechia',\n\t\t\t'East Timor':'Timor Leste',\n\t\t\t'Eswatini':'Eswatini (Swaziland)',\n\t\t\t'North Korea':'Korea DPR',\n\t\t\t'S\u00e3o Tom\u00e9 and Pr\u00edncipe':'Sao Tome & Principe',\n\t\t\t'South Korea':'Korea Rep',\n\t\t\t'Sudan South Sudan':'South Sudan',\n\t\t\t'The Gambia': 'Gambia'\n\t\t\t}\n\t\n\t\n\t\tfor original, translate in translation.items():\n\t\t\tdif_df['Country'] = dif_df['Country'].str.replace(original, translate)\n\t\tcsv1.loc[dif_df.index, 'Country'] = dif_df['Country']\n\t\t\n\t\t#THIRD PASS\n\t\tco2_comp_renew = csv1['Country'].isin(csv2['Country'])\n\t\tdif_df = csv1[~co2_comp_renew]\n\t\t\n\t\tmerged = pd.merge(csv1, csv2, on = \"Country\", how = \"inner\")\n\t\tmerged.to_csv(f'{title1}_{title2}_data.csv')\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "import requests", "import unittest", "import difflib"]}, {"term": "class", "name": "webscrapetestsuite", "data": "class webscrapetestsuite(unittest.TestCase): \n\t\n\tdef test_link_input(self):\n\t\t# test that the requests work\n\t\ttest1 = Project1('https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions')\n\t\ttest1.webscrape()\n\t\t\n\t\t# Testing that we access the website without an error\n\t\texpected = 200\n\t\t#Checking that the status code is 200 (reached without error)\n\t\tself.assertEqual(test1.r.status_code, expected)\n\t\t\n\tdef test_merge(self):\n\t\t# test that the merge method work\n\t\t#Creating a class from which to run the merge method.\n\t\t#Need to put in the website as class is designed to first scrape, then merge\n\t\ttest2 = Project1('https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions')\n\t\ttest2.merge('../Carly_only_Project1/co2_data.csv', '../Carly_only_Project1/re_data.csv')\n\t\t\n\t\t# Testing that we have the expected number of rows in our output\n\t\texpected = 196\n\t\t#The program is designed that the merged csv should be saved in the same file\n\t\t#this code so relative path is just the file name\n\t\tfile_path = \"csv1_csv2_data.csv\"\n\t\t#Reading in csv from the path name above\n\t\tdf_to_test = pd.read_csv(file_path)\n\t\t# Comparing the number of rows in the csv output to 196\n\t\tself.assertEqual(len(df_to_test['Country']), expected)\n", "description": null, "category": "webscraping", "imports": ["import pandas as pd", "import requests", "import unittest", "import difflib"]}], [{"term": "class", "name": "classWebscrapeSpiderMiddleware:", "data": "class WebscrapeSpiderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the spider middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_spider_input(self, response, spider):\n\t\t# Called for each response that goes through the spider\n\t\t# middleware and into the spider.\n\n\t\t# Should return None or raise an exception.\n\t\treturn None\n\n\tdef process_spider_output(self, response, result, spider):\n\t\t# Called with the results returned from the Spider, after\n\t\t# it has processed the response.\n\n\t\t# Must return an iterable of Request, or item objects.\n\t\tfor i in result:\n\t\t\tyield i\n\n\tdef process_spider_exception(self, response, exception, spider):\n\t\t# Called when a spider or process_spider_input() method\n\t\t# (from other spider middleware) raises an exception.\n\n\t\t# Should return either None or an iterable of Request or item objects.\n\t\tpass\n\n\tdef process_start_requests(self, start_requests, spider):\n\t\t# Called with the start requests of the spider, and works\n\t\t# similarly to the process_spider_output() method, except\n\t\t# that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n\t\t# Must return only requests (not items).\n\t\tfor r in start_requests:\n\t\t\tyield r\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}, {"term": "class", "name": "classWebscrapeDownloaderMiddleware:", "data": "class WebscrapeDownloaderMiddleware:\n\t# Not all methods need to be defined. If a method is not defined,\n\t# scrapy acts as if the downloader middleware does not modify the\n\t# passed objects.\n\n\t@classmethod\n\tdef from_crawler(cls, crawler):\n\t\t# This method is used by Scrapy to create your spiders.\n\t\ts = cls()\n\t\tcrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n\t\treturn s\n\n\tdef process_request(self, request, spider):\n\t\t# Called for each request that goes through the downloader\n\t\t# middleware.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this request\n\t\t# - or return a Response object\n\t\t# - or return a Request object\n\t\t# - or raise IgnoreRequest: process_exception() methods of\n\t\t#   installed downloader middleware will be called\n\t\treturn None\n\n\tdef process_response(self, request, response, spider):\n\t\t# Called with the response returned from the downloader.\n\n\t\t# Must either;\n\t\t# - return a Response object\n\t\t# - return a Request object\n\t\t# - or raise IgnoreRequest\n\t\treturn response\n\n\tdef process_exception(self, request, exception, spider):\n\t\t# Called when a download handler or a process_request()\n\t\t# (from other downloader middleware) raises an exception.\n\n\t\t# Must either:\n\t\t# - return None: continue processing this exception\n\t\t# - return a Response object: stops process_exception() chain\n\t\t# - return a Request object: stops process_exception() chain\n\t\tpass\n\n\tdef spider_opened(self, spider):\n\t\tspider.logger.info('Spider opened: %s' % spider.name)\n", "description": null, "category": "webscraping", "imports": ["from scrapy import signals", "from itemadapter import is_item, ItemAdapter"]}], [{"term": "def", "name": "bswebscrape", "data": "def bswebscrape(html_site):\n\t# Beautiful soup to webscrape\n\tweb_input = requests.get(html_site)\n\n\tif web_input.status_code > 399:\n\t\treturn 1\n\n\tsoup = BeautifulSoup(web_input.text, 'lxml')\n\tcontents = soup.find_all('p')\n\n\t# Store data in a new list\n\tscraped_data = []\n\tfor content in contents:\n\t\tscraped_data.append(content.text)\n\n\treturn scraped_data\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import requests", "import urllib.parse", "#import pandas as pd", "#import numpy as np", "from cs50 import SQL", "from functools import wraps", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "data_clean", "data": "def data_clean(scraped_data):\n", "description": null, "category": "webscraping", "imports": ["import os", "import requests", "import urllib.parse", "#import pandas as pd", "#import numpy as np", "from cs50 import SQL", "from functools import wraps", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "webscrapedb_update", "data": "def webscrapedb_update(unique_wordlist):\n\n\t# Update SQL database\n\tcounter = 0\n\tfor word in unique_wordlist:\n\t\tcounter += 1\n\t\tsqlcmd = db.execute(\"INSERT INTO word_list VALUES (?, ?, ?)\", counter, str(word), int(unique_wordlist[word]))\n\treturn 0\n\n", "description": null, "category": "webscraping", "imports": ["import os", "import requests", "import urllib.parse", "#import pandas as pd", "#import numpy as np", "from cs50 import SQL", "from functools import wraps", "from bs4 import BeautifulSoup"]}, {"term": "def", "name": "webscrapedb_delete", "data": "def webscrapedb_delete():\n\tsqlcmd = db.execute(\"DELETE FROM word_list\")\n", "description": null, "category": "webscraping", "imports": ["import os", "import requests", "import urllib.parse", "#import pandas as pd", "#import numpy as np", "from cs50 import SQL", "from functools import wraps", "from bs4 import BeautifulSoup"]}], [{"term": "def", "name": "render_docs", "data": "def render_docs():\n\treturn render_template_string(docs_html())\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template_string", "from scrape_dev import *", "from production.render import docs_html, get_grades", "from production.db import *", "import sys"]}, {"term": "def", "name": "single_course", "data": "def single_course(term, course, section):\n\t# Formatting\n\tterm = term.lower()\n\tcourse = course.lower()\n\turl = f\"{course}.{section}.{term}\"\n\n\t# Scrape coursebook\n\tclass_info = webscrape_single_section(url)\n\n\t# Send response\n\treturn jsonify({'data': class_info})\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template_string", "from scrape_dev import *", "from production.render import docs_html, get_grades", "from production.db import *", "import sys"]}, {"term": "def", "name": "all_courses", "data": "def all_courses(course):\n\tcourse = course.lower()\n\tcourse_list = webscrape_all_sections(course)\n\treturn jsonify({\"data\": course_list})\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template_string", "from scrape_dev import *", "from production.render import docs_html, get_grades", "from production.db import *", "import sys"]}, {"term": "def", "name": "single_course_grade", "data": "def single_course_grade(term, course, section):\n\tgrade_data = get_single_course_grade(term, course, section)\n\treturn jsonify({\"data\": grade_data})\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template_string", "from scrape_dev import *", "from production.render import docs_html, get_grades", "from production.db import *", "import sys"]}, {"term": "def", "name": "all_course_grades", "data": "def all_course_grades(term, course):\n\tgrade_data = get_all_course_grades(term, course)\n\treturn jsonify({\"data\": grade_data})\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template_string", "from scrape_dev import *", "from production.render import docs_html, get_grades", "from production.db import *", "import sys"]}, {"term": "def", "name": "get_prof_data", "data": "def get_prof_data(name):\n\tprof_data = fetch_prof(name)\n\treturn jsonify({\"data\": prof_data})\n\n", "description": null, "category": "webscraping", "imports": ["from flask import Flask, jsonify, render_template_string", "from scrape_dev import *", "from production.render import docs_html, get_grades", "from production.db import *", "import sys"]}], [], [{"term": "def", "name": "get_laliga_df", "data": "def get_laliga_df():\r\n\tlaliga_url = \"https://www.laliga.com/en-GB/laliga-santander/standing\"\r\n\r\n\tla_liga_response = requests.get(laliga_url)\r\n\r\n\tlaliga_soup = BeautifulSoup(la_liga_response.content, 'html.parser')\r\n\r\n\tresults = laliga_soup.find('div', \r\n\t\t\t\t\t\t\t  {'class': 'styled__StandingTableBody-e89col-5 cDiDQb'}).find_all('div', \r\n\t\t\t\t\t\t\t  {'class': 'styled__ContainerAccordion-e89col-11 HquGF'})\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \r\n\t# Scraping La Liga Table with list comprehension\r\n\r\n\tlaliga_teams = [result.find('div', {'class': 'styled__ShieldContainer-lo8ov8-0 bkblFd shield-desktop'}).find('p', \r\n\t\t\t\t\t\t\t\t{'class': 'styled__TextRegularStyled-sc-1raci4c-0 glrfl'}).get_text() \r\n\t\t\t\t\tfor result in results]\r\n\r\n\tlaliga_points = [result.find_all('p', \r\n\t\t\t\t\t {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[0].get_text()\r\n\t\t\t\t\t for result in results]\r\n\r\n\r\n\tlaliga_played = [result.find_all('p', \r\n\t {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[1].get_text()\r\n\t for result in results]\r\n\t\r\n\tlaliga_wins = [result.find_all('p', \r\n\t {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[2].get_text()\r\n\t for result in results]\r\n\t\r\n\tlaliga_draws = [result.find_all('p', \r\n\t {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[3].get_text()\r\n\t for result in results]\r\n\t\r\n\tlaliga_losses = [result.find_all('p', \r\n\t {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[4].get_text()\r\n\t for result in results]\r\n\t\r\n\tlaliga_goals_for = [result.find_all('p', \r\n\t {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[5].get_text()\r\n\t for result in results]\r\n\t\r\n\tlaliga_goals_against = [result.find_all('p', \r\n\t {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[6].get_text()\r\n\t for result in results]\r\n\t\r\n\tlaliga_goals_diff = [result.find_all('p', \r\n\t {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[7].get_text()\r\n\t for result in results]\t\t\t \r\n\t\r\n\t## Make pandas Dataframe of La Liga table:\r\n\t\r\n\tlaliga_df = pd.DataFrame({'Rank': range(1, len(laliga_teams) + 1), 'Team': laliga_teams, \r\n\t\t\t\t  'Pts': laliga_points, 'Pl': laliga_played, \r\n\t\t\t\t  'W': laliga_wins, 'D': laliga_draws, \r\n\t\t\t\t  'L': laliga_losses, 'GF': laliga_goals_for, \r\n\t\t\t\t  'GA': laliga_goals_against, 'GD': laliga_goals_diff})  \r\n\t# Return dataframe for Laliga\t  \r\n\treturn(laliga_df)\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import pandas as pd\r", "import streamlit as st\r"]}, {"term": "def", "name": "get_epl_df", "data": "def get_epl_df():\r\n\tepl_link = \"https://www.premierleague.com/tables\"\r\n\t\r\n\tresponse = requests.get(epl_link)\r\n\t\r\n\tepl_soup = BeautifulSoup(response.content, 'html.parser')\r\n\t\r\n\t# Table rows are in the tr tags. Each table row is for each EPL team\r\n\tepl_table_rows = epl_soup.find_all('tr')\r\n\t\r\n\t# Rank, Teams, Number of Games Played:\r\n\tepl_rank = [row.find_all('span', {'class': 'value'})[0].get_text() for row in epl_table_rows[1:40:2]]\r\n\tepl_teams = [row.find_all('span', {'class': 'long'})[0].get_text() for row in epl_table_rows[1:40:2]]\r\n\tepl_played = [row.find_all('td')[3].get_text() for row in epl_table_rows[1:40:2]]\r\n\t\r\n\t# Wins, Draws, Losses\r\n\tepl_wins = [row.find_all('td')[4].get_text() for row in epl_table_rows[1:40:2]]\r\n\tepl_draws = [row.find_all('td')[5].get_text() for row in epl_table_rows[1:40:2]]\r\n\tepl_losses = [row.find_all('td')[6].get_text() for row in epl_table_rows[1:40:2]]\r\n\t\r\n\t# Goals For, Goals Against, Goal Diff & Points:\r\n\tepl_goals_for = [row.find_all('td')[7].get_text() for row in epl_table_rows[1:40:2]]\r\n\tepl_goals_against = [row.find_all('td')[8].get_text() for row in epl_table_rows[1:40:2]]\r\n\tepl_goal_diff = [row.find_all('td')[9].get_text().strip() for row in epl_table_rows[1:40:2]]\r\n\tepl_points = [row.find_all('td')[10].get_text().strip() for row in epl_table_rows[1:40:2]]\r\n\t\r\n\t# Make EPL Table:\r\n\tepl_df = pd.DataFrame({\r\n\t\t\t 'Rank': epl_rank,\r\n\t\t\t 'Team': epl_teams,\r\n\t\t\t 'Pl': epl_played,\r\n\t\t\t 'W': epl_wins,\r\n\t\t\t 'D': epl_draws,\r\n\t\t\t 'L': epl_losses,\r\n\t\t\t 'GF': epl_goals_for,\r\n\t\t\t 'GA': epl_goals_against,\r\n\t\t\t 'GD': epl_goal_diff,\r\n\t\t\t 'Pts': epl_points\r\n\t})\r\n\t#Return EPL dataframe:\r\n\treturn(epl_df)\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import pandas as pd\r", "import streamlit as st\r"]}, {"term": "def", "name": "get_bundesliga_df", "data": "def get_bundesliga_df():\r\n\tbundesliga_url = \"https://www.bundesliga.com/en/bundesliga/table\"\r\n\t\r\n\tresponse = requests.get(bundesliga_url)\r\n\t\r\n\tbundes_soup = BeautifulSoup(response.content, 'html.parser')\r\n\t\r\n\tbundes_results = bundes_soup.find('table', {'class': 'table'}).find_all('tr')\r\n\t\r\n\t# Remove first row (header):\r\n\tbundes_results = bundes_results[1:]\r\n\t\r\n\t# Webscrape parts of the table I use list comprhension instead of for loop append method:\r\n\t\r\n\tbundes_teams = [result.find('td', {'class': 'team'}).find('span', {'class': 'd-none d-lg-inline'}).get_text() \r\n\t\t\t\t\tfor result in bundes_results]\r\n\t\t\r\n\tbundes_matches = [result.find('td', {'class': 'matches'}).get_text() for result in bundes_results]\r\n\t\r\n\tbundes_points = [result.find('td', {'class': 'pts'}).get_text() for result in bundes_results]\r\n\t\r\n\tbundes_wins = [result.find('td', {'class': 'd-none d-lg-table-cell wins'}).get_text() for result in bundes_results]\r\n\t\r\n\tbundes_draws = [result.find('td', {'class': 'd-none d-lg-table-cell draws'}).get_text() for result in bundes_results]\r\n\t\r\n\tbundes_losses = [result.find('td', {'class': 'd-none d-lg-table-cell looses'}).get_text() for result in bundes_results]\r\n\t\r\n\tbundes_goals = [result.find('td', {'class': 'd-none d-md-table-cell goals'}).get_text() for result in bundes_results]\r\n\t\r\n\tbundes_goal_diff = [result.find('td', {'class': 'difference'}).get_text().replace(\"+\", \"\") for result in bundes_results]\r\n\t\r\n\t## Make pandas Dataframe:\r\n\tbundes_df = pd.DataFrame({'Rank': range(1, 19), 'Team': bundes_teams, 'Matches': bundes_matches,\r\n\t\t\t\t\t\t\t  'Points': bundes_points, 'Wins': bundes_wins, 'Draws': bundes_draws,\r\n\t\t\t\t\t\t\t  'Losses': bundes_losses, 'Goals': bundes_goals, 'Goal Difference': bundes_goal_diff})\r\n\t\t\r\n\t# Split Goals Into Goals For & Goals Against:\r\n\tbundes_df[['Goals For','Goals Against']] = bundes_df['Goals'].str.split(\":\",expand=True,)\r\n\t\t\r\n\t# Drop Goals column\r\n\tbundes_df.drop('Goals', axis = 1, inplace = True)\r\n\t\t\r\n\t# Rearrange columns\r\n\tbundes_df = bundes_df.reindex(columns=['Rank', 'Team', 'Matches', 'Points',\r\n\t\t\t\t\t\t\t 'Wins', 'Draws', 'Losses', 'Goals For',\r\n\t\t\t\t\t\t\t 'Goals Against', 'Goal Difference'])\r\n\t\r\n\tbundes_df.columns = ['Rank', 'Team', 'Matches', 'Pts',\r\n\t\t\t\t\t\t 'W', 'D', 'L', \r\n\t\t\t\t\t\t 'GF', 'GA', 'GD']\r\n\t# Get Bundesliga dataframe:\r\n\treturn(bundes_df)\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import pandas as pd\r", "import streamlit as st\r"]}, {"term": "def", "name": "get_serieA_df", "data": "def get_serieA_df():\r\n\tserieA_url = \"https://www.legaseriea.it/en/serie-a/league-table\"\r\n\t\r\n\tserieA_response = requests.get(serieA_url)\r\n\t\r\n\tserieA_soup = BeautifulSoup(serieA_response.content, 'html.parser')\r\n\t\r\n\tserieA_table = serieA_soup.find('tbody')\r\n\t\r\n\ttable_rows = serieA_table.find_all('tr')\r\n\t\r\n\t# Obtain parts of the table:\r\n\t\t\r\n\tserieA_ranks = [row.find_all('td')[0].find('span').text for row in table_rows]\r\n\t\r\n\tserieA_team_names = [row.find_all('td')[0].text.split()[1: ] for row in table_rows]\r\n\t\r\n\t# Unnest lists, dealing with the Hellas Verona case pretty much\r\n\tserieA_teams = [\" \".join(str(x) for x in test) for test in serieA_team_names]\r\n\t\r\n\tserieA_points = [row.find_all('td')[1].text for row in table_rows]\r\n\t\r\n\tserieA_played = [row.find_all('td')[2].text for row in table_rows]\r\n\t\r\n\tserieA_wins = [row.find_all('td')[3].text for row in table_rows]\r\n\t\r\n\tserieA_draws = [row.find_all('td')[4].text for row in table_rows]\r\n\t\r\n\tserieA_losses = [row.find_all('td')[5].text for row in table_rows]\r\n\t\r\n\tserieA_goals_for = [row.find_all('td')[-2].text for row in table_rows]\r\n\t\r\n\tserieA_goals_against = [row.find_all('td')[-1].text for row in table_rows]\r\n\t\r\n\t# Make pandas Dataframe of Serie A table:\r\n\t\r\n\tserieA_df = pd.DataFrame({'Rank': serieA_ranks, 'Team': serieA_teams, 'Pts': serieA_points,\r\n\t\t\t\t\t\t\t  'Played': serieA_played, 'W': serieA_wins, 'D': serieA_draws, 'L': serieA_losses, \r\n\t\t\t\t\t\t\t  'GF': serieA_goals_for, 'GA': serieA_goals_against})\r\n\t# Return serie A dataframe:\r\n\treturn(serieA_df)\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import pandas as pd\r", "import streamlit as st\r"]}, {"term": "def", "name": "get_ligue1_df", "data": "def get_ligue1_df():\r\n\tligue_one_url = \"https://www.ligue1.com/ranking\"\r\n\t\r\n\tligue_one_response = requests.get(ligue_one_url)\r\n\t\r\n\tligue_one_soup = BeautifulSoup(ligue_one_response.content, 'html.parser')\r\n\t\r\n\ttable_rows = ligue_one_soup.find('div', {'class': 'classement-table-body'}).find('ul').find_all('li')\r\n\t\r\n\t# Ranks\r\n\tligue_one_ranks = [x.find('a').find_all('div')[0].text for x in table_rows]\r\n\t\r\n\t# Team Name\r\n\tligue_one_teams = [x.find('a').find_all('div')[1].find('span').text for x in table_rows]\r\n\t\r\n\t# Points\r\n\tligue_one_points = [x.find('a').find_all('div')[2].text for x in table_rows]\r\n\t\r\n\t# Played\r\n\tligue_one_played = [x.find('a').find_all('div')[3].text for x in table_rows]\r\n\t\r\n\t# Wins\r\n\tligue_one_wins = [x.find('a').find_all('div')[4].text for x in table_rows]\r\n\t\r\n\t# Draws\r\n\tligue_one_draws = [x.find('a').find_all('div')[5].text for x in table_rows]\r\n\t\r\n\t# Losses\r\n\tligue_one_losses = [x.find('a').find_all('div')[6].text for x in table_rows]\r\n\t\r\n\t# Goals For\r\n\tligue_one_goals_for = [x.find('a').find_all('div')[7].text for x in table_rows]\r\n\t\r\n\t# Goals Against\r\n\tligue_one_goals_against = [x.find('a').find_all('div')[8].text for x in table_rows]\r\n\t\r\n\t# Goal Difference\r\n\tligue_one_goals_diff = [x.find('a').find_all('div')[9].text for x in table_rows]\r\n\t\r\n\t## Create Ligue 1 Dataframe\r\n\tligue1_df = pd.DataFrame({'Rank': ligue_one_ranks, 'Team': ligue_one_teams, 'Pts': ligue_one_points,\r\n\t\t\t\t\t\t\t   'Pl': ligue_one_played, 'W': ligue_one_wins, 'D': ligue_one_draws, 'L': ligue_one_losses, \r\n\t\t\t\t\t\t\t   'GF': ligue_one_goals_for, 'GA': ligue_one_goals_against, 'GD': ligue_one_goals_diff})\r\n\t# Return Ligue 1 dataframe:\r\n\treturn(ligue1_df)\r\n", "description": null, "category": "webscraping", "imports": ["from bs4 import BeautifulSoup\r", "import requests\r", "import pandas as pd\r", "import streamlit as st\r"]}], [{"term": "def", "name": "main", "data": "def main():\n\t#url = \"https://www.monster.com/jobs/search/?q=Software-Developer&where=Seattle\"\n\t#cd = \"SearchResults,section,card-content\"\n\t#ced = \"[h2,div,div],[title,company,location]\"\n\turl = sys.argv[1]\n\tcd = sys.argv[2]\n\tced = sys.argv[3]\n\tresults = webscrape(url,cd)\n\tobjects, categories = parseContainer(results,ced)\n\tjsonCategories = json.dumps(categories)\n\tjsonObject = json.dumps(objects)\n\tprint(jsonCategories)\n\tprint(jsonObject)\n", "description": null, "category": "webscraping", "imports": []}], [{"term": "def", "name": "index", "data": "def index(request):\n\treturn JsonResponse({'gavel-api-version': '0.0.1'})\n", "description": null, "category": "webscraping", "imports": ["from django.shortcuts import render", "from django.http import JsonResponse", "from datetime import datetime, timedelta", "from .lib.webscrape.ManitobaCourtsScraper import ManitobaCourtsScraper"]}], [], [{"term": "def", "name": "getPrice", "data": "def getPrice(coinmarketurl):\n\n\t# Set the URL you want to webscrape from\n\turl = coinmarketurl\n\n\t# Connect to the URL\n\tresponse = requests.get(url)\n\n\t# Parse HTML and save to BeautifulSoup object\u00c2\u00b6\n\tsoup = BeautifulSoup(response.text, \"html.parser\")\n\n\tmyprice = soup.find_all(\"div\", {\"class\": \"priceValue\"})\n\n\tpricestring = myprice[0].text\n\tprint(pricestring)\n\n\treturn pricestring\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import requests", "from lxml import html", "from bs4 import BeautifulSoup"]}], [], [], [], [{"term": "class", "name": "classWebscrape:", "data": "class Webscrape:\n\tdef __init__(self):\n\t\tself.url = \"https://www.youtube.com/c/Dazbeeee/videos\"\n\t\tself.session = HTMLSession()\n\t\tself.song_title = [ ]\n\t\tself.view = [ ]\n\t\tself.like = [ ]\n\t\tself.date = [ ]\n\t\tself.link = [ ]\n\t\tself.total = [ ]\n\t\tself.totalView = 0\n\t\tself.totalLike = 0\n\t\tself.info = [ ]\n\t\tself.view_title_link = [ ]\n\t\tself.like_title_link = [ ]\n\t\tself.top25_like = [ ]\n\t\tself.top25_view = [ ]\n\n\tdef readVideoList(self):\n\t\tresponse = self.session.get(self.url)\n\t\tresponse.html.render(sleep=1, keep_page=True, scrolldown=26)\n\t\ta = response.html.find('a#video-title')\n\t\tfor links in a:\n\t\t\tlink = next(iter(links.absolute_links))\n\t\t\tthd = threading.Thread(target=self.check, args=(link, ))\n\t\t\tthd.start()\n\t\t\tthd.join()\n\t\tself.setTotal()\n\t\tself.setViewTitle()\n\t\tself.setLikeTitle()\n\t\tself.totalView = sum(self.view)\n\t\tself.totalLike = sum(self.like)\n\t\t\n\tdef videoLikes(self, i):\n\t\tr = requests.get(i, headers={'User-Agent': ''})\n\t\tlikes = r.text[:r.text.find(' likes\"')]\n\t\tlike_num = int(likes[likes.rfind('\"') + 1:].replace(',', \"\"))\n\t\tself.like.append(like_num)\n\n\tdef check(self, i):\n\t\tcondition.acquire()\n\t\tres = requests.get(i)\n\t\tsoup = BeautifulSoup(res.text, 'lxml')\n\t\tvideo_title = soup.find(\"meta\", itemprop=\"name\")[\"content\"]\n\t\tif self.splitTitle(video_title):\n\t\t\tself.link.append(i)\n\t\t\tself.videoLikes(i)\n\t\t\tself.date.append(str(soup.find(\"meta\", itemprop=\"datePublished\")[\"content\"]))\n\t\t\tself.view.append(int(soup.find(\"meta\", itemprop=\"interactionCount\")[\"content\"]))\n\t\tcondition.release()\n\n\n\tdef sql(self):\n\t\tprint(\"\\n\\nSQL to create the table from the web\\n\")\n\t\tsq = sqlite()\n\t\tsq.connect()\n\t\tsq.table(\"Database\")\n\t\tfor i in range(len(self.song_title)):\n\t\t\tsql_thd = threading.Thread(target=sq.insert, args=(i + 1, self.song_title[i], self.info[i],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   self.view[i], self.like[i],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   self.date[i], self.link[i]))\n\t\t\tsql_thd.start()\n\t\t\tsql_thd.join()\n\t\tprint(\"\\nAll of the data inserted successfully as BLOBs into a table\\n\")\n\n\tdef gr(self):\n\t\tself.set_top25_view()\n\t\tself.set_top25_like()\n\t\tgc = GraphCreator(self.top25_view, self.top25_like)\n\t\tgc.createTable()\n\n\tdef setTotal(self):\n\t\tfor i in range(len(self.song_title)):\n\t\t\tself.total.append((self.view[i], self.song_title[i], self.like[i], self.date[i], self.info[i], self.link[i]))\n\n\tdef setViewTitle(self):\n\t\tfor i in range(len(self.song_title)):\n\t\t\tself.view_title_link.append((self.view[i], self.song_title[i], self.link[i]))\n\n\tdef setLikeTitle(self):\n\t\tfor i in range(len(self.song_title)):\n\t\t\tself.like_title_link.append((self.like[i], self.song_title[i], self.link[i]))\n\n\tdef set_top25_like(self):\n\t\tself.like_title_link.sort()\n\t\tfor i in range(25):\n\t\t\tinfo_data = self.like_title_link[len(self.view_title_link) - i - 1]\n\t\t\tself.top25_like.append(info_data)\n\n\tdef set_top25_view(self):\n\t\tself.view_title_link.sort()\n\t\tfor i in range(25):\n\t\t\tinfo_data = self.view_title_link[len(self.view_title_link) - i - 1]\n\t\t\tself.top25_view.append(info_data)\n\n\n\tdef checkExcludes(self, title):\n\t\texcludes = [\"Teaser\", \"#Shorts\", \"#YouTubeMusic\", \"RELEASED\", \"Selection Album\"]\n\t\tfor j in excludes:\n\t\t\tif title.find(j) != -1:\n\t\t\t\treturn True\n\t\treturn False\n\t\n\tdef splitTitle(self, t):\n\t\tremain = r'\\(([^)]+)'\n\t\tremove = \"\\(.*\\)|\\s-\\s.\"\n\t\tremain_original = r'\\|'\n\t\texcludes2 = ['  \uff0f', \" / \", \" \uff0f\",\n\t\t\t\t\t \"/\", \"|\"]\n\t\t# M/V means the video is her original song\n\t\tta = t\n\t\tif self.checkExcludes(ta):\n\t\t\treturn False\n\t\tif t.find('M/V') != -1: # + 'Official Piano Arrange\n\t\t\tself.info.append(\"ORIGINAL\")\n\t\t\te = re.sub(remove, '', t.split(\" | \")[1]).replace('M/V', \"\")\n\t\t\tself.song_title.append(e)\n\t\t\treturn True\n\n\t\telif t.find('(') != -1 and t.find(')') != 1:  # check world.execute(me); (...)\n\t\t\tif t.count('(') != 1:\n\t\t\t\tartist = re.findall(remain, t)[-1]\n\t\t\telse:\n\t\t\t\tartist = re.findall(remain, t)[0]\n\t\t\tself.info.append(artist)\n\t\telse:\n\t\t\tself.info.append(\"UNKNOWN\")\n\n\t\tfor j in excludes2:\n\t\t\tif t.find(j) != -1:\n\t\t\t\te = re.sub(remove, '', t).split(j)\n\t\t\t\tself.song_title.append(e[0])\n\t\t\t\treturn True\n\t\tself.song_title.append(re.sub(remove, '', t))\n\t\treturn True\n\n\n", "description": null, "category": "webscraping", "imports": ["from GraphCreator import GraphCreator", "from SaveSQL import *", "from requests_html import HTMLSession", "import requests", "from bs4 import BeautifulSoup", "import threading", "import json"]}], [], [{"term": "def", "name": "Webscrape_head", "data": "def Webscrape_head(page_soup):\n\tNazev = page_soup.h1.text\n\t#BookInfoFile=open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/\"+NazevDir+\"/BookInfo.txt\", \"w\",encoding=\"utf-8\")\n\tBookInfoFile=open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/BookInfo.tsv\", \"a\",encoding=\"utf-8\")\n\n\tauthor = page_soup.find(\"a\",{\"itemprop\":\"author\"}).text\n\tgenres = page_soup.findAll(\"span\",{\"itemprop\":\"genre\"})\n\tannotation = page_soup.find(\"div\",{\"id\":\"book_annotation\"})\n\tif(annotation is not None):\n\t\tif(annotation.b is not None):\n\t\t\tannotation.b.decompose()\n\n\t\tif(annotation.text is not None):\n\t\t\tannotation=annotation.text\n\t\telse:\n\t\t\tannotation=\"\"\n\telse:\n\t\tannotation = \"\"\n\n\tgenre_list=[]\n\tfor genre in genres:\n\t\tgenre_list.append(genre.text)\n\n\tBookInfoFile.write(Nazev+'\\t'+author+'\\t'+','.join(genre_list)+'\\t'+annotation.replace('\\n',' ').replace(chr(13),'').replace('  ','').strip()+'\\n')\n\tBookInfoFile.close()\n\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen as uReq", "import urllib", "from bs4 import BeautifulSoup as soup\t", "import os, sys, time"]}, {"term": "def", "name": "WebScrape_reviews", "data": "def WebScrape_reviews(my_url):\n\t#otevre url a precte html zadaneho url\n\tmy_url=my_url.encode('utf-8').decode('ascii', 'ignore')\n\n\ttry:\n\t\tuClient = uReq(my_url)\n\texcept:\n\t\treturn\n\n\n\tpage_html = uClient.read()\n\tuClient.close()\n\n\t#vyhledani hledanych dat v html\n\tpage_soup = soup(page_html, \"html.parser\")\n\tNazev = page_soup.h1.text\n\n\tWebscrape_head(page_soup)\n\t\t\n\n\t#pocet stranek recenzi\n\treview_page_count=len(page_soup.findAll(\"a\",{\"class\":\"textlist_item_select_width round_mini\"}))+1\n\n\n\tcbdbReviews = open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/Reviews.tsv\", \"a\",encoding=\"utf-8\")\n\n\n\tfor review_page in range(1,review_page_count+1):\n\t\tif(review_page != 1):\n\t\t\t#zmena url a precteni\n\t\t\tmy_url=my_url+'&comments_page='+str(review_page)\n\t\t\t#vyhledani recenzi v html\n\t\t\tuClient = uReq(my_url)\n\t\t\tpage_html = uClient.read()\n\t\t\tuClient.close()\n\t\t\tpage_soup = soup(page_html, \"html.parser\")\n\t\treviews = page_soup.findAll(\"div\",{\"class\":\"comment\"})\n\n\t\tfor review in reviews:\n\t\t\tusername=review.div.img[\"alt\"]\n\t\t\tuserid=review.a[\"href\"].split('-')[1]\n\n\t\t\theader=review.find(\"div\",{\"class\":\"comment_header\"})\n\n\t\t\trating=header.img \n\t\t\tif(rating is not None): #pripad kde recenze nema hodnoceni\n\t\t\t\trating=header.img[\"alt\"] \n\t\t\telse:\n\t\t\t\trating=\"??\"\n\n\t\t\tdate=header.find(\"span\",{\"class\":\"date_span\"}).text\n\t\t\tcomment=review.find(\"div\",{\"class\":\"comment_content\"}).text\n\n\t\t\tcbdbReviews.write(\"cbdb\"+'\\t'+Nazev+'\\t'+username+'\\t'+userid+'\\t'+date+'\\t'+rating+'\\t'+comment.replace('\\n',' ').replace(chr(13),'').replace('  ','').strip()+'\\n') \n\t\t\n\t\ttime.sleep(2)#delay mezi pristupy na stranky recenz\u00c3\u00ad\n\ttime.sleep(2)#delay mezi pristupy na knihy\n\n", "description": null, "category": "webscraping", "imports": ["from urllib.request import urlopen as uReq", "import urllib", "from bs4 import BeautifulSoup as soup\t", "import os, sys, time"]}], [{"term": "def", "name": "get_from_csv", "data": "def get_from_csv(stat, year=None):\n\t\"\"\"Imports data for specified stat and year into a pandas DataFrame. If csv\n\tfile doesn't exist, then it will scrape the web for the data and save it to\n\ta csv file.\n\n\tParameters\n\t----------\n\tstat : str\n\t\tName of the stat. Case doesn't matter.\n\tyear : int, optional\n\t\tYear of specified statistic. If None, then it defaults to the most\n\t\trecent (including current) season.\n\t\"\"\"\n\n\t# create filepath for stat and year\n\tfilepath = \"_\".join(stat.split(\" \"))\n\tif year is not None:\n\t\tfilepath += \"_{}\".format(year)\n\tfilepath += \".csv\"\n\tfilepath = os.path.join(_DATA_DIR, filepath)\n\n\t# import data for stat and year from csv file into dataframe\n\tif not os.path.exists(filepath):\n\t\twebscrape.get_stat_data(stat, year=year)\n\n\treturn pd.read_csv(filepath_or_buffer=filepath)\n\n", "description": "Imports data for specified stat and year into a pandas DataFrame. If csv\n\tfile doesn't exist, then it will scrape the web for the data and save it to\n\ta csv file.\n\n\tParameters\n\t----------\n\tstat : str\n\t\tName of the stat. Case doesn't matter.\n\tyear : int, optional\n\t\tYear of specified statistic. If None, then it defaults to the most\n\t\trecent (including current) season.\n\t", "category": "webscraping", "imports": ["from itertools import combinations", "import os", "import matplotlib.pyplot as plt", "import numpy as np", "import pandas as pd", "from regressors import stats", "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, LinearRegression", "from sklearn.model_selection import cross_val_score", "from sklearn.model_selection import RepeatedKFold", "from statsmodels.stats.outliers_influence import variance_inflation_factor", "import webscrape", "\t# import data for stat and year from csv file into dataframe"]}, {"term": "def", "name": "create_df", "data": "def create_df(main_stat, other_stats, year=None):\n\t\"\"\"For a given dependent stat and a list of independent stats, returns a\n\tpd.DataFrame instance where the dependent stat is in the first column and\n\tpredictors are in subsequent columns\n\n\tParameters\n\t----------\n\tmain_stat : str\n\t\tDependent stat.\n\tother_stats : str or list\n\t\tIndependent stat(s). Can pass a single stat as a str or a list of str\n\t\tfor multiple stats.\n\tyear : int, optional\n\t\tYear of specified statistic. If None, then it defaults to the most\n\t\trecent (including current) season.\n\n\tReturns\n\t-------\n\tdf_combined : pd.DataFrame\n\t\tDataFrame of rankings for each stat. The dependent stat is in the first\n\t\tcolumns and predictors are in subsequent columns\n\t\"\"\"\n\n\t# if only one independent stat, turn it into list\n\tif isinstance(other_stats, str):\n\t\tother_stats = [other_stats]\n\n\t# check that the main stat isn't in the other stats\n\tif main_stat in other_stats:\n\t\traise ValueError(\"The '{}' main stat cannot be in other stats\".format(main_stat))\n\n\t# get data from csv files\n\tdf_main = get_from_csv(main_stat, year=year)\n\tdf_others = []\n\tfor stat in other_stats:\n\t\tdf_combined = get_from_csv(stat, year=year)\n\t\tdf_combined = df_combined.set_index(\"PLAYER NAME\")\n\t\tdf_combined = df_combined.reindex(index=df_main[\"PLAYER NAME\"])\n\t\tdf_combined = df_combined.reset_index()\n\t\tdf_others.append(df_combined)\n\n\t# create y and x arrays\n\ty = np.array(df_main[\"RANK THIS WEEK\"])\n\tx = []\n\tfor df_combined in df_others:\n\t\tx.append(np.array(df_combined[\"RANK THIS WEEK\"]))\n\tx = np.column_stack(tuple(x))\n\n\t# create filter that removes rows with NaN in x and corresponding rows in y\n\tif len(x.shape) == 1:\n\t\tnan_filter = ~np.isnan(x)\n\telse:\n\t\tnan_filter = np.all(~np.isnan(x), axis=1)\n\tx = x[nan_filter]\n\ty = y[nan_filter]\n\n\t# combine x and y in a pd.DataFrame instance\n\tdf_combined = pd.DataFrame(np.column_stack((y, x)), columns=[main_stat] + other_stats)\n\n\treturn df_combined\n\n", "description": "For a given dependent stat and a list of independent stats, returns a\n\tpd.DataFrame instance where the dependent stat is in the first column and\n\tpredictors are in subsequent columns\n\n\tParameters\n\t----------\n\tmain_stat : str\n\t\tDependent stat.\n\tother_stats : str or list\n\t\tIndependent stat(s). Can pass a single stat as a str or a list of str\n\t\tfor multiple stats.\n\tyear : int, optional\n\t\tYear of specified statistic. If None, then it defaults to the most\n\t\trecent (including current) season.\n\n\tReturns\n\t-------\n\tdf_combined : pd.DataFrame\n\t\tDataFrame of rankings for each stat. The dependent stat is in the first\n\t\tcolumns and predictors are in subsequent columns\n\t", "category": "webscraping", "imports": ["from itertools import combinations", "import os", "import matplotlib.pyplot as plt", "import numpy as np", "import pandas as pd", "from regressors import stats", "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, LinearRegression", "from sklearn.model_selection import cross_val_score", "from sklearn.model_selection import RepeatedKFold", "from statsmodels.stats.outliers_influence import variance_inflation_factor", "import webscrape", "\t# import data for stat and year from csv file into dataframe"]}, {"term": "class", "name": "AnalysisBase", "data": "class AnalysisBase(object):\n\n\tdef __init__(self, main_stat, other_stats, year=None):\n\t\tself.main_stat = main_stat\n\t\tself.other_stats = other_stats\n\t\tself.year = year\n\t\tself.df = create_df(main_stat, other_stats, year=year)\n\t\tself.x_mean = np.zeros(self.df.shape[1] - 1)\n\t\tself.x_std = np.ones(self.df.shape[1] - 1)\n\t\tself.y_mean = 0\n\n\tdef get_X(self, as_numpy=True):\n\t\tX = self.df.iloc[:, 1:]\n\t\tif as_numpy:\n\t\t\treturn np.array(X)\n\t\telse:\n\t\t\treturn X\n\n\tdef get_y(self):\n\t\ty = self.df.iloc[:, 0]\n\t\treturn y\n\n\tdef _resave_df(self, X, y):\n\t\tself.df = pd.DataFrame(np.column_stack((y, X)), columns=[self.main_stat] + list(self.other_stats))\n\n\tdef normalize_X(self):\n\t\tX = self.get_X()\n\t\tself.x_mean = np.mean(X, axis=0)\n\t\tself.x_std = np.std(X, axis=0)\n\t\tX = (X - self.x_mean) / self.x_std\n\t\tself._resave_df(X, self.get_y())\n\n\tdef center_y(self):\n\t\ty = self.get_y()\n\t\tself.y_mean = np.mean(y)\n\t\ty = y - self.y_mean\n\t\tself._resave_df(self.get_X(), y)\n\n\tdef fit(self, **kwargs):\n\t\tpass\n\n\tdef fit_multi(self, *args, **kwargs):\n\t\tpass\n\n\tdef variance_inflation_factor(self):\n\t\treturn [variance_inflation_factor(self.get_X(), i) for i in range(self.df.shape[1] - 1)]\n\n", "description": null, "category": "webscraping", "imports": ["from itertools import combinations", "import os", "import matplotlib.pyplot as plt", "import numpy as np", "import pandas as pd", "from regressors import stats", "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, LinearRegression", "from sklearn.model_selection import cross_val_score", "from sklearn.model_selection import RepeatedKFold", "from statsmodels.stats.outliers_influence import variance_inflation_factor", "import webscrape", "\t# import data for stat and year from csv file into dataframe"]}, {"term": "class", "name": "LassoRegression", "data": "class LassoRegression(AnalysisBase):\n\n\tREGRESSION_TYPE = Lasso\n\tREGRESSION_TYPE_CV = LassoCV\n\n\tdef __init__(self, main_stat, other_stats, year=None):\n\t\tsuper(LassoRegression, self).__init__(main_stat, other_stats, year=year)\n\t\tself.alpha = np.array([])\n\t\tself.coef = None\n\t\tself.mse = np.array([])\n\t\tself.mse_std = np.array([])\n\t\tself.best_coef = None\n\t\tself.best_mse = None\n\t\tself.CV = RepeatedKFold(n_splits=self.get_X().shape[0], n_repeats=1, random_state=2652124)\n\n\tdef fit(self, alpha):\n\t\tmodel = self.REGRESSION_TYPE(alpha=alpha)\n\t\tmodel.fit(self.get_X(), self.get_y())\n\t\tcoef = model.coef_.reshape(len(model.coef_), 1)\n\t\tself.alpha = np.append(self.alpha, alpha)\n\t\tif self.coef is None:\n\t\t\tself.coef = coef\n\t\telse:\n\t\t\tself.coef = np.column_stack((self.coef, coef))\n\t\tself.compute_mse(alpha)\n\n\tdef compute_mse(self, alpha):\n\t\tmodel = self.REGRESSION_TYPE(alpha=alpha)\n\t\tX = self.get_X()\n\t\ty = self.get_y()\n\t\tscores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=self.CV, n_jobs=-1)\n\t\tscores = np.abs(scores)\n\t\tself.mse = np.append(self.mse, np.mean(scores))\n\t\tself.mse_std = np.append(self.mse, np.std(scores))\n\n\tdef fit_multi(self, alphas):\n\t\tself.alpha = np.array([])\n\t\tself.coef = None\n\t\tfor alpha in alphas:\n\t\t\tself.fit(alpha)\n\n\tdef plot_coef(self):\n\t\tplt.figure()\n\t\tplt.plot(self.alpha, self.coef.T)\n\t\tplt.xscale('log')\n\t\tplt.xlabel(r\"$\\lambda$\")\n\t\tplt.ylabel(\"Standardized coefficients\")\n\t\tplt.legend(labels=self.other_stats)\n\t\tplt.tight_layout()\n\n\tdef plot_mse(self):\n\t\tplt.figure()\n\t\tplt.plot(self.alpha, self.mse)\n\t\tplt.xscale('log')\n\t\tplt.xlabel(r\"$\\lambda$\")\n\t\tplt.ylabel(\"MSE\")\n\t\tplt.tight_layout()\n\n\tdef fit_CV(self, alphas):\n\t\tsearch = self.REGRESSION_TYPE_CV(alphas=alphas, cv=self.CV)\n\t\tresults = search.fit(self.get_X(), self.get_y())\n\t\tself.best_coef = results.coef_\n\t\tself.best_model = search\n\n\tdef summary(self):\n\t\tstats.summary(self.best_model, self.get_X(), self.get_y())\n\n", "description": null, "category": "webscraping", "imports": ["from itertools import combinations", "import os", "import matplotlib.pyplot as plt", "import numpy as np", "import pandas as pd", "from regressors import stats", "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, LinearRegression", "from sklearn.model_selection import cross_val_score", "from sklearn.model_selection import RepeatedKFold", "from statsmodels.stats.outliers_influence import variance_inflation_factor", "import webscrape", "\t# import data for stat and year from csv file into dataframe"]}, {"term": "class", "name": "RidgeRegression", "data": "class RidgeRegression(LassoRegression):\n\n\tREGRESSION_TYPE = Ridge\n\tREGRESSION_TYPE_CV = RidgeCV\n\n\tdef fit_CV(self, alphas):\n\t\tsearch = self.REGRESSION_TYPE_CV(alphas=alphas, scoring='neg_mean_squared_error', cv=self.CV)\n\t\tresults = search.fit(self.get_X(), self.get_y())\n\t\tself.best_mse = np.abs(results.best_score_)\n\t\tself.best_coef = results.coef_\n\t\tself.best_model = search\n\n", "description": null, "category": "webscraping", "imports": ["from itertools import combinations", "import os", "import matplotlib.pyplot as plt", "import numpy as np", "import pandas as pd", "from regressors import stats", "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, LinearRegression", "from sklearn.model_selection import cross_val_score", "from sklearn.model_selection import RepeatedKFold", "from statsmodels.stats.outliers_influence import variance_inflation_factor", "import webscrape", "\t# import data for stat and year from csv file into dataframe"]}, {"term": "def", "name": "best_subset", "data": "def best_subset(estimator, X, y, max_size=8, cv=5):\n\t\"\"\"Calculates the best model of up to max_size features of X.\n\t   estimator must have a fit and score functions.\n\t   X must be a DataFrame.\"\"\"\n\n\tn_features = X.shape[1]\n\tsubsets = (combinations(range(n_features), k + 1)\n\t\t\t   for k in range(min(n_features, max_size)))\n\n\tbest_size_subset = []\n\tfor subsets_k in subsets:  # for each list of subsets of the same size\n\t\tbest_score = -np.inf\n\t\tbest_subset = None\n\t\tfor subset in subsets_k: # for each subset\n\t\t\testimator.fit(X.iloc[:, list(subset)], y)\n\t\t\t# get the subset with the best score among subsets of the same size\n\t\t\tscore = estimator.score(X.iloc[:, list(subset)], y)\n\t\t\tif score > best_score:\n\t\t\t\tbest_score, best_subset = score, subset\n\t\t# to compare subsets of different sizes we must use CV\n\t\t# first store the best subset of each size\n\t\tbest_size_subset.append(best_subset)\n\n\t# compare best subsets of each size\n\tbest_score = -np.inf\n\tbest_subset = None\n\tlist_scores = []\n\tfor subset in best_size_subset:\n\t\tscore = cross_val_score(estimator, X.iloc[:, list(subset)], y, cv=cv, scoring='neg_mean_squared_error').mean()\n\t\tlist_scores.append(score)\n\t\tif score > best_score:\n\t\t\tbest_score, best_subset = score, subset\n\n\treturn best_subset, best_score, best_size_subset, list_scores\n\n", "description": "Calculates the best model of up to max_size features of X.\n\t   estimator must have a fit and score functions.\n\t   X must be a DataFrame.", "category": "webscraping", "imports": ["from itertools import combinations", "import os", "import matplotlib.pyplot as plt", "import numpy as np", "import pandas as pd", "from regressors import stats", "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, LinearRegression", "from sklearn.model_selection import cross_val_score", "from sklearn.model_selection import RepeatedKFold", "from statsmodels.stats.outliers_influence import variance_inflation_factor", "import webscrape", "\t# import data for stat and year from csv file into dataframe"]}], [], [], [], [], [], [], [{"term": "def", "name": "extractNavigableStrings", "data": "def extractNavigableStrings(context):\n\t\"\"\" from https://stackoverflow.com/questions/29110820/how-to-scrape-between-span-tags-using-beautifulsoup\"\"\"\n\tstrings = []\n\tfor e in context.children:\n\t\tif isinstance(e, NavigableString):\n\t\t\tstrings.append(e)\n\t\tif isinstance(e, Tag):\n\t\t\tstrings.extend(extractNavigableStrings(e))\n\treturn strings\n\n\n", "description": " from https://stackoverflow.com/questions/29110820/how-to-scrape-between-span-tags-using-beautifulsoup", "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}, {"term": "def", "name": "parse_MAl", "data": "def parse_MAl(url):\n\t\"\"\"\n\tParameters\n\t----------\n\turl : string\n\t\tmyanimelist.net url string \n\n\tReturns\n\t-------\n\tdf : DataFrame\n\t\treturns a dataframe with columns \"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"\n\n\t\"\"\"\n\thtml = requests.get(url)\n\tsoup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n\tresults = soup.find_all(class_= \"ranking-list\")\n\t\n\tdf = pd.DataFrame(columns=[\"name\",\"english_name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\", \"url\"])\n\ti = 0\n\tfor result in results:\n\t\t#print(i)\n\t\turl_= result.find(class_=\"hoverinfo_trigger fl-l fs14 fw-b\")[\"href\"]\n\t\thtml_ = requests.get(url_)\n\t\tsoup_ = BeautifulSoup(html_.content, 'html.parser', from_encoding=\"utf-8\")\n\t\t\n\t\tt1name = extractNavigableStrings(soup_.find(class_=\"h1-title\"))\n\t\tif len(t1name) == 1:\n\t\t\tname = t1name[0]\n\t\t\tenglish_name = None\n\t\telif len(t1name) >= 2:\n\t\t\tname=t1name[0]\n\t\t\tenglish_name=t1name[1]\n\t\telse:\n\t\t\tname = None\n\t\t\tenglish_name = None\n\t\t\t\n\t\tType, Dates, members = result.find(class_=\"information di-ib mt4\").text.strip().splitlines()\n\t\ttry:\n\t\t\tmembers = float(\"\".join(members.split()[0].split(\",\")))\n\t\texcept:\n\t\t\tmembers = None\n\t\t\t\n\t\t[Type_, eps, n] = [\", \".join(x.split()) for x in re.split(r'[()]',Type)]\n\t\t\n\t\ttry:\n\t\t\teps = float(eps.split(\",\")[0])\n\t\texcept:\n\t\t\teps = None\n\t\t\n\t\ttry:\n\t\t\tgenres = [genre.text.strip() for genre in soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"genre\")]\n\t\t\t\n\t\texcept:\n\t\t\tgenres = None\n\t\t\n\t\ttry:\n\t\t\tscore = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingValue\")[0].text.strip())\n\t\texcept:\n\t\t\tscore = None\n\t\t#try:\n\t\t#\tscore_members = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingCount\")[0].text.strip())\n\t\t#except:\n\t\t #   score_members = None\n\t\t\n\t\tdf = df.append({\n\t\t\t\"name\": name,\n\t\t\t\"english_name\":english_name,\n\t\t\t\"type\": Type_,\n\t\t\t\"episodes\": eps,\n\t\t\t\"members\": members,\n\t\t\t#\"score_members\": score_members,\n\t\t\t\"rating\": score,\n\t\t\t\"genre\": genres,\n\t\t\t\"dates\": Dates,\n\t\t\t\"url\": url_\n\t\t},ignore_index=True)\n\t\t\n\t\ti+=1\n\treturn df\n\n", "description": "\n\tParameters\n\t----------\n\turl : string\n\t\tmyanimelist.net url string \n\n\tReturns\n\t-------\n\tdf : DataFrame\n\t\treturns a dataframe with columns \"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"\n\n\t", "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}, {"term": "def", "name": "webscrape_MAl", "data": "def webscrape_MAl(anime_limit=16750, start=0):\n\turl_template = \"https://myanimelist.net/topanime.php?limit={}\"\n\tdf = pd.DataFrame(columns=[\"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"])\n\tfor limit in tqdm(range(start,anime_limit, 50)): # iterate in steps of 50\n\t\turl = url_template.format(limit)\n\t\tdf_temp = parse_MAl(url)\n\t\tif df_temp[\"name\"].isnull().sum() >= 40:\n\t\t\tprint(\"Number of missing names, for limit {} = {}\".format(limit, df_temp[\"name\"].isnull().sum()))\n\t\t\tprint(\"--------Halting---------\")\n\t\t\traise SystemExit()\n\t\tsave_mal_temp(df_temp, limit)\n\t\t\n\t\t# I think MAL has a limit on the number of conenctions per minute/second/hour\n\t\t# and after 200-400, the site blocks access. Adding the pause for 1 minute below soleves the issue\n\t\tsleep(60) # pause the loop for 1 minute. \n\t\t\n\n\n\n\n", "description": null, "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}, {"term": "def", "name": "save_mal_temp", "data": "def save_mal_temp(df, limit):\n\tcsvTemp = \"temp/MAL_start_{}.csv\".format(limit)\n\tdf.to_csv(csvTemp)\n\t\t\n\t#print(\"Number of missing names, for limit {} = {}\".format(limit, df[\"name\"].isnull().sum()))\n\t\n\t\n", "description": null, "category": "webscraping", "imports": ["import urllib", "import requests", "import bs4", "from bs4 import BeautifulSoup", "import pandas as pd", "import re", "from tqdm import tqdm", "from time import sleep", "from bs4.element import NavigableString, Tag"]}], [{"term": "class", "name": "classvolare:", "data": "class volare:\n\tdef __init__(self,name,depart,arrive,date,people):\n\t\tself.name = name\n\t\tself.start = depart\n\t\tself.stop = arrive\n\t\tself.date = date\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}, {"term": "def", "name": "ama", "data": "def ama ():\n\tamadeus = Client(\n\tclient_id='nPw0aTw26KfiG0npH6XcoEKYCJ7N8zbf',\n\tclient_secret = 'dLqHifawmuDizlVZ'\n\t)\n\ttry:\n\t\tresponse = amadeus.shopping.flight_offers_search.get(\n\t\t\toriginLocationCode=c1.start,\n\t\t\tdestinationLocationCode=c1.stop,\n\t\t\tdepartureDate=c1.date,\n\t\t\tadults=c1.mate\n\t\t)\n\t\tprint(response.data)\n\texcept ResponseError as error:\n\t\tprint(error)\n\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}, {"term": "def", "name": "flight_retrieve", "data": "def flight_retrieve(x):\n\tflights=x[0:5]\n\ttripdf = pd.DataFrame()\n\tfor i in range(len(flights)):\n\t\ttrip = {}\n\t\tvolo=flights[i]\n\t\tflight=volo['itineraries'][0]\n\t\te = re.sub('PT','',flight['duration'])\n\t\te = re.sub('H',' Hours ',e)\n\t\ttrip['duration'] = re.sub('M',' Min',e)\n\t\tq=flight['segments'][0]\n\t\ttrip['airline']=q['carrierCode']\n\t\tq=q['departure']\n\t\ttrip['from']=q['iataCode']\n\t\ttrip['leave at']=re.sub('T',' at ',q['at'])\n\t\tq=flight['segments'][0]\n\t\tq = q['arrival']\n\t\ttrip['to']=q['iataCode']\n\t\ttrip['arrive at']=re.sub('T',' at ',q['at'])\n\t\tcost=volo['price']\n\t\ttrip['price']=cost['total']\n\t\tlisting=pd.DataFrame([trip]).T\n\t\ttripdf=pd.concat(objs=[listing,tripdf],axis=1,)\n\ttripdf.columns=[1,2,3,4,5]\n\treturn(tripdf)\n\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}, {"term": "def", "name": "get_id", "data": "def get_id(df):\n\turl = \"https://airbnb19.p.rapidapi.com/api/v1/searchDestination\"\n\n\tquerystring = {\"query\":'tbd',\"country\":df ['Country']}\n\t\n\theaders = {\n\t\"X-RapidAPI-Key\": \"4756e43d86msh629b4ee18cb5f4cp1b8e99jsncc91f7430291\",\n\t\"X-RapidAPI-Host\": \"airbnb19.p.rapidapi.com\"}\n\t\t\t\t\t\t\t\t\t\t\t\t\n\n\tresponse = requests.request(\"GET\", url, headers=headers, params=querystring)\n\ttime.sleep(3)\n\tdata = response.json()\n\ttry:\n\t\treturn data['data'][0]['id']\n\texcept:\n\t\treturn df['City']\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}, {"term": "def", "name": "get_place_info", "data": "def get_place_info(idlist,citylist):\n\tli = []\n\tnotgood = []\n\tfor ind in range(len(idlist[0])):\n\t\tprint(idlist[0][ind])\n\t\turl = \"https://airbnb19.p.rapidapi.com/api/v1/searchPropertyByPlace\"\n\n\t\tquerystring = {\"id\":idlist[0][ind],\"totalRecords\":\"10\",\"currency\":\"EUR\",\"adults\":\"1\"}\n\n\t\theaders = {\n\t\t\"X-RapidAPI-Key\": \"4756e43d86msh629b4ee18cb5f4cp1b8e99jsncc91f7430291\",\n\t\t\"X-RapidAPI-Host\": \"airbnb19.p.rapidapi.com\"}\n\n\n\t\tresponse = requests.request(\"GET\", url, headers=headers, params=querystring)\n\t\ttime.sleep(3)\n\n\t\tdata2 = response.json()\n\t\t\n\t\ttry:\n\t\t\tfor i in range(10):\n\t\t\t\tcity = data2['data'][i]['city']\n\t\t\t\trating = data2['data'][i]['avgRating']\n\t\t\t\tprice = data2['data'][i]['price']\n\n\t\t\t\tli.append({ 'City': citylist[ind], 'Rating':rating, 'Price': price})\n\t\texcept:\n\t\t\t\tnotgood.append(citylist[ind])\n\treturn pd.DataFrame(li),notgood\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}, {"term": "def", "name": "waiting", "data": "def waiting(t):\n\ttime.sleep(t)\n\tprint('.')\n\tprint()\n\ttime.sleep(t)\n\tprint('.')\n\tprint()\n\ttime.sleep(t)\n\tprint('.')\n\tprint()\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}, {"term": "def", "name": "talk_speed", "data": "def talk_speed(dialog, speed):\n\tfor character in dialog:\n\t\tsys.stdout.write(character)\n\t\tsys.stdout.flush()\n\t\ttime.sleep(speed)\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}, {"term": "def", "name": "app_title", "data": "def app_title():\n\tprint()\n\tprint()\n\tprint()\n\tprint('#######################################')\n\tprint('############# VACATION BOT ############')\n\tprint('################ v.1 ##################')\n\tprint('#######################################')\n\twaiting(0.5)\n\tapp_start()\n\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}, {"term": "def", "name": "app_start", "data": "def app_start():\n\tbot2 = 'Please tell me when you want to go on vacation.(YYYY-MM-DD)\\n'\n\ttalk_speed(bot2,0.0001)\n\tyear = (input('Year()>   ')).strip()\n\tmonth = (input('Month>   ')).strip()\n\tday = (input('Year>   ')).strip()\n\tdate=year+'-'+month+'-'+day\n\tc1.date=date\n\tbot4 = \"Pick an outdoor activity\\n\"\n\ttalk_speed(bot4,0.0001)\n\twaiting(0.5)\n\tprint((locations['Activities'].unique()))\n\tprint()\n\tactivity = input('>\t').strip().lower().capitalize()\n\tif activity == ['Hiking', 'Cycling','Surfing']:\n\t\tprint('Good choice!!')\n\twhile activity not in ['Hiking', 'Cycling','Surfing']:\n\t\tprint('error: I never said that')\n\t\tactivity = input('>\t').strip().lower().capitalize()\n\t\tif activity == ['Hiking', 'Cycling','Surfing']:\n\t\t\tprint('Good choice!!')\n\tp=list((locations['Countries'].loc[locations['Activities']==activity]).unique())\n\tp=rd.choices(p, k=10)\n\twaiting(0.4)\n\tbot6='Alright, here are some personal recomendations for Countries you might like to visit...\\n'\n\ttalk_speed(bot6,0.0001)\n\twaiting(0.3)\n\tprint(p)\n\twaiting(3)\n\tbot7='Do any of these interest you?'\n\ttalk_speed(bot7,0.0001)\n\tresp = input('Yes or No:  ').lower()\n\tif resp == 'yes':\n\t\tapp_data(p)\n\n\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}, {"term": "def", "name": "app_data", "data": "def app_data(z):\n\tprint(z)\n\t\n\n\n", "description": null, "category": "webscraping", "imports": ["import re", "import pandas as pd", "from bs4 import BeautifulSoup", "import requests", "import time", "import amadeus", "from amadeus import Client, ResponseError", "import sys", "import random as rd"]}], [{"term": "class", "name": "WebAnalysis", "data": "class WebAnalysis(WebsiteText):\n\n\tdef word_count(self):\n\t\tcount = Counter(self.remove_common_words()).most_common(7)\n\t\t\n\t\treturn count\n\t\n\tdef top_words(self):\n\t\tmost_count = Counter(self.remove_common_words()).most_common(1)\n\t\tbest_count = [i[0] for i in most_count]\n\t\t\n\t\tnew = print(f\"The top word is {best_count[0]}\")\n\t\treturn new\n\n", "description": null, "category": "webscraping", "imports": ["# Modules imported", "from webscrape import *", "from collections import Counter"]}], [], [], [], [], [], []]